papers:
- title: A Statically and Dynamically Scalable Soft GPGPU
  abstract: "Current soft processor architectures for FPGAs do not utilize the potential\n\
    of the massive parallelism available. FPGAs now support many thousands of\nembedded\
    \ floating point operators, and have similar computational densities to\nGPGPUs.\
    \ Several soft GPGPU or SIMT processors have been published, but the\nreported\
    \ large areas and modest Fmax makes their widespread use unlikely for\ncommercial\
    \ designs. In this paper we take an alternative approach, building the\nsoft GPU\
    \ microarchitecture around the FPGA resource mix available. We\ndemonstrate a\
    \ statically scalable soft GPGPU processor (where both parameters\nand feature\
    \ set can be determined at configuration time) that always closes\ntiming at the\
    \ peak speed of the slowest embedded component in the FPGA (DSP or\nhard memory),\
    \ with a completely unconstrained compile into a current Intel\nAgilex FPGA. We\
    \ also show dynamic scalability, where a subset of the thread\nspace can be specified\
    \ on an instruction-by-instruction basis.\n  For one example core type, we show\
    \ a logic range -- depending on the\nconfiguration -- of 4k to 10k ALMs, along\
    \ with 24 to 32 DSP Blocks, and 50 to\n250 M20K memories. All of these instances\
    \ close timing at 771 MHz, a\nperformance level limited only by the DSP Blocks.\
    \ We describe our methodology\nfor reliably achieving this clock rate by matching\
    \ the processor pipeline\nstructure to the physical structure of the FPGA fabric.\
    \ We also benchmark\nseveral algorithms across a range of data sizes, and compare\
    \ to a commercial\nsoft RISC processor."
  url: http://arxiv.org/abs/2401.04261v1
  keywords: ''
  document: '# A Statically and Dynamically Scalable Soft GPGPU


    Martin Langhammer Intel Corporation & Imperial College London London, UK martin.langhammer@intel.com


    ## ABSTRACT


    Current soft processor architectures for FPGAs do not utilize the potential of
    the massive parallelism available. FPGAs now support many thousands of embedded
    floating point operators, and have similar computational densities to GPGPUs.
    Several soft GPGPU or SIMT processors have been published, but the reported large
    areas and modest Fmax makes their widespread use unlikely for commercial designs.
    In this paper we take an alternative approach, building the soft GPU microarchitecture
    around the FPGA resource mix available. We demonstrate a statically scalable soft
    GPGPU processor (where both parameters and feature set can be determined at configuration
    time) that always closes timing at the peak speed of the slowest embedded component
    in the FPGA (DSP or hard memory), with a completely unconstrained compile into
    a current Intel Agilex FPGA. We also show dynamic scalability, where a subset
    of the thread space can be specified on an instruction-by-instruction basis.


    For one example core type, we show a logic range ‚Äì depending on the configuration
    ‚Äì of 4k to 10k ALMs, along with 24 to 32 DSP Blocks, and 50 to 250 M20K memories.
    All of these instances close timing at 771 MHz, a performance level limited only
    by the DSP Blocks. We describe our methodology for reliably achieving this clock
    rate by matching the processor pipeline structure to the physical structure of
    the FPGA fabric. We also benchmark several algorithms across a range of data sizes,
    and compare to a commercial soft RISC processor.


    ## 1 INTRODUCTION


    FPGAs are capable platforms, with multiple thousands of embedded memories as well
    as DSP Blocks, many of which now support IEEE 754 floating point numerics. In
    addition, there is a significant amount of high performance IP available for FPGAs,
    e.g. FFTs [\[2,](#page-10-0) [8\]](#page-10-1) and error correction such as Reed-Solomon
    codecs [\[3\]](#page-10-2). High performance systems can readily be assembled
    using a combination of original design and these IP Blocks. The value of the FPGA
    is integration: although each individual IP or function is lower performance than
    ASIC, this is offset by the flexibility. However, modifying IP - even your own
    - requires significant effort. FPGA hardware compile times (synthesis, place and
    route) can take hours, and timing closure can be a significant unknown. Implementing
    (and modifying) a complex subset of a system by a pure software approach, where
    the result of the compile or assembly is essentially instantly available, and
    loaded onto an already placed and routed processor, may be very attractive.


    Soft RISC cores (Nios [\[9\]](#page-10-3) and MicroBlaze [\[4\]](#page-10-4))
    for FPGA have been used for over two decades, and allow the inclusion of complex
    control flow, or the offload of ancillary functions. Although these RISC processors
    are very flexible, they also have a rather low


    George A. Constantinides Imperial College London London, UK g.constantinides@imperial.ac.uk


    performance. Parallel processor architectures may offer better performance, and
    SIMT (GPGPU) processors may be able to efficiently use the large number of memory
    and DSP Blocks distributed across the FPGA device. There have been a number of
    soft SIMT FPGA architectures published [\[14,](#page-10-5) [15,](#page-10-6) [17,](#page-10-7)
    [18,](#page-10-8) [24,](#page-10-9) [25,](#page-10-10) [29\]](#page-10-11), but
    these are often very large (50K-300K LUTs), and typically have a low clock frequency
    (30MHz-100MHz). Other types of parallel processors are also known for FPGA [\[22,](#page-10-12)
    [30,](#page-10-13) [31\]](#page-10-14) (and commercialized [\[5\]](#page-10-15)),
    but the Fmax is relatively low at ‚àº150MHz.


    A different approach has been taken by Xilinx (now AMD) in the Versal devices,
    with arrays of AI Engines, a hardened VLIW processor. This motivates us to consider
    whether we can combine the flexibility of a soft processor (where any number can
    be instantiated into the soft fabric), but with the performance of an ASIC implementation
    (in this case, running at the speed of the embedded hardened features).


    Our design, which we call the eGPU (for embeddedGPU), is both statically and dynamically
    scalable, features which make it particularly useful and performant for FPGA applications.
    Static scalability is the ability to parameterize the thread space, shared memory
    space, integer ALU functions, as well as major processor features (such as predicates).
    Dynamic scalability allows us to operate on a defined subset of the thread space,
    and change this on an instruction by instruction basis, without any dead time.
    We will see that this can greatly reduce the number of cycles required in some
    portions of the program, such as during a vector reduction (which is a common
    kernel of GPGPU applications).


    We make the following contributions:


    - Describe a novel parameterized SIMT processor for FPGA, with a wide range of
    user defined instructions, as well as architectural trade-offs (such as predicates).

    - Demonstrate that a soft processor can consistently close timing at a level limited
    only by the embedded features such as DSP and memory, and do so with a completely
    unconstrained compile.

    - Compare the absolute and normalized (by resource cost) results of a soft GPGPU
    with a soft RISC processor, and show that the SIMT architecture is better in the
    general case, and significantly better when using dedicated hardware extensions.


    ## 2 BACKGROUND


    Our goal for this project was to architect and implement a compact, high performance
    SIMT processor, that can be used for commercial FPGA system designs. We can use
    current and prior FPGA processors both to understand the limitations of previous
    projects, and to validate some of our design choices. The axes of comparison to
    other work include memory systems, complexity (such as workload balancing), and
    trade-offs between hard and soft implementation.


    #### Table 1: Resource Comparison


    <span id="page-1-0"></span>


    | Architecture<br>Config. |          | LUTs | DSP  | FMax | PPA | Device     |  |

    |-------------------------|----------|------|------|------|-----|------------|--|

    | FGPU [15]               | 2CUx8PE  | 57K  | 48   | 250  | 36  | Zynq-7000  |  |

    | DO-GPU [29]             | 4CUx8PE  | 360K | 1344 | 208  | 133 | Stratix 10 |  |

    | FlexGrip [17]           | 1SMx16PE | 114K | 300  | 100  | 175 | Virtex-6   |  |

    | eGPU                    | 1SMx16SP | 5K   | 24   | 771  | 1   | Agilex     |  |


    Many of the previously published GPGPUs [\[15,](#page-10-6) [17,](#page-10-7)
    [24,](#page-10-9) [29\]](#page-10-11) are SIMT processors which were compiled
    to an FPGA, whereas eGPU was designed for FPGA. The eGPU has an power-performancearea
    (PPA) metric which is one or two orders of magnitude (OOM) smaller than some of
    the earlier soft GPGPUs. Comparisons between high-performance processor designs
    are complex and multidimensional. For example, some existing soft GPUs have more
    complex memory systems, including caches and dynamic workload balancing. This
    does come with a cost, with a typical order of magnitude resource difference,
    as can be seen in Table [1,](#page-1-0) where we compare configurations of the
    other soft GPGPUs that are closest in computational structure to eGPU (PEs are
    roughly the same as SPs). Despite the much deeper pipelines (e.g. FlexGrip [\[17\]](#page-10-7)
    has a 21 deep pipeline, FGPU has a 18 deep pipeline [\[15\]](#page-10-6)), they
    also run at a considerably slower clock frequency. Although they are implemented
    in older FPGA technology (FlexGrip is in Virtex-6 at 100MHz), this does not fully
    explain the performance level, as there are soft processors that run at 450MHz
    in those devices [\[20\]](#page-10-16) [\[21\]](#page-10-17). In the benchmarking
    section we will also see that the benchmarks also run slower than expected on
    the earlier GPGPUs based on the difference in clock frequency.


    Instead, we validate eGPU against existing soft RISC processors [\[9\]](#page-10-3),
    which are extensively used in real applications. We will normalize the benchmark
    results based on cost i.e. FPGA resources consumed. The eGPU, being a parallel
    processor (with essentially 16 smaller multi-threaded processors) will naturally
    be larger; to be effective and usable, it must have a clear advantage in both
    absolute performance and normalized efficiency over the RISC processors.


    eGPU uses a single local data memory, which is configurable in size, and does
    not support a cache. Larger datasets need to be externally managed. Like the eGPU,
    the Xilinx AI Engines [\[13\]](#page-10-18), which are organized as hard VLIW
    hard processor arrays, have only a single local data memory per CPU, the loading
    and unloading of which has to be managed externally. Algorithms with larger amounts
    of data (such as 4K FFTs) need to be split across multiple AI Engines [\[6\]](#page-10-19).
    The eGPU has a greater memory flexibility, as we are able to configure a larger
    shared memory instance (we show examples with up to 128KB in this paper). The
    AI Engines give us an example of a commercial FPGA parallel processor, where using
    multiple simpler processors have been found to have an advantage over using complex
    memory systems.


    ## 3 ARCHITECTURE DESCRIPTION


    The architecture of the eGPU is based on an earlier proof-of-concept design [\[28\]](#page-10-20).
    Our new design adds significant scalability - thread and register space, shared
    memory size, instruction set support, as well as optional predicates for thread
    divergence. Figure [1](#page-2-0) shows the top level architecture of the eGPU.
    The streaming multi-processor (SM) contains 16 parallel scalar processors (SP),
    although only 8 are shown in the figure for clarity. An optional dot-product core
    and special function unit (SFU) reciprocal square root can be attached. We target
    the Intel Agilex [\[23\]](#page-10-21) family of FPGAs in this work. The eGPU
    has a very short pipeline (8 stages) compared to other GPUs; therefore, hazards
    are hidden for most programs. Consequently, we do not provide hardware support
    for tracking hazards in the current version, which in turn gives us an efficient
    and fast processor.


    Two types of embedded memories are now supported, simple dual port (DP) and the
    emulated quad port (QP) blocks [\[12\]](#page-10-22). One of the largest performance
    limitations of the earlier eGPU architecture was memory bandwidth. The QP memory
    will double the write bandwidth, while at the same time reducing the number of
    embedded memory blocks required (the 20K-bit M20K blocks) by half. The trade-off
    is that in QP mode, the memory speed is reduced from 1 GHz to 600 MHz, which then
    becomes the critical path in the processor. Resource, Fmax, and benchmark results
    are all described later in this paper.


    ## 3.1 Dynamic Scalability


    Most GPGPUs support thread divergence by predicates (threadspecific conditionals)
    but these have a potential significant performance impact, as all threads are
    run, whether or not they are written back. In addition to predicates, the eGPU
    sequencer supports an instruction by instruction specification of a subset of
    the thread space, where only the indicated threads are run. If the program can
    be constructed such that the data of interest can be written to the threads that
    can be isolated by the dynamic thread allocation, then a large number of processing
    cycles can be skipped. This is particularly noticeable in programs with many multi-cycle
    instructions, such as reads and writes to shared memory. This will have a direct
    impact on the benchmark performance (number of cycles).


    We define a wavefront as the maximum number of operations that can be run per
    clock cycle; with 16 SPs we have a wavefront width of 16. The thread block depth
    (alternately, the wavefront depth) is the number of wavefronts per instruction,
    which is the initialized thread size / 16. We feel these terms allow us to describe
    our dynamic thread scalability more concisely.


    The eGPU can be configured, on a cycle by cycle basis, to act as a standard SIMT
    processor, a multi-threaded CPU, or a single threaded MCU. While the number of
    clock cycles to execute all the threads for an operation instruction (e.g. FP
    or INT) is dependent on the depth of the thread block, loads and stores are multi-cycle
    (because of the limited number of ports to shared memory). The impact of dynamically
    adjusting the width of certain instructions (e.g. reduction, where the writeback
    data can be orders of magnitude less than the read data) can be seen in the benchmark
    section later in this paper.


    The upper 4-bit field in the instruction word (IW) allows the wavefront width
    and depth to be coded for that instruction. Perhaps the most common case will
    be using only the first SP, or even the first thread in the first SP; many GPU
    applications will have vector reduction kernels, where a reduction result(s) may
    end up in the leftmost SP. If we can operate on this SP exclusively for a certain
    subset of time during the execution of the program, we A Statically and Dynamically
    Scalable Soft GPGPU


    <span id="page-2-0"></span>![](_page_2_Figure_1.jpeg)


    Figure 1: eGPU SM Top Level Architecture


    can save significant processing time, and power. The coding of the personality
    is described in the Instruction Set section.


    Hence, we can change the scale of the SIMT span by reducing the wavefront width
    and/or depth. The eGPU can act as a multithreaded CPU if we set the wavefront
    width to one, and if we also set the thread depth to one, each instruction will
    only act on thread 0 of the first SP - this SP can then be used like a MCU. We
    will use these modes to good effect in our benchmarks later in this paper.


    <span id="page-2-1"></span>![](_page_2_Figure_5.jpeg)


    ## 3.2 Predicates


    The eGPU optionally - by user configuration - supports predicates, which enable
    thread divergence. Conditionals can be applied to each thread individually using
    a condition instruction (see Table [2\)](#page-3-0).


    As with many aspects of eGPU, the number and type of conditions can be selected
    at compile time. Although these will have only a minimal impact on area, the additional
    wireload may impact performance because of the large number of individual predicate
    stacks. There is one predicate stack per initialized thread, so there may be thousands
    of stacks per eGPU instance.


    Some algorithms, such as the bitonic sort benchmark in this paper, require predicates.
    On the other hand, many of the signal processing applications that we expect that
    the eGPU will be used for (such as FFTs and matrix decomposition) do not use data
    dependent decisions. These do not need predicates, and can be programmed using
    only loop constructs, which are supported in the eGPU sequencer. For this reason,
    the presence and complexity of predication is a parameter of our design, especially
    considering the large potential cost of the feature.


    Figure [2](#page-2-1) shows the structure of a single predicate block. Each SP
    has a separate block, which are comprised of multiple predicate stacks. Each thread
    has a unique predicate stack. Multiple nested levels of conditional operations
    (IF/ELSE/END IF) are supported


    Figure 2: One Predicate Block


    per stack, with the maximum supported depth of nesting being parameterized.


    The incremental cost of adding one level of nesting is trivial, as the control
    logic of each predicate stack is the dominant user of logic here. The wavefront
    value (for example, in our base eGPU configuration of 512 threads with 16 SPs,
    there will be 32 wavefronts - i.e. 32 threads per SP) enables the correct predicate
    stack for the current thread. If the condition instruction (IF) condition is true
    for that thread, a ''1'' will be set at the top of the predicate stack, and the
    rest of the stack pushed down. An ELSE instruction will invert the top of the
    stack, and an END IF will pop the stack and return to the previous nesting level.


    The eGPU is configured at compile time for a maximum number of threads; if the
    run time configuration of threads is less than this, there is no issue as only
    the selected threads will trigger the operation of the predicate block.


    <span id="page-3-1"></span>


    | [43:40]  |        |      | [39:34] [33:32] [31:27] | [26:22] | [21:17] | [16:1]    |

    |----------|--------|------|-------------------------|---------|---------|-----------|

    | Variable | Opcode | Typ. | RD                      | RA      | RB      | Immediate
    |


    Figure 3: Instruction Word


    The conditional value will only be applied to the current predicate block, and
    all others ignored in that clock cycle. The current thread activation thread\_active
    signal will be muxed from all the predicate blocks, selected by the current wavefront.
    The thread\_active signal is used to pass or zero the write\_enable signals to
    either the register files or shared memory, whichever is the destination for that
    instruction.


    ## 4 INSTRUCTION SET


    Table [2](#page-3-0) shows most of the instruction set for the eGPU. There are
    a total of 61 instructions, including 18 conditional cases (we omit the FP conditional
    instructions here for brevity). Usually, only a subset of instructions are included
    (by the user defined configuration of the eGPU). The 18 conditional cases depend
    on predicates being included in the parameters - as predicates typically increase
    soft logic cost by 50% they are only used when the expected class of applications
    need them. Many of the intended applications, such as FFT, matrix multiplication
    and decomposition, do not, and the required loops can be handled with the dedicated
    loop instructions. Some instructions can support multiple TYPES, such as signed
    (INT32) and unsigned (UINT32) formats for integer instructions.


    The integer ALU uses a large proportion of the soft logic (‚âà100 ALMs to ‚âà400 ALMs),
    so selecting only the required precision (16 bit or 32-bit) and feature subset
    can reduce the cost of the eGPU substantially. Extension instructions are also
    optional. We will use the dot product instruction for some of the benchmarks in
    this paper; if used, it can make significant difference to the performance of
    some functions. We can also add elementary functions (currently we support only
    reciprocal square root), which are required for algorithms such as matrix decomposition.
    In contrast, the FP instructions are almost completely contained inside the DSP
    Block, with only the FP Max() and Min() instructions having a potential impact
    on area or performance.


    Figure [3](#page-3-1) shows an instruction word, here shown in a 43-bit form.
    As the number of registers per thread changes, the three register field widths
    also change; the displayed word is for a 32 registers per thread configuration,
    which requires 5 bits to encode the register number. The 2-bit representation
    field encodes whether the number is unsigned integer, signed integer, or FP32.
    The four most significant bits encode the processing type, which allow the wavefront
    depth and the width of the wavefront to be changed on an instruction by instruction
    basis.


    Writing these results into shared memory using subset write can be 16x faster
    than using the generic write. An instruction, whether used for a full or a partial
    thread space, is almost identical, with only the four instruction type bits used
    to control the subset of the thread space. Table [3](#page-3-2) shows how the
    upper 4 bits of the IW control the width and depth of the thread space.


    | Table 2: Instruction Set |  |

    |--------------------------|--|

    |--------------------------|--|


    <span id="page-3-0"></span>


    | Group              | Instruction            |                         |  |  |  |  |

    |--------------------|------------------------|-------------------------|--|--|--|--|

    |                    | ADD.TYPE Rd,Ra,Rb      | Rd = Ra + Rb            |  |  |  |  |

    |                    | SUB.TYPE Rd,Ra,Rb      | Rd = Ra - Rb            |  |  |  |  |

    | Integer Arithmetic | NEG.TYPE Rd,Ra         | Rd = -Ra                |  |  |  |  |

    |                    | ABS.TYPE Rd,Ra         | Rd = absolute(Ra)       |  |  |  |  |

    |                    | MUL16LO.TYPE Rd,Ra,Rb  | Rd = Ra * Rb            |  |  |  |  |

    |                    | MUL16HI.TYPE Rd,Ra,Rb  | Rd = (Ra * Rb)¬ª16       |  |  |  |  |

    | Integer Multiply   | MUL24.LO.TYPE Rd,Ra,Rb | Rd = Ra * Rb            |  |  |  |  |

    |                    | MUL24.HI.TYPE Rd,Ra,Rb | Rd = (Ra * Rb)¬ª24       |  |  |  |  |

    |                    | AND Rd,Ra,Rb           | Rd = Ra & Rb            |  |  |  |  |

    |                    | OR Rd,Ra,Rb            | Rd = Ra ‚à• Rb            |  |  |  |  |

    |                    | XOR Rd,Ra,Rb           | Rd = Ra ‚äï Rb            |  |  |  |  |

    | Integer Logic      | NOT Rd,Ra              | Rd = !Ra                |  |  |  |  |

    |                    | cNOT Rd,Ra             | Rd = (Ra == 0)?1:0      |  |  |  |  |

    |                    | BVS Rd,Ra              | Rd = bit_reverse(Ra)    |  |  |  |  |

    |                    | SHL.TYPE Rd,Ra,Rb      | Rd = Ra ‚â™ Rb            |  |  |  |  |

    | Integer Shift      | SHR.TYPE Rd,Ra,Rb      | Rd = Ra ‚â´ Rb            |  |  |  |  |

    |                    | POP Rd,Ra              | Rd = unary(Ra)          |  |  |  |  |

    | Integer Other      | MAX.TYPE Rd,Ra,Rb      | Rd = (Ra>Rb)?Ra:Rb      |  |  |  |  |

    |                    | MIN.TYPE Rd,Ra,Rb      | Rd = (Ra<Rb)?Ra:Rb      |  |  |  |  |

    |                    | ADD.FP32 Rd,Ra,Rb      | Rd = Ra + Rb            |  |  |  |  |

    |                    | SUB.FP32 Rd,Ra,Rb      | Rd = Ra - Rb            |  |  |  |  |

    |                    | NEG.FP32 Rd,Ra         | Rd = -Ra                |  |  |  |  |

    | FP ALU             | ABS.FP32 Rd,Ra         | Rd = absolute(Ra)       |  |  |  |  |

    |                    | MUL.FP32 Rd,Ra,Rb      | Rd = Ra*Rb              |  |  |  |  |

    |                    | MAX.FP32 Rd,Ra,Rb      | Rd = (Ra>Rb)?Ra:Rb      |  |  |  |  |

    |                    | MIN.FP32 Rd,Ra,Rb      | Rd = (Ra<Rb)?Ra:Rb      |  |  |  |  |

    |                    | eq                     | ùëÖùëé == ùëÖùëè                |  |  |  |  |

    |                    | ne                     | ùëÖùëé ‚â† ùëÖùëè                 |  |  |  |  |

    | Int Compare        | lt (INT), lo (UINT)    | ùëÖùëé < ùëÖùëè                 |  |  |  |  |

    |                    | le (INT), ls (UINT)    | ùëÖùëé ‚â§ ùëÖùëè                 |  |  |  |  |

    |                    | gt (INT), hi (UINT)    | ùëÖùëé > ùëÖùëè                 |  |  |  |  |

    |                    | ge (INT), hs (UINT)    | ùëÖùëé ‚â• ùëÖùëè                 |  |  |  |  |

    | Memory             | LOD Rd (Ra)+offset     | Read from Shared        |  |  |  |  |

    |                    | STO Rd (Ra)+offset     | Write to Shared         |  |  |  |  |

    | Immediate          | LOD Rd #Imm            | Rd = Imm                |  |  |  |  |

    | Thread             | TDx Rd                 | Rd = Thread IDx         |  |  |  |  |

    |                    | TDy Rd                 | Rd = Thread IDy         |  |  |  |  |

    |                    | DOT Rd,Ra,Rb           | Dot Product ‚ü®ùëÖùëé, ùëÖùëè‚ü©    |  |  |  |  |

    | Extension          | SUM Rd,Ra,Rb           | Reduction ‚ü®ùëÖùëé, ùëÖùëè‚ü©<br>‚àö |  |  |  |  |

    |                    | INVSQR Rd,Ra           | ùëÖùëë = 1/<br>ùëÖùëé           |  |  |  |  |

    |                    | JMP address            | Jump to Address         |  |  |  |  |

    |                    | JSR address            | Subroutine Address      |  |  |  |  |

    |                    | RTS                    | Return from Subroutine  |  |  |  |  |

    | Control            | LOOP address           | Jump and Dec Loop Ctr   |  |  |  |  |

    |                    | INIT loops             | Set Loop Ctr            |  |  |  |  |

    |                    | STOP                   | Stop and Set Flag       |  |  |  |  |

    |                    | IF.cc                  | if cc true              |  |  |  |  |

    | Conditional        | ELSE                   | if cc false             |  |  |  |  |

    |                    | ENDIF                  | clear cc                |  |  |  |  |


    Table 3: Thread Space Control


    <span id="page-3-2"></span>


    | Coding | Width [4:3]             | Depth [2:1]          |  |  |  |

    |--------|-------------------------|----------------------|--|--|--|

    | "00"   | All (16 SPs)            | Wavefront 0 only     |  |  |  |

    | "01"   | 1/4 width (first 4 SPs) | all wavefronts       |  |  |  |

    | "10"   | SP0 only                | first 1/2 wavefronts |  |  |  |

    | "11"   | Undefined               | first 1/4 wavefronts |  |  |  |


    ## 5 RESULTS


    We compiled a number of different eGPU instances, using both DP and QP memory
    versions. We used Quartus Pro V22.4 and targeted an Intel Agilex AGIB027R29A1E1V
    device [\[7\]](#page-10-23). All of our results are reported for a single compilation
    attempt (we did not use seed sweeps).


    The DP memory results are tabulated in Table [4.](#page-6-0) We define three categories
    - small, medium, and large - to show the effects of different thread space, shared
    memory, and ALU features, as well as the impact of supporting predicates. The
    base eGPU architecture is the same for all instances: one SM with 16 SPs, a two
    read port register memory, and a four read and one write port shared memory. We
    configured all of these cases to use 512 threads, but with varying numbers of
    registers per thread. QP memory results are shown in Table [5,](#page-6-1) the
    main architectural change being the two write port shared memory.


    The ''small'' category uses a 16-bit ALU, which will likely only be used for address
    generation. The minimum specification supports only a single bit shift, as well
    as a 16-bit adder/subtractor, and arithmetic logic (AND/OR/XOR) operations. The
    memory requirements for the SPs is reduced by providing 16 registers per thread.
    The ''large'' category implements 64 registers per thread, and larger shared memory
    sizes with up to 128KB. The integer ALU supports the full set of integer instructions
    defined in the previous section. We also include a ''medium'' category for further
    examples. Many other combinations of parameters and features sets are possible
    as well.


    ## 5.1 Impact of Register and Shared Memory Size


    Both the thread registers and the shared memories are implemented using M20K memories,
    which can be configured into either DP (one read port and one write port active
    simultaneously) or QP memories (two read ports and two write ports active simultaneously).
    The natural datapath size of the eGPU is 32-bits, defined by the native IEEE 754
    single precision (FP32) floating point DSPs which will be doing the majority of
    the calculations. In DP mode, a M20K can be configured as a 512x32-bit memory.
    Port restrictions mean that in QP mode the M20K is a 2048x8-bit block, which requires
    a larger minimum thread register space to take advantage of the extra ports.


    In DP mode thread registers are implemented in two dual port memories, providing
    two read ports and one write port per clock cycle. In our most common eGPU configuration
    (with 16 registers per thread), a 512 thread machine will require two M20Ks per
    SP, or 32 M20Ks for thread registers in total, which is also the minimum size.
    Both the number of registers per thread and the number of total threads are parameterized,
    but the number of M20Ks will increase accordingly. In QP mode, the 8-bit data
    port width means that there is no point in using less than 2 thread registers
    per SP, although we will use half the number of M20K blocks compared to the DP
    version when we configure at least these number of registers.


    The shared memory is implemented as a four read port, one write port per memory
    in DP mode. The smallest possible shared memory is 512 words (2KB), which would
    require four M20Ks. This is very small, and unlikely to be useful, as the shared
    memory size would only be as large as the register space in a single SP. A more
    realistic shared memory size would be 2 words (8KB), which would require 16 M20Ks;
    the total memory usage for a small eGPU instance, including registers, would therefore
    be 48 M20Ks. The shared memory is set by parameter, and significantly larger sizes
    are possible without frequency impact. For example, a 64KB shared memory needs
    128 M20Ks, and a 128KB shared memory 256 M20Ks, which is a small fraction of the
    memories on the device. In QP


    mode, the number of M20Ks is halved, and the number of write ports doubled to
    two.


    ## 5.2 Integer ALU Architecture and Resources


    Unlike the floating point arithmetic, which can be mapped directly to a DSP Block,
    the simpler integer operations need to be implemented in soft logic. We will see
    that up to half of the soft logic and registers in an eGPU is required for the
    integer ALU. Table [6](#page-6-2) shows the resources, split by operation type,
    for a wide range of integer ALUs.


    The smallest reasonable integer ALU is a 16 bit version with single bit shifts,
    which consumes 90 ALMs and 136 registers, most of which are used for the 5 stage
    pipeline. Here we have a signed adder/subtractor, as well as logic functions (in
    this case, only AND, OR, and XOR are supported). The more typical full 16-bit
    ALU implementation supports signed and unsigned arithmetic, a more complete set
    of logic operations (AND/OR/XOR/NOT/cNOT/BVS), full 16-bit left and right shifts,
    population count, as well as max/min functions. The resource cost is approximately
    double that of the minimum ALU. The 5 stage pipeline 32-bit version again doubles
    the logic, as might be expected, but the number of registers triples, as individual
    functions (specifically the adder/subtractor and shifters) are themselves pipelined
    to ensure that the ALU always exceeds 800MHz. This contrasts with the 16-bit ALU,
    where the pipelining is used to improve the placement of the entire ALU, rather
    than improving the performance of any individual function. There is also a 4 stage
    pipeline version of the 32-bit integer ALU, which is about the size of the 16-bit
    full function ALU. This returns a lower performance (typically 700 MHz), and is
    used in order to save logic for the QP version of the eGPU (which has a lower
    target speed of 600MHz). The individual resource counts in Table [6](#page-6-2)
    may not accurately reflect the impact of each function to the overall ALU size,
    as synthesis may combine aspects of some functions together.


    ## 5.3 Predicate Resources


    In Table [4](#page-6-0) and [5](#page-6-1) the area impact of predicate support
    is clearly visible, increasing the soft logic resources by about 50%. While each
    predicate stack (including its control) is very small, each thread has a unique
    stack. The base predicate area consists of only a thread comparator (which checks
    that the SP currently executing the thread that the predicate circuit is associated
    with), an instruction decode (IF/ELSE/ENDIF), and the single bit-wide predicate
    stack. This may only be 5 ALMs per thread, but if a typical eGPU contains 1 threads,
    the predicate circuitry can quickly grow to be as large as the rest of the soft
    logic. Increasing the stack depth will have only a minimal impact on area, as
    each additional level consists of only a two input mux and a register.


    ## 5.4 Instruction Fetch, Decode, and Control


    This section will always have a modest footprint, requiring 200 to 250 ALMs, and
    a handful of M20Ks to store the instruction words. The instruction decoder takes
    about 40 ALMs, and the thread generator around 25 ALMs. A single M20K can store
    512 40-bit instruction words; the benchmarks we analyse later in this paper range
    from


    30 instructions (32 element reduction) to 250 instructions (256 element bitonic
    sort), so a multi-tenancy of programs would only need several M20Ks.


    Increasing the IW to 43 or 46 bits (which is required to support a 32 and 64 registers
    per thread, respectively), adds only a single M20K per 2 instructions, as the
    M20K containing the upper bits would be configured in 8 format. In any case, the
    number of M20Ks needed for program storage is small compared to the thread registers
    and shared memory. For example, a 1 word program space would require three M20Ks,
    and a 4 program space nine M20Ks.


    ## 5.5 Calculating Resources and Setting Performance


    Although the eGPU has a parameterized pipeline depth between the SPs and shared
    memory, it can achieve the target performance (771MHz and 600MHz respectively)
    using the minimum depth of 8 stages. The parameterized pipelining can be used
    for future applications with larger shared memories, or when the shared memories
    are placed elsewhere on the device, and not located near the SP array. We also
    report the slowest path outside the embedded (M20K and DSP) resources (see Table
    [4](#page-6-0) and [5\)](#page-6-1). If needed, there are also additional pipelining
    parameters inside the SP for the paths both to and from the FP and Integer ALUs.
    We will show in the next section how additional pipelining may not improve Fmax
    as the eGPU has been designed to fit into an Agilex sector in the base configuration.


    We can see that the SP overhead (mux and control) is ‚âà150 ALMs, the integer ALU
    ranges from ‚âà100 ALMs to ‚âà400 ALMs, and the predicates, if used, start from ‚âà
    150ALMs. A single SP will therefore be as small as 250 ALMs, and can be as large
    as 650 ALMs; this translates into a small eGPU core (16 SPs) requiring 4 ALMs,
    and over 10 ALMs for fully featured example.


    The number of M20Ks for the register memory for the DP eGPU can be calculated
    as threads√óregisters/256; for the shared memory the number of blocks is 2 √ó size().
    The number of M20K blocks required for the QP eGPU are half of this, except that
    there is a minimum size (threads √ó registers\_per\_thread/16 > 2047) for the number
    of registers, in which case the QP eGPU will need the same number of register
    blocks as the DP version.


    ## 5.6 FPGA Sector Resources and Impact


    It is most beneficial to select eGPU parameters around the available FPGA resources
    and their on-chip organization. The Intel Agilex devices are arranged in sectors,
    the most common of which contains about 16400 ALMs, 240 M20K memories, and 160
    DSP Blocks. Although we are not limited to a single sector (additional pipelining
    may be required to maintain performance across sector boundaries), this ratio
    of resources provides a good guide how to parameterize a eGPU instance. In particular,
    creating too large a register or memory space will be inefficient, as the ALMs
    between the M20K columns will likely be unreachable by other designs in the FPGA.
    Likewise, there is no point in specifying a small register space or shared memory,
    as the M20Ks between the logic structures may not be accessible by other functions.
    Further analysis is provided in the following section where we demonstrate that
    by selecting parameters in this way, the eGPU consistently achieves the reported


    performance levels by matching its architecture with the sector structure.


    ## 6 REPEATABLE HIGH PERFORMANCE


    This section provides the required information to make our design process repeatable
    for those wishing to achieve high performance in their own designs. It is therefore
    necessarily ''close to metal'' in abstraction. Although the details are specific
    to Intel FPGAs, we believe the same approaches are valid for all other FPGAs as
    well.


    The eGPU is designed to give consistent performance, which will always be limited
    by the slowest embedded (DSP or M20K memory) resource in that configuration. The
    clock network in Agilex is specified at 1GHz, which is the absolute limit of performance
    for any design. The M20K memories in DP mode also achieve 1GHz, but only 600 MHz
    in QP mode. The DSP Blocks can run at 771 MHz when implementing a FP32 multiply-add
    datapath with a 4 stage pipeline [\[11\]](#page-10-24). We are therefore limited
    to a maximum speed of 771MHz, unless we use QP memory, in which case the maximum
    frequency drops to 600MHz. The lower performance of the QP memory, however, will
    allow us to support a higher density storage, and the doubled write bandwidth
    may offer an overall higher throughput for some applications. We will examine
    some of these trade-offs in the benchmarking section.


    Using the sector architecture effectively enables the eGPU performance and efficiency.
    Sector resources are arranged in columns, each approximately 41 rows high (several
    columns are shorter because of non-user accessible device features). Achieving
    a 1GHz speed for soft logic does not require logic to be immediately adjacent
    to each other, as there are different horizontal and vertical wire lengths - too
    much pipelining can negatively impact performance as much as too little. More
    important is using the minimal number of wire resources per connection. In the
    Agilex devices, there is a constant 4 columns of logic between each column of
    either DSP or M20K. In a sector we will have 40 columns of logic, 4 columns of
    DSP, and 6 columns of M20K. There is little point in saving logic or memory if
    it is not accessible by other portions of design.


    As we have shown in the previous section, the results are deterministic and repeatable,
    in both area and performance. Ideally, the resource use would be balanced to realize
    the maximum efficiency from the device.


    To map eGPU to the device, we first sketched out a LUT level construct of an SP,
    and adjusted it so that the number of logic levels would align with the sector
    column ratios described above. Paths directly between M20K memories (which implement
    the thread registers in each SP) and the DSP Blocks had to fit into a 4 column
    group of LABs, and longer pure logic paths (e.g. the integer ALUs) were organized
    so that the total area did not spill over into a M20K or DSP column that might
    be used by another SP.


    We can see from the results (Table [6\)](#page-6-2) that a 16-bit Integer ALU
    is in the range of 100-200 ALMs and the 32-bit version requires 200- 400 ALMs.
    If predicates are used, they will cost an additional 125-250 ALMs per SP, depending
    on the defined thread space. The remaining 150 ALMs per SP are used for the data
    muxing and alignment shown in Figure [1.](#page-2-0) We were able to implement
    a small eGPU (the first example in Table [4\)](#page-6-0) that was able to close
    timing over 771 MHz with no dedicated soft logic registers (i.e. registers that
    were not


    <span id="page-6-0"></span>


    | Scale  | ALU<br>Precision | Shift<br>Precision | Threads | Reg.<br>Thread |
    Shared<br>Memory | Predicate<br>Levels | ALM   | Registers | DSP | M20K | Freq<br>(MHz)
    | SP<br>(ALM/Reg.) |

    |--------|------------------|--------------------|---------|----------------|------------------|---------------------|-------|-----------|-----|------|---------------|------------------|

    | Small  | 16               | 1                  | 512     | 16             |
    8KB              | 0                   | 4243  | 13635     | 24  | 50   | 1018/771      |
    224/707          |

    | Small  | 16               | 16                 | 512     | 16             |
    32KB             | 5                   | 7518  | 18992     | 24  | 98   | 898/771       |
    413/979          |

    | Medium | 16               | 16                 | 512     | 32             |
    32KB             | 5                   | 7579  | 19155     | 24  | 131  | 883/771       |
    426/1043         |

    | Medium | 32               | 16                 | 512     | 32             |
    32KB             | 5                   | 9754  | 25425     | 24  | 131  | 902/771       |
    461/1277         |

    | Large  | 32               | 16                 | 512     | 64             |
    32KB             | 8                   | 10127 | 26040     | 32  | 195  | 860/771       |
    575/1505         |

    | Large  | 32               | 32                 | 512     | 64             |
    64KB             | 16                  | 10697 | 26618     | 32  | 259  | 841/771       |
    600/1476         |


    #### Table 4: Fitting Results - DP Memory


    #### Table 5: Fitting Results - QP Memory


    <span id="page-6-1"></span>


    | Scale  | ALU       | Shift     | Threads | Regs./ | Shared | Predicate | ALM   |
    Registers | DSP | M20K | Freq    | SP         |

    |--------|-----------|-----------|---------|--------|--------|-----------|-------|-----------|-----|------|---------|------------|

    |        | Precision | Precision |         | Thread | Memory | Levels    |       |           |     |      |
    (MHz)   | (ALM/Reg.) |

    | Small  | 32        | 1         | 512     | 64     | 32KB   | 0         | 5468  |
    14487     | 24  | 98   | 840/600 | 287/830    |

    | Medium | 32        | 32        | 1024    | 32     | 64KB   | 0         | 7057  |
    16722     | 32  | 131  | 763/600 | 396/1016   |

    | Large  | 32        | 32        | 1024    | 32     | 64KB   | 16        | 11314
    | 25050     | 32  | 131  | 763/600 | 685/1601   |

    | Large  | 32        | 32        | 1024    | 32     | 128KB  | 10        | 10174
    | 23094     | 32  | 195  | 714/600 | 556/1391   |


    Table 6: Fitting Results - Integer ALU


    <span id="page-6-2"></span>


    | Prec. | Type  | ALM | Registers | Add/<br>Sub | Logic | SHL | SHR | Pop |

    |-------|-------|-----|-----------|-------------|-------|-----|-----|-----|

    | 16    | Min   | 90  | 136       | 3           | 9     | -   | -   | -   |

    | 16    | Small | 134 | 207       | 9           | 10    | 20  | 23  | -   |

    | 16    | Full  | 199 | 269       | 9           | 18    | 20  | 23  | 11  |

    | 32    | Min   | 208 | 406       | 5           | 27    | 28  | 28  | -   |

    | 32    | Full  | 394 | 704       | 27          | 36    | 50  | 53  | 27  |


    directly connected with a logic function, such as a mux), but for the generic
    parameterized case, we added a single additional pipeline stage register between
    the thread registers and the functional units, and also one level in the write-back
    path between the functional units and the thread registers. For all of the examples
    in Table [4](#page-6-0) there are also single pipeline stages to and from the
    shared memory. We parameterized the pipeline depth for all of these stages, along
    with the appropriate balancing delays for the data and control paths into the
    write paths of the thread registers, but found that these were not needed to be
    increased beyond one pipeline stage for any of the reported examples.


    Figure [4](#page-7-0) shows the unconstrained placement of the largest instance
    of Table [4.](#page-6-0) The shared memory and 8 out of the 16 SPs have been color
    coded for identification. The shared memory creates a spine in the middle of the
    core, with 8 SPs placed on either side of it. For purposes of illustration we
    have colored a subset of SPs: three random SPs and the left of the spine, and
    five contiguous ones on the right. Three things are evident with all SPs: (a)
    the majority of the logic is in one contiguous block, (b) there is a separate
    contiguous structure (the predicate block) placed some distance away, and (c),
    the SP straddles a columns of DSP Blocks. All of the instances of Table [4](#page-6-0)
    and [5](#page-6-1) display this pattern, including the shared memory spine.


    Figure [5](#page-7-1) shows one of the SPs in greater detail (this SP is the one
    marked by the black boxes in Figure [4\)](#page-7-0). The largest component is
    the integer ALU. The operators (adder/subtractor, shifters, arithmetic logic,
    etc.) are in the 4 columns to the right of the two DSP blocks (the DSP Block for
    the floating point operators is adjacent to the integer multiplier). To the left
    of the DSP Blocks is largely pipelining logic - of the 5 pipeline stages in the
    ALU, only one is used for pipelining the operators - the rest is used to break
    up the paths between the thread register memories and the ALUs. We examined all
    of the SP placements, and the placement of the M20Ks for the register memories
    (8 M20Ks for this instance) was in one of three layouts: (a) a contiguous single
    column (b) most of the registers in one column, with a smaller number in the next
    column further away from the integer ALU, and (c) equally split between two columns
    on either side of the integer ALU. In all of these cases, the pipeline wrapper
    around the ALU was usually grouped together, and essentially separate from the
    actual operators. Rather than having to be in a specific location relative to
    the M20Ks and operators, the ability to split up a bus so that it can be mapped
    to the same number of wire hops is what was important. Fewer pipeline stages would
    have introduced a two stage routing path, which would have likely become the critical
    path in the eGPU. (In the QP memory version, we can remove one of the pipeline
    stages as the M20K becomes the slowest component at 600 MHz, and we can see that
    the removal of some of the pipeline path reduce the non-memory path performance
    to just over 700 MHz). On the other hand, more than a 5 stage integer ALU could
    potentially decrease performance as it could spread out the placement of the SP.


    The predicate circuitry is placed in another contiguous block, but well away from
    the SP core it is associated with. From Figure [4](#page-7-0) we can see that
    the majority of the other predicates have a similar relationship with their respective
    SP. All of these have been automatically placed by Quartus. This is possible because
    the interface to and from the predicate block is very narrow, with only a single
    bit (thread\_active signal) returned. The signals to the block are relatively
    few: a thread index (typically 5 to 8 bits wide), a 3-bit decoded instruction
    signal (IF/ELSE/ENDIF), and a single bit valid condition code. Although there
    are many possible conditions from many different instructions, these can be decoded
    into a single


    <span id="page-7-0"></span>![](_page_7_Figure_1.jpeg)


    Figure 4: eGPU Placement


    valid condition bit in the main SP body. These narrow busses give us flexibility
    to wrap multiple pipes around the relatively simple (consisting largely of a chained
    registers organized in individual stacks) predicate blocks, which makes it possible
    for the tool to place them almost completely independently of the main datapaths.


    To create repeatable high performance designs, we need to understand both the
    structure, and the position of embedded features to each other. Here we are using
    integer ALUs which range in size by four times, our logic and memory density is
    very high, but our performance always exceeds that of the slowest embedded feature.
    It is possible to build a completely different type of CPU (or indeed any other
    type of core) and achieve this type of performance via a push button flow, but
    the architecture of the FPGA needs to be considered at every stage of the IP architecture
    phase.


    ## 7 BENCHMARKS


    We ran a number of workloads of different types to evaluate absolute and relative
    performance of the eGPU for varying data sizes that we might expect for embedded
    applications. We also profiled all the workloads to examine the efficiency of
    the eGPU. For continuity we selected many of the same benchmarks as used by Flexgrip
    [\[16\]](#page-10-25). We chose vector reduction, matrix transpose, and matrix-matrix
    multiply (MMM), as these would be common building blocks for many GPGPU applications.
    Bitonic sort [\[19\]](#page-10-26) is a sorting algorithm suited for parallel
    processing. Instead of the simpler autocorrelation, we used the FFT, as we felt
    this would be more representative of the


    <span id="page-7-1"></span>![](_page_7_Figure_7.jpeg)


    Figure 5: Single SP Placement


    workloads expected for the eGPU. All benchmarks were written in assembly code
    (we have not written our compiler yet).


    We report the comparison to FlexGrip only for the MMM, as the larger dataset size
    would be less affected by any overheads for setup and data transfer. We see that
    there is a significant performance advantage in favor of eGPU in cycle time alone.
    We ran all benchmarks, except the FFT, for which there are no reported FlexGrip
    results. FlexGrip underperforms eGPU by a factor of ‚âà31x, averaged over all benchmarks.
    We did not compare against DO-GPU (which is the latest iteration of FGPU), as
    DO-GPU normalized size is 50x-100x greater than eGPU.


    Our reported measurements are all based on core performance: we start the clock
    once the data has been loaded into the shared memory, and stop the clock once
    the final result has been written back to the shared memory. The most likely use
    of the eGPU is to apply multiple algorithms to the same data - .. there is no
    loading and unloading of data between different algorithms. For completeness,
    we also ran all of our benchmarks taking into account the time to load and unload
    the data over the 32-bit wide data bus. The performance impact was only 4.7%,
    averaged over all benchmarks.


    Clock frequency was 771 MHz for eGPU (including where the Dot Product operator
    is used), and 600 MHz for eGPU-QP variant. We compare both cycle counts and elapsed
    time for the eGPUs with the two shared memory architectures, and also the impact
    of the optional Dot Product core for reduction and MMM benchmarks. We then normalize
    the performance (time), by the resource cost, which we calculated on the basis
    of ALMs and DSP Blocks. We estimate that the effective cost of a DSP block is
    100 ALMs, which we calculate as follows: we start with the ALM count of the pure
    soft logic implementation of a FP32 multiply and adder (approximately 650 ALMs
    [\[10\]](#page-10-27)), and add 50% area to this number for DSP Block overhead
    (a DSP Block contains considerable additional features). We then divide by 10
    for an approximate soft logic to hard logic


    <span id="page-8-1"></span><span id="page-8-0"></span>


    |                                         |                                             |
    Vector Reduction               |                |                  | Matrix Transpose   |                      |                      |
    Matrix x Matrix          |                             |                                       |              |                            |                                             |

    |-----------------------------------------|---------------------------------------------|--------------------------------|----------------|------------------|--------------------|----------------------|----------------------|--------------------------|-----------------------------|---------------------------------------|--------------|----------------------------|---------------------------------------------|

    | Dimension                               | Metric                                      |
    Nios                           | eGPU           | eGPU             | eGPU               |
    Nios                 | eGPU                 | eGPU                     | Nios                        |
    FlexGrip                              | eGPU         | eGPU                       |
    eGPU                                        |

    |                                         |                                             |                                |
    DP             | QP               | Dot                |                      |
    DP                   | QP                       |                             |                                       |
    DP           | QP                         | Dot                                         |

    |                                         | Cycles                                      |
    459                            | 168            | 160              | 62                 |
    21809                | 1720                 | 1208                     | 1.45M                       |
    2.14M                                 | 111546       | 103354                     |
    19800                                       |

    |                                         | Time(us)                                    |
    1.32                           | 0.22           | 0.27             | 0.08               |
    62.85                | 2.23                 | 2.01                     | 4179                        |
    21400                                 | 144.7        | 172.3                      |
    25.7                                        |

    | 32                                      | Ratio(cycles)                               |
    2.73                           | 1.0            | 0.95             | 0.37               |
    12.68                | 1.0                  | 0.7                      | 13.03                       |
    19.2                                  | 1.0          | 0.93                       |
    0.18                                        |

    |                                         | Ratio(time)                                 |
    6.01                           | 1.0            | 1.23             | 0.37               |
    28.18                | 1.0                  | 0.9                      | 28.97                       |
    147.9                                 | 1.0          | 1.19                       |
    0.18                                        |

    |                                         | Normalized                                  |
    1.14                           | 1.0            | 1.4              | 0.45               |
    5.33                 | 1.0                  | 1.02                     | 5.48                        |
    -                                     | 1.0          | 1.35                       |
    0.21                                        |

    |                                         | Cycles                                      |
    1803                           | 202            | 194              | 94                 |
    86609                | 5529                 | 3481                     | 11.6M                       |
    16.6M                                 | 451066       | 418671                     |
    84425                                       |

    |                                         | Time(us)                                    |
    5.20                           | 0.26           | 0.32             | 0.12               |
    249.6                | 7.17                 | 5.80                     | 33383                       |
    166000                                | 585.0        | 697.8                      |
    109.5                                       |

    | 64                                      | Ratio(cycles)                               |
    8.93                           | 1.0            | 0.96             | 0.47               |
    15.66                | 1.0                  | 0.63                     | 25.7                        |
    36.8                                  | 1.0          | 0.93                       |
    0.19                                        |

    |                                         | Ratio(time)                                 |
    19.98                          | 1.0            | 1.23             | 0.47               |
    34.81                | 1.0                  | 0.81                     | 57.1                        |
    284                                   | 1.0          | 1.19                       |
    0.19                                        |

    |                                         | Normalized                                  |
    3.78                           | 1.0            | 1.4              | 0.60               |
    6.59                 | 1.0                  | 0.92                     | 10.80                       |
    -                                     | 1.0          | 1.35                       |
    0.23                                        |

    |                                         | Cycles                                      |
    3595                           | 216            | 208              | 101                |
    345233               | 20481                | 12649                    | 92.5M                       |
    441.2M                                | 2342356      | 2212136                    |
    886452                                      |

    |                                         | Time(us)                                    |
    10.36                          | 0.28           | 0.35             | 0.13               |
    994.91               | 26.56                | 21.08                    | 266491                      |
    4412.1                                | 3038.1       | 3686.9                     |
    1149.7                                      |

    | 128                                     | Ratio(cycles)                               |
    16.64                          | 1.0            | 0.96             | 0.47               |
    16.86                | 1.0                  | 0.62                     | 39.47                       |
    188.3                                 | 1.0          | 0.94                       |
    0.38                                        |

    |                                         | Ratio(time)                                 |
    37.00                          | 1.0            | 1.23             | 0.47               |
    37.45                | 1.0                  | 0.79                     | 87.71                       |
    1452                                  | 1.0          | 1.21                       |
    0.38                                        |

    |                                         | Normalized                                  |
    7.00                           | 1.0            | 1.4              | 0.60               |
    7.09                 | 1.0                  | 0.90                     | 1659                        |
    -                                     | 1.0          | 1.37                       |
    0.46                                        |

    |                                         |                                             |                                |                |                  |                    |                      |                      |                          |                             |                                       |              |                            |                                             |

    | Bitonic 64<br>Bitonic 32<br>Bitonic 128 | Bitonic 256<br>Bitonic 32QP<br>Bitonic
    64QP | Bitonic 128QP<br>Bitonic 256QP | FFT32<br>FFT64 | FFT128<br>FFT256 | FFT32QP<br>FFT64QP
    | FFT128QP<br>FFT256QP | Reduce32<br>Reduce64 | Reduce32Dot<br>Reduce128 | Reduce64Dot<br>Reduce128Dot
    | MMM 64x64<br>MMM 32x32<br>MMM 128x128 | MMM32x32QP   | MMM64x64QP<br>MMM128x128QP
    | MMM32x32Dot<br>MMM64x64Dot<br>MMM128x128Dot |

    | FP OP                                   | INT OP                                      |                                |
    IMM OP         |                  | Branch             | Load                 |                      |
    Save                     | Predicate                   |                                       |
    Thread Setup |                            | NOP                                         |


    Table 7: Vector and Matrix Benchmarks


    Figure 6: Benchmark Profiling (Y-Axis shows proportion of instructions executed
    by type).


    scaling factor (earlier work [\[26\]](#page-10-28) suggested a higher ratio in
    the general case, but recent work [\[27\]](#page-10-29) described more efficient
    ways of mapping arithmetic, especially multipliers, to FPGAs). We report normalized
    cost (considering both elapsed time and resources, with eGPU-DP as the baseline).


    As a comparison, we ran all of the benchmarks on Nios IIe [\[1\]](#page-10-30),
    which is a mature RISC processor for Intel FPGAs. The configuration we used consumed
    1100 ALMs (plus 3 DSP Blocks, giving a normalized cost of 1400), and closed timing
    at 347 MHz. We did not profile the Nios code, but analyzed the efficiency of operation
    (CPI). Most of the benchmarks retired an instruction every 1.7 clock cycles, except
    for the matrix-matrix multiplies and FFT, which required about 3 clocks, because
    of the way that 32√ó32 multipliers


    were implemented. (For simplicity, we replaced the FP32 arithmetic with INT32
    for the Nios examples).


    For the vector and matrix benchmarks, we chose an eGPU configuration with 32 registers
    per thread, with a 32 bit ALU, and a 128KB shared memory. This configuration has
    an equivalent cost (see Table [4](#page-6-0) and [5\)](#page-6-1) of 7400, 8400,
    and 9000 ALMs for the eGPU-DP, eGPU-QP, and eGPU-Dot variants respectively. Depending
    on the configuration, eGPU is 5√ó to 6√ó larger than Nios (but also more than twice
    the operating frequency). We would therefore expect (or at least hope) that eGPU
    would give an OOM performance increase over Nios.


    We can deduce the mechanism of the matrix transpose benchmarks from Table [7](#page-8-0)
    directly. For a given √ó matrix, we know that the eGPU will need 2 cycles to write
    the transposed elements


    to shared memory and 1/4th of those cycles to initially read them into the SP
    threads. We can see that the number of cycles clocked is marginally larger than
    this; these are largely used for the integer instructions needed to generate the
    transposed write addresses. We expect that the eGPU-QP will require about 40%
    fewer cycles, being able to write two transposed elements per clock, which indeed
    is the case.


    The vector reduction needs inter-SP communication, which go through the shared
    memory, which is the performance bottleneck in the eGPU. Table [7](#page-8-0)
    shows the impact of memory accesses on reduction performance. The actual floating
    point operations are a relatively small (‚âà10%) component of the reduction, with
    the majority of the cycles used by the memory operations. If we are using the
    dot product operator, there are even fewer FP operations required, and most of
    the time is spent waiting (NOPs) for the dot product to write back to the SP.
    All final vector reductions end up in the first SP, and we can use the multi-threaded
    CPU or MCU eGPU dynamic scaling personalities to write these values to the shared
    memory.


    The MMMs are much more complex. Although the algorithm itself is very simple,
    consisting only of a three level loop, the standard GPU implementation requires
    a vector reduction. While the cycle count increases as expected (‚àº 4√ó) from 32√ó32
    to 64√ó64, there is an unexpected jump from 64√ó64 to 128√ó128, which is particularly
    evident in the eGPU-Dot case. Analysis of the code shows that while we are able
    to store the entire matrix (or at least a majority of the matrix) in the SP registers
    (there are 16384 total registers across the 16 SPs in the configuration we have
    chosen here) for the 32√ó32 and 64√ó64 cases, we need to keep reloading portions
    of the matrix in the 128√ó128 case, which can also be seen in the profile stack
    in Figure [6.](#page-8-1) Of course, there is always the option of increasing
    the maximum thread space or registers per thread (through parameterization) if
    the expected workloads were larger matrices. Compared to the vector reduction
    (where we profile a single vector), the thread initialization and integer operations
    are amortized away as we operate on many vectors. The NOPs also disappear as the
    the thread depth increases here.


    The bitonic sort benchmark requires a wider mix of instructions. Predicates are
    required, which increases the effective cost of the eGPU core by about 50%. The
    smaller sorts require many NOPs, which progressively reduce as the number of wavefronts
    increase for the larger datasets. The nature of the bitonic sort tends to use
    many subroutine calls, which we can see here in the relatively large number of
    branch operations. Again, the memory operations take the majority of all cycles,
    as each pass of the sort requires a redistribution of the data among the SPs.
    While the eGPU-QP version requires fewer clock cycles because of the increased
    write bandwidth, the normalized cost of the QP version is higher, largely because
    of the lower clock frequency.


    A similar pattern of instruction distribution is seen in the FFT. Increasing wavefront
    depth for larger datasets reduces NOPs significantly. The number of FP instructions
    (which are doing the actual FFT calculations) is relatively small, at about 10%.
    The largest proportion of operations are once again the memory accesses, especially
    in the write to shared memory; using the QP version of the eGPU results in a 20%
    to 30% decrease in total cycles. The normalized cost of the two eGPU versions,
    however, is approximately the


    Table 8: Bitonic Sort and FFT Benchmarks


    |     |               |        | Bitonic Sort |       | FFT    |      |            |  |  |

    |-----|---------------|--------|--------------|-------|--------|------|------------|--|--|

    | Dim | Metric        |        | eGPU         | eGPU  |        | eGPU | eGPU<br>QP
    |  |  |

    |     |               | Nios   | DP           | QP    | Nios   | DP   |            |  |  |

    |     | Cycles        | 8457   | 1742         | 1543  | 9165   | 876  | 714        |  |  |

    |     | Time(us)      | 24.37  | 2.25         | 2.51  | 26.41  | 1.14 | 1.19       |  |  |

    | 32  | Ratio(cycles) | 4.89   | 1.0          | 0.86  | 10.46  | 1.0  | 0.82       |  |  |

    |     | Ratio(time)   | 10.8   | 1.0          | 1.1   | 23.16  | 1.0  | 1.04       |  |  |

    |     | Normalized    | 1.24   | 1.0          | 1.24  | 4.38   | 1.0  | 1.18       |  |  |

    |     | Cycles        | 20687  | 3728         | 3054  | 20848  | 1695 | 1312       |  |  |

    |     | Time(us)      | 59.6   | 4.83         | 5.09  | 60.08  | 2.20 | 2.19       |  |  |

    | 64  | Ratio(cycles) | 5.54   | 1.0          | 0.82  | 12.30  | 1.0  | 0.82       |  |  |

    |     | Ratio(time)   | 12.3   | 1.0          | 1.05  | 27.31  | 1.0  | 1.01       |  |  |

    |     | Normalized    | 1.42   | 1.0          | 1.18  | 5.17   | 1.0  | 1.13       |  |  |

    |     | Cycles        | 49741  | 8326         | 6536  | 46667  | 3463 | 2558       |  |  |

    |     | Time(us)      | 143.3  | 10.8         | 10.9  | 134.49 | 4.29 | 4.26       |  |  |

    | 128 | Ratio(cycles) | 5.97   | 1.0          | 0.79  | 13.48  | 1.0  | 0.74       |  |  |

    |     | Ratio(time)   | 13.2   | 1.0          | 1.01  | 31.35  | 1.0  | 0.95       |  |  |

    |     | Normalized    | 1.48   | 1.0          | 1.13  | 5.93   | 1.0  | 1.08       |  |  |

    |     | Cycles        | 149271 | 16578        | 11974 | 103636 | 6813 | 4736       |  |  |

    |     | Time(us)      | 430.2  | 21.5         | 19.9  | 298.66 | 8.84 | 7.89       |  |  |

    | 256 | Ratio(cycles) | 9.0    | 1.0          | 0.72  | 15.21  | 1.0  | 0.70       |  |  |

    |     | Ratio(time)   | 20.0   | 1.0          | 0.93  | 33.79  | 1.0  | 0.89       |  |  |

    |     | Normalized    | 2.24   | 1.0          | 1.05  | 6.39   | 1.0  | 1.01       |  |  |


    same, with the high clock frequency of the base version offsetting the higher
    memory bandwidth of the QP version. These results also point to a better optimization
    for the FFT: by using a higher radix FFT, there will be correspondingly fewer
    passes through the shared memory. (We have a extensive flexibility in specifying
    the register and thread parameters, we can easily support much higher radices,
    which will require much larger register spaces).


    Comparing against Nios, we can see that the eGPU performs very well. We see at
    least an OOM performance difference based on time, and in almost all cases on
    a cycle basis as well. This tells us that eGPU is a more efficient architecture
    than a RISC processor, and is a viable candidate for a soft accelerator core.


    ## 8 CONCLUSIONS


    We have demonstrated a GPGPU that consistently beats 770 MHz for a wide range
    of parameters, and described the design approach required to reach such frequencies.
    We are able to swap in and out features as well as change the precision of the
    integer ALU to optimize for area and resource balancing in the FPGA.


    For the eGPU to be useful in an actual system design, it must offer an improvement
    over known methods. We compare the eGPU to a mature commercial soft CPU (Nios)
    over a number of benchmarks. The eGPU is much better on a cycle by cycle or elasped
    time basis in all cases we tried (typically by one to two OOM), and is still better
    on an area normalized basis. When we add the dot product core which can be used
    directly by the eGPU in a regular GPGPU context - the advantage can increase again
    by several times. A soft GPU therefore can offer a valid implementation option
    for many types of algorithms. This does not mean that a GPGPU will replace the
    RISC, anymore than a discrete GPGPU will replace a discrete RISC, only that we
    have shown that the soft GPGPU can now be considered for commercial designs, rather
    than just being of academic interest. The eGPU only uses 1%-2% of a current mid-range
    device, making it a cost effective option to implement complex algorithms in a
    larger FPGA system design, even if multiple cores are required.


    A Statically and Dynamically Scalable Soft GPGPU


    ## REFERENCES


    - <span id="page-10-30"></span>[1] 2016. Nios II Classic Processor Reference GuideNios
    II Classic Processor Reference Guide. [https://www.intel.com/content/www/us/en/docs/programmable/683620/](https://www.intel.com/content/www/us/en/docs/programmable/683620/current/overview-67435.html)
    [current/overview-67435.html.](https://www.intel.com/content/www/us/en/docs/programmable/683620/current/overview-67435.html)

    - <span id="page-10-0"></span>[2] 2017. FFT IP Core: User Guide. [https://www.intel.co.uk/content/www/uk/en/](https://www.intel.co.uk/content/www/uk/en/products/details/fpga/intellectual-property/dsp/fft.html)
    [products/details/fpga/intellectual-property/dsp/fft.html.](https://www.intel.co.uk/content/www/uk/en/products/details/fpga/intellectual-property/dsp/fft.html)

    - <span id="page-10-2"></span>[3] 2017. High-speed Reed-Solomon IP Core User Guide.
    [https://www.intel.com/](https://www.intel.com/content/www/us/en/docs/programmable/683120/17-1/about-the-high-speed-reed-solomon-ip-core.html)
    [content/www/us/en/docs/programmable/683120/17-1/about-the-high-speed](https://www.intel.com/content/www/us/en/docs/programmable/683120/17-1/about-the-high-speed-reed-solomon-ip-core.html)[reed-solomon-ip-core.html.](https://www.intel.com/content/www/us/en/docs/programmable/683120/17-1/about-the-high-speed-reed-solomon-ip-core.html)

    - <span id="page-10-4"></span>[4] 2018. Microblaze Processor Reference Guide.
    [https://docs.xilinx.com/v/u/2018.2-](https://docs.xilinx.com/v/u/2018.2-English/ug984-vivado-microblaze-ref)
    [English/ug984-vivado-microblaze-ref.](https://docs.xilinx.com/v/u/2018.2-English/ug984-vivado-microblaze-ref)

    - <span id="page-10-15"></span>[5] 2020. HB0919 Handbook CoreVectorBlox. [https://www.microsemi.com/existing](https://www.microsemi.com/existing-parts/parts/152678)[parts/parts/152678.](https://www.microsemi.com/existing-parts/parts/152678)

    - <span id="page-10-19"></span>[6] 2021. Block-by-Block Configurable Fast Fourier
    Transform Implementation on AI Engine (XAPP1356). [https://docs.xilinx.com/r/en-US/xapp1356-fft-ai-engine/FFT](https://docs.xilinx.com/r/en-US/xapp1356-fft-ai-engine/FFT-on-Multiple-AI-Engines)[on-Multiple-AI-Engines.](https://docs.xilinx.com/r/en-US/xapp1356-fft-ai-engine/FFT-on-Multiple-AI-Engines)

    - <span id="page-10-23"></span>[7] 2021. Intel Agilex7 FPGAs and SoCs F-Series:
    Product Table. [https://www.intel.](https://www.intel.com/content/dam/www/programmable/us/en/pdfs/literature/pt/intel-agilex-f-series-product-table.pdf)
    [com/content/dam/www/programmable/us/en/pdfs/literature/pt/intel-agilex-f](https://www.intel.com/content/dam/www/programmable/us/en/pdfs/literature/pt/intel-agilex-f-series-product-table.pdf)[series-product-table.pdf.](https://www.intel.com/content/dam/www/programmable/us/en/pdfs/literature/pt/intel-agilex-f-series-product-table.pdf)

    - <span id="page-10-1"></span>[8] 2022. Fast Fourier Transform v9.1. [https://www.xilinx.com/content/dam/xilinx/](https://www.xilinx.com/content/dam/xilinx/support/documents/ip_documentation/xfft/v9_1/pg109-xfft.pdf)
    [support/documents/ip\\_documentation/xfft/v9\\_1/pg109-xfft.pdf.](https://www.xilinx.com/content/dam/xilinx/support/documents/ip_documentation/xfft/v9_1/pg109-xfft.pdf)

    - <span id="page-10-3"></span>[9] 2022. Nios V Processor Reference Manual. [https://www.intel.com/content/www/](https://www.intel.com/content/www/us/en/products/details/fpga/nios-processor/v.html)
    [us/en/products/details/fpga/nios-processor/v.html.](https://www.intel.com/content/www/us/en/products/details/fpga/nios-processor/v.html)

    - <span id="page-10-27"></span>[10] 2023. Floating-Point IP Cores User Guide.
    [https://www.intel.com/content/www/](https://www.intel.com/content/www/us/en/docs/programmable/683750/23-1/about-floating-point-ip-cores.html)
    [us/en/docs/programmable/683750/23-1/about-floating-point-ip-cores.html.](https://www.intel.com/content/www/us/en/docs/programmable/683750/23-1/about-floating-point-ip-cores.html)

    - <span id="page-10-24"></span>[11] 2023. Intel Agilex 7 Variable Precision DSP
    Blocks. [https://www.intel.com/](https://www.intel.com/content/www/us/en/docs/programmable/683037/23-3/variable-precision-dsp-blocks-overview.html)
    [content/www/us/en/docs/programmable/683037/23-3/variable-precision-dsp](https://www.intel.com/content/www/us/en/docs/programmable/683037/23-3/variable-precision-dsp-blocks-overview.html)[blocks-overview.html.](https://www.intel.com/content/www/us/en/docs/programmable/683037/23-3/variable-precision-dsp-blocks-overview.html)

    - <span id="page-10-22"></span>[12] 2023. Intel Agilex7 Embedded Memory User Guide.
    [https://www.intel.com/](https://www.intel.com/content/www/us/en/docs/programmable/683241/23-2/embedded-memory-overview.html)
    [content/www/us/en/docs/programmable/683241/23-2/embedded-memory](https://www.intel.com/content/www/us/en/docs/programmable/683241/23-2/embedded-memory-overview.html)[overview.html.](https://www.intel.com/content/www/us/en/docs/programmable/683241/23-2/embedded-memory-overview.html)

    - <span id="page-10-18"></span>[13] 2023. Versal Adaptive SoC AI Engine Architecture
    Manual (AM009). [https:](https://docs.xilinx.com/v/u/en-US/wp506-ai-engine) [//docs.xilinx.com/v/u/en-US/wp506-ai-engine.](https://docs.xilinx.com/v/u/en-US/wp506-ai-engine)

    - <span id="page-10-5"></span>[14] Abdullah Al-Dujaili, Florian Deragisch, Andrei
    Hagiescu, and Weng-Fai Wong. 2012. Guppy: A GPU-like soft-core processor. In 2012
    International Conference on Field-Programmable Technology. 57‚Äì60.<https://doi.org/10.1109/FPT.2012.6412112>

    - <span id="page-10-6"></span>[15] Muhammed Al Kadi, Benedikt Janssen, and Michael
    Huebner. 2016. FGPU: An SIMT-Architecture for FPGAs. In Proceedings of the 2016
    ACM/SIGDA International Symposium on Field-Programmable Gate Arrays (Monterey,
    California, USA) (FPGA ''16). Association for Computing Machinery, New York, NY,
    USA, 254‚Äì263.<https://doi.org/10.1145/2847263.2847273>

    - <span id="page-10-25"></span>[16] Kevin Andryc. 2018. An Architecture Evaluation
    and Implementaiton of a Soft GPGPU for FPGAs. (2018).<https://doi.org/10.7275/12722172>

    - <span id="page-10-7"></span>[17] Kevin Andryc, Murtaza Merchant, and Russell
    Tessier. 2013. FlexGrip: A soft GPGPU for FPGAs. In 2013 International Conference
    on Field-Programmable Technology (FPT). 230‚Äì237.<https://doi.org/10.1109/FPT.2013.6718358>

    - <span id="page-10-8"></span>[18] Raghuraman Balasubramanian, Vinay Gangadhar,
    Ziliang Guo, Chen-Han Ho, Cherin Joseph, Jaikrishnan Menon, Mario Paulo Drumond,
    Robin Paul, Sharath Prasad, Pradip Valathol, and Karthikeyan Sankaralingam. 2015.
    Enabling GPGPU Low-Level Hardware Explorations with MIAOW: An Open-Source RTL
    Implementation of a GPGPU. ACM Trans. Archit. Code Optim. 12, 2, Article 21 (jun
    2015), 25 pages.<https://doi.org/10.1145/2764908>

    - <span id="page-10-26"></span>[19] K. E. Batcher. 1968. Sorting Networks and
    Their Applications. In Proceedings of the April 30‚ÄìMay 2, 1968, Spring Joint Computer
    Conference (Atlantic City, New Jersey) (AFIPS ''68 (Spring)). Association for
    Computing Machinery, New York,


    NY, USA, 307‚Äì314.<https://doi.org/10.1145/1468075.1468121>


    - <span id="page-10-16"></span>[20] Hui Yan Cheah, Fredrik Brosser, Suhaib A.
    Fahmy, and Douglas L. Maskell. 2014. The IDEA DSP Block-Based Soft Processor for
    FPGAs. ACM Trans. Reconfigurable Technol. Syst. 7, 3, Article 19 (sep 2014), 23
    pages.<https://doi.org/10.1145/2629443>

    - <span id="page-10-17"></span>[21] Hui Yan Cheah, Suhaib A. Fahmy, and Nachiket
    Kapre. 2014. Analysis and optimization of a deeply pipelined FPGA soft processor.
    In 2014 International Conference on Field-Programmable Technology (FPT). 235‚Äì238.
    [https://doi.org/10.](https://doi.org/10.1109/FPT.2014.7082783) [1109/FPT.2014.7082783](https://doi.org/10.1109/FPT.2014.7082783)

    - <span id="page-10-12"></span>[22] Christopher Han-Yu Chou, Aaron Severance,
    Alex D. Brant, Zhiduo Liu, Saurabh Sant, and Guy G. Lemieux. 2011. VEGAS: soft
    vector processor with scratchpad memory. In Proceedings of the ACM/SIGDA 19th
    International Symposium on Field Programmable Gate Arrays, FPGA 2011, Monterey,
    California, USA, February 27, March 1, 2011, John Wawrzynek and Katherine Compton
    (Eds.). ACM, 15‚Äì24. <https://doi.org/10.1145/1950413.1950420>

    - <span id="page-10-21"></span>[23] Jeffrey Chromczak, Mark Wheeler, Charles Chiasson,
    Dana How, Martin Langhammer, Tim Vanderhoek, Grace Zgheib, and Ilya Ganusov. 2020.
    Architectural Enhancements in Intel¬Æ Agilex‚Ñ¢ FPGAs. In FPGA ''20: The 2020 ACM/SIGDA
    International Symposium on Field-Programmable Gate Arrays, Seaside, CA, USA, February
    23-25, 2020, Stephen Neuendorffer and Lesley Shannon (Eds.). ACM, 140‚Äì149.<https://doi.org/10.1145/3373087.3375308>

    - <span id="page-10-9"></span>[24] Pedro Duarte, Pedro Tomas, and Gabriel Falcao.
    2017. SCRATCH: An End-to-End Application-Aware Soft-GPGPU Architecture and Trimming
    Tool. In Proceedings of the 50th Annual IEEE/ACM International Symposium on Microarchitecture
    (Cambridge, Massachusetts) (MICRO-50 ''17). Association for Computing Machinery,
    New York, NY, USA, 165‚Äì177.<https://doi.org/10.1145/3123939.3123953>

    - <span id="page-10-10"></span>[25] Jeffrey Kingyens and J. Gregory Steffan. 2010.
    A GPU-inspired soft processor for high-throughput acceleration. In 2010 IEEE International
    Symposium on Parallel and Distributed Processing, Workshops and Phd Forum (IPDPSW).
    1‚Äì8. [https:](https://doi.org/10.1109/IPDPSW.2010.5470679) [//doi.org/10.1109/IPDPSW.2010.5470679](https://doi.org/10.1109/IPDPSW.2010.5470679)

    - <span id="page-10-28"></span>[26] Ian Kuon and Jonathan Rose. 2006. Measuring
    the gap between FPGAs and ASICs. In Proceedings of the ACM/SIGDA 14th International
    Symposium on Field Programmable Gate Arrays, FPGA 2006, Monterey, California,
    USA, February 22- 24, 2006, Steven J. E. Wilton and Andr√© DeHon (Eds.). ACM, 21‚Äì30.
    [https:](https://doi.org/10.1145/1117201.1117205) [//doi.org/10.1145/1117201.1117205](https://doi.org/10.1145/1117201.1117205)

    - <span id="page-10-29"></span>[27] Martin Langhammer and Gregg Baeckler. 2018.
    High Density and Performance Multiplication for FPGA. In 25th IEEE Symposium on
    Computer Arithmetic, ARITH 2018, Amherst, MA, USA, June 25-27, 2018. IEEE, 5‚Äì12.
    [https://doi.org/10.1109/](https://doi.org/10.1109/ARITH.2018.8464695) [ARITH.2018.8464695](https://doi.org/10.1109/ARITH.2018.8464695)

    - <span id="page-10-20"></span>[28] Martin Langhammer and George A. Constantinides.
    2023. eGPU: A 750 MHz Class Soft GPGPU for FPGA. In 2023 33rd International Conference
    on Field-Programmable Logic and Applications (FPL). 277‚Äì282. [https://doi.org/10.1109/](https://doi.org/10.1109/FPL60245.2023.00047)
    [FPL60245.2023.00047](https://doi.org/10.1109/FPL60245.2023.00047)

    - <span id="page-10-11"></span>[29] Rui Ma, Jia-Ching Hsu, Tian Tan, Eriko Nurvitadhi,
    Rajesh Vivekanandham, Aravind Dasu, Martin Langhammer, and Derek Chiou. 2021.
    DO-GPU: Domain Optimizable Soft GPUs. In 2021 31st International Conference on
    Field-Programmable Logic and Applications (FPL). 140‚Äì144. [https://doi.org/10.1109/FPL53798.2021.](https://doi.org/10.1109/FPL53798.2021.00031)
    [00031](https://doi.org/10.1109/FPL53798.2021.00031)

    - <span id="page-10-13"></span>[30] Aaron Severance and Guy Lemieux. 2012. VENICE:
    A compact vector processor for FPGA applications. In 2012 International Conference
    on Field-Programmable Technology, FPT 2012, Seoul, Korea (South), December 10-12,
    2012. IEEE, 261‚Äì268. <https://doi.org/10.1109/FPT.2012.6412146>

    - <span id="page-10-14"></span>[31] Aaron Severance and Guy G. F. Lemieux. 2013.
    Embedded supercomputing in FPGAs with the VectorBlox MXP Matrix Processor. In
    Proceedings of the International Conference on Hardware/Software Codesign and
    System Synthesis, CODES+ISSS 2013, Montreal, QC, Canada, September 29 - October
    4, 2013. IEEE, 6:1‚Äì6:10.<https://doi.org/10.1109/CODES-ISSS.2013.6658993>'
  decisions:
    evaluation_prompt: 'Qualified. Reason: The paper contains multiple sections and
      discussions that provide empirical evaluation, including benchmarking several
      algorithms, performance tables, and comparisons with other processors. There
      are sections labeled "Results" and "Benchmarks" that discuss the evaluation
      of the proposed method.'
    related_work_prompt: 'Qualified. Reason: The paper meaningfully engages with prior
      research throughout its text. It includes numerous academic citations, discusses
      previous work on soft GPGPU architectures, and compares its proposed method
      with existing solutions. The paper provides context and discussion for the tools,
      frameworks, and datasets it cites, and it explains and compares its methodology
      to previous work in sections such as the Introduction and Background.'
    novelty_prompt: 'Qualified. Reason: The paper proposes a novel parameterized SIMT
      processor architecture for FPGAs, called the eGPU, which is both statically
      and dynamically scalable. It claims to achieve high performance by matching
      the processor pipeline structure to the physical structure of the FPGA fabric.
      The paper also demonstrates the eGPU''s performance advantages over existing
      soft RISC processors and other soft GPGPUs, highlighting its novelty in design
      and application.'
    review_only_prompt: 'Qualified. Reason: The paper introduces a novel parameterized
      SIMT processor for FPGA, describes its architecture, and presents new contributions
      such as dynamic and static scalability, benchmarking results, and comparisons
      with existing processors. It does not primarily summarize existing work but
      rather focuses on presenting new research and findings.'
  topics:
    main_topic: Computer Architecture
    main_topic_reasoning: The paper focuses on the architecture of a soft GPGPU processor
      built for FPGAs, discussing how it utilizes FPGA resources and enhances computational
      capabilities while addressing performance limitations. This aligns directly
      with topics in computer architecture.
    secondary_topic: High-performance Computing
    secondary_topic_reasoning: The paper also pertains to high-performance computing
      as it deals with maximizing performance through scalable architectural design,
      efficient resource utilization, and benchmarking for performance in computational
      tasks.
    main_topic_sub: Accelerator-based, application-specific, and reconfigurable architectures
    secondary_topic_sub: Other
- title: "Study on the Particle Sorting Performance for Reactor Monte Carlo\n  Neutron\
    \ Transport on Apple Unified Memory GPUs"
  abstract: 'In simulation of nuclear reactor physics using the Monte Carlo neutron

    transport method on GPUs, the sorting of particles plays a significant role in

    performance of calculation. Traditionally, CPUs and GPUs are separated devices

    connected at low data transfer rate and high data transfer latency. Emerging

    computing chips tend to integrate CPUs and GPUs. One example is the Apple

    silicon chips with unified memory. Such unified memory chips have opened doors

    for new strategies of collaboration between CPUs and GPUs for Monte Carlo

    neutron transport. Sorting particle on CPU and transport on GPU is an example

    of such new strategy, which has been suffering the high CPU-GPU data transfer

    latency on the traditional devices with separated CPU and GPU. The finding is

    that for the Apple M2 max chip, sorting on CPU leads to better performance per

    power than sorting on GPU for the ExaSMR whole core benchmark problems and the

    HTR-10 high temperature gas reactor fuel pebble problem. The partially sorted

    particle order has been identified to contribute to the higher performance with

    CPU sort than GPU. The in-house code using both CPU and GPU achieves 7.5 times

    power efficiency that of OpenMC on CPU for ExaSMR whole core benchmark with

    depleted fuel, and 150 times for HTR-10 fuel pebble benchmark with depleted

    fuel.'
  url: http://arxiv.org/abs/2401.11455v2
  keywords: sorting, Monte Carlo, neutron transport, GPU, apple, unified memory
  document: '### I. INTRODUCTION


    Being the method with the highest fidelity, the Monte Carlo method has been adopted
    as a verification tool to other methods such as discrete ordinates and the method
    of characteristics. Because of its heavy computation burden, the Monte Carlo method
    has not been considered as the everyday reactor simulation tool. The great performance
    improvement on GPUs demonstrated in recent studies makes the adoption of Monte
    Carlo method as a routine practice more practical. Table [I](#page-0-0) summarizes
    some recent work.


    As discovered by Hamilton [\[1\]](#page-5-0), particle sorting is important for
    achieving high neutron transport performance by increasing the coherence in execution
    paths between GPU threads. Joo [\[2\]](#page-5-1) further elaborates the particle
    sorting strategies. In previous study, most codes such as Pragma [\[2\]](#page-5-1),
    Shift [\[1\]](#page-5-0) and MagiC [\[3\]](#page-5-2) (possibly) use GPUs for
    particle sorting, and OpenMC [\[4\]](#page-5-3) possibly uses CPUs for particle
    sorting. The Warp [\[5\]](#page-5-4) code seems sorting particles on CPUs too.


    <span id="page-0-0"></span>TABLE I SUMMARY OF CONTINUOUS ENERGY MONTE CARLO NEUTRON
    TRANSPORT CODE WITH GPU SUPPORT


    | Code       | Developer                  | Sorting on CPUs or GPUs |

    |------------|----------------------------|-------------------------|

    | Warp [5]   | Univ. California, Berkeley | CPUs                    |

    | Pragma [2] | Seoul National Univ.       | GPUs                    |

    | Shift [1]  | Oak Ridge National Lab.    | GPUs                    |

    | OpenMC [4] | Argonne National Lab.      | CPUs (Possibly)         |

    | MagiC [3]  | Univ. South China          | GPU (Possibly)          |

    | In-house   | In-house                   | CPUs and GPUs           |

    |            |                            |                         |


    As indicated in Figure [1,](#page-0-1) from chips for personal entertainment such
    as Sony Playstation 5 [\[6\]](#page-5-5) to chips for high performance computation
    such as AMD [\[7\]](#page-5-6) and Nvidia [\[8\]](#page-5-7) have merged CPU and
    GPU chips. Some chips such as PS5 and MI300A have unified memory with the CPU,
    GPU and memory connected with the high speed bus called infinity fabric.


    ![](_page_0_Figure_11.jpeg)


    <span id="page-0-1"></span>Fig. 1. A snapshot of the design of some recent merged
    CPU and GPU chips


    This work proposes to use Apple unified memory computing devices to study the
    collaboration between CPUs and GPUs in Monte Carlo neutron transport methods.
    This collaboration is previously uneconomic because of the low data transfer rate
    and high data transfer latency between CPUs and GPUs on computing devices with
    separated CPUs and GPUs. There are previous work study the Apple silicon for Monte
    Carlo methods in areas such as: CPU performance study [\[9\]](#page-5-8), multicore
    CPU work balance [\[10\]](#page-5-9), and cross section lookup on GPU [\[11\]](#page-5-10).


    The contributions are summarized as followed.


    ‚Ä¢ Discussion about programming for Apple M2 Max chip


    - Study of the sorting performance on CPU-GPU for partially sorted data

    - Verification of in-house merged CPU-GPU code with VERA pincell and assembly
    benchmark problems

    - Comparison of CPU and GPU sorting strategies on the simulation power efficiency
    for ExaSMR whole core and HTR-10 fuel pebble benchmark problems


    # II. DEVELOPMENT ON APPLE SILICON AS A UNIFIED MEMORY DEVICE


    The Apple silicon chips are system-on-chips (SoCs), where a cluster of more powerful
    performance CPU cores, and a cluster of less powerful efficiency cores, and a
    cluster of GPU cores are integrated on the same silicon die. All CPU and GPU clusters
    have its private L2 cache, and these clusters are sharing an L3 cache named as
    the System Level Cache (SLC).


    ### *A. Apple M2 Max Chip*


    In this work, the Apple M2 Max chip is studied and Figure [2](#page-1-0) gives
    a snapshot [\[12\]](#page-5-11) and an illustration of the chip components. There
    are four memory chips surrounding the SoC in the center. The memory type is LPDDR5,
    which offers an interface of 512 bit with a bandwidth of 400 GB/s. In most gaming
    GPUs, GDDR6 and GDDR6X are the most common types, and in workstation GPUs, HBM2
    and HBM3 are the most common types. Usually, LPDDR5 is used for power efficient
    mobile devices, and LPDDR5 has higher latency. The way of Apple''s use of LPDDR5
    is unusual, and it has a tight connection with the SoC. The result of this tight
    packaging is the lower latency than the usual LPDDR5 packaging while keeping the
    power consumption at low level.


    The SoC includes 8 performance CPU cores sharing 32 MB L2 cache and 4 efficiency
    CPU cores sharing 4 MB L2 cache. The L2 cache is much larger than Intel, AMD and
    many ARM based CPUs. There are 38 GPU cores sharing an unknown size of L2 cache.
    Moreover, there is a system level cache (SLC) of 48 MB for all CPU cores and GPU
    cores.


    What makes the Apple SoC unique is that the CPU and GPU are sharing the same memories
    and there is a single SLC for both CPU and GPU. Such a design enables closer collaboration
    between CPUs and GPUs. Table [II](#page-1-1) illustrates some of the difference
    between Apple SoC and systems with discrete GPUs. The close connection between
    CPU and GPU in Apple SoC enables collaborated CPU-GPU algorithms with frequent
    communication between CPU and GPU.


    <span id="page-1-1"></span>TABLE II COMPARISON OF APPLE SOC AND SYSTEMS OF CPU
    WITH DISCRETE GPU


    |                    | Apple SoC  | Discrete GPU |

    |--------------------|------------|--------------|

    | CPU-GPU bus        | in-silicon | PCI-E        |

    | Memory type        | sharing    | host/device  |

    | GPU memory latency | low        | high         |


    ![](_page_1_Picture_11.jpeg)


    Fig. 2. A snapshot [\[12\]](#page-5-11) (left) and a sketch of the design (right)
    of Apple M2 Max chip. I-Cache stands for instruction cache, and D-Cache stands
    for data cache. Avalanche and Blizzard are architecture design code names.


    # <span id="page-1-0"></span>*B. Objective-C/Swift Programming Languages and Frameworks*


    The operating systems MacOS for laptops and workstations, and iPadOS for tablets,
    and iOS for mobile phones, and watchOS for watches, and tvOS for home media stations,
    and visionOS for the recently released space computing goggles are delivered with
    user interfaces with distinguished styles. The basic design of such user interfaces
    is dated back to the 1980s, where C++ has not yet been prevailing. Another object-oriented
    language Objective-C [\[13\]](#page-5-12) inspired from the SmallTalk [\[14\]](#page-5-13)
    is adopted by Apple to develop the user interfaces.


    Later, in the last decade, the Swift [\[15\]](#page-5-14) language is further
    proposed for meeting the demand of software developers for an easier to use languages.
    Applications developed in Objective-C or Swift are integrated with system frameworks
    such as Cocoa [\[16\]](#page-5-15) for user interfaces and Metal [\[17\]](#page-5-16)
    for 3D graphics. Figure [3](#page-2-0) illustrates the layers of applications,
    frameworks and OS kernel.


    At the lowest level, Apple computing devices run the Darwin OS kernel [\[18\]](#page-5-17),
    which is different from Linux. Same as Linux, Darwin implements the Portable Operating
    Sys-


    ![](_page_2_Figure_0.jpeg)


    <span id="page-2-0"></span>Fig. 3. A sketch of application development in Objective-C
    & Swift programming language on Apple devices.


    tem Interface (POSIX) [\[19\]](#page-5-18). So migration of lower level applications
    between Linux and Darwin is much easier than that between Linux and Windows, where
    the POSIX has not been completely implemented on Windows. As a side notice, Windows
    has provided the Windows Subsystem for Linux (WSL) [\[20\]](#page-5-19) to provide
    an embedded Linux environment, in order to execute Linux application on Windows.


    ### *C. Metal Shading Language & Framework*


    At the beginning, Apple did not design its own programming languages for GPUs.
    Instead, OpenGL [\[21\]](#page-5-20) and OpenCL [\[22\]](#page-6-0) are adopted,
    which are open standards conceived by many vendors.


    However, as the Apple GPUs get more powerful, the OpenGL and OpenCL have been
    not able to keep the pace of increased hardware features provided by Apple chips.
    So, the Metal Shading Language(MSL) [\[17\]](#page-5-16) has been proposed.


    Applications written in MSL rely on toolchains provided by the Metal framework
    for compilation and execution. Although both MSL and CUDA [\[23\]](#page-6-1)
    C++ are based on C++, there are differences in the code building stages. Figure
    [4](#page-2-1) illustrates the major difference.


    In CUDA, the host code running on CPU and device code running on GPU are combined
    in the same CUDA C++ source code, while in Metal, the host code in Objective-C
    or Swift and device code in Metal are separated. Also, in CUDA the CPU and GPU
    binaries are packed in a single executable, while in Metal, the CPU executable
    will load Metal GPU code in runtime.


    ![](_page_2_Figure_8.jpeg)


    <span id="page-2-1"></span>Fig. 4. A sketch of CPU-GPU program compilation scheme
    on Nvidia and Apple GPU devices.


    ### *D. Apple GPU Programming Patterns*


    Programming with the Metal framework on Apple GPU begins with the creation of
    command queue. Then, create command buffers to submit tasks to GPUs. Each task
    may contain multiple stages. Each stage creates a command encoder, and each GPU
    kernel function binds to a command encoder. After all commands in the buffer are
    encoded, the buffer is committed, so that the GPU starts to execute the commands
    as soon as possible. Figure [5](#page-2-2) illustrates this programming pattern.


    ![](_page_2_Figure_12.jpeg)


    <span id="page-2-2"></span>Fig. 5. Programming patterns for Apple GPU.


    ### III. SORTING ALGORITHMS


    <span id="page-2-4"></span>In this section, the CPU and GPU sorting algorithms
    are discussed and studied on Apple chips.


    ### *A. Summary of Sorting Algorithms on CPU & GPU*


    There are two sorting codes on CPU, which are the C++ standard library (stdlib)
    utility and Intel TBB [\[24\]](#page-6-2) library. The C++ stdlib adopts the Introsort
    algorithm and runs on single thread. The average, best and worse case time complexity
    is O(n log n), where n is the number of elements to sort. The Intel TBB library
    adopts the Quicksort algorithm and supports multi-thread devices. The Quicksort
    algorithm has the same complexity as Introsort, except that the worse case time
    complexity is O(n 2 ). As a side notice, Introsort is a combination of the three
    algorthims: Quicksort, Heapsort, and Insertion sort.


    Because there are no sorting utilities shipped with the Metal framework, an in-house
    code has been implemented using the Bitonic sorting algorithm. The average, best
    and worse case time complexity is O(n log<sup>2</sup> n). The Bitonic algorithm
    requires the data size to be power of 2. Figure [III](#page-2-3) compares the
    CPU and GPU sorting algorithms.


    TABLE III SORTING ALGORITHMS ON CPU & GPU


    <span id="page-2-3"></span>


    | Device | Library                     | Algorithm | Time complexity |

    |--------|-----------------------------|-----------|-----------------|

    | CPU    | C++ Stdlib (single thread ) | Introsort | O(n log n)      |

    | CPU    | Intel TBB (multi-thread)    | Quicksort | O(n log n)      |

    | GPU    | In-house                    | Bitonic   | O(n log2 n)     |


    The time complexity is only a guidance, and the next two subsections propose two
    experiments to illustrate the performance on Apple chips.


    ### *B. Performance of Sorting on Apple Chip*


    *1) Random Integers:* The first experiment studies the sorting algorithms on an
    array of integers randomly sampled. If there are n integers, then each integer
    is sampled using a uniform distribution between 0 and n ‚àí 1. Figure [6](#page-3-0)
    compares the time cost for sorting integer arrays with size from 2 9 to 2 24 .


    ![](_page_3_Figure_2.jpeg)


    <span id="page-3-0"></span>Fig. 6. Comparison of time cost for sorting integer
    arrays with size from 2 9 to 2 24


    On Log-Log scale, the plot of time cost versus data size appears as straight lines.
    On GPU, this ''straight line'' appearance does not extend well below 10<sup>5</sup>
    . This is because of the GPU execution overhead. Notice that the time measured
    is purely the GPU execution cost, not including the GPU kernel launch cost.


    *2) Partially Sorted Integers:* It worths notice that the performance of sorting
    is limited by memory bandwidth. So, for partial sorted data, since there are less
    data move operations than fully random data, some algorithms may perform better.


    To test the performance of sorting of partially sorted integers, it begins with
    an array of fully sorted integers. If the are n integers, then the array is 0,
    1, 2, . . . n ‚àí 1. Next, define a ratio of swap r, and randomly swap ‚åänr‚åã pairs
    of integers in the array, with the pair indices randomly sampled. Here, ‚åänr‚åã takes
    the max integer less or equal to nr. Figure [7](#page-3-1) shows the time cost
    for integer arrays of size 2 <sup>23</sup> with ratio of swap r from 10<sup>‚àí</sup><sup>7</sup>
    to 1. When r = 10<sup>‚àí</sup><sup>7</sup> , there are no swaps, so the ratio of
    swap is essentially 0.


    When the number of swaps varies, the GPU Bitonic algorithm performance keeps nearly
    the same, but the CPU algorithms drastically varies. When there are less than
    10<sup>‚àí</sup><sup>5</sup> of elements are swapped, CPU performs better than GPU.


    ### *C. Sorting Strategy for Monte Carlo Neutron Transport*


    The particle sorting algorithm is important for accelerating Monte Carlo neutron
    transport on GPU. Hamilton [\[1\]](#page-5-0), Joo [\[2\]](#page-5-1), and Tramm
    [\[4\]](#page-5-3) have good summaries, and Liu [\[25\]](#page-6-3) discusses
    the sorting algorithms on Apple computing devices.


    ![](_page_3_Figure_10.jpeg)


    <span id="page-3-1"></span>Fig. 7. Comparison of time cost for sorting integer
    arrays with size 2 <sup>23</sup> and ratio of swaps from 0 to 1.


    ### IV. REACTOR SIMULATION BENCHMARKS


    The previous discussion of sorting algorithm on integer arrays is limited, and
    the results may not reflect the situation of reactor physics simulation. In this
    section, the VERA pincell and assembly problems [\[26\]](#page-6-4) are simulated
    to verify the correctness of the program. Then the ExaSMR [\[27\]](#page-6-5)
    whole core and HTR-10 [\[28\]](#page-6-6) fuel pebble benchmark problems are simulated
    to study the performance.


    ### *A. Simulation Configuration*


    The in-house code on GPU uses 32-bit floating point number since Apple GPUs only
    support 32-bit floating point numbers. Instead, OpenMC uses 64-bit floating point
    numbers.


    The cross sections are prepared in an optimal set of 13 temperatures for the kernel
    reconstruction Doppler broadening method, which is suitable for neutron transport
    in continuously variable media [\[9\]](#page-5-8). For OpenMC, cross sections
    at the same set of temperatures are used, and the 2-point interpolation Doppler
    broadening method is used.


    The in-house code tallies flux of a 23-group structure and the power. OpenMC code
    tallies nothing. Table [IV](#page-3-2) summarizes the simulation configuration.


    TABLE IV SIMULATION CONFIGURATION FOR NEUTRON TRANSPORT


    <span id="page-3-2"></span>


    |                                                                                     |
    OpenMC Code                                                                                                                                     |  |

    |-------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------|--|

    |                                                                                     |
    64-bit (double)                                                                                                                                 |  |

    |                                                                                     |
    turn off                                                                                                                                        |  |

    |                                                                                     |
    turn off                                                                                                                                        |  |

    |                                                                                     |
    turn off                                                                                                                                        |  |

    | Cross section temperatures (K)<br>300, 304.252, 338.681, 412.408, 530.512, 705.793,
    |                                                                                                                                                 |  |

    | 951.89, 1283.538, 1704.703, 2189.43, 2653.095,                                      |                                                                                                                                                 |  |

    | 2950, 3000                                                                          |                                                                                                                                                 |  |

    |                                                                                     |
    2-point linear interpolation                                                                                                                    |  |

    |                                                                                     |
    None                                                                                                                                            |  |

    |                                                                                     |
    ENDF/B-VIII.0                                                                                                                                   |  |

    |                                                                                     |
    In-house Code<br>32-bit (single)<br>turned off<br>turned off<br>turned off<br>kernel
    reconstruction<br>23-group flux and power<br>ENDF/B-VIII.0 |  |


    # *B. Verification: VERA Pincell & Assembly Benchmark Problem*


    In order to verify simulation on Apple GPU, the VERA pincell and assembly benchmark
    problems are studied. Table [V](#page-4-0) compares K-effective values between
    in-house code on Apple M2 Max CPU+GPU and OpenMC code on Apple M2 Max CPU. The
    probability table, thermal scattering, and resonance scattering are not considered.
    There are 1,048,576 particles per cycle with 100 inactive cycles and 200 total
    cycles.


    <span id="page-4-0"></span>TABLE V K-EFFECTIVE OF VERA PINCELL ASSEMBLY BENCHMARK
    PROBLEMS


    |    | In-house     | OpenMC       |    | In-house    | OpenMC      |

    |----|--------------|--------------|----|-------------|-------------|

    |    | CPU+GPU      | CPU only     |    | CPU+GPU     | CPU only    |

    | 1A | 1.18705 (8)  | 1.18805 (8)  | 2E | 1.06910 (7) | 1.06995 (9) |

    | 1B | 1.18190 (9)  | 1.18290 (10) | 2F | 0.97484 (8) | 0.97557 (8) |

    | 1C | 1.17186 (9)  | 1.17257 (9)  | 2G | 0.84713 (6) | 0.84804 (9) |

    | 1D | 1.16345 (10) | 1.16405 (9)  | 2H | 0.78723 (7) | 0.78799 (8) |

    | 1E | 0.77405 (7)  | 0.77529 (6)  | 2I | 1.18092 (8) | 1.18178 (8) |

    | 2A | 1.18315 (8)  | 1.18391 (8)  | 2J | 0.97392 (8) | 0.97481 (8) |

    | 2B | 1.18398 (8)  | 1.18471 (8)  | 2K | 1.02330 (8) | 1.02385 (8) |

    | 2C | 1.17466 (8)  | 1.17532 (9)  | 2L | 1.02126 (7) | 1.02146 (9) |

    | 2D | 1.16689 (8)  | 1.16772 (8)  | 2M | 0.94233 (6) | 0.94209 (9) |


    The GPU code underestimates the K-effective within 100 pcm, and the using of single
    precision floating point numbers play an important role in this discrepancy.


    # *C. Performance Study: ExaSMR Whole Core Benchmark Problem*


    Next, the influence of the sorting on the performance of whole core nuclear reactor
    simulation has been studied with the ExaSMR benchmark problems. Table [VI](#page-4-1)
    summarizes these problems. There are two versions, one contains fresh fuel with
    only 7 nuclides in fuel, and the other one contains depleted fuel with 245 nuclides
    in fuel.


    TABLE VI SUMMARY OF EXASMR WHOLE CORE BENCHMARK SIMULATION


    <span id="page-4-1"></span>


    |                                   | Fresh fuel  | Depleted fuel                   |

    |-----------------------------------|-------------|---------------------------------|

    | Number of nuclides                | 76          | 283                             |

    | Number of nuclides in fuel        | 7           | 245                             |

    | Number of cycles                  |             | 350                             |

    | Number of inactive cycles         |             | 100                             |

    | OpenMC particles per cycle        |             | 20)<br>1,048,576 (2             |

    | In-house code particles per cycle |             | 23)<br>8,388,608 (2             |

    | OpenMC tally                      |             | None                            |

    | In-house code tally               |             | fission power + 23-group fluxes
    |

    | K-effective OpenMC CPU only       | 1.00656 (6) | 1.00660 (5)                     |

    | K-effective In-house CPU+GPU      | 1.00587 (2) | 1.00586 (2)                     |


    The simulation performance is summarized in Table [VII.](#page-4-2) The sorting
    on CPU performs better than sorting on GPU. This attributes to the partially sorted
    order in the particles as studied in Section [III.](#page-2-4) For the fresh fuel
    problem, the in-house code with GPU transport achieves about 3.0 times power efficiency
    that of OpenMC, and about 7.5 times for the depleted fuel problem. The power efficiency
    has been visualized in Figure [8.](#page-4-3)


    <span id="page-4-2"></span>TABLE VII PERFORMANCE OF SORTING FOR EXASMR WHOLE CORE
    BENCHMARK PROBLEMS


    | In-house<br>In-house<br>OpenMC                                 |

    |----------------------------------------------------------------|

    |                                                                |

    | sorting on CPU<br>sorting on GPU                               |

    | active cycles<br>active cycles<br>active cycles                |

    | (particles/s/Watt)<br>(particles/s/Watt)<br>(particles/s/Watt) |

    | Fresh fuel<br>4.5E3<br>3.7E3<br>1.5E3                          |

    | Depleted fuel<br>3.0E3<br>2.5E3<br>4.0E2                       |


    ![](_page_4_Figure_12.jpeg)


    <span id="page-4-3"></span>Fig. 8. Comparison of simulation efficiency in particle
    per second per Watt for the ExaSMR whole core benchmark problem.


    ### *D. Performance Study: Pebble Fuel from HTR-10 Test Reactor*


    In order to verify the influence of sorting algorithms on the performance of simulation
    of emerging high temperature gas reactors, the fuel pebble benchmark problem of
    the HTR-10 test reactor has been studied. High temperature gas reactors have distinguished
    design from the light water reactors, and the previous study of the performance
    with ExaSMR may not apply to HTR-10. The definition of the HTR-10 pebble benchmark
    problem and the simulation configuration and calculated K-effective is summarized
    in Table [VIII.](#page-5-21) The code simulation configuration follows Table [IV.](#page-3-2)
    The material for the HTR-10 pebble problem is given in Appendix [A.](#page-6-7)
    As indicated in Table [IV,](#page-3-2) ENDF/B-VIII.0 is used.


    The simulation performance of both in-house code using CPU and GPU sorting and
    the OpenMC code on CPU is summarized in Table [IX.](#page-5-22) The in-house code
    agrees with OpenMC in terms of K-effective within in 30 pcm (consider 3 standard
    deviations). The K-effective is about 500 pcm lower than the reference [\[29\]](#page-6-8),
    which is possibly attributed to the ignorance of unresolved resonance (URR).


    Same as the ExaSMR whole benchmark, the CPU sorting algorithms perform better
    than GPU sorting on the basis of performance per power. And the in-house code
    is about 270 times more power efficient than OpenMC on CPU for pebble with fresh
    fuel, and 150 times for pebble with depleted fuel.


    TABLE VIII SUMMARY OF HTR-10 BENCHMARK SIMULATION


    <span id="page-5-21"></span>


    |                                   | HTR-10 fuel pebble              |                  |

    |-----------------------------------|---------------------------------|------------------|

    |                                   | Fresh fuel                      | Depleted
    fuel    |

    | Pebble/fuel region radius (cm)    | 3.0/2.5                         |                  |

    | TRISO particles in fuel region    | 8,335                           |                  |

    | TRISO fuel/buffer/PyC1/SiC/PyC2   | 0.025/0.034/0.038/0.0415/0.0455 |                  |

    | layers outer radius (cm)          |                                 |                  |

    | Pebble boundary condition         | Reflected                       |                  |

    | Number of nuclides                | 10                              | 248              |

    | Number of nuclides in fuel        | 5                               | 243              |

    | Temperature                       | 300K                            |                  |

    | OpenMC particles per cycle        | 18)<br>262,144 (2               | 16)<br>65,536
    (2 |

    | In-house code particles per cycle | 1,048,576 (2                    | 20)              |

    | OpenMC tally                      | None                            |                  |

    | In-house code tally               | None                            |                  |

    | K-effective OpenMC CPU only       | 1.68732 (9)                     | 1.68714
    (21)     |

    | K-effective In-house CPU+GPU      | 1.68728 (5)                     | 1.68726
    (5)      |

    | Reference [29]*                   | 1.70534 (13)                    | N/A              |


    \* Assume room temperature with unresolved resonance considered


    <span id="page-5-22"></span>TABLE IX PERFORMANCE OF SORTING FOR HTR-10 BENCHMARK
    PROBLEMS


    |                                          | In-house<br>sorting<br>on CPU | In-house<br>sorting<br>on
    GPU<br>(particles/s/Watt) | OpenMC |

    |------------------------------------------|-------------------------------|-----------------------------------------------------|--------|

    | HTR-10 fuel pebble<br>with fresh fuel    | 1.7E3                         | 1.3E3                                               |
    6.4    |

    | HTR-10 fuel pebble<br>with depleted fuel | 8.6E2                         | 7.7E2                                               |
    5.8    |


    ### V. CONCLUSIONS


    In this work, the influence of particle sorting algorithms on the VERA pin and
    assembly, ExaSMR whole core, and HTR-10 fuel pebble benchmark problems have been
    studied with the Apple unified memory chips with merged CPU and GPU. First, it
    has reviewed the programming details on Apple silicon chips. Second, it has demonstrated
    that with partially sorted data, sorting on Apple M2 Max CPU can outperform GPU.
    Third, it has verified the correctness of the in-house CPU-GPU code with VERA
    pincell and assembly benchmarks. Fourth, it has given evidence that the CPU sort
    is more efficient in power for the ExaSMR whole core benchmark than GPU sort,
    and the in-house CPU-GPU code achieve 3.0 and 7.5 times power efficiency that
    of OpenMC CPU code for the case of fresh and depleted fuel. And finally, it has
    shown that the CPU sort is more efficient in performance per power than GPU sort
    for the HTR-10 fuel pebble benchmark problem, and the in-house CPU-GPU code achieves
    270 and 150 times power efficiency that of OpenMC CPU code for the case of fresh
    and depleted fuel. In the future, when unified memory chips with merged CPU and
    GPU are prevailing, CPU and GPU collaboration methods might be considered for
    Monte Carlo reactor neutron transport method with better power efficiency.


    ### ACKNOWLEDGMENT


    Computing technologies from New Compute Laboratory are used to produce parts of
    the data in this article. New Compute Laboratory & its information providers endeavor
    to ensure the accuracy & reliability of the information provided, but do not guarantee
    completeness or reliability, or that it is up-to-date & accepts no liability (whether
    in tort or contract or otherwise) for any loss or damage, whether direct or indirect,
    arising from errors, inaccuracies or omissions or the information being upto-date.
    Any information provided is at the user''s risk.


    ### REFERENCES


    - <span id="page-5-0"></span>[1] S. Hamilton and T. Evans, "Continuous-energy
    Monte Carlo neutron transport on GPUs in the Shift code," Annuals of Nuclear Energy,
    vol. 128, pp. 236-247, 2019.

    - <span id="page-5-1"></span>[2] N. Choi and H. Joo, "Domain decomposition for
    GPU-based continuous energy Monte Carlo power reactor calculation," Nuclear Engineering
    and Technology, vol. 52, issue 11, pp. 2667-2677, 2020.

    - <span id="page-5-2"></span>[3] K. Gao, Z. Chen, A. Sun and T. Yu, "The research
    and application of GPU-based Monte Carlo Simulation in reactor calculation," Proceedings
    of RPNM2023, Jul. 26-29 Lanzhou China, 2023.

    - <span id="page-5-3"></span>[4] J. Tramm, P. Romano, J. Doerfert, A. Lund, P.
    Shriwise, A. Siegel, and et. al., "Toward Portable GPU Acceleration of the OpenMC
    Monte Carlo Particle Transport Code," Proceedings of PHYSOR2022, May 15- 20 Pittsburg
    USA. 2022.

    - <span id="page-5-4"></span>[5] R. Bergmman, J. Vujic, "Algorithmic choices in
    WARP - A framework ¬¥ for continuous energy Monte Carlo neutron transport in general
    3D geometries on GPUs," Annuals of Nuclear Energy, vol. 77, pp. 176‚Äì 193, 2015.

    - <span id="page-5-5"></span>[6] Sony Playstation 5, https://www.playstation.com/en-us/ps5/
    (Last retrieved: Jan. 21, 2024)

    - <span id="page-5-6"></span>[7] AMD Instinct‚Ñ¢ MI300A Accelerators, https://www.amd.com/en/products/accelerators/instinct/mi300/mi300a.html
    (Last retrieved: Jan. 21, 2024)

    - <span id="page-5-7"></span>[8] NVIDIA Grace Hopper Superchip, https://www.nvidia.com/en-us/datacenter/grace-hopper-superchip/
    (Last retrieved: Jan. 21, 2024)

    - <span id="page-5-8"></span>[9] C. Liu, "Doppler broadening using discrete cosine
    transform and kernel reconstruction for spatially variable media," Annuals of
    Nuclear Energy, vol. 174, pp. 109150, 2012.

    - <span id="page-5-9"></span>[10] P. Romano, J. Tramm, P. Shriwise, "Impact of
    Asymmetric Multicore Processors on Monte Carlo Particle Transport Code Performance,"
    Proceedings of M&C 2023 (394), Aug. 13-17 Niagara Falls Canada, 2023.

    - <span id="page-5-10"></span>[11] J. Tramm, K. Yoshii, P. Romano, "Power at Your
    Fingertips: Assessing the Performance of a Monte Carlo Neutron Transport Mini-App
    on Consumer Laptop GPUs," Proceedings of M&C 2023 (433), Aug. 13-17 Niagara Falls
    Canada, 2023.

    - <span id="page-5-11"></span>[12] Apple unveils M2 Pro and M2 Max: nextgeneration
    chips for next-level workflows, https://www.apple.com/newsroom/images/product/mac/standard/Apple-M2-chips-M2-Max-230117.zip
    (Last retrieved: Jan. 21, 2024)

    - <span id="page-5-12"></span>[13] About Objective-C, https://developer.apple.com/library/archive/
    documentation/Cocoa/Conceptual/ProgrammingWithObjectiveC/Introduction/ Introduction.html
    (Last retrieved: Jan. 21, 2024)

    - <span id="page-5-13"></span>[14] GNU Smalltalk, https://www.gnu.org/software/smalltalk/
    (Last retrieved: Jan. 21, 2024)

    - <span id="page-5-14"></span>[15] Swift: the powerful programming language that''s
    also easy to learn, https://developer.apple.com/swift/ (Last retrieved: Jan. 21,
    2024)

    - <span id="page-5-15"></span>[16] What Is Cocoa?, https://developer.apple.com/library/archive/
    documentation/Cocoa/Conceptual/CocoaFundamentals/WhatIsCocoa/ WhatIsCocoa.html
    (Last retrieved: Jan. 21, 2024)

    - <span id="page-5-16"></span>[17] Accelerate graphics and much more with Metal,
    https://developer.apple.com/metal/ (Last retrieved: Jan. 21, 2024)

    - <span id="page-5-17"></span>[18] Kernel Architecture Overview, https://developer.apple.com/library/archive/
    documentation/Darwin/Conceptual/KernelProgramming/Architecture/ Architecture.html
    (Last retrieved: Jan. 21, 2024)

    - <span id="page-5-18"></span>[19] POSIX (The Portable Operating System Interface),
    https://www.gnu.org/software/libc/manual/html node/POSIX.html (Last retrieved:
    Mar. 14, 2024)

    - <span id="page-5-19"></span>[20] How to install Linux on Windows with WSL, https://learn.microsoft.com/en-us/windows/wsl/install
    (Last retrieved: Mar. 14, 2024)

    - <span id="page-5-20"></span>[21] OpenGL: The Industry''s Foundation for High
    Performance Graphics, https://www.opengl.org (Last retrieved: Jan. 21, 2024)

    - <span id="page-6-0"></span>[22] OpenCL: Open Standard for Parallel Programming
    of Heterogeneous Systems, https://www.khronos.org/opencl/ (Last retrieved: Jan.
    21, 2024)

    - <span id="page-6-1"></span>[23] CUDA Toolkit, https://developer.nvidia.com/cuda-toolkit
    (Last retrieved: Jan. 21, 2024)

    - <span id="page-6-2"></span>[24] Intel¬Æ oneAPI Threading Building Blocks, https://www.intel.com/content/www/us/en/developer/tools/oneapi/onetbb.html
    (Last retrieved: Mar. 14, 2024)

    - <span id="page-6-3"></span>[25] C. Liu, "Monte Carlo neutron transport using
    low power mobile GPU devices", Arxiv, https://arxiv.org/abs/2208.06296, 2022

    - <span id="page-6-4"></span>[26] B. Godfrey, "VERA core physics benchmark progression
    problem specifications, revision 4," CASL technical report CASL-U-2012-0131- 004,
    2014.

    - <span id="page-6-5"></span>[27] E. Merzari, S. Hamilton, T. Evans, M. Min and
    et. al., "Exascale Multiphysics Nuclear Reactor Simulations for Advanced Designs,"
    Proceedings of SC23, Nov. 12-17 Denver USA, https://doi.org/10.1145/3581784.3627038,
    2023

    - <span id="page-6-6"></span>[28] International Handbook of Reactor Physics Experiments,
    "Evaluation of the Initial Critical Configuration of the HTR-10 Pebble-Bed Reactor,"
    HTR10-GCR-RESR-001, NEA/NSC/DOC(2006)1, Rev. 0., 2006

    - <span id="page-6-8"></span>[29] Y. Cheng, C. Hao and F. Li, "Uncertainty quantification
    of fuel pebble model and its effect on the uncertainty propagation of nuclear
    data in pebble bed HTR," Annuals of Nuclear Energy, vol. 139, pp. 107286, 2020.


    ### <span id="page-6-7"></span>APPENDIX


    The definitions of materials used in the HTR-10 pebble benchmark are summarized
    in Table [X.](#page-6-9)


    <span id="page-6-9"></span>


    | TABLE X                                                    |  |

    |------------------------------------------------------------|--|

    | DEFINITIONS OF MATERIALS IN THE HTR-10 PEBBLE PROBLEM WITH |  |

    | FRESH FUEL                                                 |  |


    | Material               | Nuclide | Atomic density (1024cm‚àí3<br>) |

    |------------------------|---------|-------------------------------|

    | Pebble Carbon matrix   | B-10    | 2.49298E-8                    |

    |                        | B-11    | 1.00345E-7                    |

    |                        | C-12    | 8.57768E-2                    |

    |                        | C-13    | 9.60880E-4                    |

    | Fuel kernel            | B-10    | 4.06384E-7                    |

    |                        | B-11    | 1.63575E-6                    |

    |                        | O-16    | 4.64720E-2                    |

    |                        | U-235   | 3.99198E-3                    |

    |                        | U-238   | 1.92441E-2                    |

    | Buffer                 | B-10    | 1.58513E-8                    |

    |                        | B-11    | 6.38035E-8                    |

    |                        | C-12    | 5.45401E-2                    |

    |                        | C-13    | 6.10964E-4                    |

    | Pyrolytic Carbon (PyC) | B-10    | 2.73795E-8                    |

    | inner and outer        | B-11    | 1.10206E-7                    |

    |                        | C-12    | 9.42057E-2                    |

    |                        | C-13    | 1.05530E-3                    |

    | Silicon Carbide (SiC)  | C-12    | 4.72306E-2                    |

    |                        | C-13    | 5.29082E-4                    |

    |                        | Si-28   | 4.40486E-2                    |

    |                        | Si-29   | 2.23666E-3                    |

    |                        | Si-30   | 1.47442E-3                    |'
  decisions:
    evaluation_prompt: 'Qualified. Reason: The paper includes multiple sections that
      provide evidence of evaluation, such as "Performance of Sorting on Apple Chip,"
      "Reactor Simulation Benchmarks," and "Performance Study," which discuss experiments,
      comparisons, and results. Additionally, there are performance tables and figures
      that illustrate the evaluation of the proposed methods.'
    related_work_prompt: 'Qualified. Reason: The paper meaningfully engages with prior
      research by citing and discussing various studies and codes related to Monte
      Carlo neutron transport methods, particularly in the context of GPU performance
      and particle sorting strategies. It provides a comparison of its methods with
      previous work, as seen in the discussion of sorting algorithms and the verification
      of the in-house code against established benchmarks.'
    novelty_prompt: 'Qualified. Reason: The paper proposes the use of Apple unified
      memory computing devices to study the collaboration between CPUs and GPUs in
      Monte Carlo neutron transport methods, which is a novel application of known
      techniques. Additionally, it discusses the programming for the Apple M2 Max
      chip, studies sorting performance on CPU-GPU for partially sorted data, and
      verifies in-house merged CPU-GPU code with benchmark problems, all of which
      contribute new insights and applications.'
    review_only_prompt: '- Qualified. Reason: The paper introduces new contributions,
      such as the proposal to use Apple unified memory computing devices for Monte
      Carlo neutron transport methods, the study of sorting performance on CPU-GPU
      for partially sorted data, and the verification and comparison of CPU and GPU
      sorting strategies on simulation power efficiency. It does not primarily summarize
      existing work.'
  topics:
    main_topic: Computer Architecture
    main_topic_reasoning: The paper discusses the performance of Monte Carlo neutron
      transport simulations on integrated CPU-GPU systems, specifically focusing on
      sorting algorithms and the architecture of unified memory in Apple silicon chips,
      which falls under the field of computer architecture.
    secondary_topic: High-performance Computing
    secondary_topic_reasoning: The paper deals with optimizing performance and energy
      efficiency in high-stakes simulations, which aligns it with themes of high-performance
      computing.
    main_topic_sub: Architectures for emerging technologies and applications
    secondary_topic_sub: Other
- title: "ACS: Concurrent Kernel Execution on Irregular, Input-Dependent\n  Computational\
    \ Graphs"
  abstract: 'GPUs are widely used to accelerate many important classes of workloads
    today.

    However, we observe that several important emerging classes of workloads,

    including simulation engines for deep reinforcement learning and dynamic neural

    networks, are unable to fully utilize the massive parallelism that GPUs offer.

    These applications tend to have kernels that are small in size, i.e., have few

    thread blocks that do not saturate compute resources. Executing independent

    kernels concurrently is a promising approach to improve parallelism and

    utilization. However, this inter-kernel concurrency is difficult to leverage in

    such workloads with existing approaches: First, the inter-kernel dependencies

    and computational graph are input-dependent and vary each time the application

    is executed. Second, the computational graphs tend to be irregular, requiring

    fine-grain scheduling and synchronization; thus incurring significant

    synchronization overheads if kernel execution is parallelized. In this work, we

    propose ACS, a framework that enables lightweight detection of inter-kernel

    dependencies and low overhead kernel scheduling at runtime. The key idea behind

    ACS is to perform inter-kernel dependency checks for a small window of kernels

    at runtime, similar to out-of order instruction scheduling. This enables

    concurrent execution of kernels in applications whose computational graphs are

    input dependent and require fine-grained scheduling. We propose ACS-SW, a

    software-only open-source implementation of ACS and ACS-HW, a hardware-software

    cooperative implementation. ACS-HW further reduces synchronization overheads by

    reducing communication between the CPU and GPU. We evaluate ACS for deep RL

    simulation and dynamic DNNs on both real hardware and a GPU simulator. We

    demonstrate speedups of up to 2.19x (1.56x on average) by improving GPU

    utilization with concurrent kernel execution.'
  url: http://arxiv.org/abs/2401.12377v1
  keywords: ''
  document: '#### I. INTRODUCTION


    Graphics Processing Units (GPUs) today are commonly used to accelerate a diverse
    set of applications, such as deep neural network (DNN) processing, scientific
    computing, graphics, and cryptography. The massive parallelism offered by GPUs
    enables efficient computations on large amounts of data concurrently. However,
    we observe that certain important classes of applications, such as simulation
    engines for deep reinforcement learning (RL) [1]‚Äì[5] and dynamic neural networks
    [6]‚Äì[19], are unable to fully utilize the significant compute capability that
    GPUs offer. This underutilization is because these applications comprise a large
    number of small kernels, i.e., kernels with few thread blocks that are unable
    to fully saturate the GPU cores. To understand the challenges in alleviating this
    underutilization, we evaluate two important classes of applications and introduce
    their properties.


    Simulation Engines for Deep RL. With reinforcement learning (RL) an agent (for
    example, a robot) learns to perform tasks such as robotic locomotion, manipulation,
    and navigation [20], [21] by trial and error from interactions with the environment.
    Deep RL training involves using a DNN to learn policies that optimize for rewards
    from data collected by interacting with a simulation environment. By leveraging
    the benefits of DNNs, deep RL has recently gained widespread application for many
    challenging and important tasks [20], [22]‚Äì[28]. Despite leveraging GPUs, a significant
    fraction of the deep RL runtime is the data collection phase (up to 70% of the
    runtime), where physics simulations are used to generate training data. We observe
    that these physics simulations heavily underutilize the GPU, only achieving an
    occupancy of 34% on average. The underutilization is caused by kernels that contain
    a small number of thread blocks that cannot fully utilize the GPU. Programming
    larger kernels is impractical as each instance simulates a different scenario,
    and large kernels would lead to thread divergence.


    Dynamic DNNs. Several recent types of DNNs [6], [10], [12], [29] have emerged
    as a promising approach to reduce inference latencies in resource-constrained
    devices by reconfiguring/specializing the architecture based on the input to the
    DNN. For example, InstaNAS [10] configures the network architecture at runtime
    based on the input image. Our evaluations demonstrate that, while these architectures
    require significantly fewer FLOPs and lower inference latencies, there is still
    significant underutilization of GPU resources (achieving an occupancy of only
    39% on average). Similar to the simulation engines, we find that this underutilization
    is caused by small kernels that do not fully utilize the GPU cores.


    GPU kernels from such applications are typically executed *serially*, and thus
    the utilization is determined by the size (i.e., the number of threads and thread
    blocks) of the kernel. However, we observe that many kernels are independent and
    thus can be executed concurrently. By concurrently executing independent kernels,
    we can effectively improve GPU utilization and thus performance. Existing GPU
    architectures allow for concurrent execution of kernels by using multiple command
    queues [30] which are abstracted in software (such as CUDA Stream [31]), allowing
    the programmer to identify and launch independent kernels in parallel. However,
    enabling concurrent kernel execution for these applications is still a challenging
    task for two major reasons.


    Challenge 1: Input-dependent computational graphs. For these applications, the
    computational graph (i.e. the kernels to be executed and their dependencies) is
    only resolved at runtime based on the input, and each input or set of inputs leads
    to a different computational graph. This means that identifying independent kernels
    to launch in parallel requires performing inter-kernel dependency checks at runtime.
    These workloads have short running kernels that significantly exacerbate the scheduling
    and dependency checking overheads, making this a challenging problem to solve.
    Frameworks such as CUDA Graph [32] and AMD ATMI [33] allow programmers to define
    the inter-kernel dependency information and construct a directed acyclic graph
    (DAG) of kernels. These frameworks enable concurrent kernel execution. However,
    when inter-kernel dependencies vary by input, we must incur the significant latency
    of constructing the dependency graph and scheduling independent kernels, every
    time the application is executed, significantly increasing run time (¬ß II-D and
    ¬ß VI).


    Challenge 2: Irregular inter-kernel dependencies require fine-grain scheduling.
    The computational graph for a given input tends to be highly irregular. In other
    words, the kernels cannot be easily partitioned into independent streams and fine-grain
    scheduling is required to expose inter-kernel parallelism. Thus, parallel execution
    of kernels requires frequent synchronization to ensure correctness, leading to
    significant synchronization overheads from communicating with the CPU and from
    kernel launches (¬ß II-D).


    To address these challenges, our goal in this work is to enable kernel concurrency
    with *(i)* lightweight scheduling and dependency checking of kernels that can
    be performed at runtime and *(ii)* low overhead synchronization for scheduling
    and kernel launch. To this end, we propose ACS, a new framework for Automatic
    Concurrent Scheduling with two implementations: *(i)* ACS-SW, a software-only
    mechanism to enable lightweight kernel scheduling at runtime and *(ii)* ACS-HW:
    a hardware-software mechanism to further reduce synchronization overheads for
    efficient kernel concurrency.


    The key idea of ACS is to perform dependency checks between sequentially launched
    kernels within a fixed window at runtime, similar to out-of-order instruction
    scheduling. We refer to this window as the *scheduling window*. When a kernel
    is inserted into the scheduling window, the kernels that it is dependent on are
    identified. As kernels complete execution, kernels in the scheduling window are
    marked ready based on the identified dependencies. Ready kernels can then be concurrently
    launched as they have no more dependencies. Since at any given time, only a small
    set of kernels are scheduled and tracked (instead of the entire computational
    graph), this approach enables efficient kernel parallelization and scheduling
    at runtime. To perform dependency checks between kernels, ACS leverages annotations
    from the application that specify the memory address ranges that are read/written
    by each kernel. This metadata is then used to identify inter-kernel dependencies
    at runtime when kernels are inserted into the scheduling window. Compared to prior
    approaches (¬ß III-A), this method alleviates the significant kernel scheduling
    and dependency-check overheads for kernel parallelization.


    ACS-SW implements the above out-of-order runtime kernel scheduling in software
    as an application runtime system using CUDA streams. ACS-SW however still incurs
    synchronization overheads from communication with the CPU and kernel launch. On
    the other hand, ACS-HW implements the out-oforder kernel scheduler in the GPU
    hardware and can alleviate the synchronization overheads. We propose an efficient
    implementation of ACS-HW that reduces synchronization and kernel overheads by
    reducing communication with the CPU.


    Prior works such as task superscalar [34], carbon [35], TDM [36] and ADM [37]
    propose similar out-of-order scheduling to leverage irregular parallelism between
    tasks in CPU multiprocessors. However, the major challenge in CPUs is the latency
    of runtime dependence checking. The primary bottleneck with GPUs is the latency
    for launch/signal completion of kernels rather than dependence checking (¬ß IV-D).
    ACS addresses this challenge and provides an efficient approach to enable out-of-order
    kernel scheduling in GPUs.


    We demonstrate the effectiveness of ACS in improving GPU utilization and thus
    performance for physics simulation workloads, a range of dynamic neural networks,
    as well as static neural networks with small kernels. We demonstrate an average
    speedup of up to 1.87√ó using our softwareonly approach and up to 2.19√ó from the
    hardware-software implementation. The major contributions of this work are:


    - We identify and characterize GPU underutilization as a result of small GPU kernels
    in applications with inputdependent irregular computational graphs, e.g., deepRL
    and dynamic DNNs.

    - We introduce ACS, a runtime mechanism that improves GPU utilization by enabling
    concurrent execution of GPU kernels with a lightweight dependency tracking and
    scheduling framework.

    - We will provide an open-source software-only implementation of ACS that can
    be used on real hardware to enable low overhead GPU kernel concurrency.

    - We evaluate the effectiveness of ACS-SW and ACS-HW on a range of important GPU
    applications and demonstrate significant speedups and improved GPU utilization.


    #### II. MOTIVATION


    #### *A. Baseline GPU architecture*


    Figure 1 shows an overview of the hardware model in modern GPU architectures [38].
    The host communicates with the command processor (CP) of the GPU via a virtual
    memory region which is memory mapped to the GPU, accessible by the command processor.
    This enables communication between the CPU and GPU through entries in the command
    queue. The CPU transmits kernel launch packets to the GPU by writing them to the
    user mode command queue. The CP is responsible for decoding and dispatching the
    kernels in these command queues for execution. The CP accesses the command queue
    and schedules the kernels at the head for execution. This ensures that the kernels
    are dispatched for launch from these queues in order.


    ![](_page_1_Figure_15.jpeg)


    Fig. 1: Scheduling kernels from multiple streamsw


    #### *B. Case Study 1: Simulation Engines for Deep RL*


    Deep reinforcement learning (RL) has widely gained attention as a promising approach
    to learning control policies in robotics and dynamical systems for tasks such
    as locomotion on legged robots [1], [26], [28], dexterous hand manipulation [21],
    autonomous driving [20], [24], and drone control [22], [23], [25]. Deep RL involves
    training a DNN to learn policies that maximize the reward, based on the actions
    that the agent (e.g., four-legged robot) performs in a given environment. This
    training process requires data from the agent interacting with a physics simulator.
    Typically, each training step requires data from thousands of physics simulations.
    Recent works [1]‚Äì[5], [39] accelerate this data generation phase by leveraging
    GPUs. GPUs can accelerate data generation by performing multiple simulations simultaneously
    and also parallelizing within a single simulation. Hence this makes them an appropriate
    candidate workload for GPU execution. Despite GPU acceleration, the simulation/data
    generation phase is still the predominant computation in deep RL‚Äîtaking about
    30‚àí70% of training time depending on the complexity of the simulated environment.
    Thus accelerating simulation engines is critical for deep RL performance.


    To evaluate the efficiency of physics simulations, we analyzed a set of physics
    simulations with different environments on a GPU (parameters in ¬ß V) with the
    widely used Brax [1] framework. We evaluate the utilization of the GPU by measuring
    achieved occupancy (average ratio of active warps to the maximum supported), depicted
    in Fig. 2. We find that as much as 65% of the GPU cores are underutilized on average
    (on both GPUs). To evaluate the cause of this underutilization, we analyze the
    number of kernel launches required to generate one batch of training data in Fig.
    3. We also present the average number of CTAs per kernel in Fig. 4 and depict
    the distribution of kernel sizes observed for the ant environment in Fig. 5. We
    observe that physics simulations in our evaluations generate a large number of
    *small* kernels that have few threads and CTAs. This is a fundamental problem
    because the simulation engine cannot be efficiently mapped into large kernels
    as the different threads will likely diverge in the execution path. This is because
    each thread typically simulates a different scenario in the environment. Thus
    the application is instead programmed as a large number of short-running kernels.
    This phenomenon has also been observed by recent works [39], [40].


    ![](_page_2_Figure_3.jpeg)


    Fig. 2: Simulation engines: Achieved occupancy.


    ![](_page_2_Figure_5.jpeg)


    **0-9 10-49 50-199 200+** Fig. 5: Kernel size distribution for the ant environment
    power budgets as direct deployment of large neural network architectures on these
    devices leads to high-inference times. Automated DNN architecture design (neural
    architecture search) is a promising approach to generate faster neural network
    architectures while retaining or improving accuracy [41]‚Äì[44]. These optimized
    architectures tend to have irregular elaborate connections between convolution
    operations. Fig. 6a depicts an example DNN with irregular structure. Additionally,
    an emerging trend in recent research [29] shows that *dynamic inference models*
    [6]‚Äì[8], [10], [13]‚Äì[19], [45]‚Äì[48] are very promising to significantly reduce
    inference latency and FLOPs. With these dynamic inference models, the path of
    execution through the network is determined by the *input*. Thus, the computational
    graph is not known ahead of time. For example, Fig. 6b shows an example CNN model
    with different paths of execution based on the input [10].


    Similar to ¬ß II-B, we evaluate the efficiency of these workloads on a GPU (an
    NVIDIA RTX 3060 and an NVIDIA RTX 4090) and depict the resulting utilization in
    Fig. 7 (evaluation and workload settings are in ¬ß V). We find that the total achieved
    occupancy is around 39% in the InstaNAS-A [10] workload for both GPUs. Similar
    to the simulation engines, we root cause this underutilization to the existence
    of a large number of small kernels, as depicted in Fig. 8, where a large fraction
    of the kernels have fewer than 200 CTAs. Thus, these small kernels are unable
    to fully utilize the GPU. In these workloads, the small kernels are due to convolution
    layers that were optimized for fewer FLOPs with smaller filters. avg 3x3 max 3x3


    ![](_page_2_Figure_8.jpeg)


    # *C. Case Study 2: DNNs with dynamic irregular graphs*


    Recent research has extensively investigated specialized DNNs for edge devices
    with limited compute resources and


    #### *D. Key Observations*


    While small-sized kernels lead to underutilization, we observe that there are
    typically many kernels that can be executed *concurrently*. Thus we can improve
    GPU utilization and reduce


    ![](_page_3_Figure_0.jpeg)


    Fig. 8: Kernel size distribution (in CTAs) for InstaNAS-A [10]


    runtimes by identifying independent kernels and scheduling them for concurrent
    execution. However, this is a challenging task for these classes of applications
    for the following reasons.


    (1) Input-dependent kernel dependencies. The computational graph, and hence, the
    dependencies between kernels are only determined at *runtime* for each input.
    For example, with the instance-aware dynamic DNNs [6]‚Äì[8], [10] described in ¬ß
    II-C, for the classification inference task, the computational graph is different
    for each image. As a result, the determination of kernel dependencies and scheduling
    of kernels for the entire computational graph needs to be done for *each input*.
    This adds significant latencies to the runtime.


    CUDA Graphs [32] and AMD ATMI [33] are software frameworks that allow developers
    to specify dependencies between different kernels as edges of a directed acyclic
    graph (DAG). The challenge with this approach is that the DAG needs to be constructed
    in full (with dependencies, kernel launches, and barriers determined) before the
    application is executed on the GPU, *for each input*. This process adds high latency
    in compiling the complete dependency information. We perform an experiment to
    measure the DAG construction and launch time on Brax [1] simulation engine (¬ß
    V) compared to the program execution time, shown in Fig. 9. We observe that the
    time taken to construct the graph is exceedingly high (average of 47% of overall
    execution time).


    ![](_page_3_Figure_5.jpeg)


    Fig. 9: DAG construction time as % of execution time


    Similarly, recent works for DNNs [50]‚Äì[52] perform kernel scheduling, fusion,
    or parallelization for better GPU utilization. These works, for example, partition
    the computational graph into independent sub-graphs that are scheduled into multiple
    streams. However, this scheduling and partitioning is too time-consuming to be
    done for each input at runtime and thus cannot be applied to these classes of
    workloads.


    (2) Irregular kernel dependencies. These classes of applications have *irregular*
    computational graphs that are challenging to easily partition into CUDA streams
    (¬ß II-C). Popular deep learning frameworks [53], [54] use a single stream by default.
    The stream abstraction works best if the entire graph can be partitioned into
    independent streams of kernels. However, these graphs with irregular dependencies
    would require finegrained scheduling and heavy use of synchronization (e.g., cudaDeviceSynchronize
    and cudaStreamSynchronize) when parallelizing using CUDA streams. This synchronization
    may lead to large overheads as it requires communication between the GPU and CPU.
    Fig. 10 depicts the different overheads when CUDA streams are used for fine-grained
    scheduling with irregular graphs: kernel launch overheads 1 , CPU execution overheads
    2 and the synchronization overheads 3 . Based on our profiling, the synchronization
    and launch overheads vary between 5-20us. CUDA Graphs [32] and ATMI [33] can eliminate
    the synchronization and kernel launch overhead. However, for input-dependent graphs,
    as demonstrated in (1), this benefit is lost due to DAG construction overheads.


    ![](_page_3_Figure_10.jpeg)


    Fig. 10: Kernel launch and synchronization overheads


    # III. APPROACH


    Our goal in this work is to design a framework that enables efficient concurrent
    execution of GPU kernels *(i)* whose computational graph may only be known at
    runtime, *(ii)* without incurring significant synchronization overheads. To this
    end, we introduce ACS, a new framework that concurrently schedules independent
    kernels with a lightweight runtime mechanism.


    #### *A. Prior Mechanisms*


    We consider the baseline GPU architecture as described in ¬ß II-A. The GPU runtime
    can launch kernels into different streams. These streams are mapped to one of
    the command queues in the device-mapped memory of the GPU. The command processor
    schedules kernels at the head of these queues concurrently, thus enabling concurrent
    kernel execution. However, neither the command processor nor the kernel launch
    packets in the command queues have information on inter-kernel data dependencies.
    Kernels in different queues are assumed to be independent of each other and all
    kernels within the same queue are executed in order. Hence, in order to leverage
    parallelism in kernel executions, the task of checking inter-kernel dependencies
    and determining the kernels which can execute concurrently (and thus scheduling
    into different queues) *has to be done by the host application.* However, this
    is a problem, as this adds significant dependency-checking/scheduling latency
    to the run time. It also requires communication with the host (through a synchronization
    routine) to be performed each time a kernel completes execution, adding to the
    overhead. Several prior works describe approaches to efficiently schedule kernels
    into multiple streams. Fig. 11 depicts approaches to scheduling a computational
    graph (Fig. 11a). Fig. 11b is the baseline approach used by many existing frameworks
    [53], [54], where a single CUDA stream is used to execute all kernels serially.
    This approach leads to underutilization (¬ß II-C). Fig. 11c shows prior works [50],
    [51] that use the computational graph to identify independent kernels and the
    *entire graph* is scheduled ahead of time into multiple CUDA streams. However,
    this fine-grained scheduling and synchronization leads to large overheads.


    ![](_page_4_Figure_1.jpeg)


    ![](_page_4_Figure_2.jpeg)


    One way to avoid using a device-level synchronization (like cudaDeviceSynchronize)
    and enable asynchronous execution of kernels without communication with the CPU
    is to use events provided by the CUDA stream management API. Events serve as signaling
    mechanisms to indicate the occurrence of specific operations in a stream. This
    allows synchronization between kernels across streams through the cudaStreamWaitEvent
    API, facilitating asynchronous kernel execution without blocking the host. By
    strategically placing events and using cudaStreamWaitEvent, it is possible to
    orchestrate the order in which kernels are executed on the GPU without communication
    with host. However, this approach still requires deriving dependencies between
    all kernels beforehand, and thus incurs significant scheduling overhead.


    Another set of approaches [51], [52], [55], define static dependencies between
    kernels as a DAG, which is then scheduled with DAG frameworks (CUDA Graph [32]/ATMI
    [33]). These approaches cannot be applied to input-dependent computation graphs,
    as constructing the entire computational graph is too time-consuming to be done
    at runtime. To convey the DAG information, ATMI sends barrier packets [56] along
    with kernel launch packets to the command queue. A barrier packet [57] is a 64-byte
    data packet that contains id information about a kernel and a set of kernels that
    depend on it. This packet can be inserted into the command queue by the device
    runtime. The barrier packet blocks the launching of dependent kernels until the
    independent kernel completes execution. The barrier packet however does not contain
    any information regarding the current status of the executing kernels in the GPU
    and thus cannot perform any additional runtime reordering of kernels. It simply
    follows the dependencies already specified by the DAG. While it is possible to
    devise a framework that dynamically launches barrier packets and launch commands
    onto the GPU command queue in memory, this would require hardware support and
    would still incur synchronization overheads with the CPU. Our approach is specifically
    designed to mitigate this scheduling cost by avoiding direct communication from
    the GPU to the CPU, thereby reducing potential overheads.


    Persistent threads (PT) eliminate the scheduling and launch overheads but are
    only effective when all kernels are homogeneous [58] . CUDA dynamic parallelism
    [59] (CDP) or AMD''s device enqueue [60] (DE) enables parent kernels to launch
    child kernels, only allowing data dependencies between one parent and its children.
    These workloads however involve kernels that depend on multiple kernels, and it
    is an open problem how to use CDP for these types of dependencies.


    K6 K5 K4 K3 K2 K1 N We summarize different approaches for parallel kernel scheduling
    in Table I, in terms of applicability (whether inputdependent irregular workloads
    can be effectively mapped), synchronization/launch overheads and preparation overhead
    (resolving dependencies, constructing, and scheduling the computational graph).


    | Method                  |   | Applicability Sync+Launch Preparation<br>Overhead
    | Overhead |

    |-------------------------|---|---------------------------------------------------|----------|

    | Multi-Stream [50], [51] | ‚úì | x                                                 |
    ‚úì        |

    | DAG [32], [33], [52]    | ‚úì | ‚úì                                                 |
    x        |

    | PT [58], [61], [62]     | x | ‚úì                                                 |
    ‚úì        |

    | CDP [59] / DE [60]      | x | x                                                 |
    ‚úì        |

    | ACS-SW (Our approach)   | ‚úì | x                                                 |
    ‚úì        |

    | ACS-HW (Our approach)   | ‚úì | ‚úì                                                 |
    ‚úì        |


    TABLE I: Comparison of ACS to other scheduling frameworks


    #### *B. Key Idea of ACS*


    With ACS, the key idea is to instead perform the dependence checking and scheduling
    within a small window of kernels at *runtime* similar to out-of-order instruction
    scheduling. We perform this scheduling over a single command queue (or a single
    initialized stream). Fig. 12a depicts out-of-order kernel dispatch with ACS. Fig.
    12b shows the corresponding high-level hardware modifications for ACS. A fixed
    number of kernels in the original stream (scheduling window 1 ) are evaluated
    for dependencies. When a kernel completes execution, we evaluate which kernels
    within the scheduling window are now ready for execution 2 . All such kernels
    are marked ready and can be scheduled concurrently.


    ![](_page_4_Figure_12.jpeg)


    (a) Out-of-order kernel dispatch from the scheduling window (b) CP scheduling
    kernels in out of order manner


    Fig. 12: ACS: Runtime out-of-order kernel scheduling


    We propose two implementations of ACS: ACS-SW, a SWonly approach and ACS-HW, a
    hardware-software cooperative mechanism, which we describe in the following sections.
    ACS-SW emulates the out-of-order kernel scheduling mechanism by scheduling independent
    kernels into multiple streams and can be implemented with purely software changes,
    however the hardware support in ACS-HW is more efficient as it also alleviates
    synchronization overheads.


    #### *C. Design Overview*


    To design ACS to perform the runtime kernel scheduling as depicted in Fig. 12a,
    we need *(i)* a mechanism to determine inter-kernel dependencies in the scheduling
    window; *(ii)* to identify kernels that are ready for execution; and *(iii)* alleviate
    synchronization and kernel launch overheads.


    Determining inter-kernel dependencies. In order to determine dependencies between
    kernels, the application adds additional metadata to each kernel invocation. This
    metadata defines the range of global memory addresses that are written to and
    read from by each kernel. This metadata is provided to ACS by using a kernel wrapper
    (described in ¬ß IV-B) and can be defined by the programmer, library-writer, or
    compilation tools. By checking for overlaps between read segments and write segments,
    we determine dependencies between kernels. The kernel wrapper defines the pointers
    to the read and write data segments (start\_addr) along with the size of the segments
    (Fig. 13). The actual virtual addresses associated with the pointers are resolved
    just before kernel launch in order to perform the dependence checks (¬ß IV-A).
    We refer to these memory ranges as read\_segments and write\_segments.


    ![](_page_5_Figure_3.jpeg)


    Fig. 13: Memory regions written to/accessed by the kernel


    Tracking kernel state at runtime. Fig. 14 depicts the scheduling window ( 1 ),
    with the additional state required for scheduling. The kernels in the window can
    be ready, pending, or executing ( 3 ). Kernels in the scheduling window become
    ready for launch (ready) when the kernels it is dependent on (referred to as *upstream*
    kernels 2 ) complete execution. For each kernel in the scheduling window, we track
    a list of the corresponding upstream kernels. The upstream kernels are determined
    using the above dependency checks when inserting into the scheduling window. When
    the upstream list is empty, the kernel is marked ready for execution. After each
    kernel completes execution, the upstream list is updated for all kernels in the
    scheduling window. For ACS-SW, these checks are performed in the software runtime
    system (¬ß IV-B), and for ACS-HW, we implement them in hardware (¬ß IV-C).


    ![](_page_5_Figure_6.jpeg)


    Fig. 14: Kernels in the scheduling window with their state and corresponding upstream
    kernels (i.e., dependencies)


    Eliminating CPU synchronization overheads. In order to eliminate synchronization
    and kernel launch overheads resulting from communication between the CPU and GPU,
    we implement the scheduling window in the GPU hardware in ACS-HW. We design an
    efficient implementation of ACS-HW that reduces communication with the CPU. The
    management of the scheduling window is done entirely in hardware, including the
    determination of ready kernels. Similarly, once a kernel completes execution,
    the scheduling window is updated without requiring synchronization with the CPU.


    #### *D. Mechanism Walkthrough*


    start\_addr size Fig. 15 depicts a high level walkthrough of ACS. For each GPU
    kernel invoked by the application 1 , the read and write segments are resolved
    (detailed in ¬ß IV-A). All invoked kernels along with the corresponding read/write
    segments are entered into the input FIFO queue to await scheduling 2 . Kernels
    are then added to the fixed size scheduling window in a FIFO manner 3 . When the
    kernel enters the scheduling window 4 , the write segments of the current kernel
    are compared against read and write segments of all kernels in the scheduling
    window. The kernels with overlap are added to the corresponding upstream kernel
    list and are marked pending. When an executing kernel completes execution, all
    corresponding upstream kernel lists are updated. Any kernel that has an empty
    list is marked ready for the scheduler to launch. input FIFO queue <sup>2</sup>


    ![](_page_5_Figure_12.jpeg)


    Fig. 15: High level overview of ACS


    #### IV. DETAILED DESIGN


    #### *A. ACS Kernel Wrappers*


    In order to perform runtime dependency checks, the application defines the read/write
    segments for each kernel. These segments are defined using a kernel wrapper, ACS\_wrapper
    (defined in Fig. 16). Since virtual addresses can only be resolved at runtime,
    the programmer instead defines a function get\_addresses which populates the \_\_read\_segments\_\_
    and \_\_write\_segments\_\_ lists (lines 6 and 7 in Fig. 16). The get\_addresses
    function takes the kernel''s launch arguments as the input arguments (lines 12
    to 15). These arguments are then used to compute the read/write segments.


    scheduling window 1 K2 none ready K1 none executing Just before kernel launch,
    the CUDA runtime calls the get\_addresses function. At this point, the \_\_read\_segments\_\_
    and \_\_write\_segments\_\_ lists are populated with the resolved virtual addresses.
    In our implementation of ACS-SW, since the CUDA drivers are closed-source, we
    implement an intermediate user-level kernel launch function that calls the get\_addresses
    function instead. Fig. 17 depicts an example implementation of the get\_addresses
    function. ACS assumes that the programmer or the kernel library provider has knowledge
    of the memory regions accessed by the kernel from the kernel function prototype.
    For a wide range of commonly used kernels, such as matrix multiplication, convolution,
    addition, etc., which operate on data stored as contiguous regions in memory,
    this task is straightforward. Additionally, the get\_address function can be obtained
    using a static binary analysis tool like GPUOcelot [63]. However, in situations
    where it is not possible to determine the range of memory accessed by the kernel
    (for example, indirect memory accesses), our approach assumes that the entire
    GPU memory may be accessed by the kernel.


    ```

    1 struct ACE_wrapper {

    2 //list of read,write segments defined as

    3 //[{start_adr1,size1},{start_adr2,size2}..]

    4 list __read_segments__;

    5 list __write_segments__;

    6 // function which gets called at kernel

    7 // launch to populate read,write segments

    8 void get_addresses(

    9 dim3 blocks, dim3 threads, ...

    10 );

    11 // function declaration of the kernel

    12 static __global__ void kernel(...);

    13 };

    ```

    Fig. 16: The ACS\_wrapper definition


    ```

    1 // get address function for matrix multiply

    2 // input matrices: input1 (mxn), input2(nxk)

    3 // output matrix: output(mxk)

    4 void ACE_wrapper::get_addresses(

    5 dim3 blocks, dim3 threads,

    6 int* input1, int* input2, int* output1,

    7 int m, int n, int k) {

    8 // input1 reads m*n elements

    9 // input2 reads n*k elements

    10 __read_segments__ = {

    11 {(void*)input1, m*n*sizeof(int)},

    12 {(void*)input2, n*k*sizeof(int)}

    13 };

    14 // output reads m*k elements

    15 __write_segments__ = {

    16 {(void*)output, m*k*sizeof(int)},

    17 };

    18 }

    ```

    Fig. 17: Example: get\_addresses function


    ### *B. ACS-SW Design*


    ACS-SW is implemented as a user-level runtime that is called by the application.
    The functionalities of ACS-SW are performed by multiple independent threads that
    are launched simultaneously. The ACS-SW runtime performs two major tasks: *(i)*
    implementing and maintaining the scheduling window (window module); and *(ii)*
    scheduling kernels ready for execution (scheduling module).


    *1) The window module:* The window module is implemented as a separate thread
    that manages the input FIFO queue and the scheduling window. All the functionalities
    of the scheduling window, dependency tracking, and state management are performed
    in software within this module. This module is called in two ways: First, when
    a kernel is invoked by the application thread, this module is called and the kernel
    is inserted into the input queue. Second, the scheduler module (implemented as
    a separate thread(s)) calls the window module when a kernel completes execution.
    At this point, the state of upstream lists is updated and the kernel is removed
    from the scheduling window. The window module constantly polls the input queue
    and the scheduling window. When there is a vacancy in the scheduling window and
    a pending kernel in the input queue, the kernel is moved into the scheduling window.
    At this point, the window module performs the necessary dependency checks and
    bookkeeping. Algorithm 1 describes how the dependency check is performed.


    |  |  | Algorithm 1 Dependency check algorithm |  |  |

    |--|--|----------------------------------------|--|--|

    |--|--|----------------------------------------|--|--|


    | Input: rslist1, wslist1, wslist2<br>‚ñ∑ RW segments of scheduling window   |

    |--------------------------------------------------------------------------|

    | kernel, w-segment of kernel in inputFIFO                                 |

    | Output: is dependent<br>‚ñ∑                                                |

    | 1: is dependent = false<br>‚ñ∑ initial state of is dependent               |

    | S<br>2: rwslist1 ‚Üê wslist1<br>rslist1<br>‚ñ∑ Read+Write segments           |

    | 3: for each segment1 in rwslist1 do<br>‚ñ∑ Test for every pair of segments |

    | for each ws2 in wslist2 do<br>4:                                         |

    | ‚ñ∑ get start and end virtual memory addresses                             |

    | start1 ‚Üê segment1.start<br>5:                                            |

    | end1 ‚Üê segment1.start + segment1.size<br>6:                              |

    | start2 ‚Üê ws2.start<br>7:                                                 |

    | 8:<br>end2 ‚Üê ws2.start + ws2.size                                        |

    | ‚ñ∑ check overlaps between start and end addresses                         |

    | 9:<br>if start1 < end2 and end1 > start2 then                            |

    | is dependent = true<br>‚ñ∑<br>10:                                          |

    | 11:<br>end if                                                            |

    | 12:<br>end for each                                                      |

    | 13: end for each                                                         |


    *2) The scheduler module:* This module schedules and launches ready kernels for
    execution. This module is implemented as a configurable fixed number of threads,
    each of which launches kernels into an independent CUDA stream for concurrent
    execution, as depicted in Fig. 18. Each stream contains only one kernel at any
    given time. Threads with empty streams poll the scheduling window for a ready
    kernel 1 , which is then launched in its CUDA stream 2 . The thread then waits
    for the kernel to complete execution using the StreamSync primitive 3 . Once the
    kernel completes execution, the thread calls the window module as described above.
    This algorithm is described in Algorithm 2. Thread 1 Thread 2


    ![](_page_6_Figure_12.jpeg)


    Fig. 18: ACS-SW: The scheduler module


    |    | Algorithm 2 The scheduler module in software |                                      |

    |----|----------------------------------------------|--------------------------------------|

    |    | Input: SchedulingWindow SW, stream id        |                                      |

    |    | 1: while notstop() do                        | ‚ñ∑ poll for kernels until
    stop signal |

    | 2: | ACQUIRE LOCK(SW)                             |                                      |

    | 3: | if SW.ready.exists( )then                    | ‚ñ∑ check ready kernels                |

    | 4: | kernel ‚Üê SW.ready.pop()                      | ‚ñ∑ get ready kernel                   |

    | 5: | end if                                       |                                      |

    | 6: | RELEASE LOCK(SW)                             |                                      |

    | 7: | LAUNCH(kernel, stream id)                    | ‚ñ∑ launch kernel                      |

    | 8: | STREAM SYNC(stream id)                       | ‚ñ∑ wait for completion                |

    |    | 9: end while                                 |                                      |


    #### *C. ACS-HW Design*


    While ACS-SW enables concurrent execution of kernels and can be fully realized
    in software, it still incurs overheads from *(i)* synchronization with the CPU
    when a kernel completes execution, i.e., the StreamSync primitive that blocks
    the scheduler module thread; and *(ii)* the kernel launch overhead when the scheduler
    module launches a kernel in the CPU. ACS-HW is designed to alleviate these overheads
    with hardware support for kernel scheduling in the GPU.


    Fig. 19 depicts an overview of ACS-HW. ACS-HW comprises a software runtime system
    similar to ACS-SW that maintains an input FIFO queue containing the kernels that
    were invoked by the application 1 . The scheduling window and its management are
    however implemented in hardware on the GPU side 2 . The input queue is essentially
    implemented as a CUDA stream that dispatches kernels to the GPU. In addition to
    the input FIFO queue, the software runtime also maintains a list of kernels in
    the GPU''s scheduling window, which we call the scheduled\_list 3 . To avoid frequent
    synchronization between the CPU and GPU, we allow this list to be stale. Before
    a kernel is inserted into the scheduling window, the software runtime performs
    dependency checks with the scheduled\_list to determine the upstream kernels.
    Note that since the scheduled\_list may be stale, this upstream list needs to
    be further updated before insertion into the scheduling window (discussed below).


    ![](_page_7_Figure_2.jpeg)


    Fig. 19: ACS-HW: Design overview


    The hardware component 4 consists of two modules: *(i)* the scheduling window
    and *(ii)* the upstream load module.


    The hardware scheduling window structure is depicted in Fig. 20 and comprises
    a fixed number of slots (N) 1 . Each slot contains an 8-bit kernel identifier
    and (N-1) 8-bit upstream kernel identifiers that are implemented with SRAM 2 .
    Each slot of the SRAM module is implemented as a single bank of SRAM, contaning
    N-1 fully associated units to store upstream kernel identifiers. These upstream
    identifiers are used to determine when a kernel is ready. An additional two bits
    are used to identify the state of each kernel (i.e., ready, pending, and executing).
    When a kernel completes execution, the upstream identifiers are updated and the
    corresponding state of each kernel is updated. The completed kernel is also removed
    from the scheduling window. Any kernels that are now ready are then dispatched
    to the GPU''s kernel dispatch unit for execution 3 .


    The upstream load module is responsible for refining the upstream list provided
    by the CPU which may be stale in two ways. It may contain kernels that have (1)
    already completed execution and (2) may miss long-running kernels that are still
    executing. The first case is handled by the upstream module by checking against
    a list of kernels in the scheduling window 4 . The second case is avoided by ensuring
    that the scheduled\_list (of size M) in the CPU never misses kernels that are
    still executing. The upstream load module


    ![](_page_7_Figure_7.jpeg)


    Fig. 20: HW scheduling window and upstream load module tracks the oldest scheduled
    kernel 5 . If the number of newer kernels exceeds M (size of the scheduled\_list),
    this module blocks the insertion of more kernels from the CPU 6 .


    #### *D. ACS Overheads*


    (1) Hardware area overhead. ACS-HW introduces the hardware scheduling window which
    contains N slots, where N is the size of the scheduling window. Each slot contains
    N kernel ids of upstream data of 8 bytes each and 2 bits for status. Assuming
    a scheduling window of length N = 32, we require 1KB of SRAM for the scheduling
    module (for the entire GPU). The upstream module keeps track of the oldest executing
    kernel with an 8-bit


    GPU K1 K3 window <sup>2</sup> 4 load module (2) Storage overheads. The read and
    write segments that are saved as metadata in the input FIFO and the scheduled\_list
    by the software runtime in the CPU require memory storage. Each read and write
    segment requires 48 bits to hold the start addresses and the size.


    > (3) Mechanism latencies. ACS-HW requires updating all upstream kernels in each
    slot of the scheduling window every time a kernel completes execution. ACS-HW
    updates each slot in N-1 cycles (where N is the size of the scheduling window).
    Additionally, ACS-HW requires N cycles to insert a kernel ID with its upstream
    kernel IDs into the scheduling window. For a scheduling window of size 64, this
    operation adds 64 cycles (about 50-100ns) overhead to dispatch a ready kernel
    for launch. Thus, ACS-HW adds negligible runtime to the application compared to
    the baseline kernel launch overhead (in the order of a few microseconds).


    > (4) Dependency checking overheads To determine the list of upstream kernels,
    the CPU checks for overlaps between the write segments of the kernel in the input
    queue and the read-write segments of the kernels in the scheduled\_list. As the
    scheduled\_list can fit completely into the cache (4KB), dependency-checking is
    compute-bound and dependent on the number of read and write segments. Table II
    presents the time required to do dependence checking. For a processor with P execution
    units, effective utilization requires dependency checks to be performed in no
    more than T /P, where T is the task execution time [34], [36]. We estimate T /P
    to be around 4us, which is much more than the dependency check latency.


    #### V. METHODOLOGY


    We evaluate ACS-SW on a real hardware setup with an Intel Core i7 11700K CPU (Table
    III) and an NVIDIA RTX3060


    | Window<br>size | Number of<br>RW-segments | Dependency<br>check time |

    |----------------|--------------------------|--------------------------|

    | 16             | 6<br>10                  | 410ns<br>700ns           |

    | 32             | 6<br>10                  | 510ns<br>1640ns          |


    TABLE II: Dependency checking overhead analysis


    GPU (Table IV). We model ACS-HW on GPUs using the Accel-Sim simulator [64], configured
    with parameters of RTX3070 (Table V). We use AccelWattch [64] to model GPU power.
    We choose a scheduling window size of 32.


    | CPU 3.6GHz, OOO 4-wide dispatch window, 32 entry LSQ    |

    |---------------------------------------------------------|

    | L1D + L1I Cache 32KB, 4 way LRU, 1 cycle; 64 Byte line; |

    | L2 Cache 256KB, 8 way LRU, 4 cycle; 64 Byte line;       |

    | L3 Cache 1MB, 16 way LRU, 20 cycle; 64 Byte line;       |

    | DRAM 2-channel; 16-bank; open-row policy, 4GB DDR4      |

    | TABLE III: CPU system configuration                     |


    | Shader core 28 SMs, 1.3GHz; 2 schedulers per SM             |

    |-------------------------------------------------------------|

    | SM Resources 32768 Registers, 32KB Shared memory, 128KB L1D |

    | DRAM 2-channel; 16-bank; open-row policy, 12GB DDR4         |

    | TABLE IV: GPU system configuration                          |


    | Shader core 46 SMs, 1.4GHz; 4 schedulers per SM             |

    |-------------------------------------------------------------|

    | SM Resources 32768 Registers, 32KB Shared memory, 128KB L1D |

    | DRAM 2-channel; 16-bank; open-row policy, 16GB DDR4         |

    | TABLE V: Simulated GPU configuration                        |


    Workloads. We evaluate ACS using:


    (1) Deep RL physics simulations. Brax [1] is a GPU accelerated simulation engine
    for control tasks in reinforcement learning. We evaluate ACS with the Ant (ant),
    Grasp (grasp), Humanoid (human), Cheetah (ct), and Walker2d (w2d) simulation environments.
    These environments are Mu-JoCo [65] simulations for training RL agents to perform
    a specific task. For example, ant contains a 3d robot (the agent) with one torso
    and 4 legs, each with a knee joint, and the goal is to move in a particular direction
    by controlling its legs.


    (2) Dynamic DNNs. We evaluate our approach for 3 dynamic DNN workloads: InstaNAS
    [10] (I-NAS) is a dynamic CNN for image classification. We evaluate our approach
    using the InstaNAS-A architecture on the CIFAR10 dataset. Dynamic routing [12]
    (DR) is a DNN trained for semantic segmentation of images. We evaluate our approach
    on the Dynamic-A 16 layer architecture using the Cityscapes dataset [66]. Conditional
    Convolution [46] (CC) is a mixtureof-experts CNN model for image classification
    where the weights of the convolutions are computed at runtime. We evaluate the
    version of Conditional Convolution with 4 experts that uses an efficientnet b4
    [67] network as the backbone. All three dynamic DNNs are designed for a batch
    size of 1 and the input image defines the DNN architecture. We use Pytorch [54]
    implementations.


    (3) Static DNNs. CNN architectures optimized for low inference latency using neural
    architecture search (NAS): NASNet [41] (NASNet), AmoebaNet [42] (Amoeba), SqueezeNet
    [68] (Squeeze), and RandomWire [44] (RW). These CNNs have highly irregular structures
    with many small kernels. We evaluate ACS with a batch size of 1 on CIFAR10.


    #### VI. EVALUATION


    We evaluate ACS using three designs: *(i)* Baseline: cuDNN implementation (for
    DNNs) and a jax implementation [1] (for deep RL simulation), both using CUDA streams.
    *(ii)* ACS-SW: Our software-only mechanism is evaluated on real hardware. *(iii)*
    ACS-SW-Sim: Our software-only mechanism evaluated on the GPU simulator. We also
    include these results to compare against ACS-HW. *(iv)* ACS-HW: Our hardware-software
    cooperative mechanism evaluated on the GPU simulator. *(v)* CUDAGraph: Framework
    where the inter-kernel dependencies are prepared on the CPU as a directed acyclic
    graph and sent to the GPU ahead of time. We only present ACS-SW results for the
    deep RL workloads as the dynamic and static DNNs heavily use CuDNN libraries that
    do not currently allow modifications to make use of different CUDA streams. We
    instead model the same effect with ACS-SW-Sim.


    #### *A. Deep RL Physics Simulations*


    Fig. 21 depicts the runtimes for the generation of a single batch of training
    data from different simulation environments using ACS-SW, normalized to the baseline
    approach.


    ![](_page_8_Figure_14.jpeg)


    **ant human ct w2d grasp gmean** Fig. 21: Deep RL physics simulations: Normalized
    Speedup


    Fig. 22 depicts the runtimes for ACS-SW-Sim and ACS-HW normalized to the baseline
    implementation. We make two observations. First, ACS-SW-Sim provides similar speedups
    as in real hardware compared to the baseline implementation (up to 1.79√ó and 1.66√ó
    on average). Second, ACS-HW is able to further improve performance compared to
    the software-only approach by alleviating the synchronization and kernel launch
    overheads. We observe a slowdown with CUDAGraph due to the significant latency
    of constructing the kernel dependency graph and sending the information to the
    GPU.


    ![](_page_8_Figure_17.jpeg)


    Fig. 22: Deep RL physics simulations: Normalized speedup


    The end-to-end speedup in training tasks (simulation + learning algorithm) as
    observed is shown in Fig. 23. We observe a mean speedup of 1.42√ó on ACS-HW, and
    1.30√ó on ACS-SW.


    ![](_page_8_Figure_20.jpeg)


    In Fig. 24, we depict the achieved occupancy for the three configurations. Achieved
    occupancy is calculated as the number of active warps divided by the maximum number
    of active warps supported by the GPU averaged over all clock cycles. We observe
    that the ACS is able to significantly increase the achieved occupancy and thus
    the utilization.


    ![](_page_9_Figure_1.jpeg)


    Fig. 24: Deep RL physics simulations: Achieved occupancy


    #### *B. Inference on Dynamic DNNs*


    Fig. 25 depicts speedup over the baseline for the dynamic DNNs described in ¬ß
    V. We observe that ACS is able to provide speedups of up to 1.39√ó on dynamic DNN
    workloads with ACS-HW and on average 1.05√ó with ACS-SW and 1.3√ó with ACS-HW. I-NAS
    suffers a slowdown with ACS-SW because this workload has significant kernel launch
    overheads when parallelized but are hidden in the baseline case where the kernels
    are simply launched serially into a single stream without synchronization. We
    observe that CUDAGraph exhibits a significant slowdown due to the overhead incurred
    during the construction and communication of the DAG dependencies.


    Fig. 26 depicts the corresponding achieved occupancy. We find that the ACS configurations
    are able to significantly improve utilization, leading to performance improvements.


    ![](_page_9_Figure_6.jpeg)


    *C. Inference on Static DNNs*


    While our approach is designed for applications with dynamic computational graphs,
    we also evaluate its effectiveness in improving the concurrency of static DNNs.
    We depict the speedups obtained normalized to the baseline in Fig. 27. We observe
    an average speedup of 1.31√ó with ACS-HW, and a speedup of 1.16√ó with ACS-SW. Fig.
    28 depicts the corresponding achieved occupancy. We find that ACS leads to higher
    GPU utilization, leading to performance improvements. As expected, we observe
    that CUDAGraph exhibits similar execution times as ACS-HW for static graphs. This
    is because the task graph needs to be constructed only once.


    #### *D. Sensitivity Analysis*


    Fig. 29 compares the speedups obtained on using scheduling window sizes of 16
    and 32 for ACS-HW over baseline. We observe that the Brax simulations have higher
    performance


    ![](_page_9_Figure_11.jpeg)


    (4.5% on average) with a window size of 32 compared to 16. However, the window
    size has less of an impact on the DNNs. This is because the simulation engines
    have more inter-kernel parallelism that is exposed with a larger scheduling window.


    ![](_page_9_Figure_13.jpeg)


    *E. Comparison with Persistent Thread Frameworks*


    Persistent threads (PT) [58], [61], [69], [70] are used to efficiently schedule
    multiple tasks with dynamically determined dependencies. These tasks are executed
    using threads of a *single kernel*. Thus, it assumes all tasks are *homogeneous*,
    requiring the same number of registers and shared memory. PT frameworks which
    allow heterogeneous kernels are nontrivial and would be inefficient as the persistent
    kernel must be configured to use the maximum registers/scratchpad used by any
    kernel [58]. We use the persistent thread framework implementation from juggler
    [61] and adapted it to handle heterogeneous kernels. We were only able to implement
    a section of a rigid body simulator (used for finding contacts between pairs of
    rigid bodies). This routine invokes a different kernel (with different register
    usages) for different pairs of geometries. We implement these kernels as tasks
    of our PT framework and find that it is 1.35√ó slower than baseline. This slowdown
    is due to inefficient use of registers/scratchpad by the kernel that leads to
    lower parallelism.


    #### VII. RELATED WORK


    In this work, we (i) observe that input-dependent interkernel dependencies and
    small kernels are a significant performance bottleneck in a range of important
    applications such as simulation engines in deep RL and dynamic neural networks;
    and (ii) propose both a software-only and hardware-software cooperative mechanism
    to enable concurrent execution of kernels with statically unknown inter-kernel
    dependencies. In this section, we describe prior work that aim to improve GPU
    utilization and kernel concurrency.


    Leveraging concurrent streams in DL workloads. Mainstream deep learning frameworks
    like Tensorflow [53] and Pytorch [54] launch GPU kernels into a single CUDA stream
    that executes them sequentially. Recent works [50], [52], [71] propose software
    techniques to enable concurrent execution of GPU kernels using multiple streams
    with static scheduling and stream assignment before application execution. Inter-operator
    scheduling [50] partitions a computation graph into sections of kernels that can
    execute in parallel. Out-of-order backprop [52] observes that gradient computation
    can be parallelized using CUDA streams into weight gradients and the output gradient
    computation during backpropagation. However, these works are only applicable to
    DL workloads whose computation graph is static and known ahead of time, often
    requiring significant compilation times. Furthermore, these approaches incur high
    synchronization overheads.


    Task-based programming frameworks in CPUs. Taskbased frameworks [72]‚Äì[74] enable
    programmers to describe a program as multiple tasks which are scheduled for execution
    in multiprocessor architectures [75]. Works such as task superscalar [34], carbon
    [35], TDM [36] and ADM [37] propose out-of-order scheduling of tasks to efficiently
    leverage irregular parallelism in multiprocessors. The major bottleneck in outof-order
    scheduling of tasks dynamically for multiprocessors is the long latency required
    to do dependence checks. Thus, prior work [34]‚Äì[37] propose hardware accelerators
    to address the long latency dependence checking needed at runtime. However, with
    GPUs, the primary bottleneck is the long latency to launch/signal completion of
    kernels instead, requiring a different approach to enable out-of-order scheduling.


    Programmer annotations Prior works leverage programmer-specified annotations as
    hints to the compiler to extract parallelism. DeNovo [76] uses programmer annotations
    that encode the data read and written to by each method/function. This information
    is used at compile time to determine independent tasks that can be scheduled.
    Some frameworks [74], [77]‚Äì[80] allow programmers to annotate the array regions
    accessed by each task as a compile time directive. In ACS, we use a similar approach
    of programmer annotations to help determine parallelism at runtime to enable out-of-order
    kernel scheduling.


    Software techniques to improve GPU utilization with concurrent kernel execution.
    CUDA Graphs [32] and AMD ATMI [33], [81], [82] are frameworks that allow users
    to define dependencies between kernels as a directed-acyclicgraph (DAG) prior
    to execution. This approach eliminates synchronization and kernel launch overheads
    due to communication with the CPU. Nimble [51] identifies independent GPU kernels
    prior to execution and concurrently schedules independent kernels using CUDA streams.
    This approach uses CUDA Graphs [32] to reduce synchronization and kernel launch
    overheads. Irregular graphs are also seen in solving sparse linear equations for
    CFD simulations [83] and hyperplane sweep routines [84], where DAG frameworks
    have been shown to be effective.We quantitatively compared ACS against a CUDA
    graph implementation in ¬ß VI. None of these approaches is applicable to dynamic
    input-dependent computational graphs, as caching dependency information and constructing
    CUDA Graphs incur non-trivial latencies ( ¬ß II-D).


    Hardware support for concurrent kernels. Wireframe [85] proposes merging multiple
    kernels into a single large kernel and performs CTA scheduling with data dependency
    checks between CTAs. Blockmaestro [86] enables concurrently running kernels by
    identifying dependencies between their CTAs. These approaches however perform
    dependence checks by tracing and extracting the memory loads and stores performed
    by each thread block of every kernel. Similar to the software approaches, these
    approaches are designed for static computational graphs. The proposed scheduling
    and dependency check techniques would be too time-consuming for runtime scheduling.
    GPU dynamic parallelism [59], [87]‚Äì[89] enables launching kernels from the device
    itself and allows data dependencies between a single parent and multiple child
    kernels. However, Dynamic-NN and RL simulation workloads contain kernels that
    depend on multiple kernels, making it difficult to apply GPU dynamic parallelism.


    Compilers, runtime systems for dynamic neural networks. Prior software [11], [90]‚Äì[95]
    and hardware approaches [96] aim to optimize CPU-GPU communication overheads,
    launch overheads, and blocking synchronization calls for dynamic computational
    graphs. These approaches introduce techniques such as dynamic batching and kernel
    fusion. However, these works are orthogonal to our approach. Prior works [97],
    [98] have proposed software frameworks for CPU-GPU systems that provide simplified
    and convenient abstractions to interface with GPU runtime APIs. These frameworks
    encapsulate runtime-level code, simplifying code development for programmers in
    single and multi-GPU environments. However, these works do not specifically focus
    on input-dependent dynamic computation. Instead their goal is to provide simpler
    abstractions for programming GPU tasks and expressing dataflow dependencies between
    them. Efficient GPU sharing techniques, such as Kernelet [99], GPUPool [100] introduce
    runtime systems to enable concurrent kernel execution by scheduling kernels from
    different processes which have different memory and compute usage intensities.
    However, while these works increase overall GPU utilization by kernels launched
    from different processes, they do not leverage the parallelism between kernels
    of a single application.


    # VIII. CONCLUSION


    We introduce ACS, the first framework that enables automatic concurrent kernel
    execution with low overhead runtime scheduling and dependency checks. The key
    idea behind ACS is to dynamically schedule a small window of kernels by identifying
    which kernel(s) within the window is ready for execution. ACS leverages kernel
    annotations to automatically identify kernel dependencies at runtime. We implement
    ACS as both a software framework and a hardware-software mechanism that is able
    to further reduce synchronization overheads from CPU-GPU communication. We demonstrate
    that ACS can improve the performance of important emerging classes of workloads,
    such as RL simulations and dynamic DNNs, whose kernel dependencies are irregular
    and vary with input.


    #### REFERENCES


    - [1] C. D. Freeman, E. Frey, A. Raichuk, S. Girgin, I. Mordatch, and O. Bachem,
    "Brax - a differentiable physics engine for large scale rigid body simulation,"
    *ArXiv*, vol. abs/2106.13281, 2021.

    - [2] V. Makoviychuk, L. Wawrzyniak, Y. Guo, M. Lu, K. Storey, M. Macklin, D.
    Hoeller, N. Rudin, A. Allshire, A. Handa, and G. State, "Isaac gym: High performance
    gpu-based physics simulation for robot learning," *ArXiv*, vol. abs/2108.10470,
    2021.

    - [3] B. Shacklett, E. Wijmans, A. Petrenko, M. Savva, D. Batra, V. Koltun, and
    K. Fatahalian, "Large batch simulation for deep reinforcement learning," *ArXiv*,
    vol. abs/2103.07013, 2021.

    - [4] S. Dalton and I. Frosio, "Accelerating reinforcement learning through gpu
    atari emulation," *arXiv: Learning*, 2020.

    - [5] A. Petrenko, Z. Huang, T. Kumar, G. Sukhatme, and V. Koltun, "Sample factory:
    Egocentric 3d control from pixels at 100000 fps with asynchronous reinforcement
    learning," in *ICML*, 2020.

    - [6] K. Yuan, Q. Li, S. Guo, D. Chen, A. Zhou, F. Yu, and Z. Liu, "Differentiable
    dynamic wirings for neural networks," *2021 IEEE/CVF International Conference
    on Computer Vision (ICCV)*, pp. 317‚Äì326, 2021.

    - [7] L. Liu and J. Deng, "Dynamic deep neural networks: Optimizing accuracy-efficiency
    trade-offs by selective execution," in *AAAI*, 2018.

    - [8] Z. Yuan, B. Wu, Z. Liang, S. Zhao, W. Bi, and G. Sun, "S2dnas: Transforming
    static cnn model for dynamic inference via neural architecture search," *ArXiv*,
    vol. abs/1911.07033, 2020.

    - [9] Y. Han, G. Huang, S. Song, L. Yang, H. Wang, and Y. Wang, "Dynamic neural
    networks: A survey," *IEEE Transactions on Pattern Analysis and Machine Intelligence*,
    vol. 44, pp. 7436‚Äì7456, 2022.

    - [10] A. Cheng, C. H. Lin, D.-C. Juan, W. Wei, and M. Sun, "Instanas: Instance-aware
    neural architecture search," in *AAAI*, 2020.

    - [11] J. Wei, G. Gibson, V. Vasudevan, and E. Xing, "Dynamic scheduling for dynamic
    control flow in deep learning systems," *URL http://www. cs. cmu. edu/jinlianw/papers/dynamic
    scheduling nips18 sysml. pdf*, 2018.

    - [12] S. Cai, Y. Shu, and W. Wang, "Dynamic routing networks," *2021 IEEE Winter
    Conference on Applications of Computer Vision (WACV)*, pp. 3587‚Äì3596, 2021.

    - [13] H. Wang, S. Li, S.-C. Su, Z. Qin, and X. Li, "Rdi-net: Relational dynamic
    inference networks," *2021 IEEE/CVF International Conference on Computer Vision
    (ICCV)*, pp. 4601‚Äì4610, 2021.

    - [14] P. Singh and V. P. Namboodiri, "Skipconv: skip convolution for computationally
    efficient deep cnns," in *2020 International Joint Conference on Neural Networks
    (IJCNN)*, pp. 1‚Äì8, IEEE, 2020.

    - [15] S. Teerapittayanon, B. McDanel, and H. T. Kung, "Branchynet: Fast inference
    via early exiting from deep neural networks," *2016 23rd International Conference
    on Pattern Recognition (ICPR)*, pp. 2464‚Äì 2469, 2016.

    - [16] Z. Wu, T. Nagarajan, A. Kumar, S. Rennie, L. S. Davis, K. Grauman, and
    R. Feris, "Blockdrop: Dynamic inference paths in residual networks," in *CVPR*,
    2018.

    - [17] A. Veit and S. J. Belongie, "Convolutional networks with adaptive inference
    graphs," *International Journal of Computer Vision*, vol. 128, pp. 730‚Äì741, 2019.

    - [18] Y. Li, Y. Chen, X. Dai, D. Chen, M. Liu, L. Yuan, Z. Liu, L. Zhang, and
    N. Vasconcelos, "Micronet: Improving image recognition with extremely low flops,"
    *2021 IEEE/CVF International Conference on Computer Vision (ICCV)*, pp. 458‚Äì467,
    2021.

    - [19] W. Xia, H. Yin, X. Dai, and N. K. Jha, "Fully dynamic inference with deep
    neural networks," *IEEE Transactions on Emerging Topics in Computing*, vol. 10,
    pp. 962‚Äì972, 2022.

    - [20] A. Kendall, J. Hawke, D. Janz, P. Mazur, D. Reda, J. M. Allen, V.-D. Lam,
    A. Bewley, and A. Shah, "Learning to drive in a day," *2019 International Conference
    on Robotics and Automation (ICRA)*, pp. 8248‚Äì8254, 2019.

    - [21] T. Chen, J. Xu, and P. Agrawal, "A system for general in-hand object re-orientation,"
    in *Conference on Robot Learning*, pp. 297‚Äì307, PMLR, 2022.

    - [22] J. Panerati, H. Zheng, S. Zhou, J. Xu, A. Prorok, A. P. S. U. of Toronto
    Institute for A Studies, V. I. for Artificial Intelligence, and U. of Cambridge,
    "Learning to fly‚Äîa gym environment with pybullet physics for reinforcement learning
    of multi-agent quadcopter control," *2021 IEEE/RSJ International Conference on
    Intelligent Robots and Systems (IROS)*, pp. 7512‚Äì7519, 2021.

    - [23] L. Bartolomei, L. Teixeira, and M. Chli, "Semantic-aware active perception
    for uavs using deep reinforcement learning," in *2021 IEEE/RSJ International Conference
    on Intelligent Robots and Systems (IROS)*, pp. 3101‚Äì3108, 2021.

    - [24] J. Chen, S. E. Li, and M. Tomizuka, "Interpretable end-to-end urban autonomous
    driving with latent deep reinforcement learning," *arXiv preprint arXiv:2001.08726*,
    2020.

    - [25] S. Krishnan, B. Boroujerdian, W. Fu, A. Faust, and V. J. Reddi, "Air learning:
    a deep reinforcement learning gym for autonomous aerial robot visual navigation,"
    *Mach. Learn.*, vol. 110, pp. 2501‚Äì2540, 2021.

    - [26] Z. Xie, X. Da, B. Babich, A. Garg, and M. van de Panne, "Glide: Generalizable
    quadrupedal locomotion in diverse environments with a centroidal model," *arXiv
    preprint arXiv:2104.09771*, 2021.

    - [27] Z. Si and W. Yuan, "Taxim: An example-based simulation model for gelsight
    tactile sensors," *IEEE Robotics and Automation Letters*, vol. 7, no. 2, pp. 2361‚Äì2368,
    2022.

    - [28] N. Rudin, D. Hoeller, P. Reist, and M. Hutter, "Learning to walk in minutes
    using massively parallel deep reinforcement learning," *ArXiv*, vol. abs/2109.11978,
    2021.

    - [29] S. Rajbhandari, C. Li, Z. Yao, M. Zhang, R. Y. Aminabadi, A. A. Awan, J.
    Rasley, and Y. He, "Deepspeed-moe: Advancing mixture-ofexperts inference and training
    to power next-generation ai scale," in *ICML*, 2022.

    - [30] "Nvidia inc, hyperq." https://developer.download.nvidia.com/compute/ DevZone/C/html
    x64/6 Advanced/simpleHyperQ/doc/HyperQ.pdf. Accessed: 2023-07-21.

    - [31] "Nvidia inc, cuda programming guide." https://docs.nvidia.com/cuda/ cuda-c-programming-guide/index.html#streams.
    Accessed: 2022-11- 21.

    - [32] "Nvidia inc, getting started with cuda graphs." https://developer.nvidia.
    com/blog/cuda-graphs/. Accessed: 2020-09-30.

    - [33] "Radeon open compute, atmi (asynchronous task and memory interface)." https://github.com/RadeonOpenCompute/atmi.
    Accessed: 2022- 09-30.

    - [34] Y. Etsion, F. Cabarcas, A. Rico, A. Ramirez, R. M. Badia, E. Ayguade, J.
    Labarta, and M. Valero, "Task superscalar: An out-of-order task pipeline," in
    *2010 43rd Annual IEEE/ACM International Symposium on Microarchitecture*, pp.
    89‚Äì100, IEEE, 2010.

    - [35] S. Kumar, C. J. Hughes, and A. D. Nguyen, "Carbon: architectural support
    for fine-grained parallelism on chip multiprocessors," in *International Symposium
    on Computer Architecture*, 2007.

    - [36] E. Castillo, L. Alvarez, M. Moreto, M. Casas, E. Vallejo, J. L. Bosque,
    ¬¥ R. Beivide, and M. Valero, "Architectural support for task dependence management
    with flexible software scheduling," *2018 IEEE International Symposium on High
    Performance Computer Architecture (HPCA)*, pp. 283‚Äì295, 2018.

    - [37] D. Sanchez, R. M. Yoo, and C. E. Kozyrakis, "Flexible architectural ¬¥ support
    for fine-grain scheduling," in *ASPLOS XV*, 2010.

    - [38] S. Puthoor, X. Tang, J. Gross, and B. M. Beckmann, "Oversubscribed command
    queues in gpus," *Proceedings of the 11th Workshop on General Purpose GPUs*, 2018.

    - [39] J. Gleeson, D. Snider, Y. Yang, M. Gabel, E. de Lara, and G. Pekhimenko,
    "Optimizing data collection in deep reinforcement learning," *ArXiv*, vol. abs/2207.07736,
    2022.

    - [40] J. Gleeson, S. Krishnan, M. Gabel, V. J. Reddi, E. de Lara, and G. Pekhimenko,
    "Rl-scope: Cross-stack profiling for deep reinforcement learning workloads," *ArXiv*,
    vol. abs/2102.04285, 2021.

    - [41] B. Zoph, V. Vasudevan, J. Shlens, and Q. V. Le, "Learning transferable
    architectures for scalable image recognition," *2018 IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, pp. 8697‚Äì8710, 2018.

    - [42] E. Real, A. Aggarwal, Y. Huang, and Q. V. Le, "Regularized evolution for
    image classifier architecture search," in *AAAI*, 2019.

    - [43] H. Liu, K. Simonyan, and Y. Yang, "Darts: Differentiable architecture search,"
    *arXiv preprint arXiv:1806.09055*, 2018.

    - [44] S. Xie, A. Kirillov, R. B. Girshick, and K. He, "Exploring randomly wired
    neural networks for image recognition," *2019 IEEE/CVF International Conference
    on Computer Vision (ICCV)*, pp. 1284‚Äì1293, 2019.

    - [45] H. Bai, F. Zhou, L. Hong, N. Ye, S.-H. G. Chan, and Z. Li, "Nasood: Neural
    architecture search for out-of-distribution generalization," *2021 IEEE/CVF International
    Conference on Computer Vision (ICCV)*, pp. 8300‚Äì8309, 2021.

    - [46] B. Yang, G. Bender, Q. V. Le, and J. Ngiam, "Condconv: Conditionally parameterized
    convolutions for efficient inference," in *NeurIPS*, 2019.

    - [47] Z. You, S. Feng, D. Su, and D. Yu, "Speechmoe: Scaling to large acoustic
    models with dynamic routing mixture of experts," *arXiv preprint arXiv:2105.03036*,
    2021.

    - [48] N. M. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. V. Le, G. E. Hinton,
    and J. Dean, "Outrageously large neural networks: The sparsely-gated mixture-of-experts
    layer," *ArXiv*, vol. abs/1701.06538, 2017.

    - [49] M. Sandler, A. G. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen, "Mobilenetv2:
    Inverted residuals and linear bottlenecks," *2018 IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, pp. 4510‚Äì4520, 2018.

    - [50] Y. Ding, L. Zhu, Z. Jia, G. Pekhimenko, and S. Han, "Ios: Inter-operator
    scheduler for cnn acceleration," *ArXiv*, vol. abs/2011.01302, 2021.

    - [51] W. Kwon, G.-I. Yu, E. Jeong, and B.-G. Chun, "Nimble: Lightweight and parallel
    gpu task scheduling for deep learning," in *NeurIPS*, 2020.

    - [52] H. Oh, J. Lee, H. Kim, and J. Seo, "Out-of-order backprop: an effective
    scheduling technique for deep learning," *Proceedings of the Seventeenth European
    Conference on Computer Systems*, 2022.

    - [53] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S.
    Ghemawat, G. Irving, M. Isard, M. Kudlur, J. Levenberg, R. Monga, S. Moore, D.
    G. Murray, B. Steiner, P. A. Tucker, V. Vasudevan, P. Warden, M. Wicke, Y. Yu,
    and X. Zhang, "Tensorflow: A system for large-scale machine learning," *ArXiv*,
    vol. abs/1605.08695, 2016.

    - [54] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen,
    Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, ¬® E. Yang, Z. DeVito,
    M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala,
    "Pytorch: An imperative style, highperformance deep learning library," *ArXiv*,
    vol. abs/1912.01703, 2019.

    - [55] T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang, T. Xiao, B. Xu, C. Zhang,
    and Z. Zhang, "Mxnet: A flexible and efficient machine learning library for heterogeneous
    distributed systems," *ArXiv*, vol. abs/1512.01274, 2015.

    - [56] S. Puthoor, A. M. Aji, S. Che, M. Daga, W. Wu, B. M. Beckmann, and G. P.
    Rodgers, "Implementing directed acyclic graphs with the heterogeneous system architecture,"
    *Proceedings of the 9th Annual Workshop on General Purpose Processing using Graphics
    Processing Unit*, 2016.

    - [57] HSA Foundation, "Hsa standard," 2017. http://hsafoundation.com/ standards/,
    Last accessed on 2023-02-14.

    - [58] Y. Chen, B. Brock, S. D. Porumbescu, A. Bulucc, K. A. Yelick, and J. D.
    Owens, "Atos: A task-parallel gpu dynamic scheduling framework for dynamic irregular
    computations," *ArXiv*, vol. abs/2112.00132, 2021.

    - [59] "Nvidia inc, cuda dynamic parallelism." https://developer.nvidia.com/ blog/cuda-dynamic-parallelism-api-principles/.
    Accessed: 2022-09-30.

    - [60] "Amd inc, rocm device enqueue." https://sep5.readthedocs.io/en/latest/
    Programming Guides/Opencl-programming-guide.html#device-sideenqueue. Accessed:
    2022-09-30.

    - [61] M. E. Belviranli, S. Lee, J. S. Vetter, and L. N. Bhuyan, "Juggler: a dependence-aware
    task-based execution framework for gpus," *Proceedings of the 23rd ACM SIGPLAN
    Symposium on Principles and Practice of Parallel Programming*, 2018.

    - [62] M. Steinberger, M. Kenzel, P. Boechat, B. Kerbl, M. Dokter, and D. Schmalstieg,
    "Whippletree: task-based scheduling of dynamic workloads on the gpu," *ACM Trans.
    Graph.*, vol. 33, pp. 228:1‚Äì228:11, 2014.

    - [63] N. Farooqui, A. Kerr, G. Diamos, S. Yalamanchili, and K. Schwan, "A framework
    for dynamically instrumenting gpu compute applications within gpu ocelot," in
    *Proceedings of the Fourth Workshop on General Purpose Processing on Graphics
    Processing Units*, pp. 1‚Äì9, 2011.

    - [64] M. Khairy, Z. Shen, T. M. Aamodt, and T. G. Rogers, "Accel-sim: An extensible
    simulation framework for validated gpu modeling," *2020 ACM/IEEE 47th Annual International
    Symposium on Computer Architecture (ISCA)*, pp. 473‚Äì486, 2020.

    - [65] E. Todorov, T. Erez, and Y. Tassa, "Mujoco: A physics engine for model-based
    control," in *2012 IEEE/RSJ international conference on intelligent robots and
    systems*, pp. 5026‚Äì5033, IEEE, 2012.

    - [66] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U.
    Franke, S. Roth, and B. Schiele, "The cityscapes dataset for semantic urban scene
    understanding," in *Proceedings of the IEEE conference on computer vision and
    pattern recognition*, pp. 3213‚Äì3223, 2016.

    - [67] M. Tan and Q. Le, "Efficientnet: Rethinking model scaling for convolutional
    neural networks," in *International conference on machine learning*, pp. 6105‚Äì6114,
    PMLR, 2019.

    - [68] F. N. Iandola, M. W. Moskewicz, K. Ashraf, S. Han, W. J. Dally, and K.
    Keutzer, "Squeezenet: Alexnet-level accuracy with 50x fewer parameters and ¬°1mb
    model size," *ArXiv*, vol. abs/1602.07360, 2016.

    - [69] K. Gupta, J. A. Stuart, and J. D. Owens, "A study of persistent threads
    style gpu programming for gpgpu workloads," *2012 Innovative Parallel Computing
    (InPar)*, pp. 1‚Äì14, 2012.

    - [70] T. Aila and S. Laine, "Understanding the efficiency of ray traversal on
    gpus," *Proceedings of the Conference on High Performance Graphics 2009*, 2009.

    - [71] H. Zhu, A. Phanishayee, and G. Pekhimenko, "Daydream: Accurately estimating
    the efficacy of optimizations for DNN training," in *2020 USENIX Annual Technical
    Conference (USENIX ATC 20)*, pp. 337‚Äì 352, USENIX Association, July 2020.

    - [72] J. Reinders, M. J. Voss, P. Reble, and R. Asenjo-Plaza, "++ for heterogeneous
    programming: oneapi (dpc++ and onetbb)," in *C++ for Heterogeneous Programming:
    oneAPI (DPC++ and oneTBB)*, 2020.

    - [73] R. D. Blumofe, C. F. Joerg, B. C. Kuszmaul, C. E. Leiserson, K. H. Randall,
    and Y. Zhou, "Cilk: an efficient multithreaded runtime system," in *PPOPP ''95*,
    1995.

    - [74] L. Dagum and R. Menon, "Openmp: an industry standard api for shared-memory
    programming," in *OpenMP: an industry standard API for shared-memory programming*,
    1998.

    - [75] A. Ram¬¥ƒ±rez, F. Cabarcas, B. H. H. Juurlink, M. Alvarez-Mesa, F. Sanchez,
    A. Azevedo, C. Meenderinck, C. B. Ciobanu, S. Isaza, and ¬¥ G. Gaydadjiev, "The
    sarc architecture," *IEEE Micro*, vol. 30, pp. 16‚Äì29, 2010.

    - [76] B. Choi, R. Komuravelli, H. Sung, R. Smolinski, N. Honarmand, S. V. Adve,
    V. S. Adve, N. P. Carter, and C.-T. Chou, "Denovo: Rethinking the memory hierarchy
    for disciplined parallelism," *2011 International Conference on Parallel Architectures
    and Compilation Techniques*, pp. 155‚Äì166, 2011.

    - [77] J. Planas, R. M. Badia, E. Ayguade, and J. Labarta, "Hierarchical task-
    ¬¥ based programming with starss," *The International Journal of High Performance
    Computing Applications*, vol. 23, pp. 284 ‚Äì 299, 2009.

    - [78] A. Pop and A. Cohen, "Openstream: Expressiveness and data-flow compilation
    of openmp streaming programs," *ACM Trans. Archit. Code Optim.*, vol. 9, pp. 53:1‚Äì53:25,
    2012.

    - [79] G. Gupta and G. S. Sohi, "Dataflow execution of sequential imperative programs
    on multicore architectures," in *Proceedings of the 44th annual IEEE/ACM international
    symposium on Microarchitecture*, pp. 59‚Äì70, 2011.

    - [80] M. D. Allen, S. Sridharan, and G. S. Sohi, "Serialization sets: a dynamic
    dependence-based parallel execution model," in *Proceedings of the 14th ACM SIGPLAN
    symposium on Principles and practice of parallel programming*, pp. 85‚Äì96, 2009.

    - [81] AMD Research, "Dagee," 2017. https://github.com/AMDResearch/ DAGEE.git,
    Last accessed on 2023-02-14.

    - [82] AMD Research, "Hipgraph," 2017. https://github.com/HipGraph/, Last accessed
    on 2023-02-14.

    - [83] A. E. Helal, A. M. Aji, M. L. Chu, B. M. Beckmann, and W. chun Feng, "Adaptive
    task aggregation for high-performance sparse solvers on gpus," *2019 28th International
    Conference on Parallel Architectures and Compilation Techniques (PACT)*, pp. 324‚Äì336,
    2019.

    - [84] A. M. Kaushik, A. M. Aji, M. A. Hassaan, N. Chalmers, N. Wolfe, S. Moe,
    S. Puthoor, and B. M. Beckmann, "Optimizing hyperplane sweep operations using
    asynchronous multi-grain gpu tasks," *2019 IEEE International Symposium on Workload
    Characterization (IISWC)*, pp. 59‚Äì69, 2019.

    - [85] A. Abdolrashidi, D. Tripathy, M. E. Belviranli, L. N. Bhuyan, and D. Wong,
    "Wireframe: Supporting data-dependent parallelism through dependency graph execution
    in gpus," *2017 50th Annual IEEE/ACM International Symposium on Microarchitecture
    (MICRO)*, pp. 600‚Äì611, 2017.

    - [86] A. Abdolrashidi, H. A. Esfeden, A. Jahanshahi, K. Singh, N. B. Abu-Ghazaleh,
    and D. Wong, "Blockmaestro: Enabling programmertransparent task-based execution
    in gpu systems," *2021 ACM/IEEE 48th Annual International Symposium on Computer
    Architecture (ISCA)*, pp. 333‚Äì346, 2021.

    - [87] G. Chen and X. Shen, "Free launch: Optimizing gpu dynamic kernel launches
    through thread reuse," *2015 48th Annual IEEE/ACM International Symposium on Microarchitecture
    (MICRO)*, pp. 407‚Äì419, 2015.

    - [88] I. E. Hajj, J. Gomez-Luna, C. Li, L.-W. Chang, D. S. Milojicic, and ¬¥ W.
    mei W. Hwu, "Klap: Kernel launch aggregation and promotion for optimizing dynamic
    parallelism," *2016 49th Annual IEEE/ACM International Symposium on Microarchitecture
    (MICRO)*, pp. 1‚Äì12, 2016.

    - [89] J. Wang, N. Rubin, A. Sidelnik, and S. Yalamanchili, "Dynamic thread block
    launch: A lightweight execution mechanism to support irregular applications on
    gpus," *2015 ACM/IEEE 42nd Annual International Symposium on Computer Architecture
    (ISCA)*, pp. 528‚Äì540, 2015.

    - [90] P. Fegade, T. Chen, P. Gibbons, and T. Mowry, "Cortex: A compiler for recursive
    deep learning models," *Proceedings of Machine Learning and Systems*, vol. 3,
    pp. 38‚Äì54, 2021.

    - [91] E. Jeong, S. Cho, G.-I. Yu, J. S. Jeong, D.-J. Shin, and B.-G. Chun, "{JANUS}:
    fast and flexible deep learning via symbolic graph execution of imperative programs,"
    in *16th USENIX Symposium on Networked Systems Design and Implementation (NSDI
    19)*, pp. 453‚Äì 468, 2019.

    - [92] S. Xu, H. Zhang, G. Neubig, W. Dai, J. K. Kim, Z. Deng, Q. Ho, G. Yang,
    and E. P. Xing, "Cavs: An efficient runtime system for dynamic neural networks,"
    in *2018 USENIX Annual Technical Conference (USENIX ATC 18)*, pp. 937‚Äì950, 2018.

    - [93] H. Shen, J. Roesch, Z. Chen, W. Chen, Y. Wu, M. Li, V. Sharma, Z. Tatlock,
    and Y. Wang, "Nimble: Efficiently compiling dynamic neural networks for model
    inference," *ArXiv*, vol. abs/2006.03031, 2021.

    - [94] E. Jeong, J. S. Jeong, S. Kim, G.-I. Yu, and B.-G. Chun, "Improving the
    expressiveness of deep learning frameworks with recursion," in *Proceedings of
    the Thirteenth EuroSys Conference*, pp. 1‚Äì13, 2018.

    - [95] M. Looks, M. Herreshoff, D. S. Hutchins, and P. Norvig, "Deep learning
    with dynamic computation graphs," *ArXiv*, vol. abs/1702.02181, 2017.

    - [96] F. Khorasani, H. A. Esfeden, N. B. Abu-Ghazaleh, and V. Sarkar, "In-register
    parameter caching for dynamic neural nets with virtual persistent processor specialization,"
    *2018 51st Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)*,
    pp. 377‚Äì389, 2018.

    - [97] C. J. Rossbach, J. Currey, M. Silberstein, B. Ray, and E. Witchel, "Ptask:
    operating system abstractions to manage gpus as compute devices," in *Proceedings
    of the Twenty-Third ACM Symposium on Operating Systems Principles*, pp. 233‚Äì248,
    2011.

    - [98] C. J. Rossbach, Y. Yu, J. Currey, J.-P. Martin, and D. Fetterly, "Dandelion:
    a compiler and runtime for heterogeneous systems," in *Proceedings of the Twenty-Fourth
    ACM Symposium on Operating Systems Principles*, pp. 49‚Äì68, 2013.

    - [99] J. Zhong and B. He, "Kernelet: High-throughput gpu kernel executions with
    dynamic slicing and scheduling," *IEEE Transactions on Parallel and Distributed
    Systems*, vol. 25, no. 6, pp. 1522‚Äì1532, 2013.

    - [100] X. Tan, *GPUPool: A Holistic Approach to Fine-Grained GPU Sharing in the
    Cloud*. PhD thesis, University of Toronto (Canada), 2021.'
  decisions:
    evaluation_prompt: '- Qualified. Reason: The paper includes multiple sections
      discussing evaluation, experiments, and results. It provides evidence of evaluation
      through phrases like "We evaluate our method on‚Ä¶", "Our evaluations demonstrate‚Ä¶",
      and includes performance tables and figures. The paper also discusses benchmarks,
      metrics, comparisons, and datasets, fulfilling the criteria for qualification.'
    related_work_prompt: '- Qualified. Reason: The paper meaningfully engages with
      prior research throughout the text. It includes numerous academic citations,
      discusses existing tools and frameworks with context, and compares its proposed
      method (ACS) to previous works in various sections, including the Introduction,
      Approach, and Related Work. The paper also provides a detailed comparison of
      ACS with other scheduling frameworks, demonstrating a thorough engagement with
      existing literature.'
    novelty_prompt: '- Qualified. Reason: The paper proposes ACS, a novel framework
      for Automatic Concurrent Scheduling, which addresses the challenge of GPU underutilization
      due to small kernels in applications with input-dependent irregular computational
      graphs. The paper introduces two implementations, ACS-SW and ACS-HW, and claims
      significant contributions, including a new runtime mechanism for improving GPU
      utilization and performance. The paper also provides a detailed evaluation of
      the proposed framework, demonstrating its effectiveness in various GPU applications.'
    review_only_prompt: '- Qualified. Reason: The paper introduces a new framework
      called ACS for improving GPU utilization by enabling concurrent execution of
      GPU kernels. It presents original contributions, including the design and implementation
      of ACS, and evaluates its effectiveness, rather than primarily summarizing existing
      work.'
  topics:
    main_topic: Computer Architecture
    main_topic_reasoning: The paper focuses on optimizing kernel execution on GPUs,
      which is a fundamental aspect of computer architecture, particularly in the
      context of enhancing parallelism and resource utilization.
    secondary_topic: High-performance Computing
    secondary_topic_reasoning: The application of the framework ACS for deep reinforcement
      learning and dynamic DNNs falls under the umbrella of high-performance computing
      as it aims to improve the performance and efficiency of computational workloads.
    main_topic_sub: Architectures for emerging technologies and applications
    secondary_topic_sub: Other
