papers:
- id: the_xor_cache_a_catalyst_for_compression_zhewen_pan_https_orcid_org_0009_0009_5707_1137
  title: 'The XOR Cache: A Catalyst for Compression'
  abstract: Modern computing systems allocate significant amounts of resources for
    caching, especially for the last level cache (LLC). We observe that there is untapped
    potential for compression by leveraging redundancy due to private caching and
    inclusion that are common in today's systems. We introduce the XOR Cache to exploit
    this redundancy via XOR compression. Unlike conventional cache architectures,
    XOR Cache stores bitwise XOR values of line pairs, halving the number of stored
    lines via a form of inter-line compression. When combined with other compression
    schemes, XOR Cache can further boost intra-line compression ratios by XORing lines
    of similar value, reducing the entropy of the data prior to compression. Evaluation
    results show that the XOR Cache can save LLC area by 1.93× and power by 1.92×
    at a cost of 2.06% performance overhead compared to a larger uncompressed cache,
    reducing energy-delay product by 26.3%.
  keywords: Cache hierarchy, Cache compression, Low-power architecture
  document: '![](_page_0_Picture_0.jpeg)


    # The XOR Cache: A Catalyst for Compression


    [Zhewen Pan](https://orcid.org/0009-0009-5707-1137)


    University of Wisconsin-Madison Madison, USA zhewen.pan@wisc.edu


    #### Abstract


    Modern computing systems allocate significant amounts of resources for caching,
    especially for the last level cache (LLC). We observe that there is untapped potential
    for compression by leveraging redundancy due to private caching and inclusion
    that are common in today''s systems. We introduce the XOR Cache to exploit this
    redundancy via XOR compression. Unlike conventional cache architectures, XOR Cache
    stores bitwise XOR values of line pairs, halving the number of stored lines via
    a form of inter-line compression. When combined with other compression schemes,
    XOR Cache can further boost intra-line compression ratios by XORing lines of similar
    value, reducing the entropy of the data prior to compression. Evaluation results
    show that the XOR Cache can save LLC area by 1.93× and power by 1.92× at a cost
    of 2.06% performance overhead compared to a larger uncompressed cache, reducing
    energy-delay product by 26.3%.


    ### CCS Concepts


    • Computer systems organization → Multicore architectures.


    #### Keywords


    Cache hierarchy, Cache compression, Low-power architecture


    #### ACM Reference Format:


    Zhewen Pan and Joshua San Miguel. 2025. The XOR Cache: A Catalyst for Compression.
    In Proceedings of the 52nd Annual International Symposium on Computer Architecture
    (ISCA ''25), June 21–25, 2025, Tokyo, Japan. ACM, New York, NY, USA, [14](#page-13-0)
    pages. <https://doi.org/10.1145/3695053.3730995>


    #### 1 Introduction


    Today''s computing systems dedicate tens to hundreds [\[39\]](#page-12-0) of megabytes
    of SRAM to caching, which contributes to a significant portion of die area, e.g.,
    AMD''s Zen3''s 32 MB L3 cache occupies around 40% of die area [\[38\]](#page-12-1).
    Additionally, the power consumption of these systems also surges, further straining
    the overall energy efficiency. The demand for resources in the cache hierarchy
    will continue to increase due to the growth in dataset size and the memory wall
    problem. However, large caches do not necessarily translate into better performance
    despite having more capacity; additionally, they come at the cost of high access
    latency, usually in tens of cycles. Given their resource-demanding nature, these
    factors combined can make traditional large caches potentially inefficient for
    future systems. To bridge this efficiency gap, we seek better ways to optimize
    the cache hierarchy by reducing the cache footprint and power while maintaining
    performance.


    [This work is licensed under a Creative Commons Attribution 4.0 International
    License.](https://creativecommons.org/licenses/by/4.0) ISCA ''25, Tokyo, Japan
    © 2025 Copyright held by the owner/author(s).


    ACM ISBN 979-8-4007-1261-6/25/06 <https://doi.org/10.1145/3695053.3730995>


    [Joshua San Miguel](https://orcid.org/0000-0002-6886-7183) University of Wisconsin-Madison
    Madison, USA jsanmiguel@wisc.edu


    <span id="page-0-2"></span><span id="page-0-1"></span>![](_page_0_Figure_18.jpeg)


    (a) Conventional (b) XOR Cache Figure 1: High-level overview. Unlike a conventional
    cache, XOR Cache stores the bitwise XOR of line pairs.


    Cache compression [\[3,](#page-12-2) [8–](#page-12-3)[10,](#page-12-4) [16,](#page-12-5)
    [24,](#page-12-6) [41,](#page-13-1) [42,](#page-13-2) [45](#page-13-3)[–47,](#page-13-4)
    [49,](#page-13-5) [51,](#page-13-6) [54\]](#page-13-7) is one of the promising
    lines of research. Caches can compress cache lines upon insertion and decompress
    them upon access. This approach reduces the cache footprint with the overhead
    of only a few extra cycles added to access latency. Effective cache compression
    exploits data compressibility to achieve the benefits of a larger cache but with
    significantly reduced hardware costs.


    We introduce the XOR Cache, a new compressed LLC architecture that leverages redundancy
    that spans multiple caches, inherent to the memory hierarchies of today''s systems.
    More specifically, XOR Cache harnesses redundancy due to inclusion and private
    caching. First, while prior cache compression paradigms only exploit value redundancy
    within a single cache level for compression opportunity, XOR Cache focuses on
    taming redundancy due to inclusion between the higher and lower level cache. It
    has been shown that inclusive cache hierarchies can introduce a significant amount
    of data redundancy, thereby decreasing the effective capacity of the lower-level
    cache[1](#page-0-0) [\[15,](#page-12-7) [26,](#page-12-8) [37,](#page-12-9) [40,](#page-12-10)
    [48,](#page-13-8) [55\]](#page-13-9). Unfortunately, this data duplication has
    been overlooked, beyond the trivial solution of relaxing the strictly-inclusive
    property. To bridge this gap, XOR Cache transforms what was once considered a
    drawback—redundancy due to inclusion—into untapped compressibility. Second, XOR
    Cache leverages redundancy due to private caching to decompress data via forwarding
    between the private caches, supported by its coherence protocol. XOR Cache achieves
    efficient compression by exploiting both forms of redundancy.


    Figure [1](#page-0-1) shows a high-level overview of the XOR Cache. Unlike a conventional
    cache that stores lines as-is, in XOR Cache, an inclusive cache line, e.g., line
    A, since it already exists in the L1s, can XOR with another, e.g., line B, in
    the lower level as a single line A⊕B, denoted by the pink arrows. On an LLC access,
    we can simply forward the XORed line A⊕B to the higher level and perform another
    XOR operation to reverse the compression, denoted by the green arrows. Note that
    lines in L1s are never XORed, so L1 hits do not impose extra latency. Doing so
    provides two novel benefits:


    <span id="page-0-0"></span><sup>1</sup>We refer to the cache further from the
    processors as the lower-level cache.


    1) XORing two lines allows us to save one line of storage in the LLC; and 2) when
    the two lines have similar data values, XORing them reduces entropy, making them
    more compressible and catalyzing the effectiveness of other compression schemes.
    The goal of this work is to leverage the enhanced compressibility enabled by XOR
    Cache to reduce area and power of the LLC.


    ## <span id="page-1-0"></span>1.1 Redundancy in Cache Hierarchy (Inter-Line Compression)


    Unless made strictly exclusive, the LLC typically contains either all, i.e., an
    inclusive LLC, or some of the cache lines, i.e., a non-inclusive, non-exclusive
    (NINE) LLC, that exist in the higher-level caches. This duplication is a missing
    piece in conventional cache compression works, as they typically only exploit
    redundancy within a single cache level. However, it can create extra opportunities
    for compression across the cache level boundary. To exploit such opportunities,
    upon insertion, XOR Cache performs a bitwise XOR between the inserted line and
    another selected line and stores the result in the LLC data array. By doing so,
    it effectively co-locates two cache lines in one physical slot, resulting in a
    2:1 compression ratio. A compressed line pair can stay compressed when at least
    one of the original lines is shared at the higher level to maintain the ability
    to recover, which we call the minimum sharer invariant. Leveraging XOR compression
    alone, we can achieve a best-case compression ratio of 2, allowing us to downsize
    the LLC data array by half. We denote this as a form of inter-line compression
    (inter in Figure [1b\)](#page-0-2).


    ## <span id="page-1-2"></span>1.2 Synergy of XOR Compression (Intra-Line Compression)


    XOR compression can work independently from prior cache compression schemes; however,
    synergy exists between it and other schemes when we carefully select the XOR candidate
    lines. When combined, the XOR Cache can boost (catalyze) the compression ratios
    of other schemes. For example, in Figure [1b,](#page-0-2) when the selected line
    A is similar to B, i.e., A≈B, the XORed line A⊕B can exhibit lower entropy and
    be further compressed. This compression ratio boost is achieved by exploiting
    the similarity within the XORed lines, denoted as a form of intra-line compression,
    labeled as intra in the figure. With XOR compression, each pair of cache lines
    is first XORed, and then we can apply the baseline existing compression schemes
    on the XORed data.


    We quantify this synergy by profiling the LLC snapshots when running a wide range
    of benchmarks, as shown in Figure [2.](#page-2-0) For this analysis, we assume
    any two lines in the same bank or in the same set can potentially be XORed without
    imposing the minimum sharer invariant (Section [1.1\)](#page-1-0). As examples,
    we show the potential of XOR Cache''s synergy with BΔI [\[45\]](#page-13-3) and
    Bit-plane (BPC) [\[30\]](#page-12-11) compression, which are intra-line cache
    compression schemes, and Thesaurus [\[24\]](#page-12-6) as an inter-line compression
    scheme. BΔI [\[45\]](#page-13-3) exploits the low dynamic range of values within
    a cache line and encodes a line using a single base value and an array of deltas
    with smaller sizes. BPC [\[30\]](#page-12-11) applies three transformations to
    boost data compressibility and uses run-length encoding and frequent pattern compression
    to compress the transformed data. Thesaurus [\[24\]](#page-12-6) dynamically clusters
    cache lines using locality-sensitive hashing and compress lines against the centroids.


    The XOR pair selection policy, or XOR policy for short, determines which two cache
    lines should be XORed with each other, which is the crux of XOR Cache''s design
    space. Let''s consider three hypothetical XOR policies, namely randBank, idealSet,
    and ideal-Bank. For randBank, a line randomly selects and XORs with another line
    in the same bank. For idealSet, a line considers all the lines in the same set
    as XOR candidates and selects the most ideal XOR candidate that results in the
    most compressible XORed line. For example, in Figure [3,](#page-2-1) the line
    at set 0 and way 0 can potentially XOR with any of the lines in the same set,
    i.e., in the blue box. Here, the ideal XOR candidate in set 0 is the line at way
    2, since they only have one bit difference. Similarly, for idealBank, a line considers
    all the lines in the same bank and XORs with the one that results in the smallest
    size. In Figure [3,](#page-2-1) the line at set 0 and way 0 can XOR with anyone
    in the orange box. The line at set 1 and way 3 is the most ideal as it has exactly
    the same value. Shown in Figure [2,](#page-2-0) the leftmost sets of bars are
    the baseline compression ratio without XOR; the other three sets of bars show
    the compression ratio of XOR Cache using the three aforementioned XOR policies.
    The lines are first XORed with each other, and then the selected baseline compression
    scheme further compresses the XORed line.


    We make the following observations. First, the two search-based XOR policies,
    i.e., idealBank and idealSet, achieve higher compression ratios than the random
    XOR policy, which is value-agnostic. This demonstrates the importance of considering
    the value similarity when choosing the XOR candidates. Second, idealBank outperforms
    idealSet because it has a significantly larger search scope, and is therefore
    more likely to find ideal XOR candidates. However, the implementation cost of
    idealBank can also significantly outweigh that of idealSet. Though the search-based
    policies by no means represent practical implementations (we discuss practical
    implementation later in Section [3.2\)](#page-3-0), it showcases the upper bound
    compression ratio of XOR Cache. More specifically, the idealBank XOR compression,
    which searches for the best candidates across the entire LLC bank, can boost compression
    ratio of BΔI, BPC and Thesaurus by 2.08×, 2.09× and 2.02× on average and up to
    4.7×, 3.0× and 4.6×, respectively, demonstrating XOR Cache''s potential for catalyzing
    compression.


    ## <span id="page-1-1"></span>1.3 Challenges


    There are two main challenges that must be addressed to unlock the full potential
    of XOR Cache. The first challenge lies in designing the XOR policy that achieves
    synergy. There is ample design space, ranging from random to value-aware, each
    with its own trade-offs between complexity, effectiveness, and performance. The
    second involves the coherence protocol design. Unlike prior works, XOR compression
    crosses the boundary of a single cache level; therefore, the coherence protocol
    needs a significant redesign. Besides maintaining coherence, the protocol needs
    to ensure that uncompressed data values are always recoverable.


    This work proposes XOR Cache, a compressed LLC architecture that allows aggressive
    data array downsizing, while maintaining performance. By exploring the design
    space of XOR Cache, we show how the synergy between XOR and other schemes can
    boost the compression ratio. This work makes the following contributions:


    (1) We present a novel cache compression scheme that harnesses redundancy due
    to inclusion and private caching, which are understudied in prior works.


    0


    <span id="page-2-0"></span>![](_page_2_Figure_2.jpeg)


    (c) XOR+Thesaurus Figure 2: Compression ratio from LLC profiling. (a) shows compression
    ratio of XOR with BΔI; (b) shows compression ratio of XOR with BPC; (c) shows
    compression ratio of XOR with Thesaurus. A cache line can randomly XOR with another
    from the same bank (randBank), or search the entire set/bank to find the best
    candidate that minimizes data storage (idealSet/idealBank).


    PERFECT (Multi-threaded) PARSEC 3.0 (Multi-threaded) SPEC CPU 2017 (Multi-programmed)


    2dcv dwt hist dbyr lk cd bl bo ca de fa fe fl fr ra st sw vi 1 2 3 4 5 6 7 8 9
    10 11 gmean


    <span id="page-2-1"></span>![](_page_2_Figure_4.jpeg)


    Figure 3: IdealSet and idealBank XOR policy. The example LLC bank is 4-way set
    associative with 2 sets. The boxes denote search scopes; the shaded lines represent
    the ideal XOR candidates for the line at set 0 and way 0.


    - (2) We show that XOR compression can synergistically boost the compression ratio
    when combined with other cache compression schemes.

    - (3) We implement XOR Cache using the Ruby memory model in the gem5 simulator
    [\[36\]](#page-12-12) and evaluate it in full-system simulation on multiple benchmark
    suites.


    The remainder of the paper is organized as follows. Section [2](#page-2-2) provides
    background on cache concepts and summarizes cache compression research. Section
    [3](#page-3-1) describes XOR compression and XOR policy. Sections [4](#page-4-0)
    and [5](#page-6-0) discuss XOR Cache''s coherence protocol support for data forwarding
    and architecture implementation. Section [6](#page-8-0) presents our evaluation
    methodology and experimental results. Section [7](#page-11-0) discusses related
    work, and Section [8](#page-11-1) concludes.


    ## <span id="page-2-2"></span>2 Background


    We provide background on inclusion policy in modern cache hierarchies and cache
    coherence protocol design choices, followed by a summary of low-power and compressed
    cache works.


    ## 2.1 Modern Cache Hierarchy


    Cache hierarchies are typically classified into inclusive, NINE, and exclusive,
    each with its own pros and cons [\[6,](#page-12-13) [15,](#page-12-7) [26,](#page-12-8)
    [37,](#page-12-9) [40,](#page-12-10) [48,](#page-13-8) [55\]](#page-13-9). In
    an inclusive cache hierarchy, all the lines in the higher level caches must remain
    a subset of lines in the lower level cache. In contrast, the lower level cache
    in the exclusive cache hierarchy cannot contain any line at the higher level.
    The NINE hierarchy sits in between, where a line in the higher level may or may
    not appear in the lower level. Conventionally, the inclusive hierarchy is widely
    adopted due to its low required snooping bandwidth and low complexity in the coherence
    protocol naturally provided by inclusion. However, maintaining inclusion poses
    two main challenges: back invalidation and low effective capacity. While several
    works include proposed solutions to combat the former [\[15,](#page-12-7) [26\]](#page-12-8),
    the


    latter has received less attention. A strictly inclusive cache usually adopts
    the trivial solution of keeping the higher level cache small and the lower level
    large to limit the amount of data duplication. More recently, the trend has been
    moving towards NINE cache hierarchies [\[4,](#page-12-14) [6,](#page-12-13) [25,](#page-12-15)
    [31,](#page-12-16) [53,](#page-13-10) [56\]](#page-13-11), which is free of back
    invalidation problems and has more flexibility in sizing. Regardless, both inclusive
    and NINE cache hierarchies inevitably contain redundant cache lines due to inclusion,
    causing the effective capacity to shrink.


    #### 2.2 Cache Coherence


    This section summarizes the design choices in conventional cache coherence relevant
    to the XOR Cache''s functionality.


    2.2.1 Eviction Notification. An LLC shared by all cores is usually the point of
    coherence in the system. Therefore, it needs a directory structure for storing
    coherence states, e.g., sharer list and owner of lines cached below. The most
    critical information a directory tracks is the sharer list. It is a per-line list
    of higher level cache IDs where the line exists. A higher level cache is added
    to the sharer list on read requests and removed from the list on eviction notifications.
    This sharer list can be imprecise for the following reasons, depending on the
    coherence protocol and directory organization implementation. First, some implementations
    adopt an incomplete directory, e.g., limited pointer or coarse bit vector. These
    directories only track a subset of sharers for scalability reasons [\[40\]](#page-12-10).
    Second, imprecision can also stem from the coherence protocol. On clean evictions,
    the higher level cache may support silent eviction, i.e., opt out of eviction
    notification and silently drop the clean line to avoid excess communication on
    eviction. Note this imprecise sharer information does not compromise correctness
    as the list may contain false positive sharers but never false negatives. However,
    it does have performance and cost implications as unnecessary invalidation requests
    and acknowledgments are sent out to and from the false positive sharers [\[22\]](#page-12-17).
    In XOR Cache, to ensure recoverability, a cache line pair can remain compressed
    if at least one of the two lines is shared. We call thisthe minimum sharer invariant.
    Therefore, we adopt a full bit vector directory implementation, and our cache
    coherence protocol sends explicit notifications on clean evictions.


    2.2.2 Upgrade Notification. Some protocols also opt out of upgrade notifications
    with the Exclusive state. A uniquely shared line can be directly promoted to Exclusive
    upon reading. When the owner later modifies the line, it can silently upgrade
    it to Modified state, as it is guaranteed that no other sharer exists. XOR Cache
    benefits from explicit upgrade notification when a line transitions from Shared
    to Modified state, as the LLC is informed that its copy is potentially stale and
    should not remain XORed.


    #### 2.3 Low-Power Cache Architecture


    Prior research has explored various approaches to reduce cache power consumption.
    Techniques to lower leakage power include dynamically turning off dead blocks
    [\[1,](#page-12-18) [27\]](#page-12-19) or cache ways [\[5\]](#page-12-20), and
    placing cold blocks into a low-power drowsy state [\[23\]](#page-12-21). Other
    strategies focus on reducing the overall power by operating caches near the threshold
    voltage [\[20\]](#page-12-22) or employing mixed-cell design [\[28\]](#page-12-23).
    To address dynamic power, prior work proposes using a small filter cache [\[33\]](#page-12-24)
    to handle frequent accesses more efficiently. Additionally, reducing cache size
    has been achieved by only storing reused blocks [\[4\]](#page-12-14) or through
    cache compression [\[32\]](#page-12-25).


    Among these, compression emerges as a particularly promising technique, exploiting
    data redundancy for compressibility [\[3,](#page-12-2) [7–](#page-12-26) [10,](#page-12-4)
    [16,](#page-12-5) [18,](#page-12-27) [21,](#page-12-28) [24,](#page-12-6) [29,](#page-12-29)
    [30,](#page-12-11) [35,](#page-12-30) [41](#page-13-1)[–47,](#page-13-4) [49–](#page-13-5)[51,](#page-13-6)
    [54\]](#page-13-7). A compressed cache maintains the benefits of a larger cache
    with significantly reduced hardware costs. To implement cache compression, designs
    adopt a decoupled tag data array structure. Some also require additional hardware
    structures for metadata storage [\[24,](#page-12-6) [54\]](#page-13-7). Compression
    algorithms can be classified into intra- and inter-line based on the compression
    granularity. Intra-line compression captures value similarity within a single
    memory or cache line. [\[3,](#page-12-2) [8,](#page-12-3) [30,](#page-12-11) [45,](#page-13-3)
    [54\]](#page-13-7). These works are typically dictionary-based, matching against
    predefined common values or patterns at sub-line granularity. Inter-line works
    [\[24,](#page-12-6) [29,](#page-12-29) [41,](#page-13-1) [44,](#page-13-12) [46,](#page-13-13)
    [47,](#page-13-4) [49\]](#page-13-5) compress multiple similar lines together
    and store only one copy of the line, along with additional metadata. Typically,
    a hash function selects the lines compressed together based on address or data
    value.


    #### <span id="page-3-1"></span>3 XOR Compression


    We explain how our proposed XOR compression works and address the first challenge
    mentioned in Section [1.3.](#page-1-1)


    3.1 Compression and Decompression Algorithm Our implementation of XOR compression
    employs simple and symmetric compression and decompression algorithms. As discussed
    in Section [1.1,](#page-1-0) XOR Cache stores the bitwise XOR results of line
    pairs. Upon access, the XORed line performs another bitwise XOR operation with
    one of the two original lines. Therefore, compression and decompression are perfectly
    symmetrical since XOR with a given input is a self-inverse function. Additionally,
    the compressor and decompressor hardware is extremely simple. They are merely
    an array of XOR gates of length 512, assuming 64B cache lines. Given that they
    are simple bitwise operations, they can be embedded in the cache controller or
    even closer to the SRAM cells [\[2,](#page-12-31) [51\]](#page-13-6). In fact,
    any form of reversible computing [2](#page-3-2) is compatible with this type of
    compression. In this work, we choose XOR due to its simplicity and symmetry as
    mentioned earlier. We only consider 2-way XORing, i.e., XORing exactly two lines,
    and leave exploration of other reversible functions for future work.


    #### <span id="page-3-0"></span>3.2 XOR Policy: Finding XOR Candidates


    We can adopt an opportunistic XOR policy by allowing XOR compression whenever
    any candidate, i.e., a standalone line, is available. This maximizes the inter-line
    compression ratio from XOR compression. Alternatively, we can be more selective
    about performing XOR compression by adopting a synergistic XOR policy. As mentioned
    in Section [1.2,](#page-1-2) if similar lines are allowed to XOR together, the
    resultant⊕ line is likely to exhibit lower entropy and contain many zeros, thereby
    enabling further intra-line compression. We can see this by examining a concrete
    example of two lines from the bodytrack benchmark in Figure [4.](#page-4-1) They
    are very similar, with only a few bit differences, i.e., low in hamming distance.
    Individually, they have limited intra-line compressibility, but when XORed as
    a pair, the entropy of the XORed line can be significantly reduced.


    However, the key challenge is identifying line pairs similar to each other. As
    in Figure [2,](#page-2-0) the exhaustive search across the entire bank, i.e.,
    idealBank, achieves a remarkable compression ratio boost


    <span id="page-3-2"></span><sup>2</sup>Given an operator ◦ and its reverse operator
    ◦ −1 , and = ◦ . Given A and C, operator ◦ is reversible if we can uniquely determine
    B by ◦ <sup>−</sup><sup>1</sup> .


    The XOR Cache: A Catalyst for Compression ISCA ''25, June 21–25, 2025, Tokyo,
    Japan


    <span id="page-4-1"></span>


    | Line A   |  |  | 0020 003C 6D7F 0000 7C20 003C 6D7F 0000 7C20 003C … |

    |----------|--|--|-----------------------------------------------------|

    | Line B   |  |  | 0020 004C 6D7F 0000 7C20 004C 6D7F 0000 7C20 004C … |

    | Line A⊕B |  |  | 0000 0070 0000 0000 0000 0070 0000 0000 0000 0070 … |


    Figure 4: Two similar lines A and B from bodytrack benchmark in PAESEC3.0 suite.
    The XORed line A⊕B has low entropy.


    <span id="page-4-2"></span>![](_page_4_Figure_3.jpeg)


    Figure 5: Sensitivity study of idealSet compression ratio on the effect of spatio-value
    locality. X in idealSet-X denotes the number of index bits shifted towards the
    MSBs.


    from the baseline. However, it is idealistic and prohibitively expensive in hardware.
    idealSet shows less synergy due to a smaller scope of suitable candidates, as
    a set contains significantly fewer lines than the entire bank. In the sensitivity
    study in Figure [5,](#page-4-2) we also consider variants of idealSet where we
    leverage spatio-value locality and intentionally create more similar candidates
    by shifting the index bits in the cache line address towards the MSBs by 1-4 bits.
    We find that shifting the index bits can improve exhaustive search within a set
    by 5.47% and 11.66% on average and up to 28.32% and 4.04×, respectively. The results,
    i.e., idealSet-1,2,3,4, show more prominent synergy, closing the gap between the
    ideal and more realistic compression ratios.


    As a practical implementation, we consider a map table-based synergistic XOR policy
    for finding similar lines within a bank using manageable hardware complexity.
    The map table, similar to those in prior works on deduplication [\[24,](#page-12-6)
    [46,](#page-13-13) [49\]](#page-13-5), is an additional level of indirection that
    effectively implements a hash table. A map function is applied on the cache line
    to generate a map value, which serves as a signature of the cache line value and
    is used as an index to the map table. The map table implementation and map function
    selection will be discussed in Section [5.1.3.](#page-6-1)


    #### <span id="page-4-0"></span>4 XOR Cache Coherence


    This section describes the coherence protocol, addressing the second challenge
    discussed in Section [1.3.](#page-1-1) Though it can be more natural to reason
    about coherence for XORed lines as pairs of states, we describe the protocol in
    terms of decoupled state transitions here for brevity. After performing XOR compression
    (Section [4.2\)](#page-4-3), XOR Cache relies on its coherence protocol to perform
    decompression to service data requests (Section [4.3\)](#page-4-4) and unXORing
    to maintain data recoverability (Section [4.4\)](#page-5-0). We discuss the inter-line
    dependency and its implication on deadlock-freedom in Section [4.5.](#page-6-2)


    <span id="page-4-5"></span>![](_page_4_Figure_10.jpeg)


    Figure 6: LLC transitions between stable states. **I** for **Invalid**; **S**
    for **Shared**; **M** for **Modified**; **S0** is a special **S** state when the
    number of sharers is zero; compression, decompression, and unXORing edges are
    in blue, green, and red, respectively.


    #### 4.1 Assumption


    In this work, we present XOR Cache''s protocol based on MSI, though the concepts
    generalize to other protocols. Figure [6](#page-4-5) shows the LLC transition
    between stable states. In addition to the three stable states, namely Modified,
    Shared, and Invalid, we denote another state Shared0, for clarity. It is the specific
    scenario of Shared when the line has no sharers in the private cache. Note that
    Shared0 state is fundamentally different from Exclusive state in MESI protocols,
    as Shared0 lines do not have write permission without sending explicit write requests.
    Our implementation assumes mixed inclusive cache hierarchy, where inclusion is
    maintained for clean lines, and exclusion is enforced for dirty lines since their
    owner is in the higher level, and the lower level does not directly service requests
    on dirty lines. These assumptions are clarified in Table [1,](#page-4-6) where
    S lines allocate both directory and LLC entries, while M lines only allocate a
    directory entry. When promoted to M, a line deallocates its entry in the LLC.
    S0 lines are those with no sharer at the higher level; therefore, they only allocate
    LLC entries. Note that this does not impose considerably more transient states,
    as S0 is a state that already exists in MSI; we simply highlight it for clarity
    in the description. The directory tracks accurate sharer information requiring
    explicit eviction and upgrade notification. In our work, we do not assume support
    for silent upgrades and thus do not consider the Exclusive state. In the following
    sections, we dissect the protocol by discussing three types of transitions special
    to XOR Cache: compression, decompression, and unXORing, as highlighted in blue,
    green, and red in Figure [6.](#page-4-5)


    <span id="page-4-6"></span>Table 1: Coherence stable states and mapping in storage.


    | State     | Invalid | Shared | Modified | Shared0 |

    |-----------|---------|--------|----------|---------|

    | Directory | ✗       | ✓      | ✓        | ✗       |

    | LLC       | ✗       | ✓      | ✗        | ✓       |


    ## <span id="page-4-3"></span>4.2 Compression


    Upon line insertion, XOR Cache attempts to XOR it with an existing line in the
    data array. The inserted line can be 1) a line from memory on a demand miss, denoted
    by <sup>1</sup>❣in Figure [6,](#page-4-5) or 2) a write-back line from the higher
    level due to dirty eviction, i.e., <sup>2</sup>❣, or 3) writer data update on
    a downgrade, i.e., <sup>3</sup>❣. No coherence change is required by compression;
    we only highlight the transitions at the end of which it can happen.


    ## <span id="page-4-4"></span>4.3 Decompression


    When a getS request ( <sup>4</sup>❣and <sup>5</sup>❣in Figure [6\)](#page-4-5)
    arrives at the LLC for an XORed line, XOR Cache needs to decompress it to service
    the <span id="page-5-2"></span>ISCA ''25, June 21–25, 2025, Tokyo, Japan Zhewen
    Pan and Joshua San Miguel


    ![](_page_5_Figure_1.jpeg)


    Figure 7: Three forwarding cases when A and B are XORed. From top to bottom are
    local recovery, direct forwarding, and remote recovery.


    request. To decompress, XOR Cache forwards the request to the higher level. As
    shown in Table [2,](#page-5-1) there are three data forwarding cases, and the
    sequence diagrams for handling them are shown in Figure [7.](#page-5-2) We continue
    with the same scenario as in Figure [1,](#page-0-1) where we have a miss on line
    B, i.e., getS(B), while the LLC holds A⊕B.


    <span id="page-5-1"></span>Table 2: Forwarding choices when requestor issues **getS(B)**,
    assuming A and B are XORed in LLC.


    | Case              | A State (Shared/<br>Shared0) | B State (Shared/<br>Shared0)
    | B Requestor<br>shares line A |

    |-------------------|------------------------------|------------------------------|------------------------------|

    | Local recovery    | Shared                       | -                            |
    ✓                            |

    | Direct forwarding | -                            | Shared                       |
    ✗                            |

    | Remote recovery   | Shared                       | Shared0                      |
    ✗                            |


    In the first case where A has at least one sharer, and B''s requestor happens
    to be one of them, the LLC forwards the data A⊕B to B''s requestor. The requestor
    then performs another bitwise XOR operation on A⊕B and its local copy of A to
    retrieve the demand data B. We name this case local recovery. In the second case,
    where the requestor of B does not share A, but B has at least one sharer at the
    higher level, LLC forwards the request for B to one of B''s sharers. B''s sharer
    then supplies the data. Note that this case does not need any XOR operation and
    is handled as conventional cache-to-cache forwarding. We call this case direct
    forwarding. In the third case, the requestor of B similarly does not share A,
    and B no longer has any sharer. In this case, A must have at least one sharer
    by design, so the LLC forwards the request for B along with the data A⊕B to A''s
    sharer. A''s sharer reads out its local copy of A, performs another bitwise XOR
    between A⊕B and A to retrieve B remotely, and then sends it back to B''s requestor.
    We name this case remote recovery.


    In these cases, unblock control messages (1.3, 2.3.2, and 3.3.2) are sent from
    the private cache that supplies the data to the directory, informing its data
    service completion. These are necessary for a completely unblocking cache controller
    implementation without assuming any network ordering requirement. Unblock messages
    are similarly needed in the baseline when downgrading from and upgrading to M
    state. For direct forwarding and remote recovery, when multiple sharers exist,
    we randomly select the forwarder. There are two directory-to-cache messages (1.2
    and 3.2 in Figure [7\)](#page-5-2) including both addresses A and B. We assume
    an extra 8 bytes for these packets. Note the case where both A and B are in S0
    state is impossible due to the minimum sharer invariant. This invariant is guaranteed
    by design by enforcing the necessary unXORing, discussed next in Section [4.4.](#page-5-0)


    #### <span id="page-5-0"></span>4.4 UnXORing


    4.4.1 When unXORing happens. XOR Cache occasionally needs to unXOR a pair before
    any of them goes into an unrecoverable state in the following three cases. First,
    when a line B, XORed with A, is upgrading to **Modified**, denoted by <sup>6</sup>❣and
    <sup>7</sup>❣in Figure [6,](#page-4-5) B''s writer is expected to update its value,
    rendering the LLC copy potentially stale and thus unrecoverable. Therefore, on
    getM requests, the XORed pair, A⊕B, needs to unXOR. Second, XOR Cache requires
    that an XORed line can stay compressed if at least one line in the XOR pair has
    at least one sharer (the minimum sharer invariant). Therefore, unXORing is needed
    on the last **putS** request transitioning from <sup>S</sup> to S0, i.e., <sup>8</sup>❣.
    Third, when handling **eviction** from <sup>S</sup> and S0, i.e., <sup>9</sup>❣and
    <sup>10</sup>❣, unXORing is needed to recover the original data and optionally
    write back to memory if 1) only one of the two paired lines is performing eviction
    due to insufficient tag space or 2) both lines are performing co-eviction due
    to insufficient data space and at least one of them is dirty.


    4.4.2 unXORing implementation. Note the line triggering unXORing, say B, may be
    in <sup>S</sup> or S0 state; <sup>6</sup>❣, <sup>8</sup>❣, and <sup>9</sup>❣are
    <sup>S</sup> state triggered, while <sup>7</sup>❣and <sup>10</sup>❣are S0 state
    triggered. These two cases require different implementations of unXORing. To unXOR,
    the LLC has to obtain one of the original lines in the higher level cache by issuing
    a special write-back request. If the triggering line B is in S state ( 6❣, <sup>8</sup>❣,
    and <sup>9</sup>❣), the write-back request is sent to the triggering line''s own
    sharer. It can be implemented in the protocol directly as it only involves one
    address. Otherwise, when the triggering line, B, is in S0 ( <sup>7</sup>❣and <sup>10</sup>❣),
    both A and B are involved. The paired line, A, transitions into a transient state
    and serves as the proxy for retrieving data to perform unXORing. This effectively
    creates a dependence between A and B, as a request on B may trigger state transition
    on A. The implication of this inter-line dependence on deadlock freedom is discussed
    in Section [4.5.](#page-6-2)


    4.4.3 Free of uncontrolled expansion. In other compressed cache schemes, an update
    to a line may cause data size change and thus may induce a variable number of
    LLC evictions due to insufficient space in the data array, which may lead to performance
    problems. In XOR Cache, due to unXORing, except in the co-eviction case, expansion
    also happens. Especially when XOR compression is combined with other intra-line
    compression schemes, the likelihood of data expansion increases. If the victim
    happens to be an XORed data block, both lines should perform eviction, i.e., co-eviction.
    Say that line A, XORed with line B, triggers unXORing through the


    abovementioned operations. Then the recovered line B and optionally A[3](#page-6-3)
    after intra-line compression individually try re-insertion. This data expansion
    may cause another XORed pair, C and D, to coevict. If any of C and D is dirty,
    unXORing is performed. However, this is guaranteed not to cause further expansion,
    as the recovered lines C and D only occupy the transaction buffer space as opposed
    to the actual cache space. Therefore, XORed data block eviction never causes expansion,
    so it can not cause further data eviction and is guaranteed to be sunk.


    #### <span id="page-6-2"></span>4.5 Deadlock Freedom


    <span id="page-6-4"></span>To show that XOR Cache''s coherence protocol is correct
    and practical, this section proves the following two properties: 1) there is no
    cyclic dependence between requests (free of protocol deadlock) in Section [4.5.1,](#page-6-4)
    and 2) its implementation does not require any extra virtual network (free of
    virtual network deadlock) in Section [4.5.2.](#page-6-5) 4.5.1 Free of cyclic
    dependence between requests. As in Section [4.4,](#page-5-0) XOR Cache introduces
    inter-line dependence through unXORing. To verify deadlock freedom, we combine
    model checking with analysis. We leverage the model checking tool Murphi [\[19\]](#page-12-32)
    to verify deadlock freedom with a single address. To avoid the combinatorial explosion
    problem, we analytically evaluate deadlock freedom with multiple addresses similar
    to [\[34\]](#page-12-33). We adopt an unblocking private cache controller and
    blocking LLC controller implementation. With our controller implementation, only
    LLC-bound requests may ever be blocked. Recall that the S state line A in the
    XORed pair serves as the proxy for its paired line B to send out a write-back
    request to A''s sharer in the case of getM(B). In reply to the write-back request,
    A''s sharer should send an LLC-bound write-back data response. Note that this
    response cannot be blocked by messages other than LLC-bound requests, as line
    A cannot be in other blocking transient state[4](#page-6-6) , therefore the dependence
    established by XOR Cache does not result in cyclic dependence between requests.


    <span id="page-6-5"></span>4.5.2 No extra virtual network (VN) required. With
    a single address, two virtual networks, one dedicated to LLC-bound requests and
    the other to the rest of messages, are sufficient to guarantee virtual network
    deadlock freedom. With the new dependence introduced by XOR Cache, it is sufficient
    to ensure VN deadlock freedom by enforcing that LLC-bound requests such asgetM
    cannot be in the same virtual network with this LLC-bound write-back response.
    This is already satisfied by the virtual network assignment for a single address.
    As a result, we argue that XOR Cache''s coherence protocol is deadlock-free, and
    more importantly, its implementation does not require any extra virtual network
    on top of the baseline.


    In summary, XOR Cache''s coherence protocol supports decompression on LLC access
    to an XORed line (Section [4.3\)](#page-4-4) and unXORing for maintaining data
    recoverability (Section [4.4\)](#page-5-0). The protocol requires 18.8% more transient
    states for implementing decompression and unXORing. Additionally, an additional
    pair of messages (write-back request and response in Section [4.5\)](#page-6-2)
    and a forwarded getS request (fwd-getS in Figure [7\)](#page-5-2) are needed,
    which account for 18.2% overhead in message support.


    <span id="page-6-7"></span>![](_page_6_Figure_9.jpeg)


    Figure 8: XOR Cache organization. a) Decoupled tag-data store and map table; b)
    Tag entry; c) Data entry; Grey blocks are identical to the uncompressed baseline;
    T is the number of tag entries; D is the number of data entries.


    ## <span id="page-6-0"></span>5 XOR Cache Architecture


    This section outlines the XOR Cache''s storage organization and how it handles
    cache operations.


    #### 5.1 Organization


    To allow two arbitrary lines in a bank to XOR, XOR Cache adopts a decoupled tag-data
    organization and uses a map table to identify XOR candidates, as shown in Figure
    [8a](#page-6-7).


    5.1.1 Tag Array. The tag array is managed as a linked list with each tag entry
    shown in Figure [8b](#page-6-7). XORed is a 1-bit indicator of whether the line
    is XORed with a partner. XORptr points to the tag entry of its partner. Note that
    in our implementation, a line can only have one partner, so XOR Cache only needs
    one tag pointer instead of two in the conventional case. We leave the exploration
    of XORing beyond pairs of lines to future work. DataPtr points to the corresponding
    entry in the data array.


    5.1.2 Data Array. As shown in Figure [8c](#page-6-7), the data array entry stores
    a reverse pointer to the tag array entry, i.e., tagptr. We use a random replacement
    policy for the data array. Note that when combined with other compression schemes,
    XOR Cache needs to support variable data entry size. We use a similar 8B-segmented
    data array and startmap per set as in Thesaurus to convert the ordinal index stored
    in the tag entry to the actual segment ID. Details are omitted here for brevity
    and can be found in [\[24\]](#page-12-6). We assume that data compaction happens
    after eviction, expansion, and contraction, similar to prior works.


    <span id="page-6-1"></span>5.1.3 Map Table and Map Function. At a high level,
    the map table is a small storage structure for identifying similar XOR candidates.
    As shown in Figure [8a](#page-6-7), the map table contains tag pointers of standalone
    lines. It is indexed using a map value generated by applying a map function to
    the data. On cache line insertion, it first computes the map value and accesses
    the map table. If it hits, meaning a valid XOR candidate exists, then XOR compression
    is triggered, and the map table entry is cleared. Otherwise, the line is inserted
    as an uncompressed line, and the map table allocates an entry for the standalone
    line. We will discuss XOR Cache''s insertion flow in detail in Section [5.2.5.](#page-7-0)
    The map table is introduced as an additional level of indirection, similar to
    [\[24,](#page-12-6) [47,](#page-13-4) [49\]](#page-13-5).


    We consider four hash functions as our map function candidates. Two are locality-sensitive
    hash functions based on random projection (LSH-RP) similar to [\[24\]](#page-12-6)
    and bit sampling (LSH-BS). The other two are based on byte labeling. For every
    byte in the cache line, a boolean label 0 is generated if the byte is 0x0, or
    1 is generated otherwise. Byte labeling effectively captures the sparsity of a
    cache line at a byte level. For baseline byte labeling (BL), the generated


    <span id="page-6-3"></span><sup>3</sup> If unXORing is triggered by getM, the
    triggering line is not re-inserted as exclusion is enforced on dirty lines. As
    an optimization to avoid excessive expansion, on the last putS(A) request, we
    may choose to insert B only and drop A with no back-invalidation required.


    <span id="page-6-6"></span><sup>4</sup>Otherwise, it would have caused unXORing
    in the first place, or A would not have been XORed with B


    <span id="page-7-1"></span>![](_page_7_Figure_1.jpeg)


    Figure 9: Average byte-level entropy per 8-byte word.


    <span id="page-7-5"></span>![](_page_7_Figure_3.jpeg)


    <span id="page-7-6"></span>(b) XOR Cache flow. Forward XOR refers to the forwarding
    cases in Table [2.](#page-5-1) Figure 10: Data request flow. The critical path
    is in grey.


    byte labels are first permuted and then XOR folded into the map value. Sparse
    byte labeling (SBL) only considers a subset of bytes in the line, i.e., the most
    significant 6 bytes per every 8-byte word. This sparse version of byte labeling
    exploits the fact that the lowordered bits in words exhibit the highest entropy
    [\[29,](#page-12-29) [44\]](#page-13-12) as shown in Figure [9.](#page-7-1) Therefore,
    excluding low-order bytes ideally reduces noise in the map value. For all four
    map functions, the number of map value bits can vary as a parameter. We compare
    them in a sensitivity study in Section [6.2](#page-8-1) and also discuss the coverage
    and accuracy trade-off with a varying number of map value bits. Since the map
    table only needs one entry per unique map value and we keep the map value in a
    small number of bits, it can be implemented as a direct-mapped structure to minimize
    the overhead.


    ### 5.2 Operations


    Like other compressed caches, XOR Cache performs decompression when servicing
    data requests, which is discussed in Section [5.2.1.](#page-7-2) Sections [5.2.2,](#page-7-3)
    [5.2.3](#page-7-4) revisit the necessary unXORing cases. Finally, Section [5.2.5](#page-7-0)
    describes XOR Cache''s insertion flow.


    <span id="page-7-2"></span>5.2.1 Reads. As shown in Figure [10a,](#page-7-5) a
    read request that arrives at the LLC first performs a parallel lookup in the directory
    and tag array. If the address hits in the directory, the line is either in <sup>M</sup>
    or <sup>S</sup> state. In <sup>M</sup> state <sup>1</sup>✇, the read request performs
    the normal flow by forwarding to the owner. In <sup>S</sup> state <sup>2</sup>✇,
    i.e., when we have a directory hit on S state, the line could either be standalone,
    i.e., <sup>a</sup>✇, or XORed with another line, i.e., <sup>b</sup>✇, indicated
    by the 1-bit XORed field in the tag entry. For case <sup>a</sup>✇, we perform
    the normal flow by retrieving data from the data array. In the latter case <sup>b</sup>✇,
    the tag entry has a valid partner pointed by XORPtr. This triggers the XOR Cache''s
    data request flow as shown in Figure [10b.](#page-7-6) Reading the XORed data
    can start immediately by following the DataPtr in the tag entry of the requested
    line. In parallel, the request also needs to access the XORed partner''s coherence
    metadata to decide which of the three forwarding cases discussed in Section [4.3](#page-4-4)
    to follow. First, we follow XORPtr to access the tag entry of the XOR partner.
    Then, a second lookup in the directory is performed to retrieve


    <span id="page-7-7"></span>![](_page_7_Figure_10.jpeg)


    Figure 11: Insertion flow (off critical path). F() denotes the map function.


    the coherence metadata of the partner. Recall from Table [2](#page-5-1) that depending
    on the state of the requested line and the partner, this read access is handled
    in three different cases. It can be serviced by sending the XORed line back, piggybacking
    on the partner that already exists in the requestor, i.e., local recovery. Alternatively,
    it can be forwarded to a sharer of the requested line corresponding to direct
    forwarding. Note that in this case, since the request hits in the directory, the
    line is guaranteed not to be in S0 state, so remote recovery is not needed.


    If the address misses in the directory but hits in the tag array, i.e., also case
    <sup>2</sup>✇, the line is in S0 state. Similarly, it can be standalone or XORed.
    For XORed S0 line <sup>b</sup>✇, the second lookup is also triggered, and the
    request is either piggybacked on the existing partner, i.e., local recovery, or
    forwarded to the partner''s sharer, i.e., remote recovery. Lastly, on an LLC read
    miss <sup>3</sup>✇, the request is forwarded to memory as in normal operation
    flow. After the data is returned from memory, the insertion flow in Section [5.2.5](#page-7-0)
    is triggered.


    <span id="page-7-3"></span>5.2.2 Writes and Upgrades. On write request to an I
    or M line, it is forwarded to either memory or the owner as usual. In other cases,
    when a write request arrives for an XORed line, XOR Cache must unXOR the XORed
    pair. This involves an extra writeback hop from the higher level cache to the
    LLC. As XOR Cache forces exclusion on dirty lines, it will be evicted from the
    tag and data array and inserted into the directory.


    <span id="page-7-4"></span>5.2.3 Writebacks. Clean writeback requests, i.e., putS
    are short data-less messages that allow the directory to track a precise sharer
    list. On an XORed line''s last writeback request, it looks up its partner''s coherence
    metadata to determine if unXORing is triggered. A negative acknowledgment is sent
    out as a reply, and the private cache should retry the clean writeback along with
    clean data. Dirty writebacks, i.e. putM, are handled similarly to insertion, which
    will be discussed in Section [5.2.5.](#page-7-0) The line performs XOR compression
    if a candidate exists.


    5.2.4 Evictions. When co-evicting an XORed line, unXORing is needed to perform
    dirty writeback to memory. Both tags are evicted through the reverse and forward
    pointers.


    <span id="page-7-0"></span>5.2.5 Insertions. Upon data insertion, XOR Cache tries
    to find an available candidate in the map table to co-locate with. The insertion
    could be from memory due to demand fetch or from private caches due to writeback.
    This insertion flow, shown in Figure [11,](#page-7-7) is off the critical path.
    In a map table-based implementation, the map value is calculated by applying the
    map function to the returned data. The map value is then used to index into the
    map table. On a map table hit, it reads the tag pointer and then the data, performs
    bitwise XOR, and inserts the XORed data into the data array to replace the original
    data. Conversely, if no candidate exists, i.e., on map table miss, the tag pointer
    is inserted into the map table, data is inserted, and tag is updated. For both
    lines, DataPtrs in the tag array point to the XORed data entry, and the XORPtrs
    point to each other.


    Table 3: Hardware system configuration.


    <span id="page-8-2"></span>


    | CPU    | 4 core, 3GHz x86-64                             |  |  |  |

    |--------|-------------------------------------------------|--|--|--|

    | L1I    | 32KiB, 4 way, 4 cycle, 64B line, LRU, Private   |  |  |  |

    | L1D    | 32KiB, 4 way, 4 cycle, 64B line, LRU, Private   |  |  |  |

    | L2     | 256KiB, 8 way, 9 cycles, 64B line, LRU, Private |  |  |  |

    | L3     | 1MiB per bank, 16 way, 40 cycle,                |  |  |  |

    |        | 64B line, LRU, Shared, 4 banks                  |  |  |  |

    | Memory | DualChannelDDR4-2400                            |  |  |  |


    <span id="page-8-3"></span>Table 4: Per bank LLC storage breakdown. Other refers
    to the base cache in Thesaurus and map table in XOR Cache.


    | Clusivity        |                                  | Mixed inclusive |       |                |         |             |
    Exclusive    |        |

    |------------------|----------------------------------|-----------------|-------|----------------|---------|-------------|--------------|--------|

    | Compression      |                                  | Un-<br>comp.    | BΔI   |
    The-<br>saurus | BPC     | XOR<br>+BΔI | Un-<br>comp. | BΔI    |

    |                  | #Entries                         | 16384           |       |                |         |             |
    13312        |        |

    | Tag              | Entry size                       | 32b             | 49b   |
    49b            | 49b     | 63b         | 32b          | 49b    |

    |                  | Size (KiB)                       | 64              | 98    |
    98             | 98      | 126         | 52           | 79.63  |

    |                  | #Entries                         | 16384           | 12288
    | 10240          | 10240   | 6144        | 13312        | 10240  |

    | Data             | Entry size                       | 512b            |       |                |
    512+40b | 512b        |              |        |

    |                  | Size (KiB)                       | 1024            | 768   |
    640            | 640     | 414         | 832          | 640    |

    |                  | #Entries<br>-<br>Entry size<br>- |                 |       |
    512            | -       | 128         | -            |        |

    | Other            |                                  |                 | 512b  |
    -              | 14b     | -           |              |        |

    |                  | Size (KiB)                       | -               |       |
    32             | -       | 0.22        | -            |        |

    | Total size (KiB) |                                  | 1088            | 866   |
    770            | 738     | 540.22      | 884          | 719.63 |


    #### <span id="page-8-0"></span>6 Evaluation


    XOR Cache enables effective compression through XOR compression. With it, XOR
    Cache reduces area and power with comparable performance to the uncompressed cache.
    The goal of this section is to evaluate XOR Cache''s compressibility (Sections
    [6.2](#page-8-1) and [6.3\)](#page-9-0) and its effectiveness in reducing area
    and power (Section [6.4\)](#page-10-0). We finally show its performance overhead
    (Section [6.5\)](#page-10-1) and energy-delay product improvement (Section [6.8\)](#page-11-2).


    #### 6.1 Methodology


    <span id="page-8-7"></span>6.1.1 Simulator and Simulated System. We simulate XOR
    Cache with its coherence protocol using the Ruby model in the gem5 simulator [\[36\]](#page-12-12).
    We use CACTI 7.0 [\[11\]](#page-12-34) to evaluate area, power, and latency of
    memory structures and use Synopsys design compiler for compressor hardware synthesis
    using 32nm technology. Table [3](#page-8-2) lists the configuration of the simulated
    hardware system with a 3-level cache hierarchy similar to [\[24\]](#page-12-6).
    It represents a system with a high LLC-to-MLC size ratio, i.e., 4:1, which is
    a pessimistic configuration for XOR Cache due to limited XOR compression opportunity.


    6.1.2 LLC Configuration. Table [4](#page-8-3) lists the configurations of the
    LLC used in a 3-level cache hierarchy for XOR Cache and other baselines, unless
    otherwise specified. The uncompressed baseline is 1 MiB per bank LLC (Table [3\)](#page-8-2);
    the compressed baselines include intra-line compressed BΔI and BPC LLCs, and an
    inter-line compressed Thesaurus LLC. Thesaurus [\[24\]](#page-12-6) dynamically
    clusters cache lines using locality-sensitive hashing and compresses lines against
    the centroids. Our XOR Cache is based on xor compression synergistically with
    intra-line BΔI compression. We shrink the data array size for all compressed caches
    by a factor based on our profiled geometric mean compression ratio in Figure [2.](#page-2-0)
    Specifically, we use a 1.3× smaller data array for BΔI, and a 1.5× smaller data
    array for Thesaurus and BPC. XOR Cache with BΔI adopts a 2.5× smaller data array.
    The map table is direct-mapped with 128 entries


    Table 5: SPEC CPU 2017 benchmark random mixes.


    <span id="page-8-5"></span>


    | Run | Benchmarks                                                 |

    |-----|------------------------------------------------------------|

    | 1   | 505.mcf_r, 541.leela_r, 510.parest_r, 503.bwaves_r         |

    | 2   | 520.omnetpp_r, 548.exchange2_r, 500.perlbench_r, 557.xz_r  |

    | 3   | 511.povray_r, 521.wrf_r, 538.imagick_r, 549.fotonik3d_r    |

    | 4   | 502.gcc_r, 544.nab_r, 519.lbm_r, 527.cam4_r                |

    | 5   | 523.xalancbmk_r, 525.x264_r, 508.namd_r, 554.roms_r        |

    | 6   | 531.deepsjeng_r, 507.cactuBSSN_r, 521.wrf_r, 538.imagick_r |

    | 7   | 525.x264_r, 500.perlbench_r, 549.fotonik3d_r, 519.lbm_r    |

    | 8   | 503.bwaves_r, 541.leela_r, 557.xz_r, 508.namd_r            |

    | 9   | 527.cam4_r, 511.povray_r, 507.cactuBSSN_r, 505.mcf_r       |

    | 10  | 520.omnetpp_r, 554.roms_r, 502.gcc_r, 531.deepsjeng_r      |

    | 11  | 523.xalancbmk_r, 544.nab_r, 510.parest_r, 548.exchange2_r  |


    <span id="page-8-6"></span>![](_page_8_Figure_13.jpeg)


    Figure 12: Comp. ratio with four map functions (Section [5.1.3\)](#page-6-1).
    (a) inter-line comp. ratio; (b) intra-line comp. ratio; (c) total comp. ratio.
    The x-axis is the number of map value bits.


    (Section [5.1.3\)](#page-6-1). Besides these mixed inclusive LLCs, we also include
    an exclusive LLC with the same effective capacity.[5](#page-8-4) We also include
    the exclusive LLC with BΔI applied atop.


    We pessimistically assume a uniform LLC latency of 40 cycles, despite the potential
    for lower latency given the smaller data array. We assume 1 cycle latency for
    BΔI, 5 cycles for Thesaurus, and 7 cycles for Bit-plane decompression. For XOR
    Cache''s (de)compressor, the synthesized XOR gate array only incurs 0.12 ns delay,
    so we assume performing bit-wise XOR is within the same cycle as the read. Note
    that we also model forwarding latency as part of XOR decompression, in addition
    to the above decompressor latencies. 6.1.3 Benchmarks. We evaluate three benchmark
    suites in gem5 full-system simulation. For multi-threaded workloads, we include


    the PERFECT [\[12\]](#page-12-35) openmp version, which targets image processing
    workloads, and PARSEC 3.0 [\[13\]](#page-12-36) with simlarge datasets. For PER-FECT
    and PARSEC 3.0, we simulate the entire region of interest. We include SPEC CPU
    2017 [\[14\]](#page-12-37) reference workloads for multiprogrammed evaluation.
    In each run, we launch a random mix of 4 SPEC rate benchmarks, each with one copy,
    shown in Table [5.](#page-8-5) We fast-forward 100B instructions and collect statistics
    using the following 1B detailed instructions on every core.


    #### <span id="page-8-1"></span>6.2 XOR Compression Synergy


    In our map table-based implementation of XOR Cache, we use a small map table with
    a map function to choose the XOR candidates based on value similarity. We profile
    the LLC snapshots to compare the four map functions (LSH-RP, LSH-BS, BL, and SBL)


    <span id="page-8-4"></span><sup>5</sup>We size the exclusive LLC according to
    the proportion of S0 lines as the baseline.


    <span id="page-9-4"></span><span id="page-9-2"></span><span id="page-9-1"></span>![](_page_9_Figure_2.jpeg)


    <span id="page-9-3"></span>Figure 13: Compression ratio analysis.


    introduced in Section [5.1.3.](#page-6-1) Figure [12](#page-8-6) shows the inter-line
    and intraline geometric mean compression ratios across all three benchmark suites.
    The X-axis shows the number of map value bits, and the Y-axis shows the compression
    ratio. Additionally, compression ratios of XOR idealBank with BΔI and baseline
    BΔI (without XOR) are included for reference. For all four map functions, we see
    a coverage-accuracy tradeoff.


    Coverage. As shown in Figure [12a](#page-8-6), all four map functions show decreased
    XOR compression coverage as the number of map value bits increases. The reason
    is that the number of unique map values increases exponentially and the probability
    of finding a candidate in the matching bin decreases. Hence, the inter-line XOR
    compression opportunity is reduced.


    Accuracy. At the same time, however, as shown in Figure [12b](#page-8-6), the
    accuracy of identifying similar candidate lines improves as the intra-line compression
    ratio increases due to the increased value similarity between the XORed line pairs.
    The reason is that the more map function value bits are used, the fewer false
    similar candidates appear in the same bin. LSH-BS takes more than 30 bits to achieve
    intra-line compression ratio synergy, whereas LSH-RP needs 12 bits. BL and SBL
    both achieve similar intra-line compression ratios as BΔI as early as 7 bits;
    however, SBL maintains a higher inter-line compression ratio since it effectively
    removes the noise from high entropy bits in words.


    Tradeoff. To balance the XOR compression coverage and accuracy of similar candidate
    selection, we need to select the proper number of map value bits. Figure [12c](#page-8-6)
    shows that the sweet spot balancing inter- and intra-line compression ratios occurs
    at around 7 bits for BL and SBL. Considering all the benchmarks for our profiling
    results, for the rest of the evaluation, we use 7-bit SBL, which results in an
    average compression ratio of ∼ 2.5, justifying our choice of using a 2.5× smaller
    data array for XOR Cache with BΔI.


    #### <span id="page-9-0"></span>6.3 Compression Ratio Improvement


    Figure [13](#page-9-1) shows a compression ratio analysis of XOR Cache and baselines.
    Figure [13a](#page-9-2) and [13b](#page-9-3) show the per benchmark compression
    <span id="page-9-5"></span>ratio normalized relative to the uncompressed LLC.
    We make the following observations. First, XOR Cache with BΔI uniformly boosts
    baseline BΔI compression ratio across all benchmarks, demonstrating XOR compression''s
    effectiveness. Second, it achieves a higher compression ratio than the exclusive
    LLC with BΔI. Note that the Exclusive LLC (with no BΔI) also has a greater than
    1 compression ratio. This comes from the fact that it does not store any S state
    line, unlike the baselines storing inclusive lines. However, this ratio is low
    (1.06×) due to the already limited redundancy between LLC and private caches (4:1
    ratio) in the baseline hierarchy (Table [3\)](#page-8-2). For exclusive LLC with
    BΔI, there exists no redundancy due to inclusion. Conversely, XOR Cache leverages
    this redundancy for effective compression. Third, XOR Cache also achieves a higher
    average compression ratio than Thesaurus and BPC. Thesaurus achieves a lower compression
    ratio on PERFECT, whereas BPC works well exploiting value similarity for homogeneous
    data. For the multi-programmed SPEC workloads, both baselines achieve a lower
    compression ratio due to limited value similarity at word and cache line level.


    XOR Cache''s inter-line compression ratio by XOR compression alone is marked by
    the darker blue shade. In the practical setting, it is less than the upper bound
    ratio of 2. The reasons here are threefold. First, there is limited redundancy
    between the LLC and the private caches. In our evaluated configuration (Table
    [3\)](#page-8-2), the private cache lines only make up at most a quarter of lines
    in the LLC, leaving the vast majority of LLC lines in S0 state. They have limited
    opportunity to be XORed, therefore limiting the inter-line compression ratio.
    Second, the inter-line compressibility is limited due to the existence of **Modified**
    lines. Note that even though our setup enforces exclusion for M lines in the LLC,
    they contend with S lines for private cache space, i.e., the more M lines, the
    fewer S lines in the private cache. Recall that XOR Cache leverages shared data
    in the private caches, i.e., redundancy due to inclusion for compressibility;
    the existence of M lines limits our achieved compression ratio. Third, workloads
    may have extensive sharing between cores in data and instructions; These private
    cache lines map to the same set


    The XOR Cache: A Catalyst for Compression ISCA ''25, June 21–25, 2025, Tokyo,
    Japan


    <span id="page-10-3"></span><span id="page-10-2"></span>![](_page_10_Figure_1.jpeg)


    of S lines in the LLC. In the extreme case, all the private caches may share exactly
    the same set of N lines; the LLC has only N S lines, and the remaining majority
    of lines end up as S0. Again, S0 lines have limited XOR opportunity as they can
    only be XORed with S lines (the minimum sharer invariant). This imbalanced number
    of S0 and S lines can limit the XOR compressibility. We validated the last two
    hypotheses above by examining the private cache line distribution shown in Figure
    [13c](#page-9-4) and [13d.](#page-9-5) All the valid private cache lines are classified
    into M, S non-unique, and S unique; S non-unique lines are those with more than
    one sharers, whereas S unique lines are exclusive to one private cache. Based
    on the last two reasons, XOR compression opportunity (dark blue area in Figure
    [13a](#page-9-2) and [13b\)](#page-9-3) is proportional to the weight of S unique
    lines (bottom dark gray area with vertical lines), with the exception of blacksholes
    (bl) as the private cache is not fully utilized to due to its small footprint.
    For example, dwt''s low compression ratio is because more than 90% private cache
    lines are in M state (top light gray area with dots). PERFECT and PARSEC 3.0 generally
    have more sharing (the middle S non-unique region), due to their multi-threaded
    nature. Therefore, they achieve lower inter-line compression ratios compared to
    the multi-programmed case.


    Takeaway. Unlike the exclusive LLC that eliminates redundancy due to private caching,
    XOR Cache embraces it and tames it for more compressibility by synergistic XOR
    compression. When both combined with the BΔI intra-line scheme, XOR Cache achieves
    16.2% and 27.8% higher compression ratio for multi-threaded and multi-programmed
    workloads than the Exclusive baseline, which lacks such synergy. Moreover, XOR
    Cache with BΔI also achieves a higher compression ratio than intra- and inter-line
    schemes by 23.1% (BΔI), 4.5% (BPC), and 23.4% (Thesaurus) on multi-threaded workloads,
    and 34.9% (BΔI), 28.5% (BPC), and 18.4% (Thesaurus) on multi-programmed workloads.


    #### <span id="page-10-0"></span>6.4 Area and Power Improvement


    We obtain the normalized LLC area and cache hierarchy power breakdown in Figure
    [14a](#page-10-2) and [14b](#page-10-3) assuming 32nm technology. 6.4.1 Area.
    To support XOR compression, XOR Cache only needs 0.01 <sup>2</sup> extra area.
    Though decoupling the tag and data arrays adds marginal metadata overhead, the
    data array size reduction as a result of compression dominates. Comp. includes
    compressors,


    <span id="page-10-4"></span>![](_page_10_Figure_6.jpeg)


    (a) Multi-threaded (PERFECT, PARSEC) (b) Multi-programmed (SPEC) Figure 15: Normalized
    performance overhead. (a) shows norm. runtime of multi-threaded runs; (b) shows
    the norm. geometric mean of CPI of multi-programmed runs.


    map table, and map function logic for XOR Cache and includes compressors and storage
    (e.g., base cache) for other baselines.


    Takeaway. XOR Cache achieves 1.93× smaller LLC area than the uncompressed, 1.56×,
    1.41×, and 1.35× smaller than BΔI, Thesaurus and BPC counterparts, and 1.30× than
    the Exclusive LLC with BΔI. 6.4.2 Power. Figure [14b](#page-10-3) shows the normalized
    power breakdown of the cache hierarchy. In addition to LLC and private caches,
    we also include the network dynamic power using the model in [\[52\]](#page-13-14).
    With XOR compression, XOR Cache''s additional private cache accesses due to local
    and remote recovery contribute to a mere 1.99% of total private cache accesses.
    The increased activity adds overhead to dynamic power. However, the leakage power
    still dominates the total LLC power contribution due to the filtering effect of
    private caches. XOR Cache generates 23.4% more network traffic due to additional
    forwarding messages, which translates to increased dynamic network power. Though
    this may seem a lot, especially in a scaled-out system, with the network bandwidth
    scaling trend in emerging chiplet-based systems [\[17\]](#page-12-38), we do not
    expect the additional traffic to translate to significant bandwidth overhead.
    Additionally, this overhead is still less than the Exclusive LLC, which adds 24.6%
    more traffic for maintaining strict exclusivity.


    Takeaway. In spite of its additional activity, XOR Cache still achieves a significant
    1.92× LLC power reduction and 1.46× cache hierarchy power reduction compared to
    the uncompressed cache.


    ## <span id="page-10-1"></span>6.5 Performance Overhead


    To quantify XOR Cache''s performance overhead, we use normalized runtime for multi-threaded
    benchmarks; we take the geometric mean of CPI from every core on the 1B detailed
    simulated instructions, for multi-programmed benchmarks. Figure [15](#page-10-4)
    shows performance overhead ofXOR Cache and four baselines (Table [3\)](#page-8-2),
    normalized to the uncompressed MSI baseline. BΔI adds a fixed 1-cycle decompression
    latency on the critical path; BPC and Thesaurus have higher decompression latency
    due to complex decompressors, and Thesaurus also has base cache miss. XOR Cache
    shows a slowdown of 1.45% on multi-threaded and 2.95% on multi-programmed workloads.
    This difference between these two sets of workloads is due to two reasons: 1)
    multi-programmed workloads generally observe less compressibility; and 2) more
    LLC hits (∼ 15%) follow the remote recovery decompression path (Section [4.3\)](#page-4-4),
    which is the


    1


    <span id="page-11-5"></span>G


    <span id="page-11-3"></span>![](_page_11_Figure_1.jpeg)


    Figure 17: Geometric mean of normalized inter-line compression ratio. X-axis denotes
    LLC-to-private cache size ratio.


    8:1 4:1 2:1 8:1 4:1 2:1 Multi-threaded (PERFECT, PARSEC) Multi-programmed (SPEC)


    <span id="page-11-6"></span>![](_page_11_Figure_3.jpeg)


    slowest among all three, in multi-programmed workloads. Overall, XOR Cache incurs
    a marginal geomean norm. performance overhead of 2.06%.


    #### 6.6 Case Study: Iso-Storage Performance


    Though our primary goal is to reduce LLC area and power, we include a case study
    on iso-storage performance of XOR Cache against the three compressed baselines.
    We run 4-core multiprogrammed workloads with every core running a copy of a SPEC
    benchmark. Figure [16](#page-11-3) highlights the subset of workloads that are
    most sensitive to LLC size, where the performance difference is more than 3% using
    a 2× LLC. XOR Cache achieves an average of 1.78% and up to 5.22% speedup over
    the uncompressed iso-storage LLC, which is higher than that of BΔI (−2.89%), Thesaurus
    (1.75%) and BPC (1.28%), thanks to its higher compression ratio. Across all workloads,
    XOR Cache yields a modest speedup of 0.21%, still highest among the compression
    schemes.


    #### 6.7 Sensitivity Study


    6.7.1 Core Count. In addition to the 4-core results, we provide an additional
    set of 8-core multi-threaded[6](#page-11-4) results. Scaling up XOR Cache to 8-core
    only sees 18.7% network traffic overhead and 1.55% performance overhead, comparable
    to 18.3% traffic and 1.45% performance overhead in 4-core multi-threaded case.


    6.7.2 LLC Size. Figure [17](#page-11-5) shows the inter-line compression ratio
    sensitivity to LLC size with two additional sets of experiments with doubled and
    halved LLC size, corresponding to an 8:1 and 2:1 LLCto-private cache size ratio.
    As discussed in Section [6.1.1](#page-8-7) and [6.3,](#page-9-0) since XOR Cache
    exploits the redundancy between LLC and private caches to perform XOR compression,
    as the LLC-to-private cache ratio lowers, there is more compression opportunity
    for XOR Cache in both multi-threaded and multi-programmed cases.


    #### <span id="page-11-2"></span>6.8 Summary: Energy-Delay Product


    Figure [18](#page-11-6) summarizes our evaluation by measuring the energydelay
    product (EDP), normalized to the uncompressed MSI baseline. As a result of effective
    compression, XOR Cache saves significant power and area with a marginal performance
    loss. Therefore, the EDP of XOR Cache is the lowest among all, being 26.3% lower
    than the uncompressed baseline.


    #### <span id="page-11-0"></span>7 Related Work


    Abundant works leverage inter-line compressibility for heterogeneous data. Deduplication
    [\[49\]](#page-13-5) uses a heuristic XOR foldingbased hash to identify identical
    lines and stores only unique ones, eliminating redundancy at the cache line level.
    MORC [\[41\]](#page-13-1) proposes compressing lines leveraging temporal value
    locality for logbased caches. Thesaurus [\[24\]](#page-12-6) dynamically clusters
    cache lines using locality-sensitive hashing and compress lines against the centroids.
    Several inter-line compression works identify that high-order bits in words exhibit
    low entropy while low-order bits exhibit high entropy. BCD [\[44\]](#page-13-12)
    proposes to deduplicate low entropy bits within memory lines. EPC [\[29\]](#page-12-29)
    builds on a similar observation and stores a set of frequent patterns of low entropy
    bits. XOR Cache exploits this similar observation with its map function. Wang
    et al. [\[51\]](#page-13-6) also propose a cache compression scheme by XORing
    multiple lines using in-SRAM bit-line computing. However, it does not target redundancy
    due to inclusion as XOR Cache does. Bunker [\[46\]](#page-13-13) and Doppelgänger
    [\[47\]](#page-13-4) cache propose to exploit similarity based on address and
    value, respectively. They work well for approximate computing on image processing
    workloads. Going beyond the granularity of cache lines, Zippads [\[50\]](#page-13-15)
    compresses objects of the same type. SC2 [\[9,](#page-12-39) [10\]](#page-12-4)
    builds Huffman coding based on value statistics and exploits word-level redundancy.
    XOR Cache fits into the cache compression landscape as a new inter-line compression
    work. Similar to prior works [\[24,](#page-12-6) [46,](#page-13-13) [49\]](#page-13-5)
    on inter-line compression, XOR Cache also adopts a hash table-based approach to
    capture similarity across lines. However, the goal is to pair every two lines
    to create structured sparsity to catalyze intra-line compression.


    #### <span id="page-11-1"></span>8 Conclusion


    In this paper, we introduce the XOR Cache, which exploits data redundancy due
    to inclusion and private caching to perform effective inter-line and intra-line
    compression simultaneously. Unlike conventional caches, XOR Cache stores bitwise
    XOR results of line pairs, effectively collocating a pair of lines in a single
    slot. When combined with other compression schemes, XOR Cache can further boost
    compression ratio by exploiting the compressibility of the XORed data values.
    We tackle two key challenges in XOR Cache: designing the XOR policy based on value
    similarity and implementing the cache coherence protocol. Evaluation results show
    that as a result of XOR Cache''s effective compression, it reduces LLC area by
    1.93× and power by 1.92× with a comparable performance (average overhead of 2.06%)
    to a larger uncompressed cache, reducing energy-delay product by 26.3%.


    #### Acknowledgments


    We thank all reviewers for their valuable feedback. This work is supported by
    the Wisconsin Alumni Research Foundation and NSF under award No. CNS-2045985.


    <span id="page-11-4"></span><sup>6</sup>Most 8-core multi-programmed SPEC runs
    fail to complete due to limited memory.


    The XOR Cache: A Catalyst for Compression ISCA ''25, June 21–25, 2025, Tokyo,
    Japan


    #### References


    - <span id="page-12-18"></span>[1] Jaume Abella, Antonio González, Xavier Vera,
    and Michael F. P. O''Boyle. 2005. IATAC: a smart predictor to turn-off L2 cache
    lines. ACM Trans. Archit. Code Optim. 2, 1 (March 2005), 55–77. [doi:10.1145/1061267.1061271](https://doi.org/10.1145/1061267.1061271)

    - <span id="page-12-31"></span>[2] Shaizeen Aga, Supreet Jeloka, Arun Subramaniyan,
    Satish Narayanasamy, David Blaauw, and Reetuparna Das. 2017. Compute Caches. In
    International Symposium on High Performance Computer Architecture.

    - <span id="page-12-2"></span>[3] Alaa R. Alameldeen and David A. Wood. 2004.
    Frequent Pattern Compression: A Significance-Based Compression Scheme for L2 Caches.
    Technical Report. University of Wisconsin-Madison, Department of Computer Sciences.

    - <span id="page-12-14"></span>[4] Jorge Albericio, Pablo Ibáñez, Victor Viñals,
    and José M. Llabería. 2013. The reuse cache: Downsizing the shared last-level
    cache. In International Symposium on Microarchitecture.

    - <span id="page-12-20"></span>[5] D.H. Albonesi. 1999. Selective cache ways:
    on-demand cache resource allocation. In MICRO-32. Proceedings of the 32nd Annual
    ACM/IEEE International Symposium on Microarchitecture. 248–259. [doi:10.1109/MICRO.1999.809463](https://doi.org/10.1109/MICRO.1999.809463)

    - <span id="page-12-13"></span>[6] Mohammad Alian, Siddharth Agarwal, Jongmin
    Shin, Neel Patel, Yifan Yuan Yuan, Daehoon Kim, and Ren Wang. 2022. IDIO: Network-Driven,
    Inbound Network Data Orchestration on Server Processors. In International Symposium
    on Microarchitecture.

    - <span id="page-12-26"></span>[7] Alexandra Angerd, Angelos Arelakis, Vasilis
    Spiliopoulos, Erik Sintorn, and Per Stenström. 2022. GBDI: Going Beyond Base-Delta-Immediate
    Compression with Global Bases. In 2022 IEEE International Symposium on High-Performance
    Computer Architecture (HPCA). 1115–1127. [doi:10.1109/HPCA53966.2022.00085](https://doi.org/10.1109/HPCA53966.2022.00085)

    - <span id="page-12-3"></span>[8] Angelos Arelakis, Fredrik Dahlgren, and Per
    Stenstrom. 2015. HyComp: a hybrid cache compression method for selection of data-type-specific
    compression methods. In Proceedings of the 48th International Symposium on Microarchitecture
    (Waikiki, Hawaii) (MICRO-48). Association for Computing Machinery, New York, NY,
    USA, 38–49. [doi:10.1145/2830772.2830823](https://doi.org/10.1145/2830772.2830823)

    - <span id="page-12-39"></span>[9] Angelos Arelakis and Per Stenstrom. 2014. SC2:
    a statistical compression cache scheme. In Proceeding of the 41st Annual International
    Symposium on Computer Architecuture (ISCA) (Minneapolis, Minnesota, USA) (ISCA
    ''14). IEEE Press, 145–156.

    - <span id="page-12-4"></span>[10] Angelos Arelakis and Per Stenstrom. 2014. SC2:
    a statistical compression cache scheme. SIGARCH Comput. Archit. News 42, 3 (jun
    2014), 145–156. [doi:10.1145/](https://doi.org/10.1145/2678373.2665696) [2678373.2665696](https://doi.org/10.1145/2678373.2665696)

    - <span id="page-12-34"></span>[11] Rajeev Balasubramonian, Andrew B. Kahng, Naveen
    Muralimanohar, Ali Shafiee, and Vaishnav Srinivas. 2017. CACTI 7: New Tools for
    Interconnect Exploration in Innovative Off-Chip Memories. ACM Trans. Archit. Code
    Optim. 14, 2, Article 14 (jun 2017), 25 pages. [doi:10.1145/3085572](https://doi.org/10.1145/3085572)

    - <span id="page-12-35"></span>[12] Kevin Barker, Thomas Benson, Dan Campbell,
    David Ediger, Roberto Gioiosa, Adolfy Hoisie, Darren Kerbyson, Joseph Manzano,
    Andres Marquez, Leon Song, Nathan Tallent, and Antonino Tumeo. 2013. PERFECT (Power
    Efficiency Revolution For Embedded Computing Technologies) Benchmark Suite Manual.
    Pacific Northwest National Laboratory and Georgia Tech Research Institute. [http:](http://hpc.pnnl.gov/projects/PERFECT/)
    [//hpc.pnnl.gov/projects/PERFECT/.](http://hpc.pnnl.gov/projects/PERFECT/)

    - <span id="page-12-36"></span>[13] Christian Bienia, Sanjeev Kumar, Jaswinder
    Pal Singh, and Kai Li. 2008. The PARSEC benchmark suite: Characterization and
    architectural implications. In International Conference on Parallel Architectures
    and Compilation Techniques.

    - <span id="page-12-37"></span>[14] James Bucek, Klaus-Dieter Lange, and Jóakim
    v. Kistowski. 2018. SPEC CPU2017: Next-Generation Compute Benchmark. In Companion
    of the 2018 ACM/SPEC International Conference on Performance Engineering (Berlin,
    Germany) (ICPE ''18). Association for Computing Machinery, New York, NY, USA,
    41–42. [doi:10.1145/](https://doi.org/10.1145/3185768.3185771) [3185768.3185771](https://doi.org/10.1145/3185768.3185771)

    - <span id="page-12-7"></span>[15] Mainak Chaudhuri. 2021. Zero Inclusion Victim:
    Isolating Core Caches from Inclusive Last-level Cache Evictions. In 2021 ACM/IEEE
    48th Annual International Symposium on Computer Architecture (ISCA).

    - <span id="page-12-5"></span>[16] Xi Chen, Lei Yang, Robert P. Dick, Li Shang,
    and Haris Lekatsas. 2010. C-pack: a high-performance microprocessor cache compression
    algorithm. IEEE Trans. Very Large Scale Integr. Syst. 18, 8 (aug 2010), 1196–1208.
    [doi:10.1109/TVLSI.2009.](https://doi.org/10.1109/TVLSI.2009.2020989) [2020989](https://doi.org/10.1109/TVLSI.2009.2020989)

    - <span id="page-12-38"></span>[17] Grigory Chirkov and David Wentzlaff. 2023.
    Seizing the Bandwidth Scaling of On-Package Interconnect in a Post-Moore''s Law
    World. In Proceedings of the 37th International Conference on Supercomputing (Orlando,
    FL, USA) (ICS ''23). Association for Computing Machinery, New York, NY, USA, 410–422.

    - <span id="page-12-27"></span>[18] Esha Choukse, Mattan Erez, and Alaa R. Alameldeen.
    2018. Compresso: Pragmatic Main Memory Compression. In 2018 51st Annual IEEE/ACM
    International Symposium on Microarchitecture (MICRO). 546–558. [doi:10.1109/MICRO.2018.00051](https://doi.org/10.1109/MICRO.2018.00051)

    - <span id="page-12-32"></span>[19] David L. Dill. 1996. The Mur verification
    system. In Computer Aided Verification, Rajeev Alur and Thomas A. Henzinger (Eds.).
    Springer Berlin Heidelberg, Berlin, Heidelberg, 390–393.

    - <span id="page-12-22"></span>[20] Ronald G. Dreslinski, Gregory K. Chen, Trevor
    Mudge, David Blaauw, Dennis Sylvester, and Krisztian Flautner. 2008. Reconfigurable
    energy efficient near threshold cache architectures. In 2008 41st IEEE/ACM International
    Symposium on Microarchitecture. 459–470. [doi:10.1109/MICRO.2008.4771813](https://doi.org/10.1109/MICRO.2008.4771813)

    - <span id="page-12-28"></span>[21] Albin Eldstål-Ahrens, Angelos Arelakis, and
    Ioannis Sourdis. 2023. FlatPack: Flexible Compaction of Compressed Memory. In
    Proceedings of the International Conference on Parallel Architectures and Compilation
    Techniques (Chicago, Illinois)


    (PACT ''22). Association for Computing Machinery, New York, NY, USA, 96–108. [doi:10.1145/3559009.3569653](https://doi.org/10.1145/3559009.3569653)


    - <span id="page-12-17"></span>[22] Ricardo Fernández-Pascual, Alberto Ros, and
    Manuel E. Acacio. 2017. To Be Silent or Not: On the Impact of Evictions of Clean
    Data in Cache-Coherent Multicores. J. Supercomput. 73, 10 (oct 2017), 4428–4443.

    - <span id="page-12-21"></span>[23] Krisztián Flautner, Nam Sung Kim, Steve Martin,
    David Blaauw, and Trevor Mudge. 2002. Drowsy caches: simple techniques for reducing
    leakage power. In Proceedings of the 29th Annual International Symposium on Computer
    Architecture (Anchorage, Alaska) (ISCA ''02). IEEE Computer Society, USA, 148–157.

    - <span id="page-12-6"></span>[24] Amin Ghasemazar, Prashant Nair, and Mieszko
    Lis. 2020. Thesaurus: Efficient Cache Compression via Dynamic Clustering. In Proceedings
    of the Twenty-Fifth International Conference on Architectural Support for Programming
    Languages and Operating Systems (Lausanne, Switzerland) (ASPLOS ''20). Association
    for Computing Machinery, New York, NY, USA, 527–540. [doi:10.1145/3373376.3378518](https://doi.org/10.1145/3373376.3378518)

    - <span id="page-12-15"></span>[25] Gorka Irazoqui, Thomas Eisenbarth, and Berk
    Sunar. 2016. Cross Processor Cache Attacks. In Proceedings of the 11th ACM on
    Asia Conference on Computer and Communications Security (Xi''an, China) (ASIA
    CCS ''16). Association for Computing Machinery, New York, NY, USA, 353–364. [doi:10.1145/2897845.2897867](https://doi.org/10.1145/2897845.2897867)

    - <span id="page-12-8"></span>[26] Aamer Jaleel, Eric Borch, Malini Bhandaru,
    Simon C. Steely Jr., and Joel Emer. 2010. Achieving Non-Inclusive Cache Performance
    with Inclusive Caches: Temporal Locality Aware (TLA) Cache Management Policies.
    In International Symposium on Microarchitecture.

    - <span id="page-12-19"></span>[27] S. Kaxiras, Zhigang Hu, and M. Martonosi.
    2001. Cache decay: exploiting generational behavior to reduce cache leakage power.
    In Proceedings 28th Annual International Symposium on Computer Architecture. 240–251.
    [doi:10.1109/ISCA.](https://doi.org/10.1109/ISCA.2001.937453) [2001.937453](https://doi.org/10.1109/ISCA.2001.937453)

    - <span id="page-12-23"></span>[28] Samira M. Khan, Alaa R. Alameldeen, Chris
    Wilkerson, Jaydeep Kulkarni, and Daniel A. Jiménez. 2013. Improving multi-core
    performance using mixed-cell cache architecture. In 2013 IEEE 19th International
    Symposium on High Performance Computer Architecture (HPCA). 119–130. [doi:10.1109/HPCA.2013.6522312](https://doi.org/10.1109/HPCA.2013.6522312)

    - <span id="page-12-29"></span>[29] Jinkwon Kim, Mincheol Kang, Jeongkyu Hong,
    and Soontae Kim. 2022. Exploiting Inter-block Entropy to Enhance the Compressibility
    of Blocks with Diverse Data. In 2022 IEEE International Symposium on High-Performance
    Computer Architecture (HPCA). 1100–1114. [doi:10.1109/HPCA53966.2022.00084](https://doi.org/10.1109/HPCA53966.2022.00084)

    - <span id="page-12-11"></span>[30] Jungrae Kim, Michael Sullivan, Esha Choukse,
    and Mattan Erez. 2016. Bit-Plane Compression: Transforming Data for Better Compression
    in Many-Core Architectures. In 2016 ACM/IEEE 43rd Annual International Symposium
    on Computer Architecture (ISCA). 329–340. [doi:10.1109/ISCA.2016.37](https://doi.org/10.1109/ISCA.2016.37)

    - <span id="page-12-16"></span>[31] Sowoong Kim, Myeonggyun Han, and Woongki Baek.
    2022. DPrime+DAbort: A High-Precision and Timer-Free Directory-Based Side-Channel
    Attack in Non-Inclusive Cache Hierarchies using Intel TSX. In 2022 IEEE International
    Symposium on High-Performance Computer Architecture (HPCA). 67–81. [doi:10.1109/](https://doi.org/10.1109/HPCA53966.2022.00014)
    [HPCA53966.2022.00014](https://doi.org/10.1109/HPCA53966.2022.00014)

    - <span id="page-12-25"></span>[32] Soontae Kim, Jongmin Lee, Jesung Kim, and
    Seokin Hong. 2011. Residue cache: a low-energy low-area L2 cache architecture
    via compression and partial hits. In Proceedings of the 44th Annual IEEE/ACM International
    Symposium on Microarchitecture (Porto Alegre, Brazil) (MICRO-44). Association
    for Computing Machinery, New York, NY, USA, 420–429. [doi:10.1145/2155620.2155670](https://doi.org/10.1145/2155620.2155670)

    - <span id="page-12-24"></span>[33] J. Kin, Munish Gupta, and W.H. Mangione-Smith.
    1997. The filter cache: an energy efficient memory structure. In Proceedings of
    30th Annual International Symposium on Microarchitecture. 184–193. [doi:10.1109/MICRO.1997.645809](https://doi.org/10.1109/MICRO.1997.645809)

    - <span id="page-12-33"></span>[34] Weihang Li, Andrés Goens, Nicolai Oswald,
    Vijay Nagarajan, and Daniel J. Sorin. 2024. Determining the Minimum Number of
    Virtual Networks for Different Coherence Protocols. In 2024 ACM/IEEE 51st Annual
    International Symposium on Computer Architecture (ISCA).

    - <span id="page-12-30"></span>[35] Y. Li and M. Gao. 2023. Baryon: Efficient
    Hybrid Memory Management with Compression and Sub-Blocking. In 2023 IEEE International
    Symposium on High-Performance Computer Architecture (HPCA). IEEE Computer Society,
    Los Alamitos, CA, USA, 137–151. [doi:10.1109/HPCA56546.2023.10071115](https://doi.org/10.1109/HPCA56546.2023.10071115)

    - <span id="page-12-12"></span>[36] Jason Lowe-Power, Abdul Mutaal Ahmad, Ayaz
    Akram, Mohammad Alian, Rico Amslinger, Matteo Andreozzi, Adrià Armejach, Nils
    Asmussen, Brad Beckmann, Srikant Bharadwaj, et al. 2020. The gem5 simulator: Version
    20.0+. arXiv preprint arXiv:2007.03152 (2020).

    - <span id="page-12-9"></span>[37] Milo M. K. Martin, Mark D. Hill, and Daniel
    J. Sorin. 2012. Why On-Chip Cache Coherence is Here to Stay. Commun. ACM 55, 7
    (jul 2012), 78–89. [doi:10.1145/](https://doi.org/10.1145/2209249.2209269) [2209249.2209269](https://doi.org/10.1145/2209249.2209269)

    - <span id="page-12-1"></span>[38] Hassan Mujtaba. 2020. AMD Ryzen 5000 Zen 3
    ''Vermeer'' Undressed, First Ever High-Res Die Shots Close Ups Pictured & Detailed.
    [https://wccftech.com/amd](https://wccftech.com/amd-ryzen-5000-zen-3-vermeer-undressed-high-res-die-shots-close-ups-pictured-detailed/)[ryzen-5000-zen-3-vermeer-undressed-high-res-die-shots-close-ups-pictured](https://wccftech.com/amd-ryzen-5000-zen-3-vermeer-undressed-high-res-die-shots-close-ups-pictured-detailed/)[detailed/](https://wccftech.com/amd-ryzen-5000-zen-3-vermeer-undressed-high-res-die-shots-close-ups-pictured-detailed/)

    - <span id="page-12-0"></span>[39] Ashley O. Munch, Nevine Nassif, Carleton L.
    Molnar, Jason Crop, Rich Gammack, Chinmay P. Joshi, Goran Zelic, Kambiz Munshi,
    Min Huang, Charles R. Morganti, Sireesha Kandula, and Arijit Biswas. 2024. 2.3
    Emerald Rapids: 5th-Generation Intel® Xeon® Scalable Processors. In 2024 IEEE
    International Solid-State Circuits Conference (ISSCC), Vol. 67. 40–42. [doi:10.1109/ISSCC49657.2024.10454434](https://doi.org/10.1109/ISSCC49657.2024.10454434)

    - <span id="page-12-10"></span>[40] Vijay Nagarajan, Daniel J. Sorin, Mark D.
    Hill, and David A. Wood. 2020. A Primer on Memory Consistency and Cache Coherence,
    Second Edition. Springer Cham.

    - <span id="page-13-1"></span><span id="page-13-0"></span>[41] Tri M. Nguyen and
    David Wentzlaff. 2015. MORC: a manycore-oriented compressed cache. In Proceedings
    of the 48th International Symposium on Microarchitecture (Waikiki, Hawaii) (MICRO-48).
    Association for Computing Machinery, New York, NY, USA, 76–88. [doi:10.1145/2830772.2830828](https://doi.org/10.1145/2830772.2830828)

    - <span id="page-13-2"></span>[42] Biswabandan Panda and André Seznec. 2016. Dictionary
    sharing: An efficient cache compression scheme for compressed caches. In 2016
    49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO). 1–12.
    [doi:10.1109/MICRO.](https://doi.org/10.1109/MICRO.2016.7783704) [2016.7783704](https://doi.org/10.1109/MICRO.2016.7783704)

    - [43] Gagandeep Panwar, Muhammad Laghari, Esha Choukse, and Xun Jian. 2024. DyLeCT:
    Achieving Huge-page-like Translation Performance for Hardwarecompressed Memory.
    In 2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture
    (ISCA).

    - <span id="page-13-12"></span>[44] Sungbo Park, Ingab Kang, Yaebin Moon, Jung
    Ho Ahn, and G. Edward Suh. 2021. BCD deduplication: effective memory compression
    using partial cacheline deduplication. In Proceedings of the 26th ACM International
    Conference on Architectural Support for Programming Languages and Operating Systems
    (Virtual, USA) (ASPLOS ''21). Association for Computing Machinery, New York, NY,
    USA, 52–64. [doi:10.1145/3445814.3446722](https://doi.org/10.1145/3445814.3446722)

    - <span id="page-13-3"></span>[45] Gennady Pekhimenko, Vivek Seshadri, Onur Mutlu,
    Phillip B. Gibbons, Michael A. Kozuch, and Todd C. Mowry. 2012. Base-delta-immediate
    compression: practical data compression for on-chip caches. In Proceedings of
    the 21st International Conference on Parallel Architectures and Compilation Techniques
    (Minneapolis, Minnesota, USA) (PACT ''12). Association for Computing Machinery,
    New York, NY, USA, 377–388. [doi:10.1145/2370816.2370870](https://doi.org/10.1145/2370816.2370870)

    - <span id="page-13-13"></span>[46] Joshua San Miguel, Jorge Albericio, Natalie
    Enright Jerger, and Aamer Jaleel. 2016. The Bunker Cache for spatio-value approximation.
    In 2016 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO).
    1–12. [doi:10.](https://doi.org/10.1109/MICRO.2016.7783746) [1109/MICRO.2016.7783746](https://doi.org/10.1109/MICRO.2016.7783746)

    - <span id="page-13-4"></span>[47] Joshua San Miguel, Jorge Albericio, Andreas
    Moshovos, and Natalie Enright Jerger. 2015. Doppelgänger: A cache for approximate
    computing. In 2015 48th Annual IEEE/ACM International Symposium on Microarchitecture
    (MICRO). 50–61. [doi:10.1145/2830772.2830790](https://doi.org/10.1145/2830772.2830790)

    - <span id="page-13-8"></span>[48] Yingying Tian, Samira M. Khan, and Daniel A.
    Jiménez. 2013. Temporal-Based Multilevel Correlating Inclusive Cache Replacement.
    ACM Trans. Archit. Code


    Optim. 10, 4, Article 33 (dec 2013), 24 pages. [doi:10.1145/2541228.2555290](https://doi.org/10.1145/2541228.2555290)


    - <span id="page-13-5"></span>[49] Yingying Tian, Samira M. Khan, Daniel A. Jiménez,
    and Gabriel H. Loh. 2014. Lastlevel cache deduplication. In Proceedings of the
    28th ACM International Conference on Supercomputing (Munich, Germany) (ICS ''14).
    Association for Computing Machinery, New York, NY, USA, 53–62. [doi:10.1145/2597652.2597655](https://doi.org/10.1145/2597652.2597655)

    - <span id="page-13-15"></span>[50] Po-An Tsai and Daniel Sanchez. 2019. Compress
    Objects, Not Cache Lines: An Object-Based Compressed Memory Hierarchy. In International
    Conference on Architectural Support for Programming Languages and Operating Systems.

    - <span id="page-13-6"></span>[51] Xiaowei Wang, Charles Augustine, Eriko Nurvitadhi,
    Ravi Iyer, Li Zhao, and Reetuparna Das. 2021. Cache Compression with Efficient
    in-SRAM Data Comparison. In 2021 IEEE International Conference on Networking,
    Architecture and Storage (NAS). 1–8. [doi:10.1109/NAS51552.2021.9605440](https://doi.org/10.1109/NAS51552.2021.9605440)

    - <span id="page-13-14"></span>[52] P.T. Wolkotte, G.J.M. Smit, N. Kavaldjiev,
    J.E. Becker, and J. Becker. 2005. Energy Model of Networks-on-Chip and a Bus.
    In 2005 International Symposium on System-on-Chip. 82–85. [doi:10.1109/ISSOC.2005.1595650](https://doi.org/10.1109/ISSOC.2005.1595650)

    - <span id="page-13-10"></span>[53] Mengjia Yan, Read Sprabery, Bhargava Gopireddy,
    Christopher Fletcher, Roy Campbell, and Josep Torrellas. 2019. Attack Directories,
    Not Caches: Side Channel Attacks in a Non-Inclusive World. In 2019 IEEE Symposium
    on Security and Privacy (SP). 888–904. [doi:10.1109/SP.2019.00004](https://doi.org/10.1109/SP.2019.00004)

    - <span id="page-13-7"></span>[54] Jun Yang, Youtao Zhang, and Rajiv Gupta. 2000.
    Frequent value compression in data caches. In Proceedings of the 33rd Annual ACM/IEEE
    International Symposium on Microarchitecture (Monterey, California, USA) (MICRO
    33). Association for Computing Machinery, New York, NY, USA, 258–265. [doi:10.1145/360128.360154](https://doi.org/10.1145/360128.360154)

    - <span id="page-13-9"></span>[55] Li Zhao, Ravi Iyer, Srihari Makineni, Don Newell,
    and Liqun Cheng. 2010. NCID: A Non-Inclusive Cache, Inclusive Directory Architecture
    for Flexible and Efficient Cache Hierarchies. In Proceedings of the 7th ACM International
    Conference on Computing Frontiers.

    - <span id="page-13-11"></span>[56] Zirui Neil Zhao, Adam Morrison, Christopher
    W. Fletcher, and Josep Torrellas. 2024. Last-Level Cache Side-Channel Attacks
    Are Feasible in the Modern Public Cloud. In Proceedings of the 29th ACM International
    Conference on Architectural Support for Programming Languages and Operating Systems,
    Volume 2 (La Jolla, CA, USA) (ASPLOS ''24). Association for Computing Machinery,
    New York, NY, USA, 582–600. [doi:10.1145/3620665.3640403](https://doi.org/10.1145/3620665.3640403)'
  references:
  - '- <span id="page-12-18"></span>[1] Jaume Abella, Antonio González, Xavier Vera,
    and Michael F. P. O''Boyle. 2005. IATAC: a smart predictor to turn-off L2 cache
    lines. ACM Trans. Archit. Code Optim. 2, 1 (March 2005), 55–77. [doi:10.1145/1061267.1061271](https://doi.org/10.1145/1061267.1061271)'
  - '- <span id="page-12-31"></span>[2] Shaizeen Aga, Supreet Jeloka, Arun Subramaniyan,
    Satish Narayanasamy, David Blaauw, and Reetuparna Das. 2017. Compute Caches. In
    International Symposium on High Performance Computer Architecture.'
  - '- <span id="page-12-2"></span>[3] Alaa R. Alameldeen and David A. Wood. 2004.
    Frequent Pattern Compression: A Significance-Based Compression Scheme for L2 Caches.
    Technical Report. University of Wisconsin-Madison, Department of Computer Sciences.'
  - '- <span id="page-12-14"></span>[4] Jorge Albericio, Pablo Ibáñez, Victor Viñals,
    and José M. Llabería. 2013. The reuse cache: Downsizing the shared last-level
    cache. In International Symposium on Microarchitecture.'
  - '- <span id="page-12-20"></span>[5] D.H. Albonesi. 1999. Selective cache ways:
    on-demand cache resource allocation. In MICRO-32. Proceedings of the 32nd Annual
    ACM/IEEE International Symposium on Microarchitecture. 248–259. [doi:10.1109/MICRO.1999.809463](https://doi.org/10.1109/MICRO.1999.809463)'
  - '- <span id="page-12-13"></span>[6] Mohammad Alian, Siddharth Agarwal, Jongmin
    Shin, Neel Patel, Yifan Yuan Yuan, Daehoon Kim, and Ren Wang. 2022. IDIO: Network-Driven,
    Inbound Network Data Orchestration on Server Processors. In International Symposium
    on Microarchitecture.'
  - '- <span id="page-12-26"></span>[7] Alexandra Angerd, Angelos Arelakis, Vasilis
    Spiliopoulos, Erik Sintorn, and Per Stenström. 2022. GBDI: Going Beyond Base-Delta-Immediate
    Compression with Global Bases. In 2022 IEEE International Symposium on High-Performance
    Computer Architecture (HPCA). 1115–1127. [doi:10.1109/HPCA53966.2022.00085](https://doi.org/10.1109/HPCA53966.2022.00085)'
  - '- <span id="page-12-3"></span>[8] Angelos Arelakis, Fredrik Dahlgren, and Per
    Stenstrom. 2015. HyComp: a hybrid cache compression method for selection of data-type-specific
    compression methods. In Proceedings of the 48th International Symposium on Microarchitecture
    (Waikiki, Hawaii) (MICRO-48). Association for Computing Machinery, New York, NY,
    USA, 38–49. [doi:10.1145/2830772.2830823](https://doi.org/10.1145/2830772.2830823)'
  - '- <span id="page-12-39"></span>[9] Angelos Arelakis and Per Stenstrom. 2014.
    SC2: a statistical compression cache scheme. In Proceeding of the 41st Annual
    International Symposium on Computer Architecuture (ISCA) (Minneapolis, Minnesota,
    USA) (ISCA ''14). IEEE Press, 145–156.'
  - '- <span id="page-12-4"></span>[10] Angelos Arelakis and Per Stenstrom. 2014.
    SC2: a statistical compression cache scheme. SIGARCH Comput. Archit. News 42,
    3 (jun 2014), 145–156. [doi:10.1145/](https://doi.org/10.1145/2678373.2665696)
    [2678373.2665696](https://doi.org/10.1145/2678373.2665696)'
  - '- <span id="page-12-34"></span>[11] Rajeev Balasubramonian, Andrew B. Kahng,
    Naveen Muralimanohar, Ali Shafiee, and Vaishnav Srinivas. 2017. CACTI 7: New Tools
    for Interconnect Exploration in Innovative Off-Chip Memories. ACM Trans. Archit.
    Code Optim. 14, 2, Article 14 (jun 2017), 25 pages. [doi:10.1145/3085572](https://doi.org/10.1145/3085572)'
  - '- <span id="page-12-35"></span>[12] Kevin Barker, Thomas Benson, Dan Campbell,
    David Ediger, Roberto Gioiosa, Adolfy Hoisie, Darren Kerbyson, Joseph Manzano,
    Andres Marquez, Leon Song, Nathan Tallent, and Antonino Tumeo. 2013. PERFECT (Power
    Efficiency Revolution For Embedded Computing Technologies) Benchmark Suite Manual.
    Pacific Northwest National Laboratory and Georgia Tech Research Institute. [http:](http://hpc.pnnl.gov/projects/PERFECT/)
    [//hpc.pnnl.gov/projects/PERFECT/.](http://hpc.pnnl.gov/projects/PERFECT/)'
  - '- <span id="page-12-36"></span>[13] Christian Bienia, Sanjeev Kumar, Jaswinder
    Pal Singh, and Kai Li. 2008. The PARSEC benchmark suite: Characterization and
    architectural implications. In International Conference on Parallel Architectures
    and Compilation Techniques.'
  - '- <span id="page-12-37"></span>[14] James Bucek, Klaus-Dieter Lange, and Jóakim
    v. Kistowski. 2018. SPEC CPU2017: Next-Generation Compute Benchmark. In Companion
    of the 2018 ACM/SPEC International Conference on Performance Engineering (Berlin,
    Germany) (ICPE ''18). Association for Computing Machinery, New York, NY, USA,
    41–42. [doi:10.1145/](https://doi.org/10.1145/3185768.3185771) [3185768.3185771](https://doi.org/10.1145/3185768.3185771)'
  - '- <span id="page-12-7"></span>[15] Mainak Chaudhuri. 2021. Zero Inclusion Victim:
    Isolating Core Caches from Inclusive Last-level Cache Evictions. In 2021 ACM/IEEE
    48th Annual International Symposium on Computer Architecture (ISCA).'
  - '- <span id="page-12-5"></span>[16] Xi Chen, Lei Yang, Robert P. Dick, Li Shang,
    and Haris Lekatsas. 2010. C-pack: a high-performance microprocessor cache compression
    algorithm. IEEE Trans. Very Large Scale Integr. Syst. 18, 8 (aug 2010), 1196–1208.
    [doi:10.1109/TVLSI.2009.](https://doi.org/10.1109/TVLSI.2009.2020989) [2020989](https://doi.org/10.1109/TVLSI.2009.2020989)'
  - '- <span id="page-12-38"></span>[17] Grigory Chirkov and David Wentzlaff. 2023.
    Seizing the Bandwidth Scaling of On-Package Interconnect in a Post-Moore''s Law
    World. In Proceedings of the 37th International Conference on Supercomputing (Orlando,
    FL, USA) (ICS ''23). Association for Computing Machinery, New York, NY, USA, 410–422.'
  - '- <span id="page-12-27"></span>[18] Esha Choukse, Mattan Erez, and Alaa R. Alameldeen.
    2018. Compresso: Pragmatic Main Memory Compression. In 2018 51st Annual IEEE/ACM
    International Symposium on Microarchitecture (MICRO). 546–558. [doi:10.1109/MICRO.2018.00051](https://doi.org/10.1109/MICRO.2018.00051)'
  - '- <span id="page-12-32"></span>[19] David L. Dill. 1996. The Mur verification
    system. In Computer Aided Verification, Rajeev Alur and Thomas A. Henzinger (Eds.).
    Springer Berlin Heidelberg, Berlin, Heidelberg, 390–393.'
  - '- <span id="page-12-22"></span>[20] Ronald G. Dreslinski, Gregory K. Chen, Trevor
    Mudge, David Blaauw, Dennis Sylvester, and Krisztian Flautner. 2008. Reconfigurable
    energy efficient near threshold cache architectures. In 2008 41st IEEE/ACM International
    Symposium on Microarchitecture. 459–470. [doi:10.1109/MICRO.2008.4771813](https://doi.org/10.1109/MICRO.2008.4771813)'
  - '- <span id="page-12-28"></span>[21] Albin Eldstål-Ahrens, Angelos Arelakis, and
    Ioannis Sourdis. 2023. FlatPack: Flexible Compaction of Compressed Memory. In
    Proceedings of the International Conference on Parallel Architectures and Compilation
    Techniques (Chicago, Illinois)'
  - (PACT '22). Association for Computing Machinery, New York, NY, USA, 96–108. [doi:10.1145/3559009.3569653](https://doi.org/10.1145/3559009.3569653)
  - '- <span id="page-12-17"></span>[22] Ricardo Fernández-Pascual, Alberto Ros, and
    Manuel E. Acacio. 2017. To Be Silent or Not: On the Impact of Evictions of Clean
    Data in Cache-Coherent Multicores. J. Supercomput. 73, 10 (oct 2017), 4428–4443.'
  - '- <span id="page-12-21"></span>[23] Krisztián Flautner, Nam Sung Kim, Steve Martin,
    David Blaauw, and Trevor Mudge. 2002. Drowsy caches: simple techniques for reducing
    leakage power. In Proceedings of the 29th Annual International Symposium on Computer
    Architecture (Anchorage, Alaska) (ISCA ''02). IEEE Computer Society, USA, 148–157.'
  - '- <span id="page-12-6"></span>[24] Amin Ghasemazar, Prashant Nair, and Mieszko
    Lis. 2020. Thesaurus: Efficient Cache Compression via Dynamic Clustering. In Proceedings
    of the Twenty-Fifth International Conference on Architectural Support for Programming
    Languages and Operating Systems (Lausanne, Switzerland) (ASPLOS ''20). Association
    for Computing Machinery, New York, NY, USA, 527–540. [doi:10.1145/3373376.3378518](https://doi.org/10.1145/3373376.3378518)'
  - '- <span id="page-12-15"></span>[25] Gorka Irazoqui, Thomas Eisenbarth, and Berk
    Sunar. 2016. Cross Processor Cache Attacks. In Proceedings of the 11th ACM on
    Asia Conference on Computer and Communications Security (Xi''an, China) (ASIA
    CCS ''16). Association for Computing Machinery, New York, NY, USA, 353–364. [doi:10.1145/2897845.2897867](https://doi.org/10.1145/2897845.2897867)'
  - '- <span id="page-12-8"></span>[26] Aamer Jaleel, Eric Borch, Malini Bhandaru,
    Simon C. Steely Jr., and Joel Emer. 2010. Achieving Non-Inclusive Cache Performance
    with Inclusive Caches: Temporal Locality Aware (TLA) Cache Management Policies.
    In International Symposium on Microarchitecture.'
  - '- <span id="page-12-19"></span>[27] S. Kaxiras, Zhigang Hu, and M. Martonosi.
    2001. Cache decay: exploiting generational behavior to reduce cache leakage power.
    In Proceedings 28th Annual International Symposium on Computer Architecture. 240–251.
    [doi:10.1109/ISCA.](https://doi.org/10.1109/ISCA.2001.937453) [2001.937453](https://doi.org/10.1109/ISCA.2001.937453)'
  - '- <span id="page-12-23"></span>[28] Samira M. Khan, Alaa R. Alameldeen, Chris
    Wilkerson, Jaydeep Kulkarni, and Daniel A. Jiménez. 2013. Improving multi-core
    performance using mixed-cell cache architecture. In 2013 IEEE 19th International
    Symposium on High Performance Computer Architecture (HPCA). 119–130. [doi:10.1109/HPCA.2013.6522312](https://doi.org/10.1109/HPCA.2013.6522312)'
  - '- <span id="page-12-29"></span>[29] Jinkwon Kim, Mincheol Kang, Jeongkyu Hong,
    and Soontae Kim. 2022. Exploiting Inter-block Entropy to Enhance the Compressibility
    of Blocks with Diverse Data. In 2022 IEEE International Symposium on High-Performance
    Computer Architecture (HPCA). 1100–1114. [doi:10.1109/HPCA53966.2022.00084](https://doi.org/10.1109/HPCA53966.2022.00084)'
  - '- <span id="page-12-11"></span>[30] Jungrae Kim, Michael Sullivan, Esha Choukse,
    and Mattan Erez. 2016. Bit-Plane Compression: Transforming Data for Better Compression
    in Many-Core Architectures. In 2016 ACM/IEEE 43rd Annual International Symposium
    on Computer Architecture (ISCA). 329–340. [doi:10.1109/ISCA.2016.37](https://doi.org/10.1109/ISCA.2016.37)'
  - '- <span id="page-12-16"></span>[31] Sowoong Kim, Myeonggyun Han, and Woongki
    Baek. 2022. DPrime+DAbort: A High-Precision and Timer-Free Directory-Based Side-Channel
    Attack in Non-Inclusive Cache Hierarchies using Intel TSX. In 2022 IEEE International
    Symposium on High-Performance Computer Architecture (HPCA). 67–81. [doi:10.1109/](https://doi.org/10.1109/HPCA53966.2022.00014)
    [HPCA53966.2022.00014](https://doi.org/10.1109/HPCA53966.2022.00014)'
  - '- <span id="page-12-25"></span>[32] Soontae Kim, Jongmin Lee, Jesung Kim, and
    Seokin Hong. 2011. Residue cache: a low-energy low-area L2 cache architecture
    via compression and partial hits. In Proceedings of the 44th Annual IEEE/ACM International
    Symposium on Microarchitecture (Porto Alegre, Brazil) (MICRO-44). Association
    for Computing Machinery, New York, NY, USA, 420–429. [doi:10.1145/2155620.2155670](https://doi.org/10.1145/2155620.2155670)'
  - '- <span id="page-12-24"></span>[33] J. Kin, Munish Gupta, and W.H. Mangione-Smith.
    1997. The filter cache: an energy efficient memory structure. In Proceedings of
    30th Annual International Symposium on Microarchitecture. 184–193. [doi:10.1109/MICRO.1997.645809](https://doi.org/10.1109/MICRO.1997.645809)'
  - '- <span id="page-12-33"></span>[34] Weihang Li, Andrés Goens, Nicolai Oswald,
    Vijay Nagarajan, and Daniel J. Sorin. 2024. Determining the Minimum Number of
    Virtual Networks for Different Coherence Protocols. In 2024 ACM/IEEE 51st Annual
    International Symposium on Computer Architecture (ISCA).'
  - '- <span id="page-12-30"></span>[35] Y. Li and M. Gao. 2023. Baryon: Efficient
    Hybrid Memory Management with Compression and Sub-Blocking. In 2023 IEEE International
    Symposium on High-Performance Computer Architecture (HPCA). IEEE Computer Society,
    Los Alamitos, CA, USA, 137–151. [doi:10.1109/HPCA56546.2023.10071115](https://doi.org/10.1109/HPCA56546.2023.10071115)'
  - '- <span id="page-12-12"></span>[36] Jason Lowe-Power, Abdul Mutaal Ahmad, Ayaz
    Akram, Mohammad Alian, Rico Amslinger, Matteo Andreozzi, Adrià Armejach, Nils
    Asmussen, Brad Beckmann, Srikant Bharadwaj, et al. 2020. The gem5 simulator: Version
    20.0+. arXiv preprint arXiv:2007.03152 (2020).'
  - '- <span id="page-12-9"></span>[37] Milo M. K. Martin, Mark D. Hill, and Daniel
    J. Sorin. 2012. Why On-Chip Cache Coherence is Here to Stay. Commun. ACM 55, 7
    (jul 2012), 78–89. [doi:10.1145/](https://doi.org/10.1145/2209249.2209269) [2209249.2209269](https://doi.org/10.1145/2209249.2209269)'
  - '- <span id="page-12-1"></span>[38] Hassan Mujtaba. 2020. AMD Ryzen 5000 Zen 3
    ''Vermeer'' Undressed, First Ever High-Res Die Shots Close Ups Pictured & Detailed.
    [https://wccftech.com/amd](https://wccftech.com/amd-ryzen-5000-zen-3-vermeer-undressed-high-res-die-shots-close-ups-pictured-detailed/)[ryzen-5000-zen-3-vermeer-undressed-high-res-die-shots-close-ups-pictured](https://wccftech.com/amd-ryzen-5000-zen-3-vermeer-undressed-high-res-die-shots-close-ups-pictured-detailed/)[detailed/](https://wccftech.com/amd-ryzen-5000-zen-3-vermeer-undressed-high-res-die-shots-close-ups-pictured-detailed/)'
  - '- <span id="page-12-0"></span>[39] Ashley O. Munch, Nevine Nassif, Carleton L.
    Molnar, Jason Crop, Rich Gammack, Chinmay P. Joshi, Goran Zelic, Kambiz Munshi,
    Min Huang, Charles R. Morganti, Sireesha Kandula, and Arijit Biswas. 2024. 2.3
    Emerald Rapids: 5th-Generation Intel® Xeon® Scalable Processors. In 2024 IEEE
    International Solid-State Circuits Conference (ISSCC), Vol. 67. 40–42. [doi:10.1109/ISSCC49657.2024.10454434](https://doi.org/10.1109/ISSCC49657.2024.10454434)'
  - '- <span id="page-12-10"></span>[40] Vijay Nagarajan, Daniel J. Sorin, Mark D.
    Hill, and David A. Wood. 2020. A Primer on Memory Consistency and Cache Coherence,
    Second Edition. Springer Cham.'
  - '- <span id="page-13-1"></span><span id="page-13-0"></span>[41] Tri M. Nguyen
    and David Wentzlaff. 2015. MORC: a manycore-oriented compressed cache. In Proceedings
    of the 48th International Symposium on Microarchitecture (Waikiki, Hawaii) (MICRO-48).
    Association for Computing Machinery, New York, NY, USA, 76–88. [doi:10.1145/2830772.2830828](https://doi.org/10.1145/2830772.2830828)'
  - '- <span id="page-13-2"></span>[42] Biswabandan Panda and André Seznec. 2016.
    Dictionary sharing: An efficient cache compression scheme for compressed caches.
    In 2016 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO).
    1–12. [doi:10.1109/MICRO.](https://doi.org/10.1109/MICRO.2016.7783704) [2016.7783704](https://doi.org/10.1109/MICRO.2016.7783704)'
  - '- [43] Gagandeep Panwar, Muhammad Laghari, Esha Choukse, and Xun Jian. 2024.
    DyLeCT: Achieving Huge-page-like Translation Performance for Hardwarecompressed
    Memory. In 2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture
    (ISCA).'
  - '- <span id="page-13-12"></span>[44] Sungbo Park, Ingab Kang, Yaebin Moon, Jung
    Ho Ahn, and G. Edward Suh. 2021. BCD deduplication: effective memory compression
    using partial cacheline deduplication. In Proceedings of the 26th ACM International
    Conference on Architectural Support for Programming Languages and Operating Systems
    (Virtual, USA) (ASPLOS ''21). Association for Computing Machinery, New York, NY,
    USA, 52–64. [doi:10.1145/3445814.3446722](https://doi.org/10.1145/3445814.3446722)'
  - '- <span id="page-13-3"></span>[45] Gennady Pekhimenko, Vivek Seshadri, Onur Mutlu,
    Phillip B. Gibbons, Michael A. Kozuch, and Todd C. Mowry. 2012. Base-delta-immediate
    compression: practical data compression for on-chip caches. In Proceedings of
    the 21st International Conference on Parallel Architectures and Compilation Techniques
    (Minneapolis, Minnesota, USA) (PACT ''12). Association for Computing Machinery,
    New York, NY, USA, 377–388. [doi:10.1145/2370816.2370870](https://doi.org/10.1145/2370816.2370870)'
  - '- <span id="page-13-13"></span>[46] Joshua San Miguel, Jorge Albericio, Natalie
    Enright Jerger, and Aamer Jaleel. 2016. The Bunker Cache for spatio-value approximation.
    In 2016 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO).
    1–12. [doi:10.](https://doi.org/10.1109/MICRO.2016.7783746) [1109/MICRO.2016.7783746](https://doi.org/10.1109/MICRO.2016.7783746)'
  - '- <span id="page-13-4"></span>[47] Joshua San Miguel, Jorge Albericio, Andreas
    Moshovos, and Natalie Enright Jerger. 2015. Doppelgänger: A cache for approximate
    computing. In 2015 48th Annual IEEE/ACM International Symposium on Microarchitecture
    (MICRO). 50–61. [doi:10.1145/2830772.2830790](https://doi.org/10.1145/2830772.2830790)'
  - '- <span id="page-13-8"></span>[48] Yingying Tian, Samira M. Khan, and Daniel
    A. Jiménez. 2013. Temporal-Based Multilevel Correlating Inclusive Cache Replacement.
    ACM Trans. Archit. Code'
  - Optim. 10, 4, Article 33 (dec 2013), 24 pages. [doi:10.1145/2541228.2555290](https://doi.org/10.1145/2541228.2555290)
  - '- <span id="page-13-5"></span>[49] Yingying Tian, Samira M. Khan, Daniel A. Jiménez,
    and Gabriel H. Loh. 2014. Lastlevel cache deduplication. In Proceedings of the
    28th ACM International Conference on Supercomputing (Munich, Germany) (ICS ''14).
    Association for Computing Machinery, New York, NY, USA, 53–62. [doi:10.1145/2597652.2597655](https://doi.org/10.1145/2597652.2597655)'
  - '- <span id="page-13-15"></span>[50] Po-An Tsai and Daniel Sanchez. 2019. Compress
    Objects, Not Cache Lines: An Object-Based Compressed Memory Hierarchy. In International
    Conference on Architectural Support for Programming Languages and Operating Systems.'
  - '- <span id="page-13-6"></span>[51] Xiaowei Wang, Charles Augustine, Eriko Nurvitadhi,
    Ravi Iyer, Li Zhao, and Reetuparna Das. 2021. Cache Compression with Efficient
    in-SRAM Data Comparison. In 2021 IEEE International Conference on Networking,
    Architecture and Storage (NAS). 1–8. [doi:10.1109/NAS51552.2021.9605440](https://doi.org/10.1109/NAS51552.2021.9605440)'
  - '- <span id="page-13-14"></span>[52] P.T. Wolkotte, G.J.M. Smit, N. Kavaldjiev,
    J.E. Becker, and J. Becker. 2005. Energy Model of Networks-on-Chip and a Bus.
    In 2005 International Symposium on System-on-Chip. 82–85. [doi:10.1109/ISSOC.2005.1595650](https://doi.org/10.1109/ISSOC.2005.1595650)'
  - '- <span id="page-13-10"></span>[53] Mengjia Yan, Read Sprabery, Bhargava Gopireddy,
    Christopher Fletcher, Roy Campbell, and Josep Torrellas. 2019. Attack Directories,
    Not Caches: Side Channel Attacks in a Non-Inclusive World. In 2019 IEEE Symposium
    on Security and Privacy (SP). 888–904. [doi:10.1109/SP.2019.00004](https://doi.org/10.1109/SP.2019.00004)'
  - '- <span id="page-13-7"></span>[54] Jun Yang, Youtao Zhang, and Rajiv Gupta. 2000.
    Frequent value compression in data caches. In Proceedings of the 33rd Annual ACM/IEEE
    International Symposium on Microarchitecture (Monterey, California, USA) (MICRO
    33). Association for Computing Machinery, New York, NY, USA, 258–265. [doi:10.1145/360128.360154](https://doi.org/10.1145/360128.360154)'
  - '- <span id="page-13-9"></span>[55] Li Zhao, Ravi Iyer, Srihari Makineni, Don
    Newell, and Liqun Cheng. 2010. NCID: A Non-Inclusive Cache, Inclusive Directory
    Architecture for Flexible and Efficient Cache Hierarchies. In Proceedings of the
    7th ACM International Conference on Computing Frontiers.'
  - '- <span id="page-13-11"></span>[56] Zirui Neil Zhao, Adam Morrison, Christopher
    W. Fletcher, and Josep Torrellas. 2024. Last-Level Cache Side-Channel Attacks
    Are Feasible in the Modern Public Cloud. In Proceedings of the 29th ACM International
    Conference on Architectural Support for Programming Languages and Operating Systems,
    Volume 2 (La Jolla, CA, USA) (ASPLOS ''24). Association for Computing Machinery,
    New York, NY, USA, 582–600. [doi:10.1145/3620665.3640403](https://doi.org/10.1145/3620665.3640403)'
- id: heliostat_harnessing_ray_tracing_accelerators_for_page_table_walks_yuan_feng_https_orcid_org_0009_0004_2451_9568_university_of_california_merced_merced_california_usa_yfeng44_ucmerced_edu
  title: 'Heliostat: Harnessing Ray Tracing Accelerators for Page Table Walks'
  abstract: This paper introduces Heliostat, which enhances page translation bandwidth
    on GPUs by harnessing underutilized ray tracing accelerators (RTAs). While most
    existing studies focused on better utilizing the provided translation bandwidth,
    this paper introduces a new opportunity to fundamentally increase the translation
    bandwidth. Instead of overprovisioning the GPU memory management unit (GMMU),
    Heliostat repurposes the existing RTAs by leveraging the operational similarities
    between ray tracing and page table walks. Unlike earlier studies that utilized
    RTAs for certain workloads, Heliostat democratizes RTA for supporting any workloads
    by improving virtual memory performance. Heliostat+ optimizes Heliostat by handling
    predicted future address translations proactively. Heliostat outperforms baseline
    and two state-of-the-arts by 1.93×, 1.92×, and 1.66×. Heliostat+ further speeds
    up Heliostat by 1.23×. Compared to an overprovisioned comparable solution, Heliostat
    occupies only 1.53% of the area and consumes 5.8% of the power.
  keywords: GPU, Virtual Memory, Address Translation
  document: '![](_page_0_Picture_0.jpeg)


    # Heliostat: Harnessing Ray Tracing Accelerators for Page Table Walks


    [Yuan Feng](https://orcid.org/0009-0004-2451-9568) University of California, Merced
    Merced, California, USA yfeng44@ucmerced.edu


    [Yuke Li](https://orcid.org/0009-0009-5806-7228) University of California, Merced
    Merced, California, USA yli304@ucmerced.edu


    [Jiwon Lee](https://orcid.org/0009-0009-6529-5333) Samsung Electronics Seoul,
    Republic of Korea jiwon24.lee@gmail.com


    [Won Woo Ro](https://orcid.org/0000-0001-5390-6445) Yonsei University Seoul, Republic
    of Korea wro@yonsei.ac.kr


    [Hyeran Jeon](https://orcid.org/0000-0002-1767-8198) University of California,
    Merced Merced, California, USA hjeon7@ucmerced.edu


    # Abstract


    This paper introduces Heliostat, which enhances page translation bandwidth on
    GPUs by harnessing underutilized ray tracing accelerators (RTAs). While most existing
    studies focused on better utilizing the provided translation bandwidth, this paper
    introduces a new opportunity to fundamentally increase the translation bandwidth.
    Instead of overprovisioning the GPU memory management unit (GMMU), Heliostat repurposes
    the existing RTAs by leveraging the operational similarities between ray tracing
    and page table walks. Unlike earlier studies that utilized RTAs for certain workloads,
    Heliostat democratizes RTA for supporting any workloads by improving virtual memory
    performance. Heliostat+ optimizes Heliostat by handling predicted future address
    translations proactively. Heliostat outperforms baseline and two state-of-the-arts
    by 1.93×, 1.92×, and 1.66×. Heliostat+ further speeds up Heliostat by 1.23×. Compared
    to an overprovisioned comparable solution, Heliostat occupies only 1.53% of the
    area and consumes 5.8% of the power.


    #### CCS Concepts


    • Computer systems organization → Single instruction, multiple data; • Computing
    methodologies → Graphics processors.


    # Keywords


    GPU, Virtual Memory, Address Translation


    #### ACM Reference Format:


    Yuan Feng, Yuke Li, Jiwon Lee, Won Woo Ro, and Hyeran Jeon. 2025. Heliostat: Harnessing
    Ray Tracing Accelerators for Page Table Walks. In Proceedings of the 52nd Annual
    International Symposium on Computer Architecture (ISCA ''25), June 21–25, 2025,
    Tokyo, Japan. ACM, New York, NY, USA, [15](#page-14-0) pages.<https://doi.org/10.1145/3695053.3731011>


    # <span id="page-0-0"></span>1 Introduction


    With the increasing size and complexity of GPU applications, virtual memory systems
    have become one of the critical performance


    [This work is licensed under a Creative Commons Attribution-NonCommercial-](https://creativecommons.org/licenses/by-nc-sa/4.0)[ShareAlike
    4.0 International License.](https://creativecommons.org/licenses/by-nc-sa/4.0)
    ISCA ''25, Tokyo, Japan © 2025 Copyright held by the owner/author(s). ACM ISBN
    979-8-4007-1261-6/25/06 <https://doi.org/10.1145/3695053.3731011>


    hurdles in GPU computing. Various studies improved the performance through adaptive
    TLB coalescing [\[3,](#page-12-0) [21,](#page-13-0) [38\]](#page-13-1), page mapping
    coalescing [\[12\]](#page-13-2), multi-GPU TLB sharing [\[12,](#page-13-2) [25,](#page-13-3)
    [40,](#page-13-4) [41\]](#page-13-5), and lazy invalidation [\[2,](#page-12-1)
    [20,](#page-13-6) [22\]](#page-13-7). These earlier studies mostly focused on
    reducing the latency of individual page translations or optimizing the provided
    GPU memory management unit (GMMU) bandwidth. Thus, their performance impact is
    limited to the bandwidth of the virtual memory system integrated in the GPU. To
    support larger and more diverse workloads, we should explore efficient ways to
    increase the translation bandwidth fundamentally.


    Figure [1](#page-1-0) shows the performance impact of increased translation bandwidth.
    When we integrate more page table walkers (PTWs) to the GMMU, most applications
    show dramatically improved instructions per cycle (IPC). The Low, Mid, and High
    in the graph indicate the translation intensity according to L2 TLB misses per
    kilo instructions (L2 TLB MPKI). Especially the workloads with high translation
    intensity exhibit significant speedup; more than half of the workloads derive
    over 3× speedup with 128 PTWs over the baseline 16 PTWs. This indicates that there
    is considerable potential to enhance performance by increasing the translation
    bandwidth.


    However, adding more PTWs is not a scalable solution because the speedup is achieved
    at the cost of area and power. From our RTLlevel evaluation, 128 PTWs incur over
    7.8× area and 6.8× power overhead than the baseline 16 PTWs, as shown in Figure
    [1.](#page-1-0) Such overhead might be amortized if all applications could achieve
    notable speedup. However, almost all applications with low intensity and half
    with middle intensity barely benefited from more PTWs. For those applications,
    the overprovisioned hardware resources will only occupy the die area consuming
    static power.


    To support diverse workloads without overprovisioning GMMU, we propose Heliostat,
    which expands the translation bandwidth by repurposing the underutilized existing
    hardware components. Besides general-purpose cores, modern GPUs embed various
    accelerators such as tensor cores and ray tracing accelerators. These accelerators
    use fixed-function units specially tailored for certain types of applications.
    Due to their limited functionality, the accelerators are significantly underutilized
    when the GPU isn''t running the target applications. To increase the utilization
    of these accelerators, several studies proposed to democratize the accelerators
    for various other applications that have operational similarities [\[7,](#page-12-2)
    [14,](#page-13-8) [49,](#page-14-1) [58\]](#page-14-2). These solutions require
    substantial modifications in both software


    <span id="page-1-0"></span>![](_page_1_Figure_1.jpeg)


    Figure 1: Normalized IPC, power and area with different number of PTWs


    and hardware and the performance impact is still limited to the supported applications.
    On the other hand, Heliostat is a hardware-only solution, which expands the translation
    bandwidth by leveraging these underutilized accelerators. Thus, Heliostat can
    enhance the performance of any applications experiencing translation overhead.


    We especially repurpose ray tracing accelerators (RTAs) by exploiting the operational
    similarity between ray tracing and page table walk. As the main function of ray
    tracing is to traverse the path of lights (rays) and check the intersected objects
    in the 3D scene, RTAs are equipped with hardware components that support tree
    traversals, such as ray buffers and dedicated memory units. As GMMUs use radix
    tree-based multi-level page tables, RTA''s tree traversal capability fits well
    with the page table traversal.


    However, there are challenges to repurposing RTAs for PTW operations: First, RTA''s
    pipeline and data structures are not compatible with page translation. To support
    PTW operations, we define new data structures and design a translation unit with
    lightweight modifications in the RTA architecture. The translation unit is added
    as one of the operation units in the ray tracing pipeline. Hence, the unit is
    activated similar to the existing operation units. Second, an RTA and other cores
    share a small L1 data cache. Sharing a small data cache with other cores brings
    two concerns; cache pollution and loss of page table walk locality. To resolve
    this problem, we exploit another underutilized hardware component, which is the
    constant cache. We show the significant underutilization in the shared constant
    caches and propose to reserve some space in the constant cache to be exclusively
    used by the RTA page translations. Third, there is no existing method to distribute
    translation requests between PTWs and RTAs. To arbitrate the translation job assignment
    between the GMMU and RTAs, we design a new PTW forwarding unit. By utilizing the
    resources in the RTAs, Heliostat attains a speedup (1.93×) comparable to a GMMU
    with 128 PTWs, while using only 1.53% of the area and 5.8% of the power consumption
    of 128 PTWs.


    Based on the design of Heliostat, we further harness the existing resources to
    enable proactive lookahead translations with namely Heliostat+. While Heliostat
    focuses on supporting ondemand translations, Heliostat+ enables translations for
    predicted future requests. Heliostat+ leverages the existing ray tracing function,
    secondary ray. In ray tracing, when a primary ray reaches a translucent surface,
    a secondary ray is forked and continues the traversal in the reflected direction
    with the traversal information


    inherited from the primary ray. By leveraging this existing capability, Heliostat+
    handles a pair of an on-demand translation and a lookahead translation at the
    same time. Similar to the secondary ray, the lookahead translation thread is forked
    only when its page table traversal path diverges from the on-demand translation.
    This enables saving multiple shared page table traverses. The translation operations
    can be skipped when an on-demand translation can be served from the cached lookahead
    translation result. Heliostat+ achieves speedup over Heliostat by 1.23× (2.39×
    over the baseline) and superior performance than two state-of-the-art solutions
    by 1.93× and 1.66×.


    Our contributions are as follows:


    - To the best of our knowledge, this is the first study that leverages the underutilized
    RTAs to improve the performance of the GPU virtual memory system. Unlike existing
    RTA democratization studies that can support limited scope of applications, Heliostat
    accelerates general-purpose system operations.

    - Unlike existing studies that primarily focused on optimizing the existing translation
    bandwidth, Heliostat demonstrates a lightweight method to fundamentally increase
    the translation bandwidth with significantly lower area and power overheads compared
    to the GMMU overprovisioning.

    - Heliostat+ demonstrates a novel proactive lookahead translation without a notable
    memory traffic increase by applying a ray tracing feature that allows sharing
    the page table walks between on-demand translations and lookahead translations.


    #### <span id="page-1-1"></span>2 Background


    #### 2.1 GPU Architecture and Virtual Memory


    Figure [2](#page-2-0) illustrates the baseline GPU architecture. A GPU consists
    of multiple Compute Units (CUs), each containing register files, warp schedulers,
    compute cores, and memory execution units (MEUs). Each CU uses an L1 Vector (L1V)
    cache as a private data cache. All CUs in a Shader Array (SA) share an L1 Scalar
    (L1S) cache for storing constant data, such as kernel arguments and pointers [\[1,](#page-12-3)
    [6,](#page-12-4) [48\]](#page-14-3). Each CU is equipped with a Matrix Core for
    deep learning workloads and a Ray Tracing Accelerator (RTA) for real-time ray
    tracing rendering. To support the virtual memory system, each CU uses a private
    L1 Translation Lookaside Buffer (TLB) and all CUs share an L2 TLB. Upon an L2
    TLB miss, a page translation request is sent to the GPU Memory Management Unit
    (GMMU) to be served by an available page table walker (PTW). Each PTW traverses
    over the radix-tree-based multi-level page tables. The GMMU incorporates 16 PTWs
    [\[8,](#page-12-5) [12,](#page-13-2) [16,](#page-13-9) [21,](#page-13-0) [22,](#page-13-7)
    [24,](#page-13-10) [40,](#page-13-4) [41,](#page-13-5) [46](#page-14-4)[–48,](#page-14-3)
    [53\]](#page-14-5).


    ### 2.2 Ray Tracing and Hardware Support


    2.2.1 Ray Tracing. Ray tracing (RT) is a graphic rendering algorithm that simulates
    the lighting effects to generate highly realistic scenes. The RT algorithm tests
    the intersection between rays and the graphic primitives, e.g. triangles and spheres,
    in a scene. A large number of rays are traced concurrently to generate a high-quality
    scene. Modern RT libraries organize the graphic primitives into acceleration structures
    (AS) that segment the primitives in the scene. Bounding Volume Hierarchy (BVH)
    is the de facto standard AS that


    <span id="page-2-0"></span>![](_page_2_Figure_1.jpeg)


    Figure 2: Baseline GPU Architecture


    <span id="page-2-1"></span>![](_page_2_Figure_3.jpeg)


    Figure 3: Examples on BVH Tree


    has been used in many industry products including Vulkan [\[52\]](#page-14-6),
    NVIDIA RTX GPU [\[35\]](#page-13-11), and Intel Arc GPU [\[15\]](#page-13-12).


    2.2.2 BVH and BVH Traversal. Figure [3](#page-2-1) shows an example 2D scene and
    the BVH built from the scene. In the scene, the graphic primitives are placed
    inside a graphic world A. The scene is then segmented into two sub-regions (B
    and C) by drawing the axis-aligned bounding boxes (AABBs) according to the location
    of the primitives. Such segmentation continues recursively until the leaf nodes
    contain the actual primitives. A BVH tree is built upon completion of finding
    all leaf nodes as shown in Figure [3b.](#page-2-1)


    To find all and closet graphic primitives that are intersected with individual
    rays, ray tracing runs intersection testing functions while traversing BVH trees.
    BVH trees are typically traversed through depth-first search (DFS). Whenever an
    intersected primitive is found, a callback function any-hit share is called. When
    the traversal finishes, the closest-hit function is called back.


    2.2.3 Ray Tracing Accelerator. Though GPUs are massively parallel processors,
    the lock-step nature of single-instruction-multiplethread (SIMT) execution makes
    it inefficient to process tree structures like BVH trees. To enable real-time
    RT, GPU vendors integrate RT accelerators (RTAs) into GPUs, such as raytracing
    accelerators in AMD, RT Core in NVIDIA, and RT Unit in Intel Xe-cores. Figure
    [4](#page-2-2) illustrates a typical RTA architecture. Once a warp is dispatched
    from a CU''s issue scheduler, the warp is enqueued to the Warp Buffer. The ray
    information (Ray Property) is also enqueued to the warp''s private Ray Buffer.
    An RTA can handle four concurrent warps [\[14,](#page-13-8) [43\]](#page-13-13).
    At every cycle, the warp scheduler in an RTA selects a warp from the Warp Buffer
    to traverse the input BVH tree. While traversing the tree, once a node is fetched
    via the dedicated memory unit (Mem Access FIFO), the node is passed to Node Decoder.
    Then,


    <span id="page-2-2"></span>![](_page_2_Figure_10.jpeg)


    Figure 4: Baseline Architecture of Ray Tracing Accelerator


    the Node Decoder decodes the node by using the ray properties in the Ray Buffer
    and identifies the next ray operation out of the three Operation Units. OP Dest.
    Table routes the execution to the selected ray operation unit. Once the ray operation
    is finished, the traversal stack and ray property are updated in the Ray Buffer.


    # <span id="page-2-4"></span>3 Motivation


    Methodology: We modeled an AMD GCN GPU by using a cyclelevel GPU simulator, MGPUSim
    [\[48\]](#page-14-3). A GPU has 128 CUs. Each CU has one RTA. We used the RTA
    architecture model of Vulkansim v2.0 [\[43\]](#page-13-13). We employed 16 PTWs
    in the GMMU [\[8,](#page-12-5) [12,](#page-13-2) [16,](#page-13-9) [21,](#page-13-0)
    [22,](#page-13-7) [40,](#page-13-4) [48,](#page-14-3) [53\]](#page-14-5). L2 TLB
    is a 16-way set-associative cache with 512 lines. Detailed simulation configurations
    are shown in Table [2.](#page-9-0) We used 23 general-purpose GPU applications
    selected from widely used benchmark suites. We classified individual applications
    into Low, Mid, and High, according to their address translation intensity, which
    is calculated by L2 TLB Misses Per Kilo Instructions (MPKI), as shown in Table
    [1.](#page-2-3)


    High L2 TLB misses in GPU computing: In Section [1,](#page-0-0) we showed a substantial
    speedup with increased translation bandwidth. To understand the reason behind
    the performance improvement, we measured the L2 TLB miss rates. The results in
    Figure [5](#page-3-0) show that applications in the High group barely benefited
    from an L2 TLB; on average, every other TLB access encounters a miss. Although


    Table 1: Tested Workloads and Benchmarks


    <span id="page-2-3"></span>


    | Benchmark   | Application        | L2TLB MPKI | Abbr   | Category |

    |-------------|--------------------|------------|--------|----------|

    | POLYBENCH   | doitgen            | 0.015      | doit   | Low      |

    | POLYBENCH   | syrk               | 0.017      | syrk   | Low      |

    | POLYBENCH   | gemm               | 0.028      | gemm   | Low      |

    | HeteroMark  | aes                | 0.071      | aes    | Low      |

    | HeteroMark  | fir                | 0.319      | fir    | Low      |

    | AMD APP SDK | simpleconvolution  | 0.445      | conv   | Low      |

    | SHOC        | fft                | 0.478      | fft    | Low      |

    | HeteroMark  | pagerank           | 0.807      | pr     | Low      |

    | POLYBENCH   | lu                 | 1.27       | lu     | Mid      |

    | Pannotia    | color              | 1.56       | color  | Mid      |

    | Pannotia    | mis                | 2.08       | mis    | Mid      |

    | POLYBENCH   | bicg               | 2.11       | bicg   | Mid      |

    | POLYBENCH   | jacobi2d           | 2.16       | j2d    | Mid      |

    | AMD APP SDK | floydwarshall      | 2.24       | floyd  | Mid      |

    | AMD APP SDK | fastwalshtransform | 2.27       | fwt    | Mid      |

    | POLYBENCH   | fdtd2d             | 2.49       | fdtd2d | Mid      |

    | Pannotia    | sssp               | 4.06       | sssp   | Mid      |

    | RODINIA     | nw                 | 21.1       | nw     | High     |

    | POLYBENCH   | mvt                | 48.3       | mvt    | High     |

    | SHOC        | stencil2d          | 59.3       | st2d   | High     |

    | AMD APP SDK | matrixtranspose    | 196.2      | matr   | High     |

    | MAFIA       | gups               | 557.4      | gups   | High     |

    | POLYBENCH   | gesummv            | 4784.5     | gesm   | High     |


    <span id="page-3-0"></span>![](_page_3_Figure_1.jpeg)


    Figure 5: L2 TLB miss rate and miss panelty breakdown


    <span id="page-3-1"></span>![](_page_3_Figure_3.jpeg)


    Figure 6: Cumulative distribution function (CDF) of translation reuse distances
    at the L2 TLB


    the workloads in the Low and the Mid groups encounter lower miss rates than those
    in the High group, some applications such as fft and color show almost 100% L2
    TLB misses even when the baseline L2 TLB is sufficiently large.


    Can a larger L2 TLB help?: We then were curious if a larger L2 TLB could have
    more hits. To understand the proper TLB size of each application, we measured
    the translation reuse distances. Figure [6](#page-3-1) shows the cumulative distribution
    function (CDF) of the L2 TLB reuse distances. Applications in each group are plotted
    in blue (Low), gray (Mid), and red (High). The workloads in the High group have
    significantly far reuse distances which begin from 2<sup>9</sup> up to 2<sup>16</sup>
    . We also observed that fft, which is in the Low group but has 100% L2 TLB miss
    rate, has significantly far reuse distances beginning with 214. This means that
    these applications need a huge L2 TLB having several thousand entries to reuse
    any cached translations. Given that the most common L2 TLB setting [\[12,](#page-13-2)
    [21,](#page-13-0) [24,](#page-13-10) [25,](#page-13-3) [50\]](#page-14-7) is to
    use 512 entries (as in our baseline), we should increase L2 TLB size by at least
    10 times to reduce L2 TLB misses for these applications. Such a huge L2 TLB will
    lead to significant power and area overhead. More importantly, most of the applications
    in the Low and Mid groups will encounter significant underutilization because
    their reuse distances are shorter than 512. Therefore, increasing the TLB size
    is not a scalable solution.


    Hint from miss penalty breakdown: The miss penalty statistics in Figure [5](#page-3-0)
    also provide a hint about the main performance bottleneck. In most of the applications
    across all three translation intensity groups, the queueing delay contributes
    the most to the L2 TLB miss penalty. This means that most translation requests
    cannot be served quickly, mainly due to insufficient PTW bandwidth.


    Based on these results, we can conclude that the limited GMMU bandwidth is the
    main performance bottleneck. Therefore, we propose to increase the translation
    bandwidth.


    ISCA ''25, June 21–25, 2025, Tokyo, Japan Yuan Feng, Yuke Li, Jiwon Lee, Won Woo
    Ro, and Hyeran Jeon


    <span id="page-3-2"></span>![](_page_3_Figure_10.jpeg)


    Figure 7: Multi-level Page Table Architecture


    <span id="page-3-3"></span>![](_page_3_Figure_12.jpeg)


    Figure 8: Formats of BVH tree nodes [\[29\]](#page-13-14) and PTEs.


    # 4 Address Translations with RTAs


    #### 4.1 Why RTAs for address translation?


    To increase the translation bandwidth without significant overhead, we aim to
    utilize existing underutilized hardware components for translation. To substitute
    PTWs, the hardware component should 1) be able to issue memory transactions without
    interfering with regular memory operations, 2) be able to traverse tree-based
    multilevel page tables, and 3) be available when existing PTWs are fully occupied
    without delaying their own operations.


    RTAs fulfill all three requirements. The RTA is the only architecture component
    besides the memory execution unit (MEU) (equivalent to LDST unit in NVIDIA GPUs)
    in the CU that can independently issue memory transactions. The RTA by nature
    supports tree traversal operations as described in Section [2.](#page-1-1) Although
    the page table architecture is different from the BVH tree, PTW operations use
    DFS, like in the BVH tree traversal, and the RTA is designed to accelerate DFS.
    Furthermore, RTAs are specialized cores that are used only for RT operations.
    Therefore, when a GPU runs general-purpose applications that barely use RT operations,
    RTAs are guaranteed to be idled throughout the execution. Therefore, we propose
    to leverage RTAs for address translation.


    # <span id="page-3-4"></span>4.2 Challenges of Repurposing RTAs


    There are three challenges to running PTW operations on RTAs.


    First, the RTA pipeline and data formats are not compatible with PTW operations.
    (1) Tree format: The BVH tree is a full binary tree, while the page table is not,
    as shown in Figure [7.](#page-3-2)


    (2) Traversal termination condition: Both RTA and PTW terminate the tree traversal
    when they finish processing a leaf node. In RT, the node type information is encoded
    in the tree node data structure, as shown in Figure [8.](#page-3-3) On the other
    hand, to indicate the leaf node, PTWs use two methods. For the default size pages
    (4KB), PTWs traverse for the statically determined page table depth e.g., in Figure
    [7,](#page-3-2) PTWs can retrieve the physical frame number (PFN) after four tree
    traversal steps. If the GPU uses large pages (e.g., 64KB or 2MB), the traversal
    should stop earlier because the tree depth is shorter. In this case, PTWs check
    a one-bit last-level table indicator (e.g., bit in Figure [8\)](#page-3-3) in
    the page table entry (PTE). None of both methods are implemented in the RTA''s
    Node Decoder.


    (3) Next-level child node location: In RTA, the next-level node location is also
    encoded in the node data structure. On the other


    <span id="page-4-0"></span>![](_page_4_Figure_2.jpeg)


    Figure 9: Modified Architecture with Heliostat


    hand, PTW should calculate the location by using a subset of the bits in the requested
    virtual page number (VPN), as shown in Figure [7.](#page-3-2)


    (4) Node format: As illustrated in Figure [8,](#page-3-3) the formats of a 40
    byte BVH tree node and a 64-bit PTE do not have any similarity.


    (5) Operations: The operation units in the RTA are variants of ray intersection
    testing functions. However, page translation does not share functional similarities
    with ray intersection testing. The address translation includes extracting a PFN,
    checking the access control, and triggering a page fault. None of these three
    functions can be natively executed by the RTA''s operation units.


    Second, an RTA shares L1V cache with regular memory operations. While an RTA does
    not use a MEU unit to issue memory transactions, caches are shared between MEUs
    and the RTA. Therefore, there is a potential risk of cache pollution, that may
    drop the performance of regular memory operations. On the other hand, page table
    walk operations are highly benefited from caches because a few upper-level page
    tables are frequently shared by many translations. To support such locality, GMMUs
    are typically equipped with page table caches. Therefore, RTAs should have a separate
    cache space. An RTA has internal buffers such as Ray Buffers but they are private
    resources for individual warps. Therefore, page table information cannot be shared
    across RTAs.


    Third, there needs an arbitrator to distribute the translation tasks between the
    GMMU and RTAs. As the goal of Heliostat is to expand the translation bandwidth,
    the translation requests should be assigned based on the availability of the GMMU
    and RTAs. However, existing GPU architectures do not have such arbitrators.


    #### 5 Heliostat


    #### 5.1 Overall Architecture


    Figure [9](#page-4-0) shows the updated GPU architecture with Heliostat. We modify
    RTAs to translate addresses upon L2 TLB misses (Section [5.2\)](#page-4-1). Heliostat
    triggers a page translation on an RTA when there is no available PTW. To check
    the status of PTWs in the GMMU and redirect a translation request to an RTA, we
    design an RT-PTW Forwarding Unit (RFU) (Section [5.3\)](#page-5-0). An RFU is
    shared by all CUs in a GPU. To mitigate potential L1V cache pollution and leverage
    translation locality, we reserve space in the highly


    underutilized L1S cache for the RTAs (Section [5.4\)](#page-5-1). The following
    sections describe the details of each of these components.


    #### <span id="page-4-1"></span>5.2 RTA Modification


    Figure [10](#page-5-2) shows the updated RTA architecture. We introduce a new
    operation unit, PTE decoding unit (PDU), to traverse the multi-level page table.
    The PDU is responsible for handling the computations of PTW operations by utilizing
    the existing Ray Buffers and Mem Access FIFO in the RTA.


    5.2.1 Ray PTW Property. Ray property is used for tracking the traversal status.
    Figure [11](#page-5-3) shows the original RayProperty data structure used by the
    RT and our proposed RayPTWProperty to be used for PTW operations. The original
    ray properties are mostly used for describing a ray (e.g., the origin and the
    direction of the ray). Our proposed structure incorporates information for PTW;
    V\_addr (64 bits) contains the VPN to be translated; pid (32 bits) stores the
    process ID; rwx (8 bits) stores the access control information; level (8 bits)
    is used to carry the current page table walk level; P\_addr contains the translated
    physical address.


    When a translation request is received, a warp is scheduled and a new RayPTWProperty
    instance is created for the warp. The RayPTWProperty is maintained in the Ray
    Buffer and updated at every page table traversal step. When the translation finishes,
    the Mem Access FIFO in the RTA responds to the RFU with the translated P\_addr.
    Compared to the original RayProperty (256 bits), the proposed RayPTWProperty (112
    bits) is smaller. Therefore, we can use the proposed data structure without extra
    storage space.


    5.2.2 Node Decoder. The Node Decoder in an RTA is used for decoding the node type
    (e.g., leaf or internal node) and activating a proper operation unit. In the page
    translation, as discussed in Section [4.2,](#page-3-4) when the GPU uses 4KB pages
    only, we simply traverse for the statically determined tree depth. Therefore,
    we do not need to decode any input. If the GPU uses diverse page sizes, we should
    check the bit in the PTE. To support both methods, we add a 1-bit checker in the
    Node Decoder, which checks the bit from a given PTE input. The bit value is passed
    to the PDU as an input. Upon receiving a translation request from RFU, Node Decoder
    makes the OP Dest. Table to activate the PDU with the bit value.


    5.2.3 OP Dest. Table. The main function of the OP Dest. Table is to activate the
    correct operation units upon receiving the Node Decoder output. To activate the
    proposed PDU for the translation operations, OP Dest. Table is revised to integrate
    one more output port to pass the inputs to the PDU out of the operation units.


    5.2.4 PTE Decoding Unit (PDU). The PDU handles the computation part of the PTW
    operations. Figure [12\(](#page-5-4)a) shows the architecture. A PTW has three
    functions: 1) checking if the PTE is from the last-level page table, 2) if the
    PTE is from an intermediate-level page table, retrieving the base address of the
    next-level page table from the input PTE and issuing a memory transaction to fetch
    the next-level PTE, and 3) if the PTE is from the last-level page table, retrieving
    the PFN from the PTE and checking the access control and the valid bit to finish
    the translation. The following describes how each function is implemented.


    <span id="page-5-2"></span>![](_page_5_Figure_1.jpeg)


    Figure 10: RTA with PTW data path


    <span id="page-5-3"></span>![](_page_5_Figure_3.jpeg)


    Figure 11: Ray Property for PTW


    (1) Checking the last-level PTE: The PDU first checks the bit input passed by
    the Node Decoder. If it is 1, the PDU determines that the current input is a last-level
    PTE for a large page. Otherwise, the PDU checks the level parameter of RayPTWProperty.
    The level is initialized as zero and incremented by one as the page table tree
    is traversed further. Thus, PDU checks if level value matches the system''s page
    table depth. If matched, the PDU determines that the input PTE is from the last-level
    page table. To implement this function, we add two comparators, one for checking
    the input bit and another for checking the 8-bit level value.


    (2) Calculating the next-level page table address: This function is implemented
    by taking the corresponding level''s index value from the input VPN (e.g., one
    of PML4, PDP, PD, PT in Figure [7\)](#page-3-2), multiplying the index with the
    size of a PTE, and adding it with the next-level page table base address which
    is retrieved from the input PTE. For the first-level page table, the page table
    base register (PTBR) is used for the base address. To implement this, we add one
    adder and one shifter. The calculated next-level page table address is passed
    to the Mem Access FIFO to issue a memory transaction. Then, the RayPTWProperty.level
    is incremented by one and passed to the Ray Property field in the Ray Buffer.


    (3) Retrieving the PFN and checking access control: This is the final step of
    the PTW. The input PTE has the PFN. The PFN is extracted and updated to the RayPTWProperty.P\_addr
    field when the PTE''s valid bit is set and the access control field values match
    with RayPTWProperty.rwx. If access control values mismatch or the valid bit is
    not set, the PDU returns the translation request to the RFU so that the GMMU can
    replay the translation i.e., handling the page fault. To implement this, we add
    two comparators, one for checking the valid bit and another for checking the 8-bit
    rwx value.


    ISCA ''25, June 21–25, 2025, Tokyo, Japan Yuan Feng, Yuke Li, Jiwon Lee, Won Woo
    Ro, and Hyeran Jeon


    <span id="page-5-4"></span>![](_page_5_Figure_9.jpeg)


    Figure 12: PTE Decoding Unit (PDU) and RT-PTW Forwarding Unit (RFU)


    #### <span id="page-5-0"></span>5.3 RT-PTW Forwarding Unit


    We design an RFU to arbitrate the translation requests. The RFU determines whether
    the new translation request should be handled by an RTA or by the GMMU by monitoring
    the utilization of PTWs in the GMMU. Figure [12\(](#page-5-4)b) shows the RFU
    architecture. The GMMU monitor probes the available PTWs. If there are any available
    PTWs, the incoming requests are sent to the GMMU. Otherwise, the requests are
    directed to the requesting CU''s RTA via the inter-CU interconnection network.


    #### <span id="page-5-1"></span>5.4 Locality-aware Page Table Sharing


    To avoid L1V cache pollution and also incorporate an exclusive page table cache,
    we propose reserving a space in a shared L1S cache for the RTA. Unlike an L1V
    cache, which is the main data cache for individual CUs, an L1S cache is a read-only
    cache shared by all CUs in each Shader Array. As an L1S cache is used for storing
    kernel arguments and data allocated with a constant qualifier [\[1,](#page-12-3)
    [6,](#page-12-4) [48\]](#page-14-3), an L1S cache is typically highly underutilized.
    According to our evaluation, the L1S cache utilization is from 1% up to 19%. Also,
    as the L1S cache is shared by multiple CUs, the page table walk can be shared
    across RTAs. Thus, we leverage some of the unused L1S space as the page table
    cache, namely RT-PTW cache.


    We can reserve space in a cache in several granularities e.g., one whole set,
    a subset of ways within each set, or a subset of words within each cache line
    [\[9,](#page-12-6) [10,](#page-13-15) [44,](#page-13-16) [56\]](#page-14-8). The
    way-level reservation has the least design complexity because it doesn''t need
    to change the regular cache operations. Once a set is indexed by the regular setassociative
    policy, we can skip checking or updating the reserved ways out of all ways in
    the set. The set- or word-level reservations require modifications on the set
    associativity and cache line layout. Thus, we chose to design the way-level reservation.
    To reduce performance impact for applications that might use the L1S cache actively,
    we reserve only one way per set for the RTA operations.


    Figure [13](#page-6-0) shows the modified L1S cache. We add an access mask register
    to the cache controller. The register is a bit vector that has as many bits as
    the number of ways per set in the cache. The initial value of the register is
    all 1''s. When we want to use a specific way exclusively for RTAs, we unset the
    corresponding bit. In Figure [13,](#page-6-0) way 2 is used for the RTA. The regular
    operations use the ways with corresponding bits set.


    # 5.5 Operational Example


    Figure [14](#page-6-1) illustrates an end-to-end walkthrough of Heliostat. Suppose
    a translation request misses from both L1 TLB ➀ and L2 TLB ➁. Then, the request
    is sent to the proposed RFU ➂. If the GMMU has


    <span id="page-6-0"></span>![](_page_6_Figure_1.jpeg)


    Figure 13: L1S Cache Reservation for RTA


    <span id="page-6-1"></span>![](_page_6_Figure_3.jpeg)


    Figure 14: Heliostat Walkthrough


    an available PTW, the request is sent and served by the GMMU ○B . If the GMMU
    is busy, the translation request is sent to the requester CU''s RTA ○A, and our
    proposed PTW-enabled RTA will serve the translation request ➄. During the translation,
    the RTA looks up the RT-PTW cache reserved in the L1S cache with the next-level
    page table address ➅. If the page table is not cached ➆, a memory transaction
    is issued to fetch the page table from the memory ➇. Once the translation is finished,
    the translation results are responded to the RFU and then to the L2 TLB and the
    L1 TLB.


    # 6 Heliostat+


    # 6.1 Overview


    Heliostat supports on-demand translations. However, we have not yet fully unleashed
    the potential of the RTA''s functionality. Besides BVH tree traversal, RTAs also
    support handling secondary ray, which is a built-in functionality in ray tracing
    [\[42,](#page-13-17) [43,](#page-13-13) [45\]](#page-13-18). As shown in Figure
    [15,](#page-6-2) when a ray intersects with a reflective or translucent surface
    (e.g., glass), either ray tracing software or RTA creates another ray query that
    originates from the reflection location and travels in another direction to render
    reflection and transparency. By leveraging this existing functionality, we propose
    to support lookahead translation.


    The lookahead translation is a proactive translation for the predicted future
    requests. Once translation requests follow a predictable pattern, Heliostat+ invokes
    a lookahead translation thread (secondary ray) to handle the predicted future
    translations (Section [6.2\)](#page-6-3). Similar to the secondary ray, the lookahead
    translation thread begins the operation when its traversal path diverges from
    the on-demand translation thread''s (primary ray) by inheriting the translation
    information. Therefore, the lookahead translation fits naturally with the existing
    secondary ray functionality. The lookahead translation results are stored in the
    L1S cache (Section [6.3\)](#page-7-0). Once a translation request hits in the
    L1S, the stored translation is used and the translation operations can be skipped.
    The RFU is revised to detect strides and find the most probable RTA that might


    <span id="page-6-2"></span>![](_page_6_Figure_11.jpeg)


    Figure 15: Lookahead Translation (left) and Secondary Ray Tracing (right)


    have the cached translation by monitoring the translation requests (Section [6.4\)](#page-7-1).
    When the RFU sends a translation request to an RTA, it adds a lookahead translation
    request in the same request packet by encoding the stride value in the packet
    header.


    # <span id="page-6-3"></span>6.2 Lookahead Translation in RTA


    The secondary ray can be handled by either software or hardware. We conservatively
    assume that the RTA does not have a built-in logic for the secondary ray and propose
    a lightweight modification in the RTA to support hardware-oriented lookahead translation.


    6.2.1 Lookahead Translation Thread. The secondary ray inherits the most recent
    traversal information of the primary ray. To enable information sharing, the secondary
    ray is assigned to a thread of the warp of the primary ray. When a secondary ray
    is created, the Ray Buffer entry of the primary ray is copied to a new entry in
    the same Ray Buffer. The secondary ray thread begins the execution using the new
    entry. Similarly, we modify the RTA to create a new row in the Ray Buffer when
    a lookahead translation thread is created.


    6.2.2 Ray PTW Plus Property. To accommodate the lookahead translation information
    and indicate the thread used for the lookahead translation, we added two new parameters
    in the RayPTW-Property. A 64-bit Second\_V\_Addr stores the lookahead translation
    address. The lookahead address is calculated by adding the stride to the on-demand
    translation address. A 1-bit is\_prefetching indicates the lookahead thread property.


    6.2.3 Invoking Lookahead Translation. If the stride field in the translation request
    packet is not zero, RTA prepares the lookahead translation. The on-demand translation
    and the lookahead translation might share a few upper level PTEs, as shown in
    Figure [15.](#page-6-2) To save memory transactions for the shared traversals,
    we invoke the lookahead translation only when the lookahead translation should
    access a different PTE from the on-demand translations. In Figure [15,](#page-6-2)
    the first two levels can be handled by the on-demand translation. Only from the
    third-level page table, the lookahead translation thread begins its traversal
    separately.


    To support this, we modify the PDU and name it PDU+, as shown in Figure [16.](#page-7-2)
    The modifications are highlighted with orange color. We add a comparator to check
    if the lookahead and the on-demand traversals need to access different PTEs in
    the next-level page table. If so, we invoke a lookahead translation thread. The
    invocation signal is sent to the warp scheduler within the RTA and the scheduler
    assigns a new thread, creates a Ray Buffer entry for the thread, and schedules
    the lookahead translation.


    <span id="page-7-2"></span>


    ![](_page_7_Figure_1.jpeg)


    Figure 16: PDU+ with Lookahead Translation


    #### <span id="page-7-0"></span>6.3 Lookahead Translation Buffer


    Unlike the on-demand translation, we do not update TLBs with the lookahead translation
    to avoid pollution. Given that the limited TLB reach is a pervasive performance
    problem [\[12,](#page-13-2) [13,](#page-13-19) [21,](#page-13-0) [25,](#page-13-3)
    [37,](#page-13-20) [38,](#page-13-1) [50,](#page-14-7) [55\]](#page-14-9), we
    propose to maintain the lookahead translation results in a separate storage. Unlike
    earlier solutions that used a separate prefetch buffer [\[17,](#page-13-21) [33,](#page-13-22)
    [57,](#page-14-10) [60\]](#page-14-11), Heliostat+ utilizes the L1S space that
    is reserved for Heliostat to minimize the area overhead.


    While the L1S cache is used by Heliostat to maintain the cache lines loaded during
    the page table walks, the format and the size of a cache line (64 bytes) and a
    PTE (8 bytes) are different. Thus, naive mappings will waste the limited reserved
    cache space. If we store one PTE per cache line, 56 bytes will be wasted per cache
    line. If we store eight PTEs per cache line, we will need to store eight tags
    per cache line, which will complicate the design unnecessarily.


    To resolve the mapping problem, we collocate eight PTEs having consecutive VPNs
    with a fixed stride. As Heliostat+ uses a simple stride-based lookahead translation
    prediction (Section [6.4\)](#page-7-1), the PTEs tend to be distanced with an
    integer multiple of the stride size. Figure [17](#page-7-3) shows the proposed
    mapping. Out of the reserved cache ways for Heliostat, we spare half of the cache
    lines as a lookahead translation buffer. The lines in the lookahead translation
    buffer are marked with a one-bit RT-PF indicator in the tags. Out of the eight
    PTEs having fixed strided VPNs, the smallest VPN is used as a tag for the cache
    line and filled in the Base VPN in the tag array. The stride can be any number
    but in our current design, we consider four common strides, which are 1, 64, 128,
    and 256 (Section [6.4\)](#page-7-1). To indicate one of the four strides, we spare
    two bits in the tag.


    The lookahead translation buffer is searched upon receiving the on-demand translation
    request. When there is a hit, we return the PTE to the RFU and skip the RTA translation.
    If the lookup misses, RTA begins the translation. When updating and searching
    the lookahead translation buffer with a VPN, we try VPN ± 8 × stride. The lookahead
    buffer has only 64 cache lines. Therefore, the lookup does not incur much overhead.


    #### <span id="page-7-1"></span>6.4 Lookahead Translation Prediction


    The proposed lookahead translation can support any translation prefetch algorithms
    as a separate thread handles the lookahead translation. In the current Heliostat+,
    we implement a simple stridebased algorithm to reduce the design complexity while
    demonstrating the efficiency of the RTA-based lookahead translation.


    ISCA ''25, June 21–25, 2025, Tokyo, Japan Yuan Feng, Yuke Li, Jiwon Lee, Won Woo
    Ro, and Hyeran Jeon


    <span id="page-7-3"></span>![](_page_7_Figure_11.jpeg)


    Figure 17: L1S Cache as a Lookahead Translation Buffer


    6.4.1 Stride Detection. In GPUs, due to the massive parallelism, translation requests
    can be highly mixed up. To reduce such interferences, the RFU monitors a predetermined
    number (say ) of on-demand translations and calculates the candidate strides within
    each similar VPN region. For this, we implement a circular queue with entries.
    We used 200 entries in our evaluation.


    From our preliminary results, we commonly observed four strides: 1, 64, 128, and
    256 pages. We believe this is mainly due to the number of CUs in the GPU (128
    CUs in our baseline) and the common page mapping policies in GPU computing. Stride
    1 indicates each CU accesses a few consecutive pages. Strides 64 and 128 mean
    that one page is shared by every two CUs and one CU, respectively. Thus, the current
    Heliostat+ is designed to choose one of the four strides. The stride is encoded
    in the translation request packet by leveraging the unused bits in address translation
    service packets [\[36\]](#page-13-23).


    6.4.2 Lookahead Translation-aware On-demand Translation Assignment. Depending
    on the page mapping policy, the lookahead translation results might be used by
    any CUs in the GPU. To share the lookahead translation results globally, we propose
    to predict the RTA that may have the cached translation already and send the request
    to the selected RTA. For this purpose, we add an RT-PTW lookahead filter (RPF)
    into the RFU. Out of various prediction algorithms, we use cuckoo filter [\[11\]](#page-13-24).
    A cuckoo filter is a probabilistic data structure that can predict whether a data
    item is possibly in the buffer or definitely not in the buffer. Unlike a similar
    bloom filter, a cuckoo filter supports efficient deletion operations.


    We integrate as many cuckoo filters as the number of Shader Arrays. When an RTA
    finishes a lookahead translation, it sends a filter-filling message to the RFU.
    The RFU then inserts the VPN into the filter associated with the Shader Array
    that the RTA resides in. If a lookahead buffer evicts an entry, a filter-deletion
    message is sent to the RFU. As each lookahead buffer entry has eight PTEs, eight
    VPNs are deleted from the filter. Once RFU receives a translation request, it
    checks all filters. If any filter returns a hit, the translation request is sent
    to one of the CUs in the predicted Shader Array.


    # 6.5 Operational Example


    Figure [19\(](#page-8-0)a) shows the walkthrough of triggering a lookahead translation.
    Suppose a translation request misses from L1/L2 TLBs ➀➁, the translation request
    is sent to the RFU+. If the RFU+ has a detected stride for the the requested address,
    the stride value is encoded to the on-demand translation request packet. Note
    that a lookahead translation is always accompanied by an on-demand translation,
    to take advantage of secondary ray. If the GMMU is available, only the on-demand
    translation will be handled by the


    True/False


    True/False


    True/False


    True/False


    True/False


    True/False


    GMMU


    Figure 18: RFU+ with Lookahead Translation Prediction


    <span id="page-8-0"></span>![](_page_8_Figure_3.jpeg)


    Figure 19: Lookahead Translation Walkthrough


    GMMU since GMMU does not have the lookahead translation capability. The GMMU will
    ignore the stride bits. If the GMMU is busy, the on-demand request is sent together
    with the lookahead translation request ➂. The lookahead request is served by an
    RTA ➃, and the translated lookahead PTE is stored in the L1S ➄. After the lookahead
    PTE is stored, a filter-filling message is sent to the RFU+, and the RPF for the
    Shader Array is updated.


    Figure [19\(](#page-8-0)b) illustrates how the lookahead translation is used for
    future translation. Suppose a subsequent translation request is made by a CU in
    a remote Shader Array B. If the request misses from both L1/L2 TLBs ➀➁, the request
    is sent to the RFU+ ➁. The RFU+ then checks the RPFs and has a hit with the filter
    for Shader Array A ➂. After that, the translation request is sent to the RTA in
    the Shader Array A ➃. The RTA checks the lookahead buffer in the L1S ➄. If the
    PTE is found ➅, the RTA returns the PTE to the RFU+ without doing translations
    ➆.


    #### 7 Discussions


    RT operations with Heliostat-enabled RTA: Heliostat does not change any data path
    used by RT operations. When an RT kernel is invoked, Heliostat is not activated
    to avoid potential performance degradation. This can be controlled either by the
    programmer through a driver option or by the hardware through checking the execution
    of RT APIs. Technically, Heliostat and RT can be handled by an RTA concurrently
    because Heliostat uses dedicated operation units, PDUs. Unless RT workloads need
    the full capacity of warp buffers in all RTAs, Heliostat can be activated. To
    monitor the RTA utilization, we could add an RTA monitor similar to the


    GMMU monitor in the RFU. Then, upon TLB miss, RFU locates an available (or underutilized)
    RTA and sends the translation requests. Security on PTBR added to the PDU: Heliostat
    adds PTBR in the PDU, which contains the top-level page table address. PTBR might
    be leveraged for page table leakage. Such leakage is not possible in Heliostat.
    The PDU is only accessed by hardware address translation requests, and only when
    the GMMU is busy. Therefore, TLB timing attacks to leak PTBR will be challenging
    to design.


    Applicability to broader GPU models: Heliostat is based on AMD GPUs. But, NVIDIA
    GPUs feature similar RT Cores and virtual memory systems. Similar to shared L1S
    cache, NVIDIA Hopper''s [\[34\]](#page-13-25) distributed shared memory enables
    L1 data cache sharing among SMs within graphic processing clusters. Thus, RT-PTW
    cache and lookahead buffer can be implemented on NVIDIA GPUs. TLB/Lookahead buffer
    coherence: Page faults or migrations trigger page table updates, creating stale
    PTEs in TLB. To maintain lookahead buffer coherent with TLB, Heliostat+ flushes
    lookahead buffer when system commands TLB flush upon page faults or page migrations.
    Since both GMMU and RTAs traverse the same page table copy, coherence between
    them is inherently maintained.


    Multi-programming: Heliostat+ maintains seamless support for multi-programming
    by using PID for process differentiation. Testing with application combinations
    across categories demonstrates a 1.47× speedup over the baseline.


    RPF false positive: RPF in Heliostat+ are probabilistic filters. It may cause
    false positive predictions. In this case, requests are handled as regular translation
    requests upon a miss in the lookahead buffer. Our evaluation shows a minimal false
    positive rate of 0.38%.


    # 8 Evaluation


    We evaluate Heliostat and Heliostat+ with the workloads and the simulation configurations
    as described in Section [3.](#page-2-4) The GMMU uses a conventional 4-level radix-tree-based
    page table and 16 PTWs. The PTWs share a 128-entry page walk cache.


    #### 8.1 Overall Performance


    We compare our approach against a baseline using 16 PTWs without RTA, and two
    state-of-the-art solutions [\[8,](#page-12-5) [12\]](#page-13-2). Valkyrie [\[8\]](#page-12-5)
    enables PTE and prefetched translation sharing between an L1 TLB and an L2 TLB
    upon detecting similar translation patterns, using lightweight NoC that connects
    L1 TLBs within Shader Arrays. Our Valkyrie implementation includes inter-L1 TLB
    communications and a 1024-entry prefetch history table in L2 TLB. BarreChord [\[12\]](#page-13-2),
    focusing on GPU page table walk coalescing, allocates temporally similar page
    accesses to identical relative physical locations across GPU chiplets, enabling
    translation without page table walks. We implemented it by mapping GPU chiplets
    to individual Shader Arrays with cross-Shader Array page coalescing.


    In Figure [20,](#page-9-1) Heliostat achieves 1.93×, 1.92×, and 1.66× speedups
    over the baseline, Valkyrie, and BarreChord respectively. For large applications,
    CUs within Shader Arrays rarely share L1 TLB entries due to exclusive page access
    patterns, making performance PTW-throughput bound and limiting Valkyrie''s L1
    TLB sharing benefits. While Barre''s coalescing outperforms Valkyrie, it remains
    constrained by Shader Array boundaries. Heliostat achieves significant speedups
    through RTA offloading of PTWs, while Heliostat+


    ISCA ''25, June 21–25, 2025, Tokyo, Japan Yuan Feng, Yuke Li, Jiwon Lee, Won Woo
    Ro, and Hyeran Jeon


    <span id="page-9-1"></span>![](_page_9_Figure_2.jpeg)


    <span id="page-9-2"></span>Figure 21: Performance Breakdown


    further improves performance by 1.23× (2.39× over baseline) by improving both
    PTW bandwidth and latency through lookahead translations.


    # 8.2 Performance Breakdown


    Figure [21](#page-9-2) shows performance across three optimizations. RT-PTW, which
    shares L1V cache with regular operations, achieves only 3% speedup over baseline,
    with some applications like sssp showing 4.3% degradation due to cache thrashing.
    RT-PTW w/ dedicated L1V set yields merely 1.16% improvement, as reduced cache
    capacity for regular operations impacts performance. Heliostat achieves 1.93×
    speedup by using shared L1S cache, allowing full L1V utilization for regular operations
    while providing dedicated, CU-shared cache space for RTAs. Heliostat+ further
    improves performance by 1.23× through lookahead translation with dedicated L1S
    buffers.


    |  |  | Table 2: Simulation Parameters |

    |--|--|--------------------------------|

    |--|--|--------------------------------|


    <span id="page-9-0"></span>


    | Parameter             | Value                                                       |  |  |

    |-----------------------|-------------------------------------------------------------|--|--|

    | Number of CUs         | 128                                                         |  |  |

    | L1 Vector Cache       | 64 KB, 4-Way, 32 MSHRs, LRU                                 |  |  |

    | L1 Inst Cache         | 64 KB, 4-Way, 4 MSHRs, LRU                                  |  |  |

    | L1 Scalar Cache       | 16 KB, 4-Way, 4 MSHRs, LRU                                  |  |  |

    | L2 Cache              | 2 MB, 16-way, 192 MSHRs, LRU                                |  |  |

    | Network On Chip (NoC) | single-stage crossbar[8, 12, 24, 25, 53], ≈50-cycle
    latency |  |  |

    | Warp Scheduler        | GTO                                                         |  |  |

    | DRAM                  | 512 GBps                                                    |  |  |

    | L1 TLB                | 64 entries, fully connected, 16 MSHRs,                      |  |  |

    |                       | 1 cycle lookup latency, private to CU, LRU.                 |  |  |

    | L2 TLB                | 512 entries, 16-Way, GPU shared [8, 12, 21, 22, 24,
    25].    |  |  |

    |                       | 10 cycle lookup latency, 128 MSHRs, LRU.                    |  |  |

    | Number of RTA         | 1 per CU                                                    |  |  |

    | Operational Unit      | 4 sets [43]                                                 |  |  |

    | RTA MSHR Size         | 64 [43]                                                     |  |  |

    | RTA Max Warp          | 4 [43]                                                      |  |  |

    | GMMU                  | 16 shared PTWs. 128 PW-queue entries [12, 40]               |  |  |

    | PW Cache              | 128-entry shared between PTWs [21, 24, 25, 40]              |  |  |


    <span id="page-9-3"></span>Figure 22: Normalized Page Translation Latency


    <span id="page-9-4"></span>![](_page_9_Figure_10.jpeg)


    Figure 23: Fraction of translations served by RTAs and GMMU: in each application,
    H is Heliostat, H+ is Heliostat+.


    # 8.3 Page Translation Latency


    In Figure [22,](#page-9-3) we present the normalized latency of page table walks.
    The latency includes the memory (cache) access latencies to load page tables,
    the time for PTE decoding, and the queueing delay until acquiring a PTW or an
    RT warp. With Heliostat, the translation latency is reduced by 41.5% on average.
    This is because Heliostat expands the PTW bandwidth with 128 RTAs, which reduces
    the queueing delay. Heliostat+ further reduces the translation latency by 16.9%
    because the lookahead translations eliminate the entire on-demand PTW latency
    when the lookahead buffer hits. The additional memory requests (7% and 0.6%) brought
    by lookhead translation in Heliostat+ has minor performance impacts.


    # 8.4 Translation Workload Distribution


    To understand how much translations are offloaded to RTAs, we measured the percentage
    of translation requests served by the GMMU and the RTAs. As shown in Figure [23,](#page-9-4)
    over 65.3% and 68.5% of translation are offloaded to the RTAs by Heliostat and
    Heliostat+ on average, respectively. This reflects that GMMU PTWs are supposed
    to be severely overloaded without our solution. Heliostat+ offloads more than
    Heliostat because Heliostat+ can skip some translations when lookahead buffer
    hits and serve more requests.


    <span id="page-10-0"></span>![](_page_10_Figure_1.jpeg)


    Figure 24: Hit rate of RT-PTW cache with L1V vs. L1S


    <span id="page-10-1"></span>![](_page_10_Figure_3.jpeg)


    Figure 25: PTW reduction and prefetch hit rate


    #### 8.5 RT-PTW Cache Hit Rate


    To evaluate the effectiveness of our locality-aware page table sharing, Figure
    [24](#page-10-0) compares the cache hit rates between two designs: the baseline
    design that utilizes the general-purpose data cache (L1V cache) and Heliostat
    that reserves a way in the L1S cache as a dedicated RT-PTW cache. The baseline
    design achieves only a 24.6% hit rate, primarily due to the interference with
    the general data transactions. The proposed L1S-based RT-PTW cache achieves a
    48.6% hit rate. This significant improvement demonstrates that Heliostat effectively
    reduces expensive memory transactions during the page table traversal.


    # 8.6 Lookahead Buffer Hit Rate & Translations Reduction


    In Figure [25,](#page-10-1) we present the lookahead buffer hit rate and the corresponding
    page table walk reduction. On average, 50% of the lookahead buffer entries encounter
    hits before eviction, and the hit entries reduce 31% of translation requests.
    Especially for the applications having regular access patterns such as fir, fft,
    and j2d, Heliostat+ achieves higher page table walk reduction and prefetching
    hit rate because those applications tend to follow relative uniform strides.


    #### 8.7 Sensitivities


    8.7.1 Page Size. Heliostat supports multiple page sizes beyond the 4KB baseline.
    Figure [26](#page-10-2) (left) shows Heliostat+ achieving 7% and 0.3% speedup
    with 64KB and 2MB pages respectively, despite large pages already reducing PTW
    loads through increased TLB reach and address coalescing. To assess scalability,
    similar to [\[12,](#page-13-2) [24,](#page-13-10) [41\]](#page-13-5), we tested
    with 16× larger applications (which reflects the change from 4KB to 64KB). Figure
    [26](#page-10-2) (right) shows results from a representative subset across translation
    intensities, with Heliostat+ achieving 31% and 4.2% speedup for 64KB and 2MB pages
    respectively.


    8.7.2 Number of PTWs. Figure [27](#page-10-3) shows Heliostat+''s speedup across
    different baseline PTW counts. With 32 PTWs, Heliostat+


    <span id="page-10-2"></span>![](_page_10_Figure_13.jpeg)


    Figure 26: Speedup under Larger Pages


    <span id="page-10-3"></span>![](_page_10_Figure_15.jpeg)


    Figure 27: Speedup under different number of PTWs


    <span id="page-10-4"></span>![](_page_10_Figure_17.jpeg)


    Figure 28: Speedup under different number of L2 TLB MSHRs


    achieves 1.32× speedup, while with 8 PTWs, it reaches 3.28× improvement, demonstrating
    particular effectiveness in resource-limited systems.


    8.7.3 Number of L2 TLB MSHR. The number of MSHRs impacts the maximum concurrent
    translation requests. We provide a sensitivity analysis on the number of L2 TLB
    MSHRs in Figure [28.](#page-10-4) When there are half as many MSHRs, the speedup
    decreases to 1.96×; this is because the concurrency is capped by the MSHR number.
    When the number of MSHRs doubles, Heliostat+ is able to further utilize the RTAs''
    parallelism and therefore the speedup increases to 2.44×.


    8.7.4 NoC Latency. Translation requests are delivered to RTA via NoC. To understand
    the impact of NoC latency, we evaluated the speedup of Heliostat+ when the NoC
    latency is increased by 1.5 and 2 times. As shown in Figure [29,](#page-11-0)
    even with the significant latency increase, Heliostat+ achieves notable speedup
    over the baseline (2.32× and 1.90×, respectively). Interestingly, some applications
    in the high category show even higher speedup with higher NoC latency. This is
    because the increased NoC latency slows down the PTW request traffic as well.
    In other words, Heliostat+ can derive even better speedup when the NoC is slower.


    8.7.5 TLB Latency. While we used the common TLB configurations used by many studies
    [\[12,](#page-13-2) [22–](#page-13-7)[25,](#page-13-3) [53\]](#page-14-5), we
    evaluate the impact


    <span id="page-11-0"></span>![](_page_11_Figure_1.jpeg)


    Figure 29: Performance under diverse NoC latency.


    <span id="page-11-1"></span>![](_page_11_Figure_3.jpeg)


    Figure 30: Performance under diverse TLB latency.


    <span id="page-11-2"></span>![](_page_11_Figure_5.jpeg)


    Figure 31: Performance with diverse TLB prefetchers


    of diverse TLB latency. Some recent studies used longer TLB latencies (20-cycle
    L1 TLB and 80-cycle L2 TLB) [\[21\]](#page-13-0). We tested with this new setting
    and with even twice longer latencies as a stress test. As shown in Figure [30,](#page-11-1)
    the longer TLB latencies marginally drop the speedup from 2.39× to 2.26× and 2.25×,
    respectively. This is because translation throughput determines the overall performance
    in GPU rather than individual translation latencies.


    8.7.6 TLB prefetcher. While Heliostat+ introduces an RTA-enabled lookahead translation
    mechanism, Heliostat+ primarily provides a framework for lookahead translation
    and can be integrated with any prefetching algorithms. Figure [31](#page-11-2)
    presents the performance of Heliostat+ when the lookahead algorithm is replaced
    with other TLB prefetchers, sequential prefetcher (SP) and table-based arbitrary
    stride prefetcher (ASP), which are well evaluated in CPU domain [\[51\]](#page-14-12).
    With SP and ASP, Heliostat+ achieves 8.2% and 9.1% speedup over the baseline Heliostat.
    However, Heliostat+ outperforms when integrated with our proposed lookahead mechanism
    because we detect strides within each similar VPN region, unlike CPU TLB prefetchers
    which detect strides per load instruction. We observed that ASP''s stride detection
    per load instruction leads to extremely diverse stride estimation because each
    load instruction is executed by hundreds of threads concurrently in GPU. Also,
    SP''s next page translation supports only a certain access pattern for a


    ISCA ''25, June 21–25, 2025, Tokyo, Japan Yuan Feng, Yuke Li, Jiwon Lee, Won Woo
    Ro, and Hyeran Jeon


    <span id="page-11-3"></span>![](_page_11_Figure_10.jpeg)


    Figure 32: Performance comparison between Heliostat+ and repurposed L1S as victim
    cache of L1 TLB


    ![](_page_11_Figure_12.jpeg)


    Figure 33: Speedup under memory oversubscription


    certain page mapping (e.g., linear access when consecutive pages are mapped on
    the same CU), and hence cannot support broader GPU workloads.


    # 8.8 L1 TLB extension with L1S


    Heliostat+ utilizes L1S cache as a page walk cache. To understand the impact of
    this extra cache space, we compare the performance of Heliostat+ with another
    scenario where the same area of the L1S cache is repurposed as a victim cache
    for the L1 TLB, similar to a previous work [\[19\]](#page-13-26). As shown in
    Figure [32,](#page-11-3) most applications except for mvt show negligible speedup
    with the extended L1 TLB. This reaffirms the limitations of enlarged TLBs, as
    discussed in Section [3.](#page-2-4) With the same amount of extra cache space,
    Heliostat+ fundamentally alleviates the parallelism bottleneck of page walks.


    # 8.9 Benefits under Memory Oversubscription


    We evaluate Heliostat+ under GPU memory oversubscription (150% of GPU memory),
    with considering TLB flushing, CU pipeline draining, and page migration overhead.
    Using ACUD [\[4\]](#page-12-7) for CU flushing and a counter-based migration scheme
    [\[12,](#page-13-2) [22,](#page-13-7) [48,](#page-14-3) [53\]](#page-14-5), we
    find Heliostat+ maintains 1.21× speedup despite page fault handling typically
    being the primary bottleneck during active CPU-GPU page migration [\[4,](#page-12-7)
    [12,](#page-13-2) [18,](#page-13-27) [22,](#page-13-7) [30,](#page-13-28) [48,](#page-14-3)
    [53\]](#page-14-5). This sustained benefit stems from improved translation throughput
    for resident pages and increased PTW bandwidth enabling faster page fault handling.


    # 8.10 Hardware Overhead and Energy Efficiency


    We implemented PDU+ in Verilog and synthesized using Synopsys Design Compiler
    and FreePDK45. Adding four PDU+ to each of 128 RTAs incurs 0.024 <sup>2</sup>
    area (6.14%) and 4.484 mW power (39.02%) overhead compared to baseline GMMU with
    16 PTWs and 16-entry single-ported CAM. Compared to a 128-PTW GMMU (which achieves
    similar speedup), PDU+ requires only 1.53% area


    <span id="page-12-8"></span>![](_page_12_Figure_2.jpeg)


    Figure 34: Normalized energy consumption of address translations of Heliostat+
    over baseline.


    and 5.8% power overhead. We also evaluated the energy consumption of page translation
    by accounting for the leakage and dynamic power of GMMU, PTW, PTW Cache, PDU,
    and L1S over the entire execution time. As shown in Figure [34,](#page-12-8) although
    Heliostat+ consumes slightly more energy for some applications (e.g., syrk and
    color), as Heliostat+ has significant speedup over baseline and uses existing
    L1S, the energy consumption is reduced by 41.42% on average. RPF implementation
    uses 32 cuckoo filters, each with 16 rows and 4-way associativity (2048 8-bit
    fingerprints total, using 4 packed bits with 3-bit compression and 5 direct bits)[\[11\]](#page-13-24),
    achieving 1.53% theoretical false positive rate. The 2KB total filter size represents
    1.6% area overhead versus baseline L2 TLB per CACTI[\[31\]](#page-13-29) measurements.


    #### 9 Related Works


    #### 9.1 GPU Virtual Memory


    Virtual memory on GPU is intensively studied. Power et al. [\[39\]](#page-13-30)
    discussed the x86-style virtual memory on GPUs and exploration of design spaces.
    Ausavarungnirun et al. [\[3,](#page-12-0) [5\]](#page-12-9) investigated translation
    versus paging overhead trade-offs in GPU. SnakeByte [\[21\]](#page-13-0) proposed
    opportunistic page coalescing into variable sizes upon detecting mapping contiguity.
    Li et al. [\[22,](#page-13-7) [24,](#page-13-10) [25\]](#page-13-3) explored Multi-GPU
    optimizations through TLB sharing, GMMU bypassing, and PTE lazy invalidation.
    Feng et al. [\[12\]](#page-13-2) addressed Multi-Chip GPU translation through
    structural memory allocation and allocation hints for efficient coalescing. However,
    existing works overlook the unmatched concurrency between GMMU and GPU CUs'' memory
    execution, and heavily rely on opportunistic physical memory contiguity. Heliostat
    provides extra page translation parallelism through RTAs while requiring no page
    allocation contiguity.


    #### 9.2 Democratizing RTA


    Recent research has explored RTAs'' massive memory parallelism beyond graphics
    workloads. RTNN [\[59\]](#page-14-13) reformulated spatial neighbor search as
    a ray tracing problem. Nagarajan et al.[\[28,](#page-13-31) [32\]](#page-13-32)
    extended it to non-Euclidean distances and unrestricted searching. Liu et al.[\[26\]](#page-13-33)
    adapted RTA-based neighbor search for product quantization, while Lv et al.[\[27\]](#page-13-34)
    and Wang et al.[\[54\]](#page-14-14) applied RTAs to database scanning and outlier
    detection. TTA [\[14\]](#page-13-8) and HSU [\[7\]](#page-12-2) modified RTAs
    to traverse general-purpose tree structures beyond BVH. To the best of our knowledge,
    Heliostat is the first study to extend RTA capabilities to general-purpose system
    operations, specifically


    virtual memory page table walks. We demonstrate fundamental similarities between
    RTA''s BVH traversal and radixtree-based page table walks. Heliostat introduces
    a novel approach to offload page table walks to RTAs, complementing existing solutions
    while showing superior performance compared to two state-of-the-art alternatives
    [\[8,](#page-12-5) [12\]](#page-13-2).


    #### 10 Conclusion


    This paper tackles the limited translation bandwidth by leveraging the underutilized
    ray tracing accelerators without costly GMMU overprovisioning. By exploiting the
    operational similarities between page table walk and ray tracing, we propose to
    leverage RTAs for page table walk and further leverage the existing ray functions
    for lookahead translation. With significantly lower hardware overhead compared
    to the GMMU overprovisioning, Heliostat achieves 2.39× speedup over the baseline.


    #### Acknowledgments


    This work was supported by NSF grants, CCF-2114514 and CAREER-2341039. Part of
    this research was conducted using Pinnacles (NSF MRI, # 2019144) at the Cyber
    Infrastructure and Research Technologies (CIRT) at the University of California
    Merced, and Ampere® Altra® processors in servers donated by Ampere Computing.


    #### References


    - <span id="page-12-3"></span>[1] AMD. 2025. OminiPerf Documentation. [https://rocm.docs.amd.com/projects/](https://rocm.docs.amd.com/projects/omniperf/en/latest/conceptual/shader-engine.html#desc-sl1d)
    [omniperf/en/latest/conceptual/shader-engine.html#desc-sl1d](https://rocm.docs.amd.com/projects/omniperf/en/latest/conceptual/shader-engine.html#desc-sl1d)

    - <span id="page-12-1"></span>[2] Nadav Amit, Amy Tai, and Michael Wei. 2020.
    Don''t shoot down TLB shootdowns!. In Proceedings of the Fifteenth European Conference
    on Computer Systems (Heraklion, Greece) (EuroSys ''20). Association for Computing
    Machinery, New York, NY, USA, Article 35, 14 pages. [doi:10.1145/3342195.3387518](https://doi.org/10.1145/3342195.3387518)

    - <span id="page-12-0"></span>[3] Rachata Ausavarungnirun, Joshua Landgraf, Vance
    Miller, Saugata Ghose, Jayneel Gandhi, Christopher J. Rossbach, and Onur Mutlu.
    2017. Mosaic: a GPU memory manager with application-transparent support for multiple
    page sizes. In Proceedings of the 50th Annual IEEE/ACM International Symposium
    on Microarchitecture (Cambridge, Massachusetts) (MICRO-50 ''17). Association for
    Computing Machinery, New York, NY, USA, 136–150. [doi:10.1145/3123939.3123975](https://doi.org/10.1145/3123939.3123975)

    - <span id="page-12-7"></span>[4] Rachata Ausavarungnirun, Joshua Landgraf, Vance
    Miller, Saugata Ghose, Jayneel Gandhi, Christopher J. Rossbach, and Onur Mutlu.
    2017. Mosaic: a GPU memory manager with application-transparent support for multiple
    page sizes. In Proceedings of the 50th Annual IEEE/ACM International Symposium
    on Microarchitecture (Cambridge, Massachusetts) (MICRO-50 ''17). Association for
    Computing Machinery, New York, NY, USA, 136–150. [doi:10.1145/3123939.3123975](https://doi.org/10.1145/3123939.3123975)

    - <span id="page-12-9"></span>[5] Rachata Ausavarungnirun, Vance Miller, Joshua
    Landgraf, Saugata Ghose, Jayneel Gandhi, Adwait Jog, Christopher J Rossbach, and
    Onur Mutlu. 2018. Mask: Redesigning the gpu memory hierarchy to support multi-application
    concurrency. ACM SIGPLAN Notices 53, 2 (2018), 503–518.

    - <span id="page-12-4"></span>[6] Yuhui Bao, Yifan Sun, Zlatan Feric, Michael
    Tian Shen, Micah Weston, José L. Abellán, Trinayan Baruah, John Kim, Ajay Joshi,
    and David Kaeli. 2023. NaviSim: A Highly Accurate GPU Simulator for AMD RDNA GPUs.
    In Proceedings of the International Conference on Parallel Architectures and Compilation
    Techniques (Chicago, Illinois) (PACT ''22). Association for Computing Machinery,
    New York, NY, USA, 333–345. [doi:10.1145/3559009.3569666](https://doi.org/10.1145/3559009.3569666)

    - <span id="page-12-2"></span>[7] Aaron Barnes, Fangjia Shen, and Timothy G. Rogers.
    2024. Extending GPU Ray-Tracing Units for Hierarchical Search Acceleration . In
    2024 57th IEEE/ACM International Symposium on Microarchitecture (MICRO). IEEE
    Computer Society, Los Alamitos, CA, USA, 1027–1040. [doi:10.1109/MICRO61859.2024.00079](https://doi.org/10.1109/MICRO61859.2024.00079)

    - <span id="page-12-5"></span>[8] Trinayan Baruah, Yifan Sun, Saiful A. Mojumder,
    José L. Abellán, Yash Ukidave, Ajay Joshi, Norman Rubin, John Kim, and David Kaeli.
    2020. Valkyrie: Leveraging Inter-TLB Locality to Enhance GPU Performance. In Proceedings
    of the ACM International Conference on Parallel Architectures and Compilation
    Techniques (Virtual Event, GA, USA) (PACT ''20). Association for Computing Machinery,
    New York, NY, USA, 455–466. [doi:10.1145/3410463.3414639](https://doi.org/10.1145/3410463.3414639)

    - <span id="page-12-6"></span>[9] Jichuan Chang and Gurindar S. Sohi. 2007. Cooperative
    cache partitioning for chip multiprocessors. In ACM International Conference on
    Supercomputing 25th Anniversary Volume (Munich, Germany). Association for Computing
    Machinery, New York, NY, USA, 402–412. [doi:10.1145/2591635.2667188](https://doi.org/10.1145/2591635.2667188)

    - <span id="page-13-15"></span>[10] Nosayba El-Sayed, Anurag Mukkara, Po-An Tsai,
    Harshad Kasture, Xiaosong Ma, and Daniel Sanchez. 2018. KPart: A Hybrid Cache
    Partitioning-Sharing Technique for Commodity Multicores . In 2018 IEEE International
    Symposium on High Performance Computer Architecture (HPCA). IEEE Computer Society,
    Los Alamitos, CA, USA, 104–117. [doi:10.1109/HPCA.2018.00019](https://doi.org/10.1109/HPCA.2018.00019)

    - <span id="page-13-24"></span>[11] Bin Fan, Dave G. Andersen, Michael Kaminsky,
    and Michael D. Mitzenmacher. 2014. Cuckoo Filter: Practically Better Than Bloom.
    In Proceedings of the 10th ACM International on Conference on Emerging Networking
    Experiments and Technologies (Sydney, Australia) (CoNEXT ''14). Association for
    Computing Machinery, New York, NY, USA, 75–88. [doi:10.1145/2674005.2674994](https://doi.org/10.1145/2674005.2674994)

    - <span id="page-13-2"></span>[12] Yuan Feng, Seonjin Na, Hyesoon Kim, and Hyeran
    Jeon. 2024. Barre Chord: Efficient Virtual Memory Translation for Multi-Chip-Module
    GPUs . In 2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture
    (ISCA). IEEE Computer Society, Los Alamitos, CA, USA, 834–847. [doi:10.1109/ISCA59077.](https://doi.org/10.1109/ISCA59077.2024.00065)
    [2024.00065](https://doi.org/10.1109/ISCA59077.2024.00065)

    - <span id="page-13-19"></span>[13] Krishnan Gosakan, Jaehyun Han, William Kuszmaul,
    Ibrahim N. Mubarek, Nirjhar Mukherjee, Karthik Sriram, Guido Tagliavini, Evan
    West, Michael A. Bender, Abhishek Bhattacharjee, Alex Conway, Martin Farach-Colton,
    Jayneel Gandhi, Rob Johnson, Sudarsun Kannan, and Donald E. Porter. 2023. Mosaic
    Pages: Big TLB Reach with Small Pages. In Proceedings of the 28th ACM International
    Conference on Architectural Support for Programming Languages and Operating Systems,
    Volume 3 (<conf-loc>, <city>Vancouver</city>, <state>BC</state>, <country>Canada</country>,
    </conf-loc>) (ASPLOS 2023). Association for Computing Machinery, New York, NY,
    USA, 433–448. [doi:10.1145/3582016.3582021](https://doi.org/10.1145/3582016.3582021)

    - <span id="page-13-8"></span>[14] Dongho Ha, Lufei Liu, Yuan Hsi Chou, Seokjin
    Go, Won Woo Ro, Hung-Wei Tseng, and Tor M. Aamodt. 2024. Generalizing Ray Tracing
    Accelerators for Tree Traversals on GPUs . In 2024 57th IEEE/ACM International
    Symposium on Microarchitecture (MICRO). IEEE Computer Society, Los Alamitos, CA,
    USA, 1041–1057. [doi:10.1109/MICRO61859.2024.00080](https://doi.org/10.1109/MICRO61859.2024.00080)

    - <span id="page-13-12"></span>[15] Intel. 2024. Arc A-series GPU architecture
    whitepaper. [https://cdrdv2-public.](https://cdrdv2-public.intel.com/758302/introduction-to-the-xe-hpg-architecture-white-paper.pdf)
    [intel.com/758302/introduction-to-the-xe-hpg-architecture-white-paper.pdf](https://cdrdv2-public.intel.com/758302/introduction-to-the-xe-hpg-architecture-white-paper.pdf)

    - <span id="page-13-9"></span>[16] Aamer Jaleel, Eiman Ebrahimi, and Sam Duncan.
    2019. Ducati: High-performance address translation by extending tlb reach of gpu-accelerated
    systems. ACM Transactions on Architecture and Code Optimization (TACO) 16, 1 (2019),
    1–24.

    - <span id="page-13-21"></span>[17] Norman P Jouppi. 1990. Improving direct-mapped
    cache performance by the addition of a small fully-associative cache and prefetch
    buffers. ACM SIGARCH Computer Architecture News 18, 2SI (1990), 364–373.

    - <span id="page-13-27"></span>[18] Hyojong Kim, Jaewoong Sim, Prasun Gera, Ramyad
    Hadidi, and Hyesoon Kim. 2020. Batch-Aware Unified Memory Management in GPUs for
    Irregular Workloads. In Proceedings of the Twenty-Fifth International Conference
    on Architectural Support for Programming Languages and Operating Systems (Lausanne,
    Switzerland) (ASPLOS ''20). Association for Computing Machinery, New York, NY,
    USA, 1357–1370. [doi:10.1145/3373376.3378529](https://doi.org/10.1145/3373376.3378529)

    - <span id="page-13-26"></span>[19] Jagadish B. Kotra, Michael LeBeane, Mahmut
    T. Kandemir, and Gabriel H. Loh. 2021. Increasing GPU Translation Reach by Leveraging
    Under-Utilized On-Chip Resources. In MICRO-54: 54th Annual IEEE/ACM International
    Symposium on Microarchitecture (Virtual Event, Greece) (MICRO ''21). Association
    for Computing Machinery, New York, NY, USA, 1169–1181. [doi:10.1145/3466752.3480105](https://doi.org/10.1145/3466752.3480105)

    - <span id="page-13-6"></span>[20] Mohan Kumar Kumar, Steffen Maass, Sanidhya
    Kashyap, Ján Veselý, Zi Yan, Taesoo Kim, Abhishek Bhattacharjee, and Tushar Krishna.
    2018. LATR: Lazy Translation Coherence. In Proceedings of the Twenty-Third International
    Conference on Architectural Support for Programming Languages and Operating Systems
    (Williamsburg, VA, USA) (ASPLOS ''18). Association for Computing Machinery, New
    York, NY, USA, 651–664. [doi:10.1145/3173162.3173198](https://doi.org/10.1145/3173162.3173198)

    - <span id="page-13-0"></span>[21] Jiwon Lee, Ju Min Lee, Yunho Oh, William J.
    Song, and Won Woo Ro. 2023. SnakeByte: A TLB Design with Adaptive and Recursive
    Page Merging in GPUs . In 2023 IEEE International Symposium on High-Performance
    Computer Architecture (HPCA). IEEE Computer Society, Los Alamitos, CA, USA, 1195–1207.
    [doi:10.1109/](https://doi.org/10.1109/HPCA56546.2023.10071063) [HPCA56546.2023.10071063](https://doi.org/10.1109/HPCA56546.2023.10071063)

    - <span id="page-13-7"></span>[22] Bingyao Li, Yanan Guo, Yueqi Wang, Aamer Jaleel,
    Jun Yang, and Xulong Tang. 2023. IDYLL: Enhancing Page Translation in Multi-GPUs
    via Light Weight PTE Invalidations. In Proceedings of the 56th Annual IEEE/ACM
    International Symposium on Microarchitecture (<conf-loc>, <city>Toronto</city>,
    <state>ON</state>, <country>Canada</country>, </conf-loc>) (MICRO ''23). Association
    for Computing Machinery, New York, NY, USA, 1163–1177. [doi:10.1145/3613424.3614269](https://doi.org/10.1145/3613424.3614269)

    - [23] Bingyao Li, Yueqi Wang, Tianyu Wang, Lieven Eeckhout, Jun Yang, Aamer Jaleel,
    and Xulong Tang. 2024. STAR: Sub-Entry Sharing-Aware TLB for Multi-Instance GPU
    . In 2024 57th IEEE/ACM International Symposium on Microarchitecture (MICRO).
    IEEE Computer Society, Los Alamitos, CA, USA, 309–323. [doi:10.1109/](https://doi.org/10.1109/MICRO61859.2024.00031)
    [MICRO61859.2024.00031](https://doi.org/10.1109/MICRO61859.2024.00031)

    - <span id="page-13-10"></span>[24] Bingyao Li, Jieming Yin, Anup Holey, Youtao
    Zhang, Jun Yang, and Xulong Tang. 2023. Trans-FW: Short Circuiting Page Table
    Walk in Multi-GPU Systems via Remote Forwarding . In 2023 IEEE International Symposium
    on High-Performance Computer Architecture (HPCA). IEEE Computer Society, Los Alamitos,
    CA, USA, 456–470. [doi:10.1109/HPCA56546.2023.10071054](https://doi.org/10.1109/HPCA56546.2023.10071054)

    - <span id="page-13-3"></span>[25] Bingyao Li, Jieming Yin, Youtao Zhang, and
    Xulong Tang. 2021. Improving Address Translation in Multi-GPUs via Sharing and
    Spilling Aware TLB Design. In MICRO-54: 54th Annual IEEE/ACM International Symposium
    on Microarchitecture


    (Virtual Event, Greece) (MICRO ''21). Association for Computing Machinery, New
    York, NY, USA, 1154–1168. [doi:10.1145/3466752.3480083](https://doi.org/10.1145/3466752.3480083)


    - <span id="page-13-33"></span>[26] Zihan Liu, Wentao Ni, Jingwen Leng, Yu Feng,
    Cong Guo, Quan Chen, Chao Li, Minyi Guo, and Yuhao Zhu. 2024. JUNO: Optimizing
    High-Dimensional Approximate Nearest Neighbour Search with Sparsity-Aware Algorithm
    and Ray-Tracing Core Mapping. In Proceedings of the 29th ACM International Conference
    on Architectural Support for Programming Languages and Operating Systems, Volume
    2 (La Jolla, CA, USA) (ASPLOS ''24). Association for Computing Machinery, New
    York, NY, USA, 549–565. [doi:10.1145/3620665.3640360](https://doi.org/10.1145/3620665.3640360)

    - <span id="page-13-34"></span>[27] Yangming Lv, Kai Zhang, Ziming Wang, Xiaodong
    Zhang, Rubao Lee, Zhenying He, Yinan Jing, and X Sean Wang. 2024. RTScan: Efficient
    Scan with Ray Tracing Cores. Proceedings of the VLDB Endowment 17, 6 (2024), 1460–1472.

    - <span id="page-13-31"></span>[28] Durga Keerthi Mandarapu, Vani Nagarajan, Artem
    Pelenitsyn, and Milind Kulkarni. 2024. Arkade: k-Nearest Neighbor Search With
    Non-Euclidean Distances using GPU Ray Tracing. In Proceedings of the 38th ACM
    International Conference on Supercomputing (Kyoto, Japan) (ICS ''24). Association
    for Computing Machinery, New York, NY, USA, 14–25. [doi:10.1145/3650200.3656601](https://doi.org/10.1145/3650200.3656601)

    - <span id="page-13-14"></span>[29] Daniel Meister, Paritosh Kulkarni, Aaryaman
    Vasishta, and Takahiro Harada. 2024. HIPRT: A Ray Tracing Framework in HIP. Proceedings
    of the ACM on Computer Graphics and Interactive Techniques 7, 3 (2024), 1–18.

    - <span id="page-13-28"></span>[30] Ugljesa Milic, Oreste Villa, Evgeny Bolotin,
    Akhil Arunkumar, Eiman Ebrahimi, Aamer Jaleel, Alex Ramirez, and David Nellans.
    2017. Beyond the Socket: NUMA-Aware GPUs . In 2017 50th Annual IEEE/ACM International
    Symposium on Microarchitecture (MICRO). IEEE Computer Society, Los Alamitos, CA,
    USA, 123–135. <https://doi.ieeecomputersociety.org/>

    - <span id="page-13-29"></span>[31] Naveen Muralimanohar, Rajeev Balasubramonian,
    and Norman P Jouppi. 2009. CACTI 6.0: A tool to model large caches. HP laboratories
    27 (2009), 28.

    - <span id="page-13-32"></span>[32] Vani Nagarajan, Durga Mandarapu, and Milind
    Kulkarni. 2023. RT-kNNS Unbound: Using RT Cores to Accelerate Unrestricted Neighbor
    Search. In Proceedings of the 37th ACM International Conference on Supercomputing
    (Orlando, FL, USA) (ICS ''23). Association for Computing Machinery, New York,
    NY, USA, 289–300. [doi:10.1145/3577193.3593738](https://doi.org/10.1145/3577193.3593738)

    - <span id="page-13-22"></span>[33] Kyle J. Nesbit and James E. Smith. 2004. Data
    Cache Prefetching Using a Global History Buffer. In Proceedings of the 10th International
    Symposium on High Performance Computer Architecture (HPCA ''04). IEEE Computer
    Society, USA, 96. [doi:10.1109/HPCA.2004.10030](https://doi.org/10.1109/HPCA.2004.10030)

    - <span id="page-13-25"></span>[34] NVIDIA. 2024. Hopper architecture whitepaper.
    [https://resources.nvidia.com/en](https://resources.nvidia.com/en-us-tensor-core/gtc22-whitepaper-hopper)[us-tensor-core/gtc22-whitepaper-hopper](https://resources.nvidia.com/en-us-tensor-core/gtc22-whitepaper-hopper)

    - <span id="page-13-11"></span>[35] NVIDIA. 2024. Turing architecture whitepaper.
    [https://images.nvidia.com/aem](https://images.nvidia.com/aem-dam/en-zz/Solutions/design-visualization/technologies/turing-architecture/NVIDIA-Turing-Architecture-Whitepaper.pdf)[dam/en-zz/Solutions/design-visualization/technologies/turing-architecture/](https://images.nvidia.com/aem-dam/en-zz/Solutions/design-visualization/technologies/turing-architecture/NVIDIA-Turing-Architecture-Whitepaper.pdf)
    [NVIDIA-Turing-Architecture-Whitepaper.pdf](https://images.nvidia.com/aem-dam/en-zz/Solutions/design-visualization/technologies/turing-architecture/NVIDIA-Turing-Architecture-Whitepaper.pdf)

    - <span id="page-13-23"></span>[36] PCI-SIG. 2025. PCI-SIG 6.0 Specification.
    [https://pcisig.com/pci-express-6.0](https://pcisig.com/pci-express-6.0-specification)
    [specification](https://pcisig.com/pci-express-6.0-specification)

    - <span id="page-13-20"></span>[37] Binh Pham, Abhishek Bhattacharjee, Yasuko
    Eckert, and Gabriel H. Loh. 2014. Increasing TLB reach by exploiting clustering
    in page translations . In 2014 IEEE 20th International Symposium on High Performance
    Computer Architecture (HPCA). IEEE Computer Society, Los Alamitos, CA, USA, 558–567.
    [doi:10.1109/HPCA.](https://doi.org/10.1109/HPCA.2014.6835964) [2014.6835964](https://doi.org/10.1109/HPCA.2014.6835964)

    - <span id="page-13-1"></span>[38] Binh Pham, Viswanathan Vaidyanathan, Aamer
    Jaleel, and Abhishek Bhattacharjee. 2012. CoLT: Coalesced Large-Reach TLBs. In
    Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture
    (Vancouver, B.C., CANADA) (MICRO-45). IEEE Computer Society, USA, 258–269. [doi:10.1109/MICRO.2012.32](https://doi.org/10.1109/MICRO.2012.32)

    - <span id="page-13-30"></span>[39] Jason Power, Mark D. Hill, and David A. Wood.
    2014. Supporting x86-64 address translation for 100s of GPU lanes. In 2014 IEEE
    20th International Symposium on High Performance Computer Architecture (HPCA).
    IEEE, Orlando, FL, USA, 568–578. [doi:10.1109/HPCA.2014.6835965](https://doi.org/10.1109/HPCA.2014.6835965)

    - <span id="page-13-4"></span>[40] B. Pratheek, Neha Jawalkar, and Arkaprava Basu.
    2021. Improving GPU Multitenancy with Page Walk Stealing. In 2021 IEEE International
    Symposium on High-Performance Computer Architecture (HPCA). IEEE, Seoul, Korea
    (South), 626–639. [doi:10.1109/HPCA51647.2021.00059](https://doi.org/10.1109/HPCA51647.2021.00059)

    - <span id="page-13-5"></span>[41] B Pratheek, Neha Jawalkar, and Arkaprava Basu.
    2022. Designing Virtual Memory System of MCM GPUs. In 2022 55th IEEE/ACM International
    Symposium on Microarchitecture (MICRO). IEEE, Chicago, IL, USA, 404–422. [doi:10.1109/](https://doi.org/10.1109/MICRO56248.2022.00036)
    [MICRO56248.2022.00036](https://doi.org/10.1109/MICRO56248.2022.00036)

    - <span id="page-13-17"></span>[42] Timothy J Purcell, Ian Buck, William R Mark,
    and Pat Hanrahan. 2005. Ray tracing on programmable graphics hardware. In ACM
    SIGGRAPH 2005 Courses. ACM, ACM New York, NY, USA, 268–es.

    - <span id="page-13-13"></span>[43] Mohammadreza Saed, Yuan Hsi Chou, Lufei Liu,
    Tyler Nowicki, and Tor M. Aamodt. 2022. Vulkan-Sim: A GPU Architecture Simulator
    for Ray Tracing. In 2022 55th IEEE/ACM International Symposium on Microarchitecture
    (MICRO). IEEE, Chicago, IL, USA, 263–281. [doi:10.1109/MICRO56248.2022.00027](https://doi.org/10.1109/MICRO56248.2022.00027)

    - <span id="page-13-16"></span>[44] Daniel Sanchez and Christos Kozyrakis. 2011.
    Vantage: scalable and efficient fine-grain cache partitioning. SIGARCH Comput.
    Archit. News 39, 3 (June 2011), 57–68. [doi:10.1145/2024723.2000073](https://doi.org/10.1145/2024723.2000073)

    - <span id="page-13-18"></span>[45] Carlo H Séquin and Eliot K Smyrl. 1989. Parameterized
    ray-tracing. ACM SIGGRAPH Computer Graphics 23, 3 (1989), 307–314.


    - <span id="page-14-4"></span><span id="page-14-0"></span>[46] Seunghee Shin,
    Guilherme Cox, Mark Oskin, Gabriel H. Loh, Yan Solihin, Abhishek Bhattacharjee,
    and Arkaprava Basu. 2018. Scheduling Page Table Walks for Irregular GPU Applications.
    In 2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture
    (ISCA). IEEE, Los Angeles, CA, USA, 180–192. [doi:10.1109/ISCA.2018.00025](https://doi.org/10.1109/ISCA.2018.00025)

    - [47] Seunghee Shin, Michael LeBeane, Yan Solihin, and Arkaprava Basu. 2018.
    Neighborhood-Aware Address Translation for Irregular GPU Applications. In 2018
    51st Annual IEEE/ACM International Symposium on Microarchitecture (MI-CRO). IEEE,
    Fukuoka, Japan, 352–363. [doi:10.1109/MICRO.2018.00036](https://doi.org/10.1109/MICRO.2018.00036)

    - <span id="page-14-3"></span>[48] Yifan Sun, Trinayan Baruah, Saiful A. Mojumder,
    Shi Dong, Xiang Gong, Shane Treadway, Yuhui Bao, Spencer Hance, Carter McCardwell,
    Vincent Zhao, Harrison Barclay, Amir Kavyan Ziabari, Zhongliang Chen, Rafael Ubal,
    José L. Abellán, John Kim, Ajay Joshi, and David Kaeli. 2019. MGPUSim: Enabling
    Multi-GPU Performance Modeling and Optimization. In 2019 ACM/IEEE 46th Annual
    International Symposium on Computer Architecture (ISCA). IEEE, Phoenix, AZ, USA,
    197–209.

    - <span id="page-14-1"></span>[49] Seunghwan Sung, Sujin Hur, Sungwoo Kim, Dongho
    Ha, Yunho Oh, and Won Woo Ro. 2023. MAD MAcce: Supporting Multiply-Add Operations
    for Democratizing Matrix-Multiplication Accelerators. In Proceedings of the 56th
    Annual IEEE/ACM International Symposium on Microarchitecture (Toronto, ON, Canada)
    (MICRO ''23). Association for Computing Machinery, New York, NY, USA, 367–379.
    [doi:10.](https://doi.org/10.1145/3613424.3614247) [1145/3613424.3614247](https://doi.org/10.1145/3613424.3614247)

    - <span id="page-14-7"></span>[50] Xulong Tang, Ziyu Zhang, Weizheng Xu, Mahmut
    Taylan Kandemir, Rami Melhem, and Jun Yang. 2020. Enhancing Address Translations
    in Throughput Processors via Compression. In Proceedings of the ACM International
    Conference on Parallel Architectures and Compilation Techniques (Virtual Event,
    GA, USA) (PACT ''20). Association for Computing Machinery, New York, NY, USA,
    191–204. [doi:10.1145/3410463.3414633](https://doi.org/10.1145/3410463.3414633)

    - <span id="page-14-12"></span>[51] Georgios Vavouliotis, Lluc Alvarez, Vasileios
    Karakostas, Konstantinos Nikas, Nectarios Koziris, Daniel A. Jiménez, and Marc
    Casas. 2021. Exploiting page table locality for agile TLB prefetching. In Proceedings
    of the 48th Annual International Symposium on Computer Architecture (Virtual Event,
    Spain) (ISCA ''21). IEEE Press, Valencia, Spain, 85–98. [doi:10.1109/ISCA52012.2021.00016](https://doi.org/10.1109/ISCA52012.2021.00016)

    - <span id="page-14-6"></span>[52] Vulkan. 2025. https://www.vulkan.org/.

    - <span id="page-14-5"></span>[53] Yueqi Wang, Bingyao Li, Aamer Jaleel, Jun Yang,
    and Xulong Tang. 2024. GRIT: Enhancing Multi-GPU Performance with Fine-Grained
    Dynamic Page Placement. In 2024 IEEE International Symposium on High-Performance
    Computer Architecture (HPCA). IEEE, Edinburgh, United Kingdom, 1080–1094. [doi:10.1109/HPCA57654.](https://doi.org/10.1109/HPCA57654.2024.00085)
    [2024.00085](https://doi.org/10.1109/HPCA57654.2024.00085)

    - <span id="page-14-14"></span>[54] Ziming Wang, Kai Zhang, Yangming Lv, Yinglong
    Wang, Zhigang Zhao, Zhenying He, Yinan Jing, and X. Sean Wang. 2024. RTOD: Efficient
    Outlier Detection With Ray Tracing Cores. IEEE Transactions on Knowledge and Data
    Engineering 36, 12 (2024), 9192–9204. [doi:10.1109/TKDE.2024.3453901](https://doi.org/10.1109/TKDE.2024.3453901)

    - <span id="page-14-9"></span>[55] Zi Yan, Daniel Lustig, David Nellans, and Abhishek
    Bhattacharjee. 2019. Nimble Page Management for Tiered Memory Systems. In Proceedings
    of the Twenty-Fourth International Conference on Architectural Support for Programming
    Languages and Operating Systems (Providence, RI, USA) (ASPLOS ''19). Association
    for Computing Machinery, New York, NY, USA, 331–345. [doi:10.1145/3297858.](https://doi.org/10.1145/3297858.3304024)
    [3304024](https://doi.org/10.1145/3297858.3304024)

    - <span id="page-14-8"></span>[56] Ying Ye, Richard West, Zhuoqun Cheng, and Ye
    Li. 2014. COLORIS: A dynamic cache partitioning system using page coloring. In
    2014 23rd International Conference on Parallel Architecture and Compilation Techniques
    (PACT). IEEE, Edmonton, AB, Canada, 381–392. [doi:10.1145/2628071.2628104](https://doi.org/10.1145/2628071.2628104)

    - <span id="page-14-10"></span>[57] Xiangyao Yu, Christopher J. Hughes, Nadathur
    Satish, and Srinivas Devadas. 2015. IMP: indirect memory prefetcher. In Proceedings
    of the 48th International Symposium on Microarchitecture (Waikiki, Hawaii) (MICRO-48).
    Association for Computing Machinery, New York, NY, USA, 178–190. [doi:10.1145/2830772.2830807](https://doi.org/10.1145/2830772.2830807)

    - <span id="page-14-2"></span>[58] Yunan Zhang, Po-An Tsai, and Hung-Wei Tseng.
    2022. SIMD2: a generalized matrix instruction set for accelerating tensor computation
    beyond GEMM. In Proceedings of the 49th Annual International Symposium on Computer
    Architecture (New York, New York) (ISCA ''22). Association for Computing Machinery,
    New York, NY, USA, 552–566. [doi:10.1145/3470496.3527411](https://doi.org/10.1145/3470496.3527411)

    - <span id="page-14-13"></span>[59] Yuhao Zhu. 2022. RTNN: accelerating neighbor
    search using hardware ray tracing. In Proceedings of the 27th ACM SIGPLAN Symposium
    on Principles and Practice of Parallel Programming (Seoul, Republic of Korea)
    (PPoPP ''22). Association for Computing Machinery, New York, NY, USA, 76–89. [doi:10.1145/3503221.3508409](https://doi.org/10.1145/3503221.3508409)

    - <span id="page-14-11"></span>[60] Xiaotong Zhuang and S Lee Hsien-Hsin. 2006.
    Reducing cache pollution via dynamic data prefetch filtering. IEEE Trans. Comput.
    56, 1 (2006), 18–31.'
  references:
  - '- <span id="page-12-3"></span>[1] AMD. 2025. OminiPerf Documentation. [https://rocm.docs.amd.com/projects/](https://rocm.docs.amd.com/projects/omniperf/en/latest/conceptual/shader-engine.html#desc-sl1d)
    [omniperf/en/latest/conceptual/shader-engine.html#desc-sl1d](https://rocm.docs.amd.com/projects/omniperf/en/latest/conceptual/shader-engine.html#desc-sl1d)'
  - '- <span id="page-12-1"></span>[2] Nadav Amit, Amy Tai, and Michael Wei. 2020.
    Don''t shoot down TLB shootdowns!. In Proceedings of the Fifteenth European Conference
    on Computer Systems (Heraklion, Greece) (EuroSys ''20). Association for Computing
    Machinery, New York, NY, USA, Article 35, 14 pages. [doi:10.1145/3342195.3387518](https://doi.org/10.1145/3342195.3387518)'
  - '- <span id="page-12-0"></span>[3] Rachata Ausavarungnirun, Joshua Landgraf, Vance
    Miller, Saugata Ghose, Jayneel Gandhi, Christopher J. Rossbach, and Onur Mutlu.
    2017. Mosaic: a GPU memory manager with application-transparent support for multiple
    page sizes. In Proceedings of the 50th Annual IEEE/ACM International Symposium
    on Microarchitecture (Cambridge, Massachusetts) (MICRO-50 ''17). Association for
    Computing Machinery, New York, NY, USA, 136–150. [doi:10.1145/3123939.3123975](https://doi.org/10.1145/3123939.3123975)'
  - '- <span id="page-12-7"></span>[4] Rachata Ausavarungnirun, Joshua Landgraf, Vance
    Miller, Saugata Ghose, Jayneel Gandhi, Christopher J. Rossbach, and Onur Mutlu.
    2017. Mosaic: a GPU memory manager with application-transparent support for multiple
    page sizes. In Proceedings of the 50th Annual IEEE/ACM International Symposium
    on Microarchitecture (Cambridge, Massachusetts) (MICRO-50 ''17). Association for
    Computing Machinery, New York, NY, USA, 136–150. [doi:10.1145/3123939.3123975](https://doi.org/10.1145/3123939.3123975)'
  - '- <span id="page-12-9"></span>[5] Rachata Ausavarungnirun, Vance Miller, Joshua
    Landgraf, Saugata Ghose, Jayneel Gandhi, Adwait Jog, Christopher J Rossbach, and
    Onur Mutlu. 2018. Mask: Redesigning the gpu memory hierarchy to support multi-application
    concurrency. ACM SIGPLAN Notices 53, 2 (2018), 503–518.'
  - '- <span id="page-12-4"></span>[6] Yuhui Bao, Yifan Sun, Zlatan Feric, Michael
    Tian Shen, Micah Weston, José L. Abellán, Trinayan Baruah, John Kim, Ajay Joshi,
    and David Kaeli. 2023. NaviSim: A Highly Accurate GPU Simulator for AMD RDNA GPUs.
    In Proceedings of the International Conference on Parallel Architectures and Compilation
    Techniques (Chicago, Illinois) (PACT ''22). Association for Computing Machinery,
    New York, NY, USA, 333–345. [doi:10.1145/3559009.3569666](https://doi.org/10.1145/3559009.3569666)'
  - '- <span id="page-12-2"></span>[7] Aaron Barnes, Fangjia Shen, and Timothy G.
    Rogers. 2024. Extending GPU Ray-Tracing Units for Hierarchical Search Acceleration
    . In 2024 57th IEEE/ACM International Symposium on Microarchitecture (MICRO).
    IEEE Computer Society, Los Alamitos, CA, USA, 1027–1040. [doi:10.1109/MICRO61859.2024.00079](https://doi.org/10.1109/MICRO61859.2024.00079)'
  - '- <span id="page-12-5"></span>[8] Trinayan Baruah, Yifan Sun, Saiful A. Mojumder,
    José L. Abellán, Yash Ukidave, Ajay Joshi, Norman Rubin, John Kim, and David Kaeli.
    2020. Valkyrie: Leveraging Inter-TLB Locality to Enhance GPU Performance. In Proceedings
    of the ACM International Conference on Parallel Architectures and Compilation
    Techniques (Virtual Event, GA, USA) (PACT ''20). Association for Computing Machinery,
    New York, NY, USA, 455–466. [doi:10.1145/3410463.3414639](https://doi.org/10.1145/3410463.3414639)'
  - '- <span id="page-12-6"></span>[9] Jichuan Chang and Gurindar S. Sohi. 2007. Cooperative
    cache partitioning for chip multiprocessors. In ACM International Conference on
    Supercomputing 25th Anniversary Volume (Munich, Germany). Association for Computing
    Machinery, New York, NY, USA, 402–412. [doi:10.1145/2591635.2667188](https://doi.org/10.1145/2591635.2667188)'
  - '- <span id="page-13-15"></span>[10] Nosayba El-Sayed, Anurag Mukkara, Po-An Tsai,
    Harshad Kasture, Xiaosong Ma, and Daniel Sanchez. 2018. KPart: A Hybrid Cache
    Partitioning-Sharing Technique for Commodity Multicores . In 2018 IEEE International
    Symposium on High Performance Computer Architecture (HPCA). IEEE Computer Society,
    Los Alamitos, CA, USA, 104–117. [doi:10.1109/HPCA.2018.00019](https://doi.org/10.1109/HPCA.2018.00019)'
  - '- <span id="page-13-24"></span>[11] Bin Fan, Dave G. Andersen, Michael Kaminsky,
    and Michael D. Mitzenmacher. 2014. Cuckoo Filter: Practically Better Than Bloom.
    In Proceedings of the 10th ACM International on Conference on Emerging Networking
    Experiments and Technologies (Sydney, Australia) (CoNEXT ''14). Association for
    Computing Machinery, New York, NY, USA, 75–88. [doi:10.1145/2674005.2674994](https://doi.org/10.1145/2674005.2674994)'
  - '- <span id="page-13-2"></span>[12] Yuan Feng, Seonjin Na, Hyesoon Kim, and Hyeran
    Jeon. 2024. Barre Chord: Efficient Virtual Memory Translation for Multi-Chip-Module
    GPUs . In 2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture
    (ISCA). IEEE Computer Society, Los Alamitos, CA, USA, 834–847. [doi:10.1109/ISCA59077.](https://doi.org/10.1109/ISCA59077.2024.00065)
    [2024.00065](https://doi.org/10.1109/ISCA59077.2024.00065)'
  - '- <span id="page-13-19"></span>[13] Krishnan Gosakan, Jaehyun Han, William Kuszmaul,
    Ibrahim N. Mubarek, Nirjhar Mukherjee, Karthik Sriram, Guido Tagliavini, Evan
    West, Michael A. Bender, Abhishek Bhattacharjee, Alex Conway, Martin Farach-Colton,
    Jayneel Gandhi, Rob Johnson, Sudarsun Kannan, and Donald E. Porter. 2023. Mosaic
    Pages: Big TLB Reach with Small Pages. In Proceedings of the 28th ACM International
    Conference on Architectural Support for Programming Languages and Operating Systems,
    Volume 3 (<conf-loc>, <city>Vancouver</city>, <state>BC</state>, <country>Canada</country>,
    </conf-loc>) (ASPLOS 2023). Association for Computing Machinery, New York, NY,
    USA, 433–448. [doi:10.1145/3582016.3582021](https://doi.org/10.1145/3582016.3582021)'
  - '- <span id="page-13-8"></span>[14] Dongho Ha, Lufei Liu, Yuan Hsi Chou, Seokjin
    Go, Won Woo Ro, Hung-Wei Tseng, and Tor M. Aamodt. 2024. Generalizing Ray Tracing
    Accelerators for Tree Traversals on GPUs . In 2024 57th IEEE/ACM International
    Symposium on Microarchitecture (MICRO). IEEE Computer Society, Los Alamitos, CA,
    USA, 1041–1057. [doi:10.1109/MICRO61859.2024.00080](https://doi.org/10.1109/MICRO61859.2024.00080)'
  - '- <span id="page-13-12"></span>[15] Intel. 2024. Arc A-series GPU architecture
    whitepaper. [https://cdrdv2-public.](https://cdrdv2-public.intel.com/758302/introduction-to-the-xe-hpg-architecture-white-paper.pdf)
    [intel.com/758302/introduction-to-the-xe-hpg-architecture-white-paper.pdf](https://cdrdv2-public.intel.com/758302/introduction-to-the-xe-hpg-architecture-white-paper.pdf)'
  - '- <span id="page-13-9"></span>[16] Aamer Jaleel, Eiman Ebrahimi, and Sam Duncan.
    2019. Ducati: High-performance address translation by extending tlb reach of gpu-accelerated
    systems. ACM Transactions on Architecture and Code Optimization (TACO) 16, 1 (2019),
    1–24.'
  - '- <span id="page-13-21"></span>[17] Norman P Jouppi. 1990. Improving direct-mapped
    cache performance by the addition of a small fully-associative cache and prefetch
    buffers. ACM SIGARCH Computer Architecture News 18, 2SI (1990), 364–373.'
  - '- <span id="page-13-27"></span>[18] Hyojong Kim, Jaewoong Sim, Prasun Gera, Ramyad
    Hadidi, and Hyesoon Kim. 2020. Batch-Aware Unified Memory Management in GPUs for
    Irregular Workloads. In Proceedings of the Twenty-Fifth International Conference
    on Architectural Support for Programming Languages and Operating Systems (Lausanne,
    Switzerland) (ASPLOS ''20). Association for Computing Machinery, New York, NY,
    USA, 1357–1370. [doi:10.1145/3373376.3378529](https://doi.org/10.1145/3373376.3378529)'
  - '- <span id="page-13-26"></span>[19] Jagadish B. Kotra, Michael LeBeane, Mahmut
    T. Kandemir, and Gabriel H. Loh. 2021. Increasing GPU Translation Reach by Leveraging
    Under-Utilized On-Chip Resources. In MICRO-54: 54th Annual IEEE/ACM International
    Symposium on Microarchitecture (Virtual Event, Greece) (MICRO ''21). Association
    for Computing Machinery, New York, NY, USA, 1169–1181. [doi:10.1145/3466752.3480105](https://doi.org/10.1145/3466752.3480105)'
  - '- <span id="page-13-6"></span>[20] Mohan Kumar Kumar, Steffen Maass, Sanidhya
    Kashyap, Ján Veselý, Zi Yan, Taesoo Kim, Abhishek Bhattacharjee, and Tushar Krishna.
    2018. LATR: Lazy Translation Coherence. In Proceedings of the Twenty-Third International
    Conference on Architectural Support for Programming Languages and Operating Systems
    (Williamsburg, VA, USA) (ASPLOS ''18). Association for Computing Machinery, New
    York, NY, USA, 651–664. [doi:10.1145/3173162.3173198](https://doi.org/10.1145/3173162.3173198)'
  - '- <span id="page-13-0"></span>[21] Jiwon Lee, Ju Min Lee, Yunho Oh, William J.
    Song, and Won Woo Ro. 2023. SnakeByte: A TLB Design with Adaptive and Recursive
    Page Merging in GPUs . In 2023 IEEE International Symposium on High-Performance
    Computer Architecture (HPCA). IEEE Computer Society, Los Alamitos, CA, USA, 1195–1207.
    [doi:10.1109/](https://doi.org/10.1109/HPCA56546.2023.10071063) [HPCA56546.2023.10071063](https://doi.org/10.1109/HPCA56546.2023.10071063)'
  - '- <span id="page-13-7"></span>[22] Bingyao Li, Yanan Guo, Yueqi Wang, Aamer Jaleel,
    Jun Yang, and Xulong Tang. 2023. IDYLL: Enhancing Page Translation in Multi-GPUs
    via Light Weight PTE Invalidations. In Proceedings of the 56th Annual IEEE/ACM
    International Symposium on Microarchitecture (<conf-loc>, <city>Toronto</city>,
    <state>ON</state>, <country>Canada</country>, </conf-loc>) (MICRO ''23). Association
    for Computing Machinery, New York, NY, USA, 1163–1177. [doi:10.1145/3613424.3614269](https://doi.org/10.1145/3613424.3614269)'
  - '- [23] Bingyao Li, Yueqi Wang, Tianyu Wang, Lieven Eeckhout, Jun Yang, Aamer
    Jaleel, and Xulong Tang. 2024. STAR: Sub-Entry Sharing-Aware TLB for Multi-Instance
    GPU . In 2024 57th IEEE/ACM International Symposium on Microarchitecture (MICRO).
    IEEE Computer Society, Los Alamitos, CA, USA, 309–323. [doi:10.1109/](https://doi.org/10.1109/MICRO61859.2024.00031)
    [MICRO61859.2024.00031](https://doi.org/10.1109/MICRO61859.2024.00031)'
  - '- <span id="page-13-10"></span>[24] Bingyao Li, Jieming Yin, Anup Holey, Youtao
    Zhang, Jun Yang, and Xulong Tang. 2023. Trans-FW: Short Circuiting Page Table
    Walk in Multi-GPU Systems via Remote Forwarding . In 2023 IEEE International Symposium
    on High-Performance Computer Architecture (HPCA). IEEE Computer Society, Los Alamitos,
    CA, USA, 456–470. [doi:10.1109/HPCA56546.2023.10071054](https://doi.org/10.1109/HPCA56546.2023.10071054)'
  - '- <span id="page-13-3"></span>[25] Bingyao Li, Jieming Yin, Youtao Zhang, and
    Xulong Tang. 2021. Improving Address Translation in Multi-GPUs via Sharing and
    Spilling Aware TLB Design. In MICRO-54: 54th Annual IEEE/ACM International Symposium
    on Microarchitecture'
  - (Virtual Event, Greece) (MICRO '21). Association for Computing Machinery, New
    York, NY, USA, 1154–1168. [doi:10.1145/3466752.3480083](https://doi.org/10.1145/3466752.3480083)
  - '- <span id="page-13-33"></span>[26] Zihan Liu, Wentao Ni, Jingwen Leng, Yu Feng,
    Cong Guo, Quan Chen, Chao Li, Minyi Guo, and Yuhao Zhu. 2024. JUNO: Optimizing
    High-Dimensional Approximate Nearest Neighbour Search with Sparsity-Aware Algorithm
    and Ray-Tracing Core Mapping. In Proceedings of the 29th ACM International Conference
    on Architectural Support for Programming Languages and Operating Systems, Volume
    2 (La Jolla, CA, USA) (ASPLOS ''24). Association for Computing Machinery, New
    York, NY, USA, 549–565. [doi:10.1145/3620665.3640360](https://doi.org/10.1145/3620665.3640360)'
  - '- <span id="page-13-34"></span>[27] Yangming Lv, Kai Zhang, Ziming Wang, Xiaodong
    Zhang, Rubao Lee, Zhenying He, Yinan Jing, and X Sean Wang. 2024. RTScan: Efficient
    Scan with Ray Tracing Cores. Proceedings of the VLDB Endowment 17, 6 (2024), 1460–1472.'
  - '- <span id="page-13-31"></span>[28] Durga Keerthi Mandarapu, Vani Nagarajan,
    Artem Pelenitsyn, and Milind Kulkarni. 2024. Arkade: k-Nearest Neighbor Search
    With Non-Euclidean Distances using GPU Ray Tracing. In Proceedings of the 38th
    ACM International Conference on Supercomputing (Kyoto, Japan) (ICS ''24). Association
    for Computing Machinery, New York, NY, USA, 14–25. [doi:10.1145/3650200.3656601](https://doi.org/10.1145/3650200.3656601)'
  - '- <span id="page-13-14"></span>[29] Daniel Meister, Paritosh Kulkarni, Aaryaman
    Vasishta, and Takahiro Harada. 2024. HIPRT: A Ray Tracing Framework in HIP. Proceedings
    of the ACM on Computer Graphics and Interactive Techniques 7, 3 (2024), 1–18.'
  - '- <span id="page-13-28"></span>[30] Ugljesa Milic, Oreste Villa, Evgeny Bolotin,
    Akhil Arunkumar, Eiman Ebrahimi, Aamer Jaleel, Alex Ramirez, and David Nellans.
    2017. Beyond the Socket: NUMA-Aware GPUs . In 2017 50th Annual IEEE/ACM International
    Symposium on Microarchitecture (MICRO). IEEE Computer Society, Los Alamitos, CA,
    USA, 123–135. <https://doi.ieeecomputersociety.org/>'
  - '- <span id="page-13-29"></span>[31] Naveen Muralimanohar, Rajeev Balasubramonian,
    and Norman P Jouppi. 2009. CACTI 6.0: A tool to model large caches. HP laboratories
    27 (2009), 28.'
  - '- <span id="page-13-32"></span>[32] Vani Nagarajan, Durga Mandarapu, and Milind
    Kulkarni. 2023. RT-kNNS Unbound: Using RT Cores to Accelerate Unrestricted Neighbor
    Search. In Proceedings of the 37th ACM International Conference on Supercomputing
    (Orlando, FL, USA) (ICS ''23). Association for Computing Machinery, New York,
    NY, USA, 289–300. [doi:10.1145/3577193.3593738](https://doi.org/10.1145/3577193.3593738)'
  - '- <span id="page-13-22"></span>[33] Kyle J. Nesbit and James E. Smith. 2004.
    Data Cache Prefetching Using a Global History Buffer. In Proceedings of the 10th
    International Symposium on High Performance Computer Architecture (HPCA ''04).
    IEEE Computer Society, USA, 96. [doi:10.1109/HPCA.2004.10030](https://doi.org/10.1109/HPCA.2004.10030)'
  - '- <span id="page-13-25"></span>[34] NVIDIA. 2024. Hopper architecture whitepaper.
    [https://resources.nvidia.com/en](https://resources.nvidia.com/en-us-tensor-core/gtc22-whitepaper-hopper)[us-tensor-core/gtc22-whitepaper-hopper](https://resources.nvidia.com/en-us-tensor-core/gtc22-whitepaper-hopper)'
  - '- <span id="page-13-11"></span>[35] NVIDIA. 2024. Turing architecture whitepaper.
    [https://images.nvidia.com/aem](https://images.nvidia.com/aem-dam/en-zz/Solutions/design-visualization/technologies/turing-architecture/NVIDIA-Turing-Architecture-Whitepaper.pdf)[dam/en-zz/Solutions/design-visualization/technologies/turing-architecture/](https://images.nvidia.com/aem-dam/en-zz/Solutions/design-visualization/technologies/turing-architecture/NVIDIA-Turing-Architecture-Whitepaper.pdf)
    [NVIDIA-Turing-Architecture-Whitepaper.pdf](https://images.nvidia.com/aem-dam/en-zz/Solutions/design-visualization/technologies/turing-architecture/NVIDIA-Turing-Architecture-Whitepaper.pdf)'
  - '- <span id="page-13-23"></span>[36] PCI-SIG. 2025. PCI-SIG 6.0 Specification.
    [https://pcisig.com/pci-express-6.0](https://pcisig.com/pci-express-6.0-specification)
    [specification](https://pcisig.com/pci-express-6.0-specification)'
  - '- <span id="page-13-20"></span>[37] Binh Pham, Abhishek Bhattacharjee, Yasuko
    Eckert, and Gabriel H. Loh. 2014. Increasing TLB reach by exploiting clustering
    in page translations . In 2014 IEEE 20th International Symposium on High Performance
    Computer Architecture (HPCA). IEEE Computer Society, Los Alamitos, CA, USA, 558–567.
    [doi:10.1109/HPCA.](https://doi.org/10.1109/HPCA.2014.6835964) [2014.6835964](https://doi.org/10.1109/HPCA.2014.6835964)'
  - '- <span id="page-13-1"></span>[38] Binh Pham, Viswanathan Vaidyanathan, Aamer
    Jaleel, and Abhishek Bhattacharjee. 2012. CoLT: Coalesced Large-Reach TLBs. In
    Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture
    (Vancouver, B.C., CANADA) (MICRO-45). IEEE Computer Society, USA, 258–269. [doi:10.1109/MICRO.2012.32](https://doi.org/10.1109/MICRO.2012.32)'
  - '- <span id="page-13-30"></span>[39] Jason Power, Mark D. Hill, and David A. Wood.
    2014. Supporting x86-64 address translation for 100s of GPU lanes. In 2014 IEEE
    20th International Symposium on High Performance Computer Architecture (HPCA).
    IEEE, Orlando, FL, USA, 568–578. [doi:10.1109/HPCA.2014.6835965](https://doi.org/10.1109/HPCA.2014.6835965)'
  - '- <span id="page-13-4"></span>[40] B. Pratheek, Neha Jawalkar, and Arkaprava
    Basu. 2021. Improving GPU Multitenancy with Page Walk Stealing. In 2021 IEEE International
    Symposium on High-Performance Computer Architecture (HPCA). IEEE, Seoul, Korea
    (South), 626–639. [doi:10.1109/HPCA51647.2021.00059](https://doi.org/10.1109/HPCA51647.2021.00059)'
  - '- <span id="page-13-5"></span>[41] B Pratheek, Neha Jawalkar, and Arkaprava Basu.
    2022. Designing Virtual Memory System of MCM GPUs. In 2022 55th IEEE/ACM International
    Symposium on Microarchitecture (MICRO). IEEE, Chicago, IL, USA, 404–422. [doi:10.1109/](https://doi.org/10.1109/MICRO56248.2022.00036)
    [MICRO56248.2022.00036](https://doi.org/10.1109/MICRO56248.2022.00036)'
  - '- <span id="page-13-17"></span>[42] Timothy J Purcell, Ian Buck, William R Mark,
    and Pat Hanrahan. 2005. Ray tracing on programmable graphics hardware. In ACM
    SIGGRAPH 2005 Courses. ACM, ACM New York, NY, USA, 268–es.'
  - '- <span id="page-13-13"></span>[43] Mohammadreza Saed, Yuan Hsi Chou, Lufei Liu,
    Tyler Nowicki, and Tor M. Aamodt. 2022. Vulkan-Sim: A GPU Architecture Simulator
    for Ray Tracing. In 2022 55th IEEE/ACM International Symposium on Microarchitecture
    (MICRO). IEEE, Chicago, IL, USA, 263–281. [doi:10.1109/MICRO56248.2022.00027](https://doi.org/10.1109/MICRO56248.2022.00027)'
  - '- <span id="page-13-16"></span>[44] Daniel Sanchez and Christos Kozyrakis. 2011.
    Vantage: scalable and efficient fine-grain cache partitioning. SIGARCH Comput.
    Archit. News 39, 3 (June 2011), 57–68. [doi:10.1145/2024723.2000073](https://doi.org/10.1145/2024723.2000073)'
  - '- <span id="page-13-18"></span>[45] Carlo H Séquin and Eliot K Smyrl. 1989. Parameterized
    ray-tracing. ACM SIGGRAPH Computer Graphics 23, 3 (1989), 307–314.'
  - '- <span id="page-14-4"></span><span id="page-14-0"></span>[46] Seunghee Shin,
    Guilherme Cox, Mark Oskin, Gabriel H. Loh, Yan Solihin, Abhishek Bhattacharjee,
    and Arkaprava Basu. 2018. Scheduling Page Table Walks for Irregular GPU Applications.
    In 2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture
    (ISCA). IEEE, Los Angeles, CA, USA, 180–192. [doi:10.1109/ISCA.2018.00025](https://doi.org/10.1109/ISCA.2018.00025)'
  - '- [47] Seunghee Shin, Michael LeBeane, Yan Solihin, and Arkaprava Basu. 2018.
    Neighborhood-Aware Address Translation for Irregular GPU Applications. In 2018
    51st Annual IEEE/ACM International Symposium on Microarchitecture (MI-CRO). IEEE,
    Fukuoka, Japan, 352–363. [doi:10.1109/MICRO.2018.00036](https://doi.org/10.1109/MICRO.2018.00036)'
  - '- <span id="page-14-3"></span>[48] Yifan Sun, Trinayan Baruah, Saiful A. Mojumder,
    Shi Dong, Xiang Gong, Shane Treadway, Yuhui Bao, Spencer Hance, Carter McCardwell,
    Vincent Zhao, Harrison Barclay, Amir Kavyan Ziabari, Zhongliang Chen, Rafael Ubal,
    José L. Abellán, John Kim, Ajay Joshi, and David Kaeli. 2019. MGPUSim: Enabling
    Multi-GPU Performance Modeling and Optimization. In 2019 ACM/IEEE 46th Annual
    International Symposium on Computer Architecture (ISCA). IEEE, Phoenix, AZ, USA,
    197–209.'
  - '- <span id="page-14-1"></span>[49] Seunghwan Sung, Sujin Hur, Sungwoo Kim, Dongho
    Ha, Yunho Oh, and Won Woo Ro. 2023. MAD MAcce: Supporting Multiply-Add Operations
    for Democratizing Matrix-Multiplication Accelerators. In Proceedings of the 56th
    Annual IEEE/ACM International Symposium on Microarchitecture (Toronto, ON, Canada)
    (MICRO ''23). Association for Computing Machinery, New York, NY, USA, 367–379.
    [doi:10.](https://doi.org/10.1145/3613424.3614247) [1145/3613424.3614247](https://doi.org/10.1145/3613424.3614247)'
  - '- <span id="page-14-7"></span>[50] Xulong Tang, Ziyu Zhang, Weizheng Xu, Mahmut
    Taylan Kandemir, Rami Melhem, and Jun Yang. 2020. Enhancing Address Translations
    in Throughput Processors via Compression. In Proceedings of the ACM International
    Conference on Parallel Architectures and Compilation Techniques (Virtual Event,
    GA, USA) (PACT ''20). Association for Computing Machinery, New York, NY, USA,
    191–204. [doi:10.1145/3410463.3414633](https://doi.org/10.1145/3410463.3414633)'
  - '- <span id="page-14-12"></span>[51] Georgios Vavouliotis, Lluc Alvarez, Vasileios
    Karakostas, Konstantinos Nikas, Nectarios Koziris, Daniel A. Jiménez, and Marc
    Casas. 2021. Exploiting page table locality for agile TLB prefetching. In Proceedings
    of the 48th Annual International Symposium on Computer Architecture (Virtual Event,
    Spain) (ISCA ''21). IEEE Press, Valencia, Spain, 85–98. [doi:10.1109/ISCA52012.2021.00016](https://doi.org/10.1109/ISCA52012.2021.00016)'
  - '- <span id="page-14-6"></span>[52] Vulkan. 2025. https://www.vulkan.org/.'
  - '- <span id="page-14-5"></span>[53] Yueqi Wang, Bingyao Li, Aamer Jaleel, Jun
    Yang, and Xulong Tang. 2024. GRIT: Enhancing Multi-GPU Performance with Fine-Grained
    Dynamic Page Placement. In 2024 IEEE International Symposium on High-Performance
    Computer Architecture (HPCA). IEEE, Edinburgh, United Kingdom, 1080–1094. [doi:10.1109/HPCA57654.](https://doi.org/10.1109/HPCA57654.2024.00085)
    [2024.00085](https://doi.org/10.1109/HPCA57654.2024.00085)'
  - '- <span id="page-14-14"></span>[54] Ziming Wang, Kai Zhang, Yangming Lv, Yinglong
    Wang, Zhigang Zhao, Zhenying He, Yinan Jing, and X. Sean Wang. 2024. RTOD: Efficient
    Outlier Detection With Ray Tracing Cores. IEEE Transactions on Knowledge and Data
    Engineering 36, 12 (2024), 9192–9204. [doi:10.1109/TKDE.2024.3453901](https://doi.org/10.1109/TKDE.2024.3453901)'
  - '- <span id="page-14-9"></span>[55] Zi Yan, Daniel Lustig, David Nellans, and
    Abhishek Bhattacharjee. 2019. Nimble Page Management for Tiered Memory Systems.
    In Proceedings of the Twenty-Fourth International Conference on Architectural
    Support for Programming Languages and Operating Systems (Providence, RI, USA)
    (ASPLOS ''19). Association for Computing Machinery, New York, NY, USA, 331–345.
    [doi:10.1145/3297858.](https://doi.org/10.1145/3297858.3304024) [3304024](https://doi.org/10.1145/3297858.3304024)'
  - '- <span id="page-14-8"></span>[56] Ying Ye, Richard West, Zhuoqun Cheng, and
    Ye Li. 2014. COLORIS: A dynamic cache partitioning system using page coloring.
    In 2014 23rd International Conference on Parallel Architecture and Compilation
    Techniques (PACT). IEEE, Edmonton, AB, Canada, 381–392. [doi:10.1145/2628071.2628104](https://doi.org/10.1145/2628071.2628104)'
  - '- <span id="page-14-10"></span>[57] Xiangyao Yu, Christopher J. Hughes, Nadathur
    Satish, and Srinivas Devadas. 2015. IMP: indirect memory prefetcher. In Proceedings
    of the 48th International Symposium on Microarchitecture (Waikiki, Hawaii) (MICRO-48).
    Association for Computing Machinery, New York, NY, USA, 178–190. [doi:10.1145/2830772.2830807](https://doi.org/10.1145/2830772.2830807)'
  - '- <span id="page-14-2"></span>[58] Yunan Zhang, Po-An Tsai, and Hung-Wei Tseng.
    2022. SIMD2: a generalized matrix instruction set for accelerating tensor computation
    beyond GEMM. In Proceedings of the 49th Annual International Symposium on Computer
    Architecture (New York, New York) (ISCA ''22). Association for Computing Machinery,
    New York, NY, USA, 552–566. [doi:10.1145/3470496.3527411](https://doi.org/10.1145/3470496.3527411)'
  - '- <span id="page-14-13"></span>[59] Yuhao Zhu. 2022. RTNN: accelerating neighbor
    search using hardware ray tracing. In Proceedings of the 27th ACM SIGPLAN Symposium
    on Principles and Practice of Parallel Programming (Seoul, Republic of Korea)
    (PPoPP ''22). Association for Computing Machinery, New York, NY, USA, 76–89. [doi:10.1145/3503221.3508409](https://doi.org/10.1145/3503221.3508409)'
  - '- <span id="page-14-11"></span>[60] Xiaotong Zhuang and S Lee Hsien-Hsin. 2006.
    Reducing cache pollution via dynamic data prefetch filtering. IEEE Trans. Comput.
    56, 1 (2006), 18–31.'
- id: pd_constraint_aware_physical_logical_topology_co_design_for_network_on_wafer_qize_yang_https_orcid_org_0009_0006_2221_4706_tsinghua_university_school_of_integrated_circuits_bnrist_beijing_china_yqz23_mails_tsinghua_edu_cn
  title: PD Constraint-aware Physical/Logical Topology Co-Design for Network on Wafer
  abstract: 'As cluster scales for LLM training expand, waferscale chips, characterized
    by the high integration density and bandwidth, emerge as a promising approach
    to enhancing training performance. The role of Network on Wafer (NoW) is becoming
    increasingly significant, which puts an emphasis on two facts: physical and logical
    topology. However, existing networks fail to co-design both aspects. Additionally,
    physical topology typically focuses on optimizing communication or computation
    separately, neglecting opportunities to improve overall training performance.

    In this paper, we propose a physical design (PD) constraint-aware joint optimization
    strategy, developing mesh-switch physical topology and a dual-granularity logical
    topology. Mesh-switch leverages the high integration density of mesh and the efficient
    communication performance of fat tree, optimizing the allocation of on-chip

    <sup>∗</sup>Corresponding author

    [This work is licensed under a Creative Commons Attribution 4.0 International
    License.](https://creativecommons.org/licenses/by/4.0) ISCA ''25, Tokyo, Japan
    © 2025 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-1261-6/25/06
    <https://doi.org/10.1145/3695053.3731045>

    communication and computation resources thoroughly considering the physical constraints
    of waferscale chips. Furthermore, we conduct a DSE algorithm to search for the
    optimal mesh-switch configuration. Based on the proposed physical topology, we
    design the most appropriate logical topology, and further enhance bandwidth utilization
    through a fine-grained overlap strategy. Evaluation results demonstrate that our
    NoW design achieves nearly a 2.39× performance improvement in LLM training compared
    to existing networks. Our comprehensive design approach, which integrates physical
    and logical topologies with constraint considerations, can also be applied to
    network designs in other contexts.'
  keywords: Network on Wafer, Physical topology, Logical topology, Waferscale chips,
    LLM training
  document: '![](_page_0_Picture_0.jpeg)


    # PD Constraint-aware Physical/Logical Topology Co-Design for Network on Wafer


    [Qize Yang](https://orcid.org/0009-0006-2221-4706) Tsinghua University School
    of Integrated Circuits, BNRist Beijing, China yqz23@mails.tsinghua.edu.cn


    [Chengran Li](https://orcid.org/0009-0005-2650-6506) Tsinghua University School
    of Integrated Circuits, BNRist Beijing, China licr23@mails.tsinghua.edu.cn


    [Huizheng Wang](https://orcid.org/0000-0002-9763-8208) Tsinghua University School
    of Integrated Circuits, BNRist Beijing, China wanghz22@mails.tsinghua.edu.cn


    [Yan Zhang](https://orcid.org/0009-0005-9480-728X) Shanghai Artificial Intelligence
    Laboratory Shanghai, China zhangyan1@pjlab.org.cn


    [Taiquan Wei](https://orcid.org/0009-0005-3501-3148) Tsinghua University School
    of Integrated Circuits, BNRist Beijing, China weitq24@mails.tsinghua.edu.cn


    [Haoran Shang](https://orcid.org/0009-0004-6972-9235) Tsinghua University School
    of Integrated Circuits, BNRist Beijing, China shanghr23@mails.tsinghua.edu.cn


    > [Chao Li](https://orcid.org/0000-0001-6218-4659) SJTU Shanghai, China lichao@cs.sjtu.edu.cn


    [Shouyi Yin](https://orcid.org/0000-0002-8438-8588) Tsinghua University School
    of Integrated Circuits, BNRist Beijing, China Shanghai Artificial Intelligence
    Laboratory Shanghai, China yinsy@tsinghua.edu.cn


    [Sihan Guan](https://orcid.org/0009-0001-0282-6039) Tsinghua University School
    of Integrated Circuits, BNRist Beijing, China guan-sh23@mails.tsinghua.edu.cn


    [Jinyi Deng](https://orcid.org/0000-0001-8666-8463) Tsinghua University School
    of Integrated Circuits, BNRist Beijing, China dengjinyi@sina.com


    [Lei Wang](https://orcid.org/0009-0008-6971-6572) Shanghai Artificial Intelligence
    Laboratory Shanghai, China wanglei@pjlab.org.cn


    [Yang Hu](https://orcid.org/0000-0001-6942-4395)<sup>∗</sup> Tsinghua University
    School of Integrated Circuits, BNRist Beijing, China hu\_yang@tsinghua.edu.cn


    # Abstract


    As cluster scales for LLM training expand, waferscale chips, characterized by
    the high integration density and bandwidth, emerge as a promising approach to
    enhancing training performance. The role of Network on Wafer (NoW) is becoming
    increasingly significant, which puts an emphasis on two facts: physical and logical
    topology. However, existing networks fail to co-design both aspects. Additionally,
    physical topology typically focuses on optimizing communication or computation
    separately, neglecting opportunities to improve overall training performance.


    In this paper, we propose a physical design (PD) constraint-aware joint optimization
    strategy, developing mesh-switch physical topology and a dual-granularity logical
    topology. Mesh-switch leverages the high integration density of mesh and the efficient
    communication performance of fat tree, optimizing the allocation of on-chip


    <sup>∗</sup>Corresponding author


    [This work is licensed under a Creative Commons Attribution 4.0 International
    License.](https://creativecommons.org/licenses/by/4.0) ISCA ''25, Tokyo, Japan
    © 2025 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-1261-6/25/06
    <https://doi.org/10.1145/3695053.3731045>


    communication and computation resources thoroughly considering the physical constraints
    of waferscale chips. Furthermore, we conduct a DSE algorithm to search for the
    optimal mesh-switch configuration. Based on the proposed physical topology, we
    design the most appropriate logical topology, and further enhance bandwidth utilization
    through a fine-grained overlap strategy. Evaluation results demonstrate that our
    NoW design achieves nearly a 2.39× performance improvement in LLM training compared
    to existing networks. Our comprehensive design approach, which integrates physical
    and logical topologies with constraint considerations, can also be applied to
    network designs in other contexts.


    # Keywords


    Network on Wafer, Physical topology, Logical topology, Waferscale chips, LLM training


    #### ACM Reference Format:


    Qize Yang, Taiquan Wei, Sihan Guan, Chengran Li, Haoran Shang, Jinyi Deng, Huizheng
    Wang, Chao Li, Lei Wang, Yan Zhang, Shouyi Yin, and Yang Hu. 2025. PD Constraint-aware
    Physical/Logical Topology Co-Design for Network on Wafer. In Proceedings of the
    52nd Annual International Symposium on Computer Architecture (ISCA ''25), June
    21–25, 2025, Tokyo, Japan. ACM, New York, NY, USA, [16](#page-15-0) pages[. https://doi.org/10.1145/3695053.3731045](https://doi.org/10.1145/3695053.3731045)


    <span id="page-1-0"></span>![](_page_1_Figure_1.jpeg)


    Figure 1: The progress of physical and logical topology codesign for a network
    on wafer in a ticktock style.


    #### 1 Introduction


    Large language models (LLMs) have emerged as pivotal components in advancing artificial
    intelligence (AI) in recent years [\[1,](#page-12-0) [5,](#page-13-0) [8,](#page-13-1)
    [22,](#page-13-2) [27–](#page-13-3) [29,](#page-13-4) [31,](#page-13-5) [33,](#page-13-6)
    [60,](#page-14-0) [61,](#page-14-1) [64,](#page-14-2) [74,](#page-14-3) [75,](#page-14-4)
    [86,](#page-14-5) [87\]](#page-14-6). With advancements in neural networks, the
    scale of LLMs has significantly increased. The evolution from BERT [\[28\]](#page-13-7)
    to GPT-3 [\[1\]](#page-12-0) has resulted in a parameter increase exceeding 1,000×.
    Furthermore, larger models generally correlate with enhanced performance [\[76\]](#page-14-7),
    suggesting that this trend is likely to continue. This growth has imposed stringent
    computational and communication demands on underlying hardware, driving advancements
    in single-chip solutions such as GPUs [\[19,](#page-13-8) [20\]](#page-13-9).
    Nevertheless, progress is constrained by reticle limits [\[4,](#page-13-10) [32\]](#page-13-11).
    Concurrently, advanced architectures, including TPUv4 [\[50\]](#page-14-8), DGX
    [\[21\]](#page-13-12) and MCM [\[83,](#page-14-9) [92\]](#page-15-1), have been
    proposed to enhance computational power.


    Recently the waferscale chip is proposed to address this scalability issue [\[2,](#page-12-1)
    [7,](#page-13-13) [25,](#page-13-14) [30,](#page-13-15) [37,](#page-13-16) [46,](#page-13-17)
    [62,](#page-14-10) [72,](#page-14-11) [73,](#page-14-12) [91\]](#page-14-13).
    The insight is to convert traditional chip cluster connected with low-bandwidth
    network into densely integrated dies with high-performance network, which could
    be treated as a monolithic huge compute node with chip-like feature. Network on
    wafer (NoW), which interconnects on-chip modules, plays a critical role in the
    distribution of communication and computation resources. It directly influences
    both computational performance and communication efficiency, which are crucial
    factors in determining overall training performance [\[15,](#page-13-18) [98\]](#page-15-2).


    NoW puts an emphasis on two factors: 1) physical topology, the topology of physical
    interconnection on the wafer; 2) logical topology, the communication algorithms
    and mapping on top of the physical network. The physical topology defines the
    interconnections between dies and accounts for the area allocated for the communication
    network. Note that due to wafer area constraints, the area of the interconnect
    network plus the area of the compute dies must not exceed the total wafer area.
    Therefore, the space available for compute dies as well as the computational power
    are also determined by the physical topology. Meanwhile, the logical topology
    governs the execution of communication tasks, directly impacting communication
    performance. Through our thorough exploration of existing studies, we have observed
    the following issues: existing studies often adopt orphan designs of physical
    or logical topologies, lacking the coordination.


    Orphan design of logical topology: Prior works [\[15,](#page-13-18) [18,](#page-13-19)
    [42,](#page-13-20) [56,](#page-14-14) [67,](#page-14-15) [98\]](#page-15-2) have
    proposed logical topologies tailored to mesh such as RingBiOdd and TTO, aiming
    to enhance real performance as illustrated in the step1 of Fig[.1.](#page-1-0)
    However, the performance ceiling remains fundamentally limited by the constraints
    of mesh. Its potential performance is inherently low. Achieving higher training
    performance requires designing a superior physical topology link step2 in the
    Fig[.1.](#page-1-0)


    Orphan design of physical topology: Some existing studies focus exclusively on
    physical topology [\[41,](#page-13-21) [78\]](#page-14-16), leading to incomplete
    optimization. To fully unlock the potential of the optimized physical topology,
    it is necessary to co-design matching logical topologies. Designing an advanced
    physical topology while continuing to use a mismatched logical topology results
    in a performance bottleneck after step 2, due to a lack of compatibility. Additionally,
    the performance ceiling remains fundamentally limited by the physical design (PD)
    constraints of existing physical topologies. As illustrated in the Fig[.1,](#page-1-0)
    its potential performance is inherently low. Achieving higher training performance
    requires designing a superior physical topology which have not been proposed yet
    lack of the through modeling and exploration of on-chip resources allocation.


    Previous research has made significant advancements in the physical and logical
    topologies of MCM and waferscale systems [\[39,](#page-13-22) [48,](#page-14-17)
    [56,](#page-14-14) [78,](#page-14-16) [100,](#page-15-3) [102\]](#page-15-4).
    Building upon their foundation, we target to overcome the above issues in a manner
    of PD constraint-aware physical and logical topologies co-design following a ticktock
    framework. As Fig[.2](#page-2-0) shown, the first key novelty of this work presents
    the framework comprehensively explore from physical and logical topology to parallelism,
    from hardware to software. Targeting LLM training workloads, a PD-aware DSE is
    developed to identify the optimal physical topology for NoW. Based on the physical
    topology, we develop a a dual-granularity logical topology with the best communication
    performance, offering deadlock-free routing and a collective algorithm. Fully
    aware of physical and logical topologies, a hybrid parallelism and sharding strategies
    searching for the supreme software configurations is proposed.


    In summary, we make the following contributions:


    - We propose a constraint-aware physical and logical co-design framework which
    is able to develop NoW achieving the optimal training performance by ticktock
    process.

    - We propose a hybrid mesh-switch physical topology that balances computational
    and communication performance achieving the optimal training throughput compared
    with SOTA physical topologies for NoW.

    - We design a dual-granularity logical topology including an efficient, deadlock-free
    routing algorithm for both intra-wafer and inter-wafer networks. Additionally,
    a collective algorithm is developed with incorporate chunk segmentation and pipelining
    strategies to enhance communication performance and bandwidth utilization.

    - We develop a NoW including mesh-switch physical topology and the dual-granularity
    logical topology following the codesign framework, delivering 2.39× performance
    in LLMs training compared to SOTA NoW.


    <span id="page-2-0"></span>![](_page_2_Figure_1.jpeg)


    Figure 2: The illustration of PD constraints of WSC and the methodology framework
    of the co-design process.


    #### 2 Background


    ### 2.1 Waferscale Chips


    Waferscale integration(WSI) has overcome the limitations of photomask reticles
    on individual chip area through advanced packaging techniques, such as CoWoS [\[15,](#page-13-18)
    [16\]](#page-13-23) and InFO\_SoW[\[17,](#page-13-24) [23\]](#page-13-25). These
    innovations enable the construction of integrated circuits at the scale of the
    entire wafer. Currently, there are two primary approaches to achieving WSI [\[3,](#page-13-26)
    [9,](#page-13-27) [12,](#page-13-28) [57,](#page-14-18) [58,](#page-14-19) [81\]](#page-14-20).
    The first is monolithic WSI [\[80,](#page-14-21) [99\]](#page-15-5), where the
    wafer network is embedded within the substrate. All components are integrated
    onto the wafer using conventional step-and-repeat methods, resulting in homogeneous
    dies. Cerebras employs this method for its waferscale Engine(WSE) [\[62,](#page-14-10)
    [63,](#page-14-22) [66\]](#page-14-23). The second approach involves integrating
    multiple components with different technologies on the wafer substrate, such as
    power, thermal, and storage modules [\[2,](#page-12-1) [35,](#page-13-29) [38,](#page-13-30)
    [43,](#page-13-31) [45,](#page-13-32) [83\]](#page-14-9). Tesla''s Dojo chip is
    manufactured in this way[\[91\]](#page-14-13). Since the dies in the latter approach
    can be individually tested before integration, it tends to yield better results.
    WSI reduces the interconnection distance between dies, enhancing link bandwidth,
    while also decreasing communication latency and power consumption.


    In waferscale chips(WSCs), Die-to-Die(D2D) links are employed for on-chip communication,
    enabling scale-up by integrating multiple dies within a single wafer, while Chip-to-Chip(C2C)
    links facilitate inter-chip communication, supporting scale-out by connecting
    multiple wafers.


    #### 2.2 Constraints


    To ensure the feasibility of our topology design on waferscale chips, it is essential
    to define the physical area constraints of the chip and the range of interconnect
    lengths required for high bandwidth.


    <span id="page-2-1"></span>![](_page_2_Figure_10.jpeg)


    Figure 3: The variations of training time due to the different physical and logical
    topology configurations.


    Fig[.2](#page-2-0) shows that the total area of all components integrated on the
    chip must not exceed the wafer''s total area. We assume the wafer area is 70,
    000 <sup>2</sup> , with 20, 000 <sup>2</sup> of the perimeter allocated for scale-out
    connections and other interfaces. Therefore, the remaining 50, 000 <sup>2</sup>
    will be the available area for placing the computing units, memory and the interconnect
    network [\[49,](#page-14-24) [88,](#page-14-25) [89\]](#page-14-26).


    WSI introduces communication links with extremely high bandwidth, but also imposes
    limitations on interconnect distance and wiring density. Once the distance of
    data transmission between dies exceeds 50mm, the bit error rate increases by <sup>10</sup>8×.
    Then, forward error correction must be introduced, which however, raises the transmission
    latency to 210 ns, which is 14× higher than normal scenario [\[70,](#page-14-27)
    [82\]](#page-14-28). Therefore, we assume the longest length of on-chip links
    must be shorter than 50 mm [\[14,](#page-13-33) [24,](#page-13-34) [84,](#page-14-29)
    [104\]](#page-15-6) as demonstrated in Fig[.2.](#page-2-0)


    # 2.3 Physical & Logical Topologies


    Fig[.2](#page-2-0) shows the traditional physical and logical topologies. The
    physical topology refers to circuit-level interconnections. Most waferscale chips
    use mesh topology due to its simple layout, routing, and minimal interconnection
    area, enabling the integration for more compute dies[\[30,](#page-13-15) [37,](#page-13-16)
    [56,](#page-14-14) [62,](#page-14-10) [91\]](#page-14-13). However, as the scale
    increases, the diameter also expands, leading to a degradation in communication
    performance.


    Fat tree is a completely symmetric physical topology that exhibits several advantageous
    characteristics, including a small network diameter and fault tolerance.


    Logical topology represents the implementation of communication algorithms at
    the software level and may be different from the physical topology. As Fig[.2](#page-2-0)
    shows, typical logical topologies include ring [\[94\]](#page-15-7) and tree [\[79\]](#page-14-30).
    Each node is connected to two adjacent nodes, forming a closed loop in ring topology.
    It performs well in small-scale networks but exhibits significantly poor latency
    as the scale increases. In tree topology, the network is organized into a hierarchical
    structure where each node has a parent node and multiple child nodes resembling
    an inverted tree with the root typically being a central node.


    <span id="page-3-0"></span>![](_page_3_Figure_1.jpeg)


    Figure 4: (a) The Uneven traffic distribution of DP/TP allreduce process; (b)
    The large diameter related to the scale and the unbalanced communication hops.


    ### 2.4 Parallelism in Distributed Training


    In distributed training, five primary parallelization methods are employed: data
    parallelism(DP), tensor parallelism(TP), pipeline parallelism(PP), sequence parallelism(SP)
    and expert parallelism(EP). DP replicates the entire model on each die, with each
    instance processing a distinct data subset. TP reduces memory consumption by partitioning
    tensors within a layer across multiple devices. PP distributes model layers across
    different computing devices, enabling sequential execution. SP partitions input
    sequences across multiple devices, enabling each to process a subset of tokens.
    Employed in mixture-of-experts(MoE) models, EP assigns different experts to separate
    devices, balancing computation while leveraging model sparsity to optimize resource
    utilization.


    #### 3 Motivation


    #### 3.1 The Untapped Potential of Co-Design


    Fig[.3](#page-2-1) illustrates the influence of physical and logical topologies
    on training performance. The physical topology directly determines the allocation
    of limited on-chip resources, thereby impacting both computation and communication
    performance. Building on the physical topology, the logical topology further affects
    communication efficiency, depending on the compatibility between the physical
    and logical topologies.


    Existing researches primarily focus on optimizing physical or logical topologies
    independently, overlooking the performance gains from joint optimization.


    #### <span id="page-3-2"></span>3.2 Insufficient Optimized Physical Topology


    Mesh facilitates the division of longer interconnects into shorter segments, thereby
    enhancing energy efficiency, reducing latency, and improving yield [\[108\]](#page-15-8).
    In addition, the short-distance interconnections provided by mesh enhance integration
    density, allowing for the accommodation of up to 48 compute dies within the wafer
    area, facilitating a straightforward layout and routing process [\[2\]](#page-12-1).
    Prior waferscale systems have also adopted a physical mesh configuration [\[62,](#page-14-10)
    [63,](#page-14-22) [72,](#page-14-11) [90,](#page-14-31) [91\]](#page-14-13).
    However, collectives during training are both substantial and frequent, which
    pose challenges that mesh struggles to address effectively. As Fig[.4](#page-3-0)
    demonstrates, there is a communication bottleneck during the training process
    caused by congestion at the center and the large diameter.


    To solve the communication issue, FRED, a two-level fat tree physical NoW is constructed
    [\[78\]](#page-14-16), theoretically achieving optimal


    <span id="page-3-1"></span>![](_page_3_Figure_12.jpeg)


    (a) Fat tree network on wafer (b) Insufficient Comp. area


    Figure 5: (a) The FRED physical topology on wafer; (b) The compute dies occupy
    only 25.03% of the wafer area leading to insufficient computational power.


    communication performance with a fully connected topology. However, as Fig[.5](#page-3-1)
    shows, the introduction of a two-level switch network, leaving 25.03% of the wafer
    area to integrate compute dies which is insufficient. This ultimately results
    in a bottleneck in computational power.


    Experimental Setup. Software Platform: ASTRA-SIM [\[77,](#page-14-32) [101\]](#page-15-9),
    which is a precise open-source simulator supporting waferscale chips architecture
    and configurable topology design, is used to evaluate the performance.


    Hardware Configuration: The compute die is based on the specifications of the
    Dojo D1 chip, delivering 362 TFLOPs of performance within an area of 645<sup>2</sup>
    . Four HBM3 are integrated near each compute die, providing a total storage capacity
    of 64 and a bandwidth of 819/ within an area of 140<sup>2</sup> . Si-IF interconnections
    are utilized, providing D2D bandwidth of 2 /. C2C bandwidth is 4.5 / per edge.
    The switch die is designed based on FRED [\[78\]](#page-14-16), whose each side
    handles up to 600/ within an area of 685<sup>2</sup> for connection inside WSCs.
    If a single switch cannot meet the bandwidth requirements, multiple switches are
    clustered to achieve higher bandwidth, as described in [\[15,](#page-13-18) [98\]](#page-15-2).


    Workloads: Llama2 70B[\[93\]](#page-15-10) and GPT-3 175B [\[8\]](#page-13-1)
    are chosen to evaluate the communication and computation time of WSCs under mesh
    and fat-tree topologies during training, as they are two of the most representative
    LLMs.


    Methodology. As we described in Sec[.3.2,](#page-3-2) mesh exhibits significant
    communication bottlenecks, while FRED becomes compute-bound due to insufficient
    allocation of computational resources. We focus on the communication bottleneck
    caused by mesh and the compute bottleneck of FRED under different model loads,
    parallel dimensions, and bandwidth configurations, demonstrating that significant
    optimization space remains between them.


    #### Evaluation of the Bottleneck in Training for Mesh and FRED.


    As shown in Fig[.6,](#page-4-0) with a 2TB/s bandwidth configuration, mesh communication
    time averages 2.51 × the computation time, increasing to 3.14 × at 1TB/s. Lower
    bandwidth increases compute die area, boosting performance but worsening the communication
    bottleneck. Higher TP improves efficiency but adds overhead, while DP and PP reduce
    communication costs. FRED consistently faces computational bottlenecks, with computation
    time averaging 2.95× communication time as Fig[.7](#page-4-1) shown. Our switch
    design study shows multi-level small switches need more area than a single-level
    central switch but enhance performance. To optimize efficiency, we adopt a single-level
    centrally integrated switch to minimize multi-hop communication.


    4


    <span id="page-4-0"></span>![](_page_4_Figure_2.jpeg)


    <span id="page-4-1"></span>Figure 7: The computation and communication time among
    various configurations on FRED physical topology.


    Under the workloads of LLM training, both of mesh and FRED still present considerable
    opportunities for further optimization. Therefore, it is vital to develop physical
    topology on wafer that considers the layout and resource allocation of both the
    communication network and computation, while balancing the constraints imposed
    by waferscale physical limitations.


    Key insight 1: Mesh and FRED physical topologies focus solely on optimizing compute
    and communication while failing to comprehensively explore the wafer design space
    for an overall optimal solution.


    # 3.3 Separated Logical Topology from the Physical Topology


    The logical topology is a subset of the physical topology. The greater the discrepancy
    between them, the poorer the communication performance, manifested as higher latency
    and lower bandwidth utilization. As shown in Fig[.3,](#page-2-1) when using a
    tree logical topology on mesh, its communication performance improves by about
    1.4× compared to ring. When designing a new physical topology based on constraints,
    it is crucial to design a matching logical topology. Otherwise, applying an existing
    logical topology may lead to inefficient communication, preventing the physical
    topology from achieving its ideal performance.


    # 4 The TickTock Framework of Physical and Logical Topology Co-Design


    Since designing only the physical or logical topology alone results in suboptimal
    performance, we propose an optimization framework for co-designing physical and
    logical topologies while adhering to the physical constraints of waferscale chips.
    We first focus on PD constraint-aware physical topology design. The total wafer
    area and interconnect constraints are considered as design boundaries for the
    on-wafer topology. At this stage, the trade-off between communication and computation
    is fully modeled and jointly optimized,


    |  |  |  |  | Table 1: The symbols sculpturing mesh-switch topology. |  |

    |--|--|--|--|--------------------------------------------------------|--|

    |  |  |  |  |                                                        |  |


    | Symbols | Description                                 |

    |---------|---------------------------------------------|

    | 𝑁       | The total number of compute dies            |

    | 𝑎       | The horizontal scale of mesh group          |

    | 𝑏       | The longitudinal scale of mesh group        |

    | 𝐺       | The total number of mesh groups             |

    | 𝑥       | The horizontal index of die in mesh group   |

    | 𝑦       | The longitudinal index of die in mesh group |

    | 𝑔       | The mesh group index                        |


    resulting in the final physical layout. Based on the physical topology design,
    we propose a logical topology that is most compatible with it. Finally, fully
    aware of the physical and logical topologies, hybrid parallelism configuration
    and sharding strategies are proposed.


    ### <span id="page-4-3"></span>5 Mesh-Switch Physical Topology


    #### <span id="page-4-2"></span>5.1 Topology Description


    Mesh offers high integration density but performs poorly in terms of communication,
    while the switch-based FRED exhibits the opposite characteristics. In small-scale
    systems, the communication performance of mesh is almost indistinguishable from
    fully connected. Therefore, we propose a hybrid topology, mesh-switch, which integrates
    small-scale mesh groups and use only L1 switches connecting each group to build
    a fully connected network between groups. Unlike DGX systems, where all GPUs are
    directly connected to the switch. Mesh-switch topology can be seen as a mesh group
    connecting to a central switch. (, , ) represents the position of each die where
    (, ) are its index with in the mesh group and represents the group index.


    As shown in Fig[.8,](#page-5-0) mesh-switch physical topology on waferscale chips
    includes four physical levels: die, mesh group, wafer and wafer cluster. Compared
    to FRED, we replace the single die into a mesh group consists of × dies interconnected
    in mesh.


    Die: As presented in Fig[.8\(](#page-5-0)a), the die, consisting of core arrays,
    stacked DRAM and communication interfaces, is the basic component of the waferscale
    chip. Each die can communicate with its east, west, north, and south dies through
    NoC.


    <span id="page-5-0"></span>![](_page_5_Figure_2.jpeg)


    Figure 8: Hierarchical architecture of the waferscale mesh-switch topology.


    <span id="page-5-1"></span>![](_page_5_Figure_4.jpeg)


    Figure 9: (a) Wiring constraints in WSCs; (b) High-radix physical topologies are
    difficult to expand on wafers due to the dense wiring; (c) SOTA topologies are
    unable to achieve high-bandwidth communication on wafers because their extremely
    long interconnects suffer significant signal loss at high frequencies.


    #### Mesh Group:


    As shown in Fig[.8\(](#page-5-0)b), a mesh group consists of × dies and there
    are two kinds of dies in the group. One type is the basic die, which connects
    only to its adjacent neighbors. The other type is the port die, which, additionally,
    is connected to the switch and access a fully connected network formed with port
    dies.


    Wafer: As presented in Fig[.8\(](#page-5-0)c), mesh groups are interconnected
    through the center switch forming a fully connected network. Based on the analysis
    of mesh traffic, we position the switch at the center to address the data exchange
    bottleneck concentrated in this region. Wafer Cluster: Fig[.8\(](#page-5-0)d)
    shows the wafer cluster employs a switchless dragonfly network, offering excellent
    scalability, low cost, and high local and global throughput [\[30\]](#page-13-15).


    Fig[.9\(](#page-5-1)a) illustrates two primary constraints in waferscale chip
    routing: (1) wiring density is limited to three metal layers, and (2) wiring distance
    must remain below 50 to ensure signal integrity. Fig[.9\(](#page-5-1)b) depicts
    the wiring density variation across different SOTA physical topologies as the
    number of compute dies increases. Low-radix topologies such as mesh, mesh-switch,
    and torus consistently fits to the density constraint across all configurations.
    In contrast, 2D NoC flattened butterfly and dragonfly exhibit exponential growth
    in wiring density as compute die count increases, surpassing physical limits and
    making them unsuitable for deployment in WSCs. Fig[.9\(](#page-5-1)c) illustrates
    signal loss as frequency increases. Mesh and mesh-switch present minimal attenuation
    even at 10, benefiting from shorter wiring paths. Conversely, the other three
    topologies rely on wafer-spanning links, with dragonfly requiring near-diameter
    direct connections between groups. This severely limits their capacity to sustain
    high-bandwidth signal transmission, rendering them impractical for employment
    in WSCs.


    Routing in NoW is more challenging than in NoC due to PD constraints on limited
    wiring length and fewer number of metal layers. Moreover, the allocation of communication
    and computation resources on chip makes design trade-offs ranging from architecture
    to floorplan more complex due to the exclusive relationship. Furthermore, by thoroughly
    exploring the optimization space of NoW, we have achieved a co-design of hardware
    and software, encompassing physical design, parallel strategies, and logical topology.


    Key insight 2: Flattening high-radix and long-distance routing topologies including
    flattened butterfly, dragonfly, etc. fail to scale due to waferscale wiring constraints.


    #### 5.2 Design Space Construction of Mesh-Switch


    Mesh-switch is defined by, the number of mesh groups, and (, ), the dimensions.
    It connects a total of × × compute dies. To comprehensively explore the design
    of physical topology on wafer, we consider various factors. The size of the mesh
    group determines the number of compute dies and influences the network diameter,
    making it a key factor in both communication and computational performance. The
    shape of the mesh group also affects its placement on the wafer. A mesh group
    with a greater disparity between its length and width will have fewer possible
    placement positions. For instance, consider a mesh group size of 4: a 1×4 mesh
    fits 32 dies, while 2×2 fits 40. The position of the port die affects the network
    diameter. For example, in a 2×3 mesh, the diameter of a corner die is 3, while
    for an edge die, it is 2. When considering these parameters comprehensively, the
    priorities, in descending order, are the size of the mesh group, its shape, and
    the position of port die. The


    <span id="page-6-0"></span>![](_page_6_Figure_1.jpeg)


    Figure 10: Overview of the physical topology DSE methods.


    Table 2: Design specification of the DSE process.


    <span id="page-6-6"></span>


    | Parameters                    | Specification          |

    |-------------------------------|------------------------|

    | ,<br>𝑆𝑐𝑜𝑚𝑝<br>𝑆𝑠𝑤𝑖𝑡𝑐ℎ         | 785𝑚𝑚2<br>,685𝑚𝑚2      |

    | ,𝐵𝑊𝑠𝑤𝑖𝑡𝑐ℎ,<br>𝐵𝑊𝐷2𝐷<br>𝐵𝑊𝑤𝑖𝑟𝑒 | 2𝑇 𝐵/𝑠,1.6𝑇 𝐵/𝑠,10𝐺𝑏𝑝𝑠 |

    | 𝑤,𝑠,𝐿,𝑎                       | 35𝑢𝑚,35𝑢𝑚,30𝑚𝑚,25.4𝑚𝑚  |


    size of mesh group is the fundamental factor that determines both communication
    capacity and computational power.


    #### 5.3 DSE Methods


    As shown in Fig[.10,](#page-6-0) the input of the DSE algorithm includes the relevant
    configuration details of the waferscale chip, such as the maximum area, interconnect
    distance constraints, the area of a single compute and switch die, and the total
    bandwidth supported by a individual switch die. The central component of the algorithm
    is a hierarchical iteration corresponding to the architectural factors discussed.
    The outermost loop iterates through mesh group sizes in increasing order. Once
    the mesh group size is determined, it is factorized to identify possible shapes,
    and for each shape, the algorithm explores the potential port die positions. The
    key of the evaluation process is calculating the size of based on the area constraint.
    In each iteration, the total area is evaluated based on the current value of to
    verify whether they satisfy the are constraint. The number of mesh groups is incremented
    until the wafer area limit is reached. Once this limit is exceeded, the algorithm
    returns − 1 and determines the maximum number of compute dies that can be integrated
    under the configuration. This process reflects architecture parameters such as
    the network diameter , which represents the maximum communication hop. The individual
    compute die size remains unchanged with cluster parameters such as , , and , it
    depends solely on computing power. In the DSE process, the return of −1 represents
    the maximum group number for each configuration of and . The horizontal axis of
    Fig. [11](#page-6-1) represents the values of and , not the value of . Taking
    2 × 2 as an example, it represents = = 2, and its corresponding = 10. We use ASTRA-SIM
    analytical model for evaluation across different (, ) configurations and select
    the one with the best performance as the physical topology configuration for mesh-switch.


    128TFLOPs 362TFLOPs 512TFLOPs 1024TFLOPs


    <span id="page-6-1"></span>![](_page_6_Figure_9.jpeg)


    Figure 11: Comparison of overall training performance across different configurations.


    2.8


    <span id="page-6-7"></span>![](_page_6_Figure_11.jpeg)


    Figure 12: Comparison of overall training performance across different applications.


    $$A\_{"wafer} = A\_{comp} + A\_{sweitch} + A\_{wire} + A\_{others} \tag{1}$$


    <span id="page-6-3"></span><span id="page-6-2"></span>

    $$A\_{comp} = N\_{comp} S\_{comp} = GabS\_{comp} \tag{2}$$


    $$A\_{s\,with} = N\_{s\,with} S\_{s\,with} = G \frac{BW\_{D2D}}{BW\_{s\,with}}
    S\_{s\,with} \tag{3}$$


    <span id="page-6-5"></span><span id="page-6-4"></span>

    $$A\_{wire} = N\_{wire} S\_{wire} = \frac{BW\_{D2D}}{BW\_{wire} \rho\_{wire}}
    = \frac{BW\_{D2D}(\omega + \mathbf{s})L}{BW\_{wire} N\_{Layers}} \quad (4)$$


    The total area of the waferscale chip comprises the computation module, switch,
    interconnect and other functional circuits, satisfying Eq. [1.](#page-6-2) The
    computation unit area and switch area are both obtained by multiplying the number
    of corresponding dies by the area of a single die. The number of compute dies
    are determined by the DSE parameters (, , ), and the area of a single compute
    die comprises the combined area of the compute unit and the surrounding HBM. The
    specific calculation is provided in Eq. [2.](#page-6-3) The number of switch dies
    are determined by the switching bandwidth required by the mesh group, as detailed
    in Eq. [3.](#page-6-4) The interconnect area is computed by dividing the total
    number of interconnects by the wiring density. The total number of D2D wires is
    dictated by bandwidth requirements, while the wiring density is derived from wire
    width , spacing , length , and the number of metal layers , as shown in Eq. [4.](#page-6-5)
    The peripheral circuits, including the power network and other supporting components,
    occupy approximately 20, 000<sup>2</sup> [\[2,](#page-12-1) [72,](#page-14-11)
    [78\]](#page-14-16). For mesh-switch, we performed back-end design and obtained
    precise parameter values, as shown in Tab. [2.](#page-6-6)Though back-end design
    enables precise area evaluation, it is extremely time-consuming (∼ 24 hours).
    To maintain the efficiency of the DSE process, we employ the analytical model
    discussed above, which achieves an average error below 15% while requiring only
    approximately 0.67 of computation time.


    # <span id="page-6-8"></span>5.4 Physical Layout of Mesh-Switch WSCs Evaluation
    Setup and Methodology:


    计算die占据的面积 switch面积


    <span id="page-7-0"></span>![](_page_7_Figure_2.jpeg)


    Figure 13: The area variation of compute dies and switch dies among different
    configurations.


    To better model mesh-switch, we extend ASTRA-SIM by modifying hierarchical hybrid
    topology. The first level consists of multiple mesh groups, where compute nodes
    within each group are interconnected in mesh. The second level introduces a fully
    connected network that links all mesh groups. Specifically, we extend the routing
    mechanisms to enable intra-group communication through local mesh paths while
    leveraging the fully connected second tier for low-diameter inter-group transfers
    as the discussion in Sec. [5.1.](#page-4-2) During DSE, each candidate design
    is evaluated by ASTRA-SIM in event-driven mode.


    The hardware configuration is the same as [3.2.](#page-3-0) During the DSE process,
    we fully explore the optimal configuration of mesh-switch under different chip
    computing capacities, The computing power is set to 128, 362, 512, and 1024 TFLOPs,
    with the compute die area varying based on transistor density estimation. The
    baseline is the Dojo D1 chip, which has a computing power of 362 TFLOPs and an
    area of 645<sup>2</sup> . To explore the scalability of our DSE algorithm, we
    fully consider the future development of wafer area and evaluate the relationship
    between DSE search time and the wafer area to verify the scalability and efficiency
    of the algorithm.


    DSE Results: As shown in Fig[.11,](#page-6-1) the horizontal axis represents the
    mesh group configuration, while the vertical axis indicates the training performance
    of GPT-3 175B [\[8\]](#page-13-1). This analysis considers design options for
    single compute dies with performance ranging from 128 TFLOPs to 1024 TFLOPs, where
    greater computational power corresponds to increased die area. The Fig. [12](#page-6-7)
    compares training performance across various hardware configurations. In training
    scenarios involving five LLMs. Among all benchmarks in Fig. [12](#page-6-7) and
    hardware configurations in Fig. [11,](#page-6-1) the 2×2 mesh group configuration
    achieves the best performance. Therefore, we select this configuration for designing
    the physical layout and adopt it as the baseline architecture for mesh-switch.
    As Fig[.13](#page-7-0) illustrates, configurations smaller than 2×2 require more
    groups, increasing the area occupied by switches. This reduces the space available
    for compute dies, thereby lowering computational power. Once the mesh group size
    reaches 2×2, the integrable area begins to fluctuate due to the discrete design
    space, which causes suboptimal resource utilization. As the mesh group size increases
    further, the potential for resource waste becomes more pronounced. Fig. [13](#page-7-0)
    shows that in 2×3 or 3×3 configurations, the area occupied by compute dies and
    switches remains small because of the discrete optimization process. However,
    = 6 meets the wafer area constraints, while = 7 exceeds the limit. Consequently,
    only 36 dies can be integrated in the 2×3 configuration, leading to wasted on-chip
    area.


    Mesh-switch jointly optimizes both computation and communication, making it more
    resilient than FRED and mesh, which are optimized solely for communication or
    computation. Furthermore, the central switch provides flexible interconnections,
    enabling the


    <span id="page-7-1"></span>![](_page_7_Figure_8.jpeg)


    Figure 14: Physical views of mesh-switch topology on wafer.


    application of arbitrary parallelism configurations and SOTA shading strategies.
    For dynamic applications such as inference, software strategies [\[85\]](#page-14-33)
    can fully address the issue. For instance, input requests with various lengths
    are first managed by batching, and the data is then segmented according to the
    compute unit''s processing capacity to achieve high MFU.


    Key insight 3: Across various hardware configurations and applications, the DSE
    for mesh-switch physical topology consistently converges to 2×2 mesh group configuration.


    Based on the DSE results, we designed the corresponding waferscale mesh-switch
    physical network layout, as shown in Fig[.14.](#page-7-1) Ten mesh groups of size
    2 × 2 are integrated, with the groups interconnected via central switch dies.
    The compute dies occupy 41,800 <sup>2</sup> and the switch dies occupy 5606 <sup>2</sup>
    .


    # 5.5 Multiwafer Cluster Architecture


    As the performance of an individual wafer improves, the overall performance of
    the wafer cluster also increases. The C2C ports at the edges of mesh do not employ
    intra-wafer interconnections. This issue persists because mesh-switch continues
    to operate within the mesh physical topology of the local network surrounding
    the wafer. The current scale of LLM is substantial and stays in a growing trend,
    [\[105\]](#page-15-11) making it challenging for the storage and computation resources
    of a single wafer to meet these demands. Therefore, forming multi-cluster wafers
    is essential, as it also creates opportunities to utilize the unoccupied bandwidth
    effectively. By leveraging the C2C ports at the edges, each wafer routes high-density,
    short-distance wiring to off-wafer electrical or optical connectors via the edge
    I/O interface [\[30,](#page-13-15) [36,](#page-13-35) [53,](#page-14-34) [97\]](#page-15-12).
    Long distance inter-wafer interconnection links are physically connected off-wafer
    via a backplane or cables, forming a fully connected topology for the multi-wafer
    cluster. The topology can be configured as dragonfly, fat tree, etc., depending
    on specific needs.


    # 5.6 The Scalability Analysis of DSE


    To evaluate the scalability of the DSE algorithm, we simulate future wafers with
    glass panels expanding to a 600 diameter. We


    time/ms


    <span id="page-8-0"></span>![](_page_8_Figure_1.jpeg)


    0 1,1 2,3 3,4 4,4 4,6 5,5 6,6 6,8 8,10 18,20 Figure 15: (a)DSE execution time
    with searching area expanding; (b)DSE for 300, 000<sup>2</sup> glass panel.


    measure the runtime of DSE varying the area from 70, 000<sup>2</sup> to 300, 000<sup>2</sup>
    . As shown in Fig. [15\(](#page-8-0)a), although search time increases with area,
    the maximum execution time remains only 22.59. For the current 12-inch wafer,
    hardware configuration optimization is completed in just 15.85. These results
    indicate that the DSE algorithm exhibits low computational overhead and strong
    scalability.


    Exploring the optimal configuration for 300, 000<sup>2</sup> glass panel, the
    highest performance is achieved with a 4×4 mesh group configuration, delivering
    2.24× the performance of mesh and 1.43× that of FRED, as shown in Fig[.15\(](#page-8-0)b).
    Mesh-switch topology improves performance in waferscale chips under current manufacturing
    constraints and retains its advantages with increasing area.


    Key insight 4: The DSE algorithm exhibits strong scalability, requiring only ∼
    22ms to complete optimization on a 300, 000<sup>2</sup> glass panel.


    #### 6 Dual-Granularity Logical Topology


    ### 6.1 Routing Algorithms


    Routing: For intra-group,we choose XY routing [\[13\]](#page-13-36) as the default
    routing algorithm due to its easy implementation, low link delay, and excellent
    performance. Forming a fully connected network, the inter-group routing follows
    an all-to-all strategy. When the source and destination nodes are located in different
    groups, the packet is first routed through XY routing to reach the port die in
    its group. From there, the packet travels to the destination group via the switch
    in a single hop, and then it is routed from the port die to the destination node
    through XY routing.


    Switch Arbitration: Switch uses a round-robin arbitration mechanism to determine
    which group''s request to respond to, based on priority, while blocking the request
    signals from the other group [\[96\]](#page-15-13). Once a group''s signal is
    responded to, the priority of that group is lowered to the minimum by hardware
    to ensure load balancing. This prevents one group from monopolizing the destination
    port for too long, thus avoiding a situation where other groups'' signals are
    continuously blocked and must wait.


    Deadlock-Free Proof: We use the Channel Dependency Graph (CDG) to prove the routing
    algorithm is deadlock-free [\[26\]](#page-13-37). First,


    <span id="page-8-1"></span>![](_page_8_Figure_11.jpeg)


    Figure 16: (a) Each node has input and output channels of four directions; (b)
    The CDG of nodes within the mesh group; (c) Nodes directly connected to switch
    have two additional channels: switch to/from the group.


    a directed graph = ( ,) is built, where represents the set of computing nodes
    and represents the set of channels. For each source-destination pair ( , ), the
    routing function ( , ) gives the sequence of channels that connect them. The channel
    dependency graph = (, ) is generated, where vertices are channels and edges connect
    channels that are linked by :


    $$E = \{ (c\_l, c\_f) | (c\_l, c\_f) \in R(n\_s, n\_d) \text{ for } n\_s, n\_d
    \in N \}\tag{5}$$


    For example, the CDGs for the nodes within the mesh group and those directly connected
    to the switch are shown in Fig[.16\(](#page-8-1)b) and (c), respectively.


    Theorem 6.1. A deterministic routing function for the interconnection network
    is deadlock-free if and only if there are no cycles in the channel dependency
    graph .


    The depth-first algorithm is used to prove that there are no cycles in the CDG
    of our interconnection network [\[54\]](#page-14-35).


    ## <span id="page-8-2"></span>6.2 Collective Algorithms


    To achieve the potential training performance, it is necessary to match logical
    and physical topology. For mesh-switch with local mesh group and central fat tree
    network hierarchically, a corresponding dual-granularity logical topology is employed
    to achieve the highest performance. Despite the challenges posed by the mesh physical
    topology within the mesh group, we propose a fine-grained pipeline overlap to
    enhance communication performance by improving bandwidth utilization. To evaluate
    the communication latency, we utilize the alpha-beta model as:


    $$Latency\_{comm} = \mathcal{N}\_{comm} \* \alpha + V/\beta \tag{6}$$


    where , , and mean the communication steps, link latency, message size and the
    link bandwidth.


    Currently, many high-performance logical topologies implementations are based
    on ring and tree. Therefore, we have chosen to use them as basic options. Ring
    is characterized by evenly distributing data traffic across devices. Tree, on
    the other hand, features several root nodes responsible for aggregating the data
    from all devices. For mesh-switch physical topology, there are 4 possible hierarchical
    patterns: ring+ring, ring+tree, tree+ring, and tree+tree.


    Without In-Network Computing Switch: If the tree pattern is used in the switch,
    it implies that each port-die in the mesh group must first aggregate all the other
    data within the group before executing tree-pattern communication through the
    switch. However, obtaining the reduction result using ring is inefficient, as
    it tends to distribute the data, rather than aggregating. The ring+ring pattern
    is constrained by an excessive number of dies, leading


    9


    <span id="page-9-0"></span>![](_page_9_Figure_1.jpeg)


    Figure 17: The logical topologies designed for with & without in-network computing
    network.


    to severe communication latency issues. Therefore, tree+ring and tree+tree pattern
    are more suitable for mesh-switch , with the tree+ring pattern yielding superior
    performance as Fig[.17](#page-9-0) shows. Detailed comparison data will be presented
    in the sec [8.](#page-10-0)


    With In-Network Computing Switch: A switch with in-network computing acts as the
    root node of the level-1 topology, so the tree pattern is preferred if we aim
    to exploit the advantages of innetwork computing. Therefore, as Fig[.17](#page-9-0)
    demonstrates, tree+tree logical topology is selected for mesh-switch with in-network
    computing configuration.


    Fine-Grained Overlap Scheduling Methods: Once the hierarchical collective pattern
    is established, fine-grained overlap is applied to enhance communication performance.
    For intra-pattern communication, we employ chunk-splitting to maximize bandwidth
    utilization by scheduling as many links as possible in each time step. For inter-pattern
    communication, we ensure seamless transitions within the hierarchical pattern.
    As Fig[.18](#page-9-1) demonstrates, dies within the mesh group aggregate the
    data into the port-die, and then all port-dies pass the reduced data to the switch
    for further computation. The moment switch network completes the reduction, it
    delivers the results to the port-dies, which subsequently forward them to other
    dies in the mesh group. Reduction and broadcasting are overlapped by utilizing
    both up-link and down-link bandwidth.


    #### 7 Topology-Aware Sharding Strategies


    Parallelism configurations: Hybrid parallelism is commonly used in the training
    process of LLMs. Mesh-switch forms 2D topology on a single wafer, including two
    dimensions: within the mesh group and between groups. Based on the expanding method
    mentioned in section [5.1,](#page-4-2) mesh-switch on-wafer network is extended
    to a multiwafer cluster via the C2C interface of edge dies, adding a wafer dimension
    to the physical topology. Therefore, the physical dimensions of a cluster constructed
    with mesh-switch are represented as (, ,), where is the scale of the mesh group,
    is the number of mesh groups, and is the number of wafers.


    In this paper, we consider five parallel dimensions: DP(Data Parallelism), TP(Tensor
    Parallelism), PP(Pipeline Parallelism), SP(Sequence Parallelism), and EP(Expert
    Parallelism) dimension for MoE models. The implementation of parallel strategies
    entails mapping four or five dimensional parallesim configurations onto a three
    dimensional wafer cluster. According to the analysis in Sec. [5,](#page-4-3) the
    optimal choice under the current wafer size is = 4, = 10. The final topology handles
    arbitrary parallelism strategies for distributed training by slicing the ,, or
    dimensions to adapt to the parallesim configuration. The choice of parallel strategy
    affects both computation and


    <span id="page-9-1"></span>**Fine-grained overlap Serial Improvement R RS/R AG/B
    B R RS/R AG/B B R RS/R AG/B B R RS/R AG/B B Reduce Reduce Scatter/Reduce All Gather/Broadcast
    Broadcast Full Utilization Intra Overlap Inter Overlap**


    Figure 18: The chunk pipeline of all-reduce in mesh-switch.


    communication during training, thereby impacting overall throughput. We develop
    the following strategy to select parallel parameters, prioritized as follows:


    - 1) Memory requirements for training have to be satisfied.

    - 2) A larger DP should be chosen to maximize throughput.

    - 3) The priority order of other dimensions from low to high is TP/EP, SP, PP.


    Fig[.19\(](#page-10-1)a) presents the performance variations of clusters composed
    of four mesh-switch-based wafers under different parallesim configurations, providing
    further insight into our parallel strategy choices. When DP is large, model parameters
    are split into fewer partitions, increasing intermediate activation storage demands
    and leading to out-of-memory (OOM). When memory constraints are met, increasing
    DP enhances performance, as its associated communication can be overlapped and
    occurs less frequently. Conversely, increasing other dimensions degrades performance
    due to the nonoverlapped communication, where the additional communication overhead
    increases in the order of PP, SP, and TP/EP. However, the optimal throughput configuration
    ultimately chooses TP instead of SP because SP is less effective in reducing memory
    usage. If SP were chosen, the DP dimension would have to be further reduced, increasing
    great communication overheads, resulting in worse training performance.


    Fig[.19\(](#page-10-1)b) illustrates an example of mapping a Hybrid parallesim
    configuration onto mesh-switch physical topology. The model is sliced along the
    PP dimension and distributed across various wafers. Since the point-to-point traffic
    introduced by PP is minimal and does not require high bandwidth, the PP communication
    process is allocated to the inter-wafer network. To minimize TP/SP/EP communication
    latency, they are mapped as much as possible to the same mesh group or to adjacent
    mesh groups. For DP mapping, grouping is done considering the latency between
    dies, balancing both traffic and delay during the all-reduce process.


    Topology-Aware sharding strategies: Advanced sharding techniques, such as GPipe
    [\[40\]](#page-13-38) and DeepSpeed Ulysses [\[44\]](#page-13-39), can further
    enhance performance but require adaptation to WSCs. For instance, Ulysses replaces
    high-volume all-reduce operations with lowervolume all-to-all communication, significantly
    improving performance in fully connected architectures. However, in mesh-switch,
    executing all-to-all within a mesh group is inefficient.


    As shown in Fig[.19\(](#page-10-1)c), Ulysses reduces communication volume compared
    to Megatron only when Sp > 9. This is due to the additional intermediate communication
    hops required for the all-to-all operation in mesh group. When the SP is small,
    the reduction in communication volume achieved by Ulysses is outweighed by the
    extra communication overhead it introduces. Based on this observation, we apply
    Megatron when SP < 9 and only consider using Ulysses when SP > 9.


    图表标题


    图表标题


    <span id="page-10-1"></span>![](_page_10_Figure_2.jpeg)


    Figure 19: (a) Comparison of training performance for wafer clusters under different
    parallelism configurations; (b) Mapping of hybrid parallelism on (m,g,w) = (4,4,2)
    wafer; (c) The comm. size comparison between Megatron and Ulysses across different
    SP.


    |  | Table 3: The configurations of physical topologies. |  |  |  |

    |--|-----------------------------------------------------|--|--|--|

    |  |                                                     |  |  |  |


    <span id="page-10-2"></span>


    | Config.     | Description                                                         |

    |-------------|---------------------------------------------------------------------|

    | Mesh[72]    | 2D mesh with 6 × 8 compute dies                                     |

    | FRED[78]    | Fat tree with 20 compute dies                                       |

    | FRED_IC[78] | Fat tree support in-network computing with 20 compute dies          |

    | MS          | 2 × 2 mesh-switch with 40 compute dies                              |

    | MS_IC       | 2 × 2 mesh-switch support in-network computing with 40 compute
    dies |


    Key insight 5: DP is optimal for improving throughput, whereas TP is the most
    effective for reducing memory usage but introduces the highest communication overhead.
    Priority is given to allocating TP groups in the mapping process to minimize communication
    overhead.


    Key insight 6: Current SOTA sharding techniques may not be directly applicable
    to WSCs. Thus, multiple strategies must be evaluated collaboratively to determine
    the optimal one for different configurations.


    # <span id="page-10-0"></span>8 Evaluation


    ### 8.1 Experimental Setup


    Physical Topology: As shown in Tabl[e3,](#page-10-2) there are five physical topology
    configurations. We select FRED [\[78\]](#page-14-16) proposed in 2024 and Mesh
    as baselines. Mesh, the most widely used topology for waferscale chips, has been
    adopted by Cerebras [\[63\]](#page-14-22), Dojo [\[91\]](#page-14-13), and other
    research efforts. FRED addresses the limitations of Mesh by introducing a communication-optimal
    physical topology, representing one of the latest advancements in NoW topology
    research. Following the evaluation method of Sec. [5.4,](#page-6-8) other widely
    used cluster topologies such as Dragonfly [\[52\]](#page-14-36), Torus [\[11\]](#page-13-40),
    and Flattened Butterfly [\[51\]](#page-14-37) exceed the maximum D2D interconnect
    length of 50 or require more than 3 metal layers for routing, violating waferscale
    PD constraints and thus being excluded from the physical topology baseline.


    Depending on whether the switch supports in-network computation, both the FRED
    and mesh-switch topologies are further divided into two categories: those with
    in-network computation support and those without.


    Logical Topology: For mesh and FRED, the corresponding designs employ either tree
    or ring logical topologies, selected as the most optimal configurations. We adopt
    the TTO logical topology proposed in 2024 for Mesh as our baseline, due to its
    superior communication performance compared to other existing algorithms


    59


    <span id="page-10-3"></span>


    | Table 4: The parameter configurations of LLM models. |  |

    |------------------------------------------------------|--|

    |------------------------------------------------------|--|


    | Model               | Attn. heads | Batch | Hiddensize | # of layers | Seq  |

    |---------------------|-------------|-------|------------|-------------|------|

    | Llama2 70B[93]      | 32          | 320   | 4096       | 32          | 4096
    |

    | GPT-3 175B[1]       | 96          | 320   | 12288      | 96          | 2048
    |

    | OPT 175B[107]       | 96          | 160   | 12288      | 96          | 4096
    |

    | GShard 137B-MoE[59] | 96          | 160   | 12288      | 80          | 4096
    |

    | DeepSeekV3 671B[65] | 128         | 640   | 7168       | 61          | 4096
    |

    |                     |             |       |            |             |      |


    <span id="page-10-4"></span>


    | Table 5: The parallesimn configurations for different LLMs |  |  |

    |------------------------------------------------------------|--|--|

    | and physical topologies.                                   |  |  |


    |                     | Parallesim Config. |             |              |  |  |

    |---------------------|--------------------|-------------|--------------|--|--|

    | Model               | Mesh               | FRED        | MS           |  |  |

    | Llama2 70B[93]      | [24,2,4,1]         | [5,4,4,1]   | [20,2,4,1]   |  |  |

    | GPT-3 175B[1]       | [12,4,4,1]         | [2,5,4,2]   | [10,4,4,1]   |  |  |

    | OPT 175B[107]       |                    |             |              |  |  |

    | GShard 137B-MoE[59] | [12,1,4,1,4]       | [5,2,4,1,2] | [10,1,4,1,4] |  |  |

    | DeepSeekV3 671B[65] | [2,4,4,1,6]        | [1,5,4,1,4] | [2,4,4,1,5]  |  |  |


    in LLM training [\[56\]](#page-14-14), while FRED utilizes the DBT algorithm [\[79\]](#page-14-30).
    Beyond these configurations, for mesh-switch, we adopt tree+ring and tree+tree
    based on the analysis in Sectio[n6.2.](#page-8-2)


    LLMs and parallesim configurations: We select three mainstream dense LLMs: GPT-3
    175B [\[8\]](#page-13-1), Llama2 70B [\[93\]](#page-15-10), and OPT 175B [\[107\]](#page-15-14).
    Considering the widespread use of MoE, GShard 137B-MoE [\[59\]](#page-14-38) and
    DeepSeekV3 671B[\[65\]](#page-14-39) are included in the benchmark. The parameter
    configurations are detailed in Tab[.4.](#page-10-3) Based on the discussion on
    parallesim configurations, we adopt the optimal parallelism configurations for
    various LLMs and physical topologies as shown in Tab. [5.](#page-10-4) The parallelism
    configurations represent [DP,TP,PP,SP] and [DP,TP,PP,SP,EP] for dense and MoE
    LLMs respectively.


    Simulator: ASTRA-SIM [\[77,](#page-14-32) [101\]](#page-15-9) is chosen for the
    evaluation as Sec[.3.2](#page-3-0) and Sec[.5.4](#page-6-8) described.


    #### <span id="page-10-5"></span>8.2 Physical Topology Design


    To evaluate the performance improvements across different physical topologies,
    we fix the logical topology, selecting either Ring or Tree. The x-axis represents
    the corresponding physical and logical topology configurations, such as ''Mesh+Ring,''
    where Mesh is the physical topology and Ring is the logical topology.


    Fig[.20](#page-11-0) illustrates the performance improvements achieved by mesh-switch
    across different workloads ranging from Llama2 70B [\[93\]](#page-15-10) to DeepSeekV3
    671B [\[65\]](#page-14-39). A breakdown of computation and communication times
    reveals that mesh-switch delivers computational power comparable to mesh and communication
    performance nearly matching FRED, resulting in superior overall throughput.


    <span id="page-11-0"></span>![](_page_11_Figure_2.jpeg)


    <span id="page-11-1"></span>![](_page_11_Figure_3.jpeg)


    Figure 21: The communication time comparison of different physical topologies.


    Across all benchmarks, mesh-switch achieves throughput improvements of 1.64×,
    1.31×, 1.24×, 2.47× and 2.05× over mesh and 1.39×, 1.67×, 1.62×, 1.76× and 1.46×
    over FRED, respectively. As model size increases, computational demands in dense
    models outpace communication demands. Consequently, for 175B dense models, the
    throughput improvement over mesh is less significant but remains more pronounced
    when compared to the compute-limited FRED. Conversely, in high-communication-demand
    scenarios, such as Llama2 70B and MoE models, mesh-switch demonstrates greater
    throughput gains over mesh than FRED.


    Key insight 7: Mesh-switch achieves the highest throughput across all models,
    driven by a thorough DSE to obtain optimal communication and computation resource
    allocation.


    # 8.3 Logical Topology Design


    Fig[.21](#page-11-1) presents the optimization results for the logical topology
    on mesh-switch physical topology. Without in-network computing capabilities, the
    tree-ring logical topology emerges as the optimal choice, improving performance
    by 1.25× compared to other topologies by effectively utilizing the bandwidth of
    the fully connected central network. For configurations with in-network computing
    capabilities, the tree-tree logical topology delivers the best performance, enhancing
    communication efficiency by 2.14× over other topologies. This improvement stems
    from the reduction in both the number and steps of communication achieved through
    in-network computing. While the tree-tree topology incurs some bandwidth utilization
    loss compared to the ring topology, this drawback is mitigated by employing a
    fine-grained overlap strategy.


    To highlight the communication performance gains of the logical topology, we assess
    the bandwidth during all-reduce collectives compared to SOTA algorithms including
    TTO [\[56\]](#page-14-14), TACOS [\[100\]](#page-15-3), DBT [\[79\]](#page-14-30)
    and 2D Ring [\[95\]](#page-15-15) for data sizes ranging from 1MB to 512MB as
    Fig[.22](#page-11-2) demonstrated. For mesh-switch physical topology


    ![](_page_11_Figure_10.jpeg)


    <span id="page-11-2"></span>2D ring DBT TTO TACOS Ours


    Figure 22: All-reduce bandwidth for different data size compared to SOTA collective
    algorithms.


    <span id="page-11-3"></span>![](_page_11_Figure_12.jpeg)


    Figure 23: The overall training performance of various LLMs across different networks
    on wafer.


    with in-network computing, chunk partitioning achieves a 2.58×, 3.01×, 1.50× and
    1.29× bandwidth improvement compared to 2D Ring, DBT, TTO and TACOS, respectively.
    This gain is attributed to two key optimizations: enhanced bandwidth utilization
    within mesh groups facilitated by chunk partitioning, and exposing overlap opportunities
    between the local mesh network and the central fully connected network.


    Key insight 8: The dual-granularity logical topology enhances communication performance
    by closely aligning with the physical topology, while our fine-grained overlap
    strategy maximizes bandwidth utilization.


    # 8.4 Overall Training Performance


    We compare the performance of each physical topology with its corresponding optimal
    logical topology to ensure a fair evaluation of co-design benefits. We include
    DeepSeekV3 [\[65\]](#page-14-39) to give a more comprehensive view of the resilience
    of advance LLM and various applications. As shown in Fig[.23,](#page-11-3) across
    all benchmarks, mesh-switch without in-network computing outperforms mesh by factors
    of 1.41×, 1.31×, 1.38×, 2.08× and 1.71×, and FRED by


    1.34×, 1.55×, 1.51×, 1.29× and 1.58×, respectively. Meanwhile, meshswitch with
    in-network computing surpasses mesh by 1.78×, 1.75×, 1.82×, 2.60× and 1.85×, and
    FRED by 1.49×, 2.09×, 1.99×, 1.38× and 1.63×, consistently achieving the highest
    performance. As analyzed in [8.2,](#page-10-5) these performance improvements
    vary with the computational and communication demands of different workloads.


    We will break down the contributions of individual design choices including physical
    topology optimization, dual-granularity logical topology and the optimization
    for parallelism to show their impact on overall performance more explicitly. The
    baseline logical topology is ring, and the performance gains are based on the
    average results across all benchmarks. As illustrated in the Fig[.24,](#page-12-2)
    mesh-switch achieves 1.38× and 1.22× performance improvements compared to mesh
    and FRED, respectively, primarily attributing to the DSE process which optimizes
    the allocation of on-chip interconnects and computation resources. Additionally,
    we design a dual-granularity logical topology tailored to the hierarchical physical
    topology, which delivers a 1.42× performance improvement. We further explore the
    optimal parallelism configuration for the topologies which results in a 1.22×
    performance gain. In conclusion, our proposed NoW achieves 2.39× and 2.11× throughput
    improvements compared to mesh and FRED, respectively.


    Key insight 9: The co-design methodology further boosts performance, with 1.38×,
    1.42× and 1.22× brought by physical & logical topologies and parallelism, respectively.


    #### 9 Related Works


    Waferscale chips and chiplets: Numerous studies have explored performance improvements
    in waferscale chips and multi chiplet modules [\[2,](#page-12-1) [30,](#page-13-15)
    [37,](#page-13-16) [71,](#page-14-40) [72\]](#page-14-11), focusing on design
    space exploration [\[10,](#page-13-41) [106,](#page-15-16) [108\]](#page-15-8),
    dataflow design [\[69,](#page-14-41) [103\]](#page-15-17), on-chip memory optimization
    [\[83\]](#page-14-9), and floorplan & routing [\[47,](#page-13-42) [68\]](#page-14-42).
    However, all these efforts lack awareness of the network on wafer. Additionally,
    they fail to recognize the optimization potential of physical and logical topologies,
    which play a critical role in resource allocation and communication efficiency.
    Instead, much of the focus has been on factors such as compute die performance,
    size, integration scale, on-chip bandwidth, storage capacity and so on.


    Logical/physical topology: There has been another line of research has focused
    on enhancing mesh communication performance by modifying logical topologies, with
    applications in collective communication algorithms [\[18,](#page-13-19) [56,](#page-14-14)
    [100\]](#page-15-3), switch network optimization [\[15,](#page-13-18) [98\]](#page-15-2),
    and other areas. While these studies have proposed logical topologies that are
    better suited to mesh than to ring or tree topologies, improving bandwidth utilization
    and reducing communication latency, challenges such as contention, uneven traffic
    distribution, and excessive diameter, the limitations of mesh physical topology,
    remain unresolved. Moreover, these studies overlook the optimization of the physical
    topology itself.


    Most prior research on physical topology has focused on building large scale clusters
    and achieving cost-effective scale out expansion. These efforts have made significant
    contributions to enhancing system scalability [\[6,](#page-13-43) [30,](#page-13-15)
    [34,](#page-13-44) [41,](#page-13-21) [55\]](#page-14-43). However, they often
    overlook the design of the NoW and lack detailed modeling or discussion of on-chip
    resource allocation. Recently, some physical topology


    <span id="page-12-2"></span>![](_page_12_Figure_9.jpeg)


    Figure 24: (a) Throughput gain and breakdown based on Mesh; (b) Throughput gain
    and breakdown based on FRED.


    designs have addressed communication bottlenecks in mesh by proposing FRED, improving
    communication performance by nearly twofold [\[78\]](#page-14-16). However, this
    communication-centric optimization increases network area at the expense of computational
    power, revealing limitations when handling compute-intensive tasks. Still, these
    works are not comprehensive enough for NoW no matter at the physical or logical
    topology.


    #### 10 Summary


    NoW plays an important role in waferscale chips architectural optimization. It
    puts an emphasis on two facts: physical and logical topology which ultimately
    shape the computational performance and communication efficiency. Achieving higher
    training performance clearly requires a well-optimized physical topology to manage
    on-chip resources effectively and an efficient logical topology to streamline
    communication task scheduling. Proposing a comprehensive network design methodology
    that hierarchically clarifies the relationships among physical/logical topology,
    and parallelism, which is important due to the constrained design space of wafer
    and lays the theoretical foundation of our design and implementation. We propose
    the mesh-switch physical topology and systematically construct the design space
    for NoW. Furthermore, we develop the DSE algorithm and explore optimal configurations.
    Additionally, we design a dual-granularity logical topology tailored to the mesh-switch
    architecture and further propose a fine-grained overlap strategy to optimize communication
    performance. Experimental results indicate that our design outperforms 2.39× and
    2.11× compared to mesh and FRED respectively.


    #### Acknowledgments


    This work was supported in part by the National Science and Technology Major Project
    under Grant 2022ZD0115200; in part by the Northern IC Technology Innovation Center
    (Beijing) Co., Ltd under Grant QYJS20232801B; in part by the NSFC under Grant
    62125403, Grant 92464302, Grant U24B20164 and Grant 92164301; in part by Shanghai
    Municipal Science and Technology Major Project; in part by the Natural Science
    Foundation of Jiangsu Province Basic Research Program under Grant BK20243042;
    in part by the Beijing National Research Center for Information Science and Technology;
    and in part by the Beijing Advanced Innovation Center for Integrated Circuits.


    #### References


    - <span id="page-12-0"></span>[1] Josh Achiam, Steven Adler, Sandhini Agarwal,
    Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt,
    Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint
    arXiv:2303.08774 (2023).

    - <span id="page-12-1"></span>[2] Akhil Arunkumar, Evgeny Bolotin, Benjamin Cho,
    Ugljesa Milic, Eiman Ebrahimi, Oreste Villa, Aamer Jaleel, Carole-Jean Wu, and
    David Nellans. 2017.


    MCM-GPU: Multi-chip-module GPUs for continued performance scalability. ACM SIGARCH
    Computer Architecture News 45, 2 (2017), 320–332.


    - <span id="page-13-26"></span>[3] Russel Aubusson. 1979. Wafer-scale integration
    of semiconductor memory. Ph. D. Dissertation. Middlesex Polytechnic.

    - <span id="page-13-10"></span>[4] Vivek Bakshi. 2009. EUV lithography. (2009).

    - <span id="page-13-0"></span>[5] Soroush Bateni, Zhendong Wang, Yuankun Zhu,
    Yang Hu, and Cong Liu. 2020. Co-optimizing performance and memory footprint via
    integrated cpu/gpu memory management, an implementation on autonomous driving
    platform. In 2020 IEEE Real-Time and Embedded Technology and Applications Symposium
    (RTAS). IEEE, 310–323.

    - <span id="page-13-43"></span>[6] Maciej Besta and Torsten Hoefler. 2014. Slim
    fly: A cost effective low-diameter network topology. In SC''14: proceedings of
    the international conference for high performance computing, networking, storage
    and analysis. IEEE, 348–359.

    - <span id="page-13-13"></span>[7] Srikant Bharadwaj, Jieming Yin, Bradford Beckmann,
    and Tushar Krishna. 2020. Kite: A family of heterogeneous interposer topologies
    enabled via accurate interconnect modeling. In 2020 57th ACM/IEEE Design Automation
    Conference (DAC). IEEE, 1–6.

    - <span id="page-13-1"></span>[8] Tom B Brown. 2020. Language models are few-shot
    learners. arXiv preprint arXiv:2005.14165 (2020).

    - <span id="page-13-27"></span>[9] James A Burns, Brian F Aull, Chenson K Chen,
    Chang-Lee Chen, Craig L Keast, Jeffrey M Knecht, Vyshanavi Suntharalingam, Keith
    Warner, Peter W Wyatt, and D-RW Yost. 2006. A wafer-scale 3-D circuit integration
    technology. IEEE Transactions on Electron Devices 53, 10 (2006), 2507–2516.

    - <span id="page-13-41"></span>[10] Jingwei Cai, Zuotong Wu, Sen Peng, Yuchen
    Wei, Zhanhong Tan, Guiming Shi, Mingyu Gao, and Kaisheng Ma. 2024. Gemini: Mapping
    and Architecture Coexploration for Large-scale DNN Chiplet Accelerators. In 2024
    IEEE International Symposium on High-Performance Computer Architecture (HPCA).
    IEEE, 156–171.

    - <span id="page-13-40"></span>[11] Jose M Camara, Miquel Moreto, Enrique Vallejo,
    Ramon Beivide, Jose Miguel-Alonso, Carmen Martínez, and Javier Navaridas. 2010.
    Twisted torus topologies for enhanced interconnection networks. IEEE Transactions
    on Parallel and Distributed Systems 21, 12 (2010), 1765–1778.

    - <span id="page-13-28"></span>[12] Richard O Carlson and Constantine A Neugebauer.
    1986. Future trends in wafer scale integration. Proc. IEEE 74, 12 (1986), 1741–1752.

    - <span id="page-13-36"></span>[13] Shubhangi D Chawade, Mahendra A Gaikwad, and
    Rajendra M Patrikar. 2012. Review of XY routing algorithm for network-on-chip
    architecture. International Journal of Computer Applications 43, 21 (2012), 975–8887.

    - <span id="page-13-33"></span>[14] Shixin Chen, Shanyi Li, Zhen Zhuang, Su Zheng,
    Zheng Liang, Tsung-Yi Ho, Bei Yu, and Alberto L Sangiovanni-Vincentelli. 2023.
    Floorplet: Performance-Aware Floorplan Framework for Chiplet Integration. IEEE
    Transactions on Computer-Aided Design of Integrated Circuits and Systems (2023).

    - <span id="page-13-18"></span>[15] Shuangliang Chen, Saptadeep Pal, and Rakesh
    Kumar. 2024. Waferscale Network Switches. In 2024 ACM/IEEE 51st Annual International
    Symposium on Computer Architecture (ISCA). IEEE, 215–229.

    - <span id="page-13-23"></span>[16] W Chris Chen, Clark Hu, KC Ting, Vincent Wei,
    TH Yu, SY Huang, VCY Chang, CT Wang, SY Hou, CH Wu, et al. 2017. Wafer level integration
    of an advanced logic-memory system through 2 nd generation CoWoS® technology.
    In 2017 Symposium on VLSI Technology. IEEE, T54–T55.

    - <span id="page-13-24"></span>[17] YP Chiang, SP Tai, WC Wu, John Yeh, CT Wang,
    and CH Douglas. 2021. InFO\_oS (integrated fan-out on substrate) technology for
    advanced chiplet integration. In 2021 IEEE 71st Electronic Components and Technology
    Conference (ECTC). IEEE, 130–135.

    - <span id="page-13-19"></span>[18] Sanghun Cho, Hyojun Son, and John Kim. 2023.
    Logical/physical topology-aware collective communication in deep learning training.
    In 2023 IEEE International Symposium on High-Performance Computer Architecture
    (HPCA). IEEE, 56–68.

    - <span id="page-13-8"></span>[19] Jack Choquette. 2023. Nvidia hopper h100 gpu:
    Scaling performance. IEEE Micro 43, 3 (2023), 9–17.

    - <span id="page-13-9"></span>[20] Jack Choquette and Wish Gandhi. 2020. Nvidia
    a100 gpu: Performance & innovation for gpu computing. In 2020 IEEE Hot Chips 32
    Symposium (HCS). IEEE Computer Society, 1–43.

    - <span id="page-13-12"></span>[21] Jack Choquette, Wishwesh Gandhi, Olivier Giroux,
    Nick Stam, and Ronny Krashinsky. 2021. Nvidia a100 tensor core gpu: Performance
    and innovation. IEEE Micro 41, 2 (2021), 29–35.

    - <span id="page-13-2"></span>[22] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
    Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles
    Sutton, Sebastian Gehrmann, et al. 2023. Palm: Scaling language modeling with
    pathways. Journal of Machine Learning Research 24, 240 (2023), 1–113.

    - <span id="page-13-25"></span>[23] Shu-Rong Chun, Tin-Hao Kuo, Hao-Yi Tsai, Chung-Shi
    Liu, Chuei-Tang Wang, Jeng-Shien Hsieh, Tsung-Shu Lin, Terry Ku, and Douglas Yu.
    2020. InFO\_SoW (system-on-wafer) for high performance computing. In 2020 IEEE
    70th Electronic Components and Technology Conference (ECTC). IEEE, 1–6.

    - <span id="page-13-34"></span>[24] Ayse Coskun, Furkan Eris, Ajay Joshi, Andrew
    B Kahng, Yenai Ma, Aditya Narayan, and Vaishnav Srinivas. 2020. Cross-layer co-optimization
    of network design and chiplet placement in 2.5-D systems. IEEE Transactions on
    Computer-Aided Design of Integrated Circuits and Systems 39, 12 (2020), 5183–5196.

    - <span id="page-13-14"></span>[25] Sajed Dadashi, Midia Reshadi, Akram Reza,
    and Ahmad Khademzadeh. 2019. An expandable topology with low wiring congestion
    for silicon interposerbased network-on-chip systems. Transactions on Emerging
    Telecommunications Technologies 30, 12 (2019), e3747.

    - <span id="page-13-37"></span>[26] Dally and Seitz. 1987. Deadlock-free message
    routing in multiprocessor interconnection networks. IEEE Transactions on computers
    100, 5 (1987), 547–553.

    - <span id="page-13-3"></span>[27] Mostafa Dehghani, Josip Djolonga, Basil Mustafa,
    Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde
    Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. 2023. Scaling vision transformers
    to 22 billion parameters. In International Conference on Machine Learning. PMLR,
    7480–7512. [28] Jacob Devlin. 2018. Bert: Pre-training of deep bidirectional transformers
    for

    - <span id="page-13-7"></span>language understanding. arXiv preprint arXiv:1810.04805
    (2018).

    - <span id="page-13-4"></span>[29] Yassir Fathullah, Chunyang Wu, Egor Lakomkin,
    Junteng Jia, Yuan Shangguan, Ke Li, Jinxi Guo, Wenhan Xiong, Jay Mahadeokar, Ozlem
    Kalinli, et al. 2024. Prompting large language models with speech recognition
    abilities. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech
    and Signal Processing (ICASSP). IEEE, 13351–13355.

    - <span id="page-13-15"></span>[30] Yinxiao Feng and Kaisheng Ma. 2024. Switch-Less
    Dragonfly on Wafers: A Scalable Interconnection Architecture based on Wafer-Scale
    Integration. arXiv preprint arXiv:2407.10290 (2024).

    - <span id="page-13-5"></span>[31] Stefan Feuerriegel, Jochen Hartmann, Christian
    Janiesch, and Patrick Zschech. 2024. Generative ai. Business & Information Systems
    Engineering 66, 1 (2024), 111–126.

    - <span id="page-13-11"></span>[32] Nan Fu, Yanxiang Liu, Xiaolong Ma, and Zanfeng
    Chen. 2019. EUV lithography: state-of-the-art review. J. Microelectron. Manuf
    2, 2 (2019), 1–6.

    - <span id="page-13-6"></span>[33] Fiona Fui-Hoon Nah, Ruilin Zheng, Jingyuan
    Cai, Keng Siau, and Langtao Chen. 2023. Generative AI and ChatGPT: Applications,
    challenges, and AI-human collaboration. 277–304 pages.

    - <span id="page-13-44"></span>[34] Torsten Hoefler, Tommaso Bonato, Daniele De
    Sensi, Salvatore Di Girolamo, Shigang Li, Marco Heddes, Jon Belk, Deepak Goel,
    Miguel Castro, and Steve Scott. 2022. HammingMesh: a network topology for large-scale
    deep learning. In SC22: International Conference for High Performance Computing,
    Networking, Storage and Analysis. IEEE, 1–18.

    - <span id="page-13-29"></span>[35] Xiaofeng Hou, Chao Li, Jiacheng Liu, Lu Zhang,
    Yang Hu, and Minyi Guo. 2020. ANT-Man: Towards agile power management in the microservice
    era. In SC20: International Conference for High Performance Computing, Networking,
    Storage and Analysis. IEEE, 1–14.

    - <span id="page-13-35"></span>[36] Harry Hsia, SP Tai, CS Liu, CW Tseng, SW Lu,
    Yutung Wu, CC Chang, Jason Wu, KC Yee, CY Wu, et al. 2023. Integrated Optical
    Interconnect Systems (iOIS) for silicon photonics applications in HPC. In 2023
    IEEE 73rd Electronic Components and Technology Conference (ECTC). IEEE, 612–616.

    - <span id="page-13-16"></span>[37] Yang Hu, Xinhan Lin, Huizheng Wang, Zhen He,
    Xingmao Yu, Jiahao Zhang, Qize Yang, Zheng Xu, Sihan Guan, Jiahao Fang, et al.
    2024. Wafer-Scale Computing: Advancements, Challenges, and Future Perspectives
    [Feature]. IEEE Circuits and Systems Magazine 24, 1 (2024), 52–81.

    - <span id="page-13-30"></span>[38] Yang Hu, Mingcong Song, and Tao Li. 2017.
    Towards" full containerization" in containerized network function virtualization.
    In Proceedings of the Twenty-Second International Conference on Architectural
    Support for Programming Languages and Operating Systems. 467–481.

    - <span id="page-13-22"></span>[39] Jiayi Huang, Pritam Majumder, Sungkeun Kim,
    Abdullah Muzahid, Ki Hwan Yum, and Eun Jung Kim. 2021. Communication algorithm-architecture
    codesign for distributed deep learning. In 2021 ACM/IEEE 48th Annual International
    Symposium on Computer Architecture (ISCA). IEEE, 181–194.

    - <span id="page-13-38"></span>[40] Yanping Huang, Youlong Cheng, Ankur Bapna,
    Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui
    Wu, et al. 2019. Gpipe: Efficient training of giant neural networks using pipeline
    parallelism. Advances in neural information processing systems 32 (2019).

    - <span id="page-13-21"></span>[41] Patrick Iff, Maciej Besta, Matheus Cavalcante,
    Tim Fischer, Luca Benini, and Torsten Hoefler. 2023. HexaMesh: Scaling to Hundreds
    of Chiplets with an Optimized Chiplet Arrangement. In 2023 60th ACM/IEEE Design
    Automation Conference (DAC). IEEE, 1–6.

    - <span id="page-13-20"></span>[42] Patrick Iff, Maciej Besta, Matheus Cavalcante,
    Tim Fischer, Luca Benini, and Torsten Hoefler. 2023. Sparse Hamming Graph: A Customizable
    Network-on-Chip Topology. In 2023 60th ACM/IEEE Design Automation Conference (DAC).
    IEEE, 1–6.

    - <span id="page-13-31"></span>[43] Subramanian S Iyer, S Jangam, and Boris Vaisband.
    2019. Silicon interconnect fabric: A versatile heterogeneous integration platform
    for AI systems. IBM Journal of Research and Development 63, 6 (2019), 5–1.

    - <span id="page-13-39"></span>[44] Sam Ade Jacobs, Masahiro Tanaka, Chengming
    Zhang, Minjia Zhang, Shuaiwen Leon Song, Samyam Rajbhandari, and Yuxiong He. 2023.
    Deepspeed ulysses: System optimizations for enabling training of extreme long
    sequence transformer models. arXiv preprint arXiv:2309.14509 (2023).

    - <span id="page-13-32"></span>[45] SivaChandra Jangam, Adeel Ahmed Bajwa, Kannan
    K Thankkappan, Premsagar Kittur, and Subramanian Srikantes Iyer. 2018. Electrical
    characterization of high performance fine pitch interconnects in silicon-interconnect
    fabric. In 2018 IEEE 68th Electronic Components and Technology Conference (ECTC).
    IEEE, 1283–1288.

    - <span id="page-13-17"></span>[46] Natalie Enright Jerger, Ajaykumar Kannan,
    Zimo Li, and Gabriel H Loh. 2014. NoC architectures for silicon interposer systems:
    Why pay for more wires when you can get them (from your interposer) for free?.
    In 2014 47th Annual IEEE/ACM International Symposium on Microarchitecture. IEEE,
    458–470.

    - <span id="page-13-42"></span>[47] Bentian Jiang, Jingsong Chen, Jinwei Liu,
    Lixin Liu, Fangzhou Wang, Xiaopeng Zhang, and Evangeline FY Young. 2020. CU. POKer:
    placing DNNs on wafer-scale


    AI accelerator with optimal kernel sizing. In Proceedings of the 39th International
    Conference on Computer-Aided Design. 1–9.


    - <span id="page-14-17"></span>[48] Yuho Jin, Eun Jung Kim, and Timothy Mark Pinkston.
    2011. Communicationaware globally-coordinated on-chip networks. IEEE Transactions
    on Parallel and Distributed Systems 23, 2 (2011), 242–254.

    - <span id="page-14-24"></span>[49] Tom Jose and Deepak Shankar. 2023. Performance
    modeling of a heterogeneous computing system based on the UCIe Interconnect Architecture.
    In 2023 IEEE Space Computing Conference (SCC). IEEE, 5–10.

    - <span id="page-14-8"></span>[50] Norm Jouppi, George Kurian, Sheng Li, Peter
    Ma, Rahul Nagarajan, Lifeng Nai, Nishant Patil, Suvinay Subramanian, Andy Swing,
    Brian Towles, et al. 2023. Tpu v4: An optically reconfigurable supercomputer for
    machine learning with hardware support for embeddings. In Proceedings of the 50th
    Annual International Symposium on Computer Architecture. 1–14.

    - <span id="page-14-37"></span>[51] John Kim, James Balfour, and William Dally.
    2007. Flattened butterfly topology for on-chip networks. In 40th Annual IEEE/ACM
    International Symposium on Microarchitecture (MICRO 2007). IEEE, 172–182.

    - <span id="page-14-36"></span>[52] John Kim, Wiliam J Dally, Steve Scott, and
    Dennis Abts. 2008. Technology-driven, highly-scalable dragonfly topology. ACM
    SIGARCH Computer Architecture News 36, 3 (2008), 77–88.

    - <span id="page-14-34"></span>[53] Christophe Kopp, Stephane Bernabe, Badhise
    Ben Bakir, Jean-Marc Fedeli, Regis Orobtchouk, Franz Schrank, Henri Porte, Lars
    Zimmermann, and Tolga Tekin. 2010. Silicon photonic circuits: on-CMOS integration,
    fiber optical coupling, and packaging. IEEE Journal of selected topics in quantum
    electronics 17, 3 (2010), 498–509.

    - <span id="page-14-35"></span>[54] Richard E Korf. 1985. Depth-first iterative-deepening:
    An optimal admissible tree search. Artificial intelligence 27, 1 (1985), 97–109.

    - <span id="page-14-43"></span>[55] Kartik Lakhotia, Maciej Besta, Laura Monroe,
    Kelly Isham, Patrick Iff, Torsten Hoefler, and Fabrizio Petrini. 2022. PolarFly:
    a cost-effective and flexible lowdiameter topology. In SC22: International Conference
    for High Performance Computing, Networking, Storage and Analysis. IEEE, 1–15.

    - <span id="page-14-14"></span>[56] Sabuj Laskar, Pranati Majhi, Sungkeun Kim,
    Farabi Mahmud, Abdullah Muzahid, and Eun Jung Kim. 2024. Enhancing Collective
    Communication in MCM Accelerators for Deep Learning Training. In 2024 IEEE International
    Symposium on High-Performance Computer Architecture (HPCA). IEEE, 1–16.

    - <span id="page-14-18"></span>[57] Gary Lauterbach. 2021. The path to successful
    wafer-scale integration: The cerebras story. IEEE Micro 41, 6 (2021), 52–57.

    - <span id="page-14-19"></span>[58] Leighton and Leiserson. 1985. Wafer-scale
    integration of systolic arrays. IEEE Transactions on computers 100, 5 (1985),
    448–461.

    - <span id="page-14-38"></span>[59] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong
    Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng
    Chen. 2020. Gshard: Scaling giant models with conditional computation and automatic
    sharding. arXiv preprint arXiv:2006.16668 (2020).

    - <span id="page-14-0"></span>[60] Chao Li, Yang Hu, Ruijin Zhou, Ming Liu, Longjun
    Liu, Jingling Yuan, and Tao Li. 2013. Enabling datacenter servers to scale out
    economically and sustainably. In Proceedings of the 46th annual IEEE/ACM international
    symposium on microarchitecture. 322–333.

    - <span id="page-14-1"></span>[61] Yuang Li, Yu Wu, Jinyu Li, and Shujie Liu.
    2023. Prompting large language models for zero-shot domain adaptation in speech
    recognition. In 2023 IEEE Automatic Speech Recognition and Understanding Workshop
    (ASRU). IEEE, 1–8.

    - <span id="page-14-10"></span>[62] Sean Lie. 2022. Cerebras architecture deep
    dive: First look inside the hw/sw codesign for deep learning: Cerebras systems.
    In 2022 IEEE Hot Chips 34 Symposium (HCS). IEEE Computer Society, 1–34.

    - <span id="page-14-22"></span>[63] Sean Lie. 2023. Cerebras architecture deep
    dive: First look inside the hardware/software co-design for deep learning. IEEE
    Micro 43, 3 (2023), 18–30.

    - <span id="page-14-2"></span>[64] Xiao Lin, Shuzhou Sun, Wei Huang, Bin Sheng,
    Ping Li, and David Dagan Feng. 2021. EAPT: efficient attention pyramid transformer
    for image processing. IEEE Transactions on Multimedia 25 (2021), 50–61.

    - <span id="page-14-39"></span>[65] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang,
    Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan,
    et al. 2024. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437 (2024).

    - <span id="page-14-23"></span>[66] Hatem Ltaief, Yuxi Hong, Leighton Wilson,
    Mathias Jacquelin, Matteo Ravasi, and David Elliot Keyes. 2023. Scaling the "memory
    wall" for multi-dimensional seismic processing with algebraic compression on cerebras
    cs-2 systems. In Proceedings of the International Conference for High Performance
    Computing, Networking, Storage and Analysis. 1–12.

    - <span id="page-14-15"></span>[67] Piotr Luczynski, Lukas Gianinazzi, Patrick
    Iff, Leighton Wilson, Daniele De Sensi, and Torsten Hoefler. 2024. Near-optimal
    wafer-scale reduce. In Proceedings of the 33rd International Symposium on High-Performance
    Parallel and Distributed Computing. 334–347.

    - <span id="page-14-42"></span>[68] Canhui Luo, Zhouxing Su, and Zhipeng Lü. 2023.
    MS-CLS: An Effective Partitioning and Placement Metaheuristic for Wafer-Scale
    Physics Modeling. IEEE Transactions on Emerging Topics in Computational Intelligence
    (2023).

    - <span id="page-14-41"></span>[69] Marcelo Orenes-Vera, Esin Tureci, Margaret
    Martonosi, and David Wentzlaff. 2023. DCRA: A distributed chiplet-based reconfigurable
    architecture for irregular applications. arXiv preprint arXiv:2311.15443 (2023).

    - <span id="page-14-27"></span>[70] Saptadeep Pal and Puneet Gupta. 2020. Pathfinding
    for 2.5 D interconnect technologies. In Proceedings of the Workshop on System-Level
    Interconnect: Problems and Pathfinding Workshop. 1–8.

    - <span id="page-14-40"></span>[71] Saptadeep Pal, Jingyang Liu, Irina Alam, Nicholas
    Cebry, Haris Suhail, Shi Bu, Subramanian S Iyer, Sudhakar Pamarti, Rakesh Kumar,
    and Puneet Gupta. 2021. Designing a 2048-chiplet, 14336-core waferscale processor.
    In 2021 58th ACM/IEEE Design Automation Conference (DAC). IEEE, 1183–1188.

    - <span id="page-14-11"></span>[72] Saptadeep Pal, Daniel Petrisko, Matthew Tomei,
    Puneet Gupta, Subramanian S Iyer, and Rakesh Kumar. 2019. Architecting waferscale
    processors-a GPU case study. In 2019 IEEE International Symposium on High Performance
    Computer Architecture (HPCA). IEEE, 250–263.

    - <span id="page-14-12"></span>[73] Sunyoung Park. 2021. High Bandwidth Interposer
    Switch (HBI-S) Topology in Modular System on Chip. Ph. D. Dissertation.

    - <span id="page-14-3"></span>[74] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit,
    Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. 2018. Image transformer.
    In International conference on machine learning. PMLR, 4055–4064.

    - <span id="page-14-4"></span>[75] Yubin Qin, Yang Wang, Dazheng Deng, Zhiren
    Zhao, Xiaolong Yang, Leibo Liu, Shaojun Wei, Yang Hu, and Shouyi Yin. 2023. Fact:
    Ffn-attention co-optimized transformer architecture with eager correlation prediction.
    In Proceedings of the 50th Annual International Symposium on Computer Architecture.
    1–14.

    - <span id="page-14-7"></span>[76] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
    Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020.
    Exploring the limits of transfer learning with a unified text-to-text transformer.
    Journal of machine learning research 21, 140 (2020), 1–67.

    - <span id="page-14-32"></span>[77] Saeed Rashidi, Srinivas Sridharan, Sudarshan
    Srinivasan, and Tushar Krishna. 2020. Astra-sim: Enabling sw/hw co-design exploration
    for distributed dl training platforms. In 2020 IEEE International Symposium on
    Performance Analysis of Systems and Software (ISPASS). IEEE, 81–92.

    - <span id="page-14-16"></span>[78] Saeed Rashidi, William Won, Sudarshan Srinivasan,
    Puneet Gupta, and Tushar Krishna. 2024. FRED: Flexible REduction-Distribution
    Interconnect and Communication Implementation for Wafer-Scale Distributed Training
    of DNN Models. arXiv preprint arXiv:2406.19580 (2024).

    - <span id="page-14-30"></span>[79] Peter Sanders, Jochen Speck, and Jesper Larsson
    Träff. 2009. Two-tree algorithms for full bandwidth broadcast, reduction and scan.
    Parallel Comput. 35, 12 (2009), 581–594.

    - <span id="page-14-21"></span>[80] Johannes Schemmel, Daniel Brüderle, Andreas
    Grübl, Matthias Hock, Karlheinz Meier, and Sebastian Millner. 2010. A wafer-scale
    neuromorphic hardware system for large-scale neural modeling. In 2010 IEEE International
    Symposium on Circuits and Systems (ISCAS). IEEE, 1947–1950.

    - <span id="page-14-20"></span>[81] Tom Schram, Surajit Sutar, Iuliana Radu, and
    Inge Asselberghs. 2022. Challenges of wafer-scale integration of 2D semiconductors
    for high-performance transistor circuits. Advanced Materials 34, 48 (2022), 2109796.

    - <span id="page-14-28"></span>[82] Omer S Sella, Andrew W Moore, and Noa Zilberman.
    2018. FEC killed the cutthrough switch. In Proceedings of the 2018 Workshop on
    Networking for Emerging Applications and Technologies. 15–20.

    - <span id="page-14-9"></span>[83] Yakun Sophia Shao, Jason Clemons, Rangharajan
    Venkatesan, Brian Zimmer, Matthew Fojtik, Nan Jiang, Ben Keller, Alicia Klinefelter,
    Nathaniel Pinckney, Priyanka Raina, et al. 2019. Simba: Scaling deep-learning
    inference with multichip-module-based architecture. In Proceedings of the 52nd
    Annual IEEE/ACM International Symposium on Microarchitecture. 14–27.

    - <span id="page-14-29"></span>[84] Debendra Das Sharma, Gerald Pasdast, Zhiguo
    Qian, and Kemal Aygun. 2022. Universal chiplet interconnect express (UCIe): An
    open industry standard for innovations with chiplets at package level. IEEE Transactions
    on Components, Packaging and Manufacturing Technology 12, 9 (2022), 1423–1431.

    - <span id="page-14-33"></span>[85] Mingcong Song, Xinru Tang, Fengfan Hou, Jing
    Li, Wei Wei, Yipeng Ma, Runqiu Xiao, Hongjie Si, Dingcheng Jiang, Shouyi Yin,
    et al. 2024. Tackling the dynamicity in a production llm serving system with sota
    optimizations via hybrid prefill/decode/verify scheduling on efficient meta-kernels.
    arXiv preprint arXiv:2412.18106 (2024).

    - <span id="page-14-5"></span>[86] Mingcong Song, Jiechen Zhao, Yang Hu, Jiaqi
    Zhang, and Tao Li. 2018. Prediction based execution on deep neural networks. In
    2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA).
    IEEE, 752–763.

    - <span id="page-14-6"></span>[87] Mingcong Song, Kan Zhong, Jiaqi Zhang, Yang
    Hu, Duo Liu, Weigong Zhang, Jing Wang, and Tao Li. 2018. In-situ ai: Towards autonomous
    and incremental deep learning for iot systems. In 2018 IEEE International Symposium
    on High Performance Computer Architecture (HPCA). IEEE, 92–103.

    - <span id="page-14-25"></span>[88] Dylan Stow, Itir Akgun, and Yuan Xie. 2019.
    Investigation of cost-optimal network-on-chip for passive and active interposer
    systems. In 2019 ACM/IEEE International Workshop on System Level Interconnect
    Prediction (SLIP). IEEE, 1–8.

    - <span id="page-14-26"></span>[89] Dylan Stow, Yuan Xie, Taniya Siddiqua, and
    Gabriel H Loh. 2017. Cost-effective design of scalable high-performance systems
    using active and passive interposers. In 2017 IEEE/ACM International Conference
    on Computer-Aided Design (ICCAD). IEEE, 728–735.

    - <span id="page-14-31"></span>[90] Emil Talpes, Debjit Das Sarma, Doug Williams,
    Sahil Arora, Thomas Kunjan, Benjamin Floering, Ankit Jalote, Christopher Hsiong,
    Chandrasekhar Poorna, Vaidehi Samant, et al. 2023. The microarchitecture of dojo,
    tesla''s exa-scale computer. IEEE Micro 43, 3 (2023), 31–39.

    - <span id="page-14-13"></span>[91] Emil Talpes, Douglas Williams, and Debjit
    Das Sarma. 2022. Dojo: The microarchitecture of tesla''s exa-scale computer. In
    2022 IEEE Hot Chips 34 Symposium (HCS). IEEE Computer Society, 1–28.

    - <span id="page-15-1"></span><span id="page-15-0"></span>[92] Zhanhong Tan, Hongyu
    Cai, Runpei Dong, and Kaisheng Ma. 2021. Nn-baton: Dnn workload orchestration
    and chiplet granularity exploration for multichip accelerators. In 2021 ACM/IEEE
    48th Annual International Symposium on Computer Architecture (ISCA). IEEE, 1013–1026.

    - <span id="page-15-10"></span>[93] Hugo Touvron, Thibaut Lavril, Gautier Izacard,
    Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman
    Goyal, Eric Hambro, Faisal Azhar, et al. 2023. LLaMA: open and efficient foundation
    language models. arXiv. arXiv preprint arXiv:2302.13971 (2023).

    - <span id="page-15-7"></span>[94] Yuichiro Ueno and Rio Yokota. 2019. Exhaustive
    study of hierarchical allreduce patterns for large messages between GPUs. In 2019
    19th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID).
    IEEE, 430–439.

    - <span id="page-15-15"></span>[95] Yuichiro Ueno and Rio Yokota. 2019. Exhaustive
    study of hierarchical allreduce patterns for large messages between GPUs. In 2019
    19th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID).
    IEEE, 430–439.

    - <span id="page-15-13"></span>[96] Mary K Vernon and Udi Manber. 1988. Distributed
    round-robin and first-come first-serve protocols and their applications to multiprocessor
    bus arbitration. ACM SIGARCH Computer Architecture News 16, 2 (1988), 269–279.

    - <span id="page-15-12"></span>[97] Mark Wade, Erik Anderson, Shahab Ardalan,
    Pavan Bhargava, Sidney Buchbinder, Michael L Davenport, John Fini, Haiwei Lu,
    Chen Li, Roy Meade, et al. 2020. TeraPHY: a chiplet technology for low-power,
    high-bandwidth in-package optical I/O. IEEE Micro 40, 2 (2020), 63–71.

    - <span id="page-15-2"></span>[98] Zhiquan Wan, Zhipeng Cao, Shunbin Li, Peijie
    Li, Qingwen Deng, Weihao Wang, Kun Zhang, Guandong Liu, Ruyun Zhang, and Qinrang
    Liu. 2024. Architectural Exploration for Waferscale Switching System. IEEE Transactions
    on Very Large Scale Integration (VLSI) Systems (2024).

    - <span id="page-15-5"></span>[99] Ronald Williams and Ogden Marsh. 1993. Future
    WSI technology: stacked monolithic WSI. IEEE transactions on components, hybrids,
    and manufacturing technology 16, 7 (1993), 610–614.

    - <span id="page-15-3"></span>[100] William Won, Midhilesh Elavazhagan, Sudarshan
    Srinivasan, Ajaya Durg, Samvit Kaul, Swati Gupta, and Tushar Krishna. 2023. TACOS:
    Topology-Aware Collective Algorithm Synthesizer for Distributed Machine Learning.
    arXiv


    preprint arXiv:2304.05301 (2023).


    - <span id="page-15-9"></span>[101] William Won, Taekyung Heo, Saeed Rashidi,
    Srinivas Sridharan, Sudarshan Srinivasan, and Tushar Krishna. 2023. Astra-sim2.
    0: Modeling hierarchical networks and disaggregated systems for large-model training
    at scale. In 2023 IEEE International Symposium on Performance Analysis of Systems
    and Software (ISPASS). IEEE, 283–294.

    - <span id="page-15-4"></span>[102] William Won, Saeed Rashidi, Sudarshan Srinivasan,
    and Tushar Krishna. 2024. LIBRA: Enabling Workload-Aware Multi-Dimensional Network
    Topology Optimization for Distributed Training of Large AI Models. In 2024 IEEE
    International Symposium on Performance Analysis of Systems and Software (ISPASS).
    IEEE, 205–216.

    - <span id="page-15-17"></span>[103] Jiantao Wu, Wan-Ping Lee, Alistair Ward,
    Jerilyn A Walker, Miriam K Konkel, Mark A Batzer, and Gabor T Marth. 2014. Tangram:
    a comprehensive toolbox for mobile element insertion detection. BMC genomics 15
    (2014), 1–15.

    - <span id="page-15-6"></span>[104] FENG Yinxiao and MA Kaisheng. 2022. Chiplet
    actuary: A quantitative cost model and multi-chiplet architecture exploration
    [C]. In The 59th ACM/IEEE Design Automation Conference, San Francisco, USA. 121–126.

    - <span id="page-15-11"></span>[105] Fanlong Zeng, Wensheng Gan, Yongheng Wang,
    and S Yu Philip. 2023. Distributed training of large language models. In 2023
    IEEE 29th International Conference on Parallel and Distributed Systems (ICPADS).
    IEEE, 840–847.

    - <span id="page-15-16"></span>[106] Hengrui Zhang, August Ning, Rohan Baskar
    Prabhakar, and David Wentzlaff. 2024. Llmcompass: Enabling efficient hardware
    design for large language model inference. In 2024 ACM/IEEE 51st Annual International
    Symposium on Computer Architecture (ISCA). IEEE, 1080–1096.

    - <span id="page-15-14"></span>[107] Susan Zhang, Stephen Roller, Naman Goyal,
    Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li,
    Xi Victoria Lin, et al. 2023. Opt: Open pre-trained transformer language models,
    2022. URL https://arxiv. org/abs/2205.01068 3 (2023), 19–0.

    - <span id="page-15-8"></span>[108] Jingchen Zhu, Chenhao Xue, Yiqi Chen, Zhao
    Wang, and Guangyu Sun. 2024. Theseus: Exploring Efficient Wafer-Scale Chip Design
    for Large Language Models. arXiv preprint arXiv:2407.02079 (2024).'
  references:
  - '- <span id="page-12-0"></span>[1] Josh Achiam, Steven Adler, Sandhini Agarwal,
    Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt,
    Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint
    arXiv:2303.08774 (2023).'
  - '- <span id="page-12-1"></span>[2] Akhil Arunkumar, Evgeny Bolotin, Benjamin Cho,
    Ugljesa Milic, Eiman Ebrahimi, Oreste Villa, Aamer Jaleel, Carole-Jean Wu, and
    David Nellans. 2017.'
  - 'MCM-GPU: Multi-chip-module GPUs for continued performance scalability. ACM SIGARCH
    Computer Architecture News 45, 2 (2017), 320–332.'
  - '- <span id="page-13-26"></span>[3] Russel Aubusson. 1979. Wafer-scale integration
    of semiconductor memory. Ph. D. Dissertation. Middlesex Polytechnic.'
  - '- <span id="page-13-10"></span>[4] Vivek Bakshi. 2009. EUV lithography. (2009).'
  - '- <span id="page-13-0"></span>[5] Soroush Bateni, Zhendong Wang, Yuankun Zhu,
    Yang Hu, and Cong Liu. 2020. Co-optimizing performance and memory footprint via
    integrated cpu/gpu memory management, an implementation on autonomous driving
    platform. In 2020 IEEE Real-Time and Embedded Technology and Applications Symposium
    (RTAS). IEEE, 310–323.'
  - '- <span id="page-13-43"></span>[6] Maciej Besta and Torsten Hoefler. 2014. Slim
    fly: A cost effective low-diameter network topology. In SC''14: proceedings of
    the international conference for high performance computing, networking, storage
    and analysis. IEEE, 348–359.'
  - '- <span id="page-13-13"></span>[7] Srikant Bharadwaj, Jieming Yin, Bradford Beckmann,
    and Tushar Krishna. 2020. Kite: A family of heterogeneous interposer topologies
    enabled via accurate interconnect modeling. In 2020 57th ACM/IEEE Design Automation
    Conference (DAC). IEEE, 1–6.'
  - '- <span id="page-13-1"></span>[8] Tom B Brown. 2020. Language models are few-shot
    learners. arXiv preprint arXiv:2005.14165 (2020).'
  - '- <span id="page-13-27"></span>[9] James A Burns, Brian F Aull, Chenson K Chen,
    Chang-Lee Chen, Craig L Keast, Jeffrey M Knecht, Vyshanavi Suntharalingam, Keith
    Warner, Peter W Wyatt, and D-RW Yost. 2006. A wafer-scale 3-D circuit integration
    technology. IEEE Transactions on Electron Devices 53, 10 (2006), 2507–2516.'
  - '- <span id="page-13-41"></span>[10] Jingwei Cai, Zuotong Wu, Sen Peng, Yuchen
    Wei, Zhanhong Tan, Guiming Shi, Mingyu Gao, and Kaisheng Ma. 2024. Gemini: Mapping
    and Architecture Coexploration for Large-scale DNN Chiplet Accelerators. In 2024
    IEEE International Symposium on High-Performance Computer Architecture (HPCA).
    IEEE, 156–171.'
  - '- <span id="page-13-40"></span>[11] Jose M Camara, Miquel Moreto, Enrique Vallejo,
    Ramon Beivide, Jose Miguel-Alonso, Carmen Martínez, and Javier Navaridas. 2010.
    Twisted torus topologies for enhanced interconnection networks. IEEE Transactions
    on Parallel and Distributed Systems 21, 12 (2010), 1765–1778.'
  - '- <span id="page-13-28"></span>[12] Richard O Carlson and Constantine A Neugebauer.
    1986. Future trends in wafer scale integration. Proc. IEEE 74, 12 (1986), 1741–1752.'
  - '- <span id="page-13-36"></span>[13] Shubhangi D Chawade, Mahendra A Gaikwad,
    and Rajendra M Patrikar. 2012. Review of XY routing algorithm for network-on-chip
    architecture. International Journal of Computer Applications 43, 21 (2012), 975–8887.'
  - '- <span id="page-13-33"></span>[14] Shixin Chen, Shanyi Li, Zhen Zhuang, Su Zheng,
    Zheng Liang, Tsung-Yi Ho, Bei Yu, and Alberto L Sangiovanni-Vincentelli. 2023.
    Floorplet: Performance-Aware Floorplan Framework for Chiplet Integration. IEEE
    Transactions on Computer-Aided Design of Integrated Circuits and Systems (2023).'
  - '- <span id="page-13-18"></span>[15] Shuangliang Chen, Saptadeep Pal, and Rakesh
    Kumar. 2024. Waferscale Network Switches. In 2024 ACM/IEEE 51st Annual International
    Symposium on Computer Architecture (ISCA). IEEE, 215–229.'
  - '- <span id="page-13-23"></span>[16] W Chris Chen, Clark Hu, KC Ting, Vincent
    Wei, TH Yu, SY Huang, VCY Chang, CT Wang, SY Hou, CH Wu, et al. 2017. Wafer level
    integration of an advanced logic-memory system through 2 nd generation CoWoS®
    technology. In 2017 Symposium on VLSI Technology. IEEE, T54–T55.'
  - '- <span id="page-13-24"></span>[17] YP Chiang, SP Tai, WC Wu, John Yeh, CT Wang,
    and CH Douglas. 2021. InFO\_oS (integrated fan-out on substrate) technology for
    advanced chiplet integration. In 2021 IEEE 71st Electronic Components and Technology
    Conference (ECTC). IEEE, 130–135.'
  - '- <span id="page-13-19"></span>[18] Sanghun Cho, Hyojun Son, and John Kim. 2023.
    Logical/physical topology-aware collective communication in deep learning training.
    In 2023 IEEE International Symposium on High-Performance Computer Architecture
    (HPCA). IEEE, 56–68.'
  - '- <span id="page-13-8"></span>[19] Jack Choquette. 2023. Nvidia hopper h100 gpu:
    Scaling performance. IEEE Micro 43, 3 (2023), 9–17.'
  - '- <span id="page-13-9"></span>[20] Jack Choquette and Wish Gandhi. 2020. Nvidia
    a100 gpu: Performance & innovation for gpu computing. In 2020 IEEE Hot Chips 32
    Symposium (HCS). IEEE Computer Society, 1–43.'
  - '- <span id="page-13-12"></span>[21] Jack Choquette, Wishwesh Gandhi, Olivier
    Giroux, Nick Stam, and Ronny Krashinsky. 2021. Nvidia a100 tensor core gpu: Performance
    and innovation. IEEE Micro 41, 2 (2021), 29–35.'
  - '- <span id="page-13-2"></span>[22] Aakanksha Chowdhery, Sharan Narang, Jacob
    Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung,
    Charles Sutton, Sebastian Gehrmann, et al. 2023. Palm: Scaling language modeling
    with pathways. Journal of Machine Learning Research 24, 240 (2023), 1–113.'
  - '- <span id="page-13-25"></span>[23] Shu-Rong Chun, Tin-Hao Kuo, Hao-Yi Tsai,
    Chung-Shi Liu, Chuei-Tang Wang, Jeng-Shien Hsieh, Tsung-Shu Lin, Terry Ku, and
    Douglas Yu. 2020. InFO\_SoW (system-on-wafer) for high performance computing.
    In 2020 IEEE 70th Electronic Components and Technology Conference (ECTC). IEEE,
    1–6.'
  - '- <span id="page-13-34"></span>[24] Ayse Coskun, Furkan Eris, Ajay Joshi, Andrew
    B Kahng, Yenai Ma, Aditya Narayan, and Vaishnav Srinivas. 2020. Cross-layer co-optimization
    of network design and chiplet placement in 2.5-D systems. IEEE Transactions on
    Computer-Aided Design of Integrated Circuits and Systems 39, 12 (2020), 5183–5196.'
  - '- <span id="page-13-14"></span>[25] Sajed Dadashi, Midia Reshadi, Akram Reza,
    and Ahmad Khademzadeh. 2019. An expandable topology with low wiring congestion
    for silicon interposerbased network-on-chip systems. Transactions on Emerging
    Telecommunications Technologies 30, 12 (2019), e3747.'
  - '- <span id="page-13-37"></span>[26] Dally and Seitz. 1987. Deadlock-free message
    routing in multiprocessor interconnection networks. IEEE Transactions on computers
    100, 5 (1987), 547–553.'
  - '- <span id="page-13-3"></span>[27] Mostafa Dehghani, Josip Djolonga, Basil Mustafa,
    Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde
    Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. 2023. Scaling vision transformers
    to 22 billion parameters. In International Conference on Machine Learning. PMLR,
    7480–7512. [28] Jacob Devlin. 2018. Bert: Pre-training of deep bidirectional transformers
    for'
  - '- <span id="page-13-7"></span>language understanding. arXiv preprint arXiv:1810.04805
    (2018).'
  - '- <span id="page-13-4"></span>[29] Yassir Fathullah, Chunyang Wu, Egor Lakomkin,
    Junteng Jia, Yuan Shangguan, Ke Li, Jinxi Guo, Wenhan Xiong, Jay Mahadeokar, Ozlem
    Kalinli, et al. 2024. Prompting large language models with speech recognition
    abilities. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech
    and Signal Processing (ICASSP). IEEE, 13351–13355.'
  - '- <span id="page-13-15"></span>[30] Yinxiao Feng and Kaisheng Ma. 2024. Switch-Less
    Dragonfly on Wafers: A Scalable Interconnection Architecture based on Wafer-Scale
    Integration. arXiv preprint arXiv:2407.10290 (2024).'
  - '- <span id="page-13-5"></span>[31] Stefan Feuerriegel, Jochen Hartmann, Christian
    Janiesch, and Patrick Zschech. 2024. Generative ai. Business & Information Systems
    Engineering 66, 1 (2024), 111–126.'
  - '- <span id="page-13-11"></span>[32] Nan Fu, Yanxiang Liu, Xiaolong Ma, and Zanfeng
    Chen. 2019. EUV lithography: state-of-the-art review. J. Microelectron. Manuf
    2, 2 (2019), 1–6.'
  - '- <span id="page-13-6"></span>[33] Fiona Fui-Hoon Nah, Ruilin Zheng, Jingyuan
    Cai, Keng Siau, and Langtao Chen. 2023. Generative AI and ChatGPT: Applications,
    challenges, and AI-human collaboration. 277–304 pages.'
  - '- <span id="page-13-44"></span>[34] Torsten Hoefler, Tommaso Bonato, Daniele
    De Sensi, Salvatore Di Girolamo, Shigang Li, Marco Heddes, Jon Belk, Deepak Goel,
    Miguel Castro, and Steve Scott. 2022. HammingMesh: a network topology for large-scale
    deep learning. In SC22: International Conference for High Performance Computing,
    Networking, Storage and Analysis. IEEE, 1–18.'
  - '- <span id="page-13-29"></span>[35] Xiaofeng Hou, Chao Li, Jiacheng Liu, Lu Zhang,
    Yang Hu, and Minyi Guo. 2020. ANT-Man: Towards agile power management in the microservice
    era. In SC20: International Conference for High Performance Computing, Networking,
    Storage and Analysis. IEEE, 1–14.'
  - '- <span id="page-13-35"></span>[36] Harry Hsia, SP Tai, CS Liu, CW Tseng, SW
    Lu, Yutung Wu, CC Chang, Jason Wu, KC Yee, CY Wu, et al. 2023. Integrated Optical
    Interconnect Systems (iOIS) for silicon photonics applications in HPC. In 2023
    IEEE 73rd Electronic Components and Technology Conference (ECTC). IEEE, 612–616.'
  - '- <span id="page-13-16"></span>[37] Yang Hu, Xinhan Lin, Huizheng Wang, Zhen
    He, Xingmao Yu, Jiahao Zhang, Qize Yang, Zheng Xu, Sihan Guan, Jiahao Fang, et
    al. 2024. Wafer-Scale Computing: Advancements, Challenges, and Future Perspectives
    [Feature]. IEEE Circuits and Systems Magazine 24, 1 (2024), 52–81.'
  - '- <span id="page-13-30"></span>[38] Yang Hu, Mingcong Song, and Tao Li. 2017.
    Towards" full containerization" in containerized network function virtualization.
    In Proceedings of the Twenty-Second International Conference on Architectural
    Support for Programming Languages and Operating Systems. 467–481.'
  - '- <span id="page-13-22"></span>[39] Jiayi Huang, Pritam Majumder, Sungkeun Kim,
    Abdullah Muzahid, Ki Hwan Yum, and Eun Jung Kim. 2021. Communication algorithm-architecture
    codesign for distributed deep learning. In 2021 ACM/IEEE 48th Annual International
    Symposium on Computer Architecture (ISCA). IEEE, 181–194.'
  - '- <span id="page-13-38"></span>[40] Yanping Huang, Youlong Cheng, Ankur Bapna,
    Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui
    Wu, et al. 2019. Gpipe: Efficient training of giant neural networks using pipeline
    parallelism. Advances in neural information processing systems 32 (2019).'
  - '- <span id="page-13-21"></span>[41] Patrick Iff, Maciej Besta, Matheus Cavalcante,
    Tim Fischer, Luca Benini, and Torsten Hoefler. 2023. HexaMesh: Scaling to Hundreds
    of Chiplets with an Optimized Chiplet Arrangement. In 2023 60th ACM/IEEE Design
    Automation Conference (DAC). IEEE, 1–6.'
  - '- <span id="page-13-20"></span>[42] Patrick Iff, Maciej Besta, Matheus Cavalcante,
    Tim Fischer, Luca Benini, and Torsten Hoefler. 2023. Sparse Hamming Graph: A Customizable
    Network-on-Chip Topology. In 2023 60th ACM/IEEE Design Automation Conference (DAC).
    IEEE, 1–6.'
  - '- <span id="page-13-31"></span>[43] Subramanian S Iyer, S Jangam, and Boris Vaisband.
    2019. Silicon interconnect fabric: A versatile heterogeneous integration platform
    for AI systems. IBM Journal of Research and Development 63, 6 (2019), 5–1.'
  - '- <span id="page-13-39"></span>[44] Sam Ade Jacobs, Masahiro Tanaka, Chengming
    Zhang, Minjia Zhang, Shuaiwen Leon Song, Samyam Rajbhandari, and Yuxiong He. 2023.
    Deepspeed ulysses: System optimizations for enabling training of extreme long
    sequence transformer models. arXiv preprint arXiv:2309.14509 (2023).'
  - '- <span id="page-13-32"></span>[45] SivaChandra Jangam, Adeel Ahmed Bajwa, Kannan
    K Thankkappan, Premsagar Kittur, and Subramanian Srikantes Iyer. 2018. Electrical
    characterization of high performance fine pitch interconnects in silicon-interconnect
    fabric. In 2018 IEEE 68th Electronic Components and Technology Conference (ECTC).
    IEEE, 1283–1288.'
  - '- <span id="page-13-17"></span>[46] Natalie Enright Jerger, Ajaykumar Kannan,
    Zimo Li, and Gabriel H Loh. 2014. NoC architectures for silicon interposer systems:
    Why pay for more wires when you can get them (from your interposer) for free?.
    In 2014 47th Annual IEEE/ACM International Symposium on Microarchitecture. IEEE,
    458–470.'
  - '- <span id="page-13-42"></span>[47] Bentian Jiang, Jingsong Chen, Jinwei Liu,
    Lixin Liu, Fangzhou Wang, Xiaopeng Zhang, and Evangeline FY Young. 2020. CU. POKer:
    placing DNNs on wafer-scale'
  - AI accelerator with optimal kernel sizing. In Proceedings of the 39th International
    Conference on Computer-Aided Design. 1–9.
  - '- <span id="page-14-17"></span>[48] Yuho Jin, Eun Jung Kim, and Timothy Mark
    Pinkston. 2011. Communicationaware globally-coordinated on-chip networks. IEEE
    Transactions on Parallel and Distributed Systems 23, 2 (2011), 242–254.'
  - '- <span id="page-14-24"></span>[49] Tom Jose and Deepak Shankar. 2023. Performance
    modeling of a heterogeneous computing system based on the UCIe Interconnect Architecture.
    In 2023 IEEE Space Computing Conference (SCC). IEEE, 5–10.'
  - '- <span id="page-14-8"></span>[50] Norm Jouppi, George Kurian, Sheng Li, Peter
    Ma, Rahul Nagarajan, Lifeng Nai, Nishant Patil, Suvinay Subramanian, Andy Swing,
    Brian Towles, et al. 2023. Tpu v4: An optically reconfigurable supercomputer for
    machine learning with hardware support for embeddings. In Proceedings of the 50th
    Annual International Symposium on Computer Architecture. 1–14.'
  - '- <span id="page-14-37"></span>[51] John Kim, James Balfour, and William Dally.
    2007. Flattened butterfly topology for on-chip networks. In 40th Annual IEEE/ACM
    International Symposium on Microarchitecture (MICRO 2007). IEEE, 172–182.'
  - '- <span id="page-14-36"></span>[52] John Kim, Wiliam J Dally, Steve Scott, and
    Dennis Abts. 2008. Technology-driven, highly-scalable dragonfly topology. ACM
    SIGARCH Computer Architecture News 36, 3 (2008), 77–88.'
  - '- <span id="page-14-34"></span>[53] Christophe Kopp, Stephane Bernabe, Badhise
    Ben Bakir, Jean-Marc Fedeli, Regis Orobtchouk, Franz Schrank, Henri Porte, Lars
    Zimmermann, and Tolga Tekin. 2010. Silicon photonic circuits: on-CMOS integration,
    fiber optical coupling, and packaging. IEEE Journal of selected topics in quantum
    electronics 17, 3 (2010), 498–509.'
  - '- <span id="page-14-35"></span>[54] Richard E Korf. 1985. Depth-first iterative-deepening:
    An optimal admissible tree search. Artificial intelligence 27, 1 (1985), 97–109.'
  - '- <span id="page-14-43"></span>[55] Kartik Lakhotia, Maciej Besta, Laura Monroe,
    Kelly Isham, Patrick Iff, Torsten Hoefler, and Fabrizio Petrini. 2022. PolarFly:
    a cost-effective and flexible lowdiameter topology. In SC22: International Conference
    for High Performance Computing, Networking, Storage and Analysis. IEEE, 1–15.'
  - '- <span id="page-14-14"></span>[56] Sabuj Laskar, Pranati Majhi, Sungkeun Kim,
    Farabi Mahmud, Abdullah Muzahid, and Eun Jung Kim. 2024. Enhancing Collective
    Communication in MCM Accelerators for Deep Learning Training. In 2024 IEEE International
    Symposium on High-Performance Computer Architecture (HPCA). IEEE, 1–16.'
  - '- <span id="page-14-18"></span>[57] Gary Lauterbach. 2021. The path to successful
    wafer-scale integration: The cerebras story. IEEE Micro 41, 6 (2021), 52–57.'
  - '- <span id="page-14-19"></span>[58] Leighton and Leiserson. 1985. Wafer-scale
    integration of systolic arrays. IEEE Transactions on computers 100, 5 (1985),
    448–461.'
  - '- <span id="page-14-38"></span>[59] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong
    Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng
    Chen. 2020. Gshard: Scaling giant models with conditional computation and automatic
    sharding. arXiv preprint arXiv:2006.16668 (2020).'
  - '- <span id="page-14-0"></span>[60] Chao Li, Yang Hu, Ruijin Zhou, Ming Liu, Longjun
    Liu, Jingling Yuan, and Tao Li. 2013. Enabling datacenter servers to scale out
    economically and sustainably. In Proceedings of the 46th annual IEEE/ACM international
    symposium on microarchitecture. 322–333.'
  - '- <span id="page-14-1"></span>[61] Yuang Li, Yu Wu, Jinyu Li, and Shujie Liu.
    2023. Prompting large language models for zero-shot domain adaptation in speech
    recognition. In 2023 IEEE Automatic Speech Recognition and Understanding Workshop
    (ASRU). IEEE, 1–8.'
  - '- <span id="page-14-10"></span>[62] Sean Lie. 2022. Cerebras architecture deep
    dive: First look inside the hw/sw codesign for deep learning: Cerebras systems.
    In 2022 IEEE Hot Chips 34 Symposium (HCS). IEEE Computer Society, 1–34.'
  - '- <span id="page-14-22"></span>[63] Sean Lie. 2023. Cerebras architecture deep
    dive: First look inside the hardware/software co-design for deep learning. IEEE
    Micro 43, 3 (2023), 18–30.'
  - '- <span id="page-14-2"></span>[64] Xiao Lin, Shuzhou Sun, Wei Huang, Bin Sheng,
    Ping Li, and David Dagan Feng. 2021. EAPT: efficient attention pyramid transformer
    for image processing. IEEE Transactions on Multimedia 25 (2021), 50–61.'
  - '- <span id="page-14-39"></span>[65] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang,
    Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan,
    et al. 2024. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437 (2024).'
  - '- <span id="page-14-23"></span>[66] Hatem Ltaief, Yuxi Hong, Leighton Wilson,
    Mathias Jacquelin, Matteo Ravasi, and David Elliot Keyes. 2023. Scaling the "memory
    wall" for multi-dimensional seismic processing with algebraic compression on cerebras
    cs-2 systems. In Proceedings of the International Conference for High Performance
    Computing, Networking, Storage and Analysis. 1–12.'
  - '- <span id="page-14-15"></span>[67] Piotr Luczynski, Lukas Gianinazzi, Patrick
    Iff, Leighton Wilson, Daniele De Sensi, and Torsten Hoefler. 2024. Near-optimal
    wafer-scale reduce. In Proceedings of the 33rd International Symposium on High-Performance
    Parallel and Distributed Computing. 334–347.'
  - '- <span id="page-14-42"></span>[68] Canhui Luo, Zhouxing Su, and Zhipeng Lü.
    2023. MS-CLS: An Effective Partitioning and Placement Metaheuristic for Wafer-Scale
    Physics Modeling. IEEE Transactions on Emerging Topics in Computational Intelligence
    (2023).'
  - '- <span id="page-14-41"></span>[69] Marcelo Orenes-Vera, Esin Tureci, Margaret
    Martonosi, and David Wentzlaff. 2023. DCRA: A distributed chiplet-based reconfigurable
    architecture for irregular applications. arXiv preprint arXiv:2311.15443 (2023).'
  - '- <span id="page-14-27"></span>[70] Saptadeep Pal and Puneet Gupta. 2020. Pathfinding
    for 2.5 D interconnect technologies. In Proceedings of the Workshop on System-Level
    Interconnect: Problems and Pathfinding Workshop. 1–8.'
  - '- <span id="page-14-40"></span>[71] Saptadeep Pal, Jingyang Liu, Irina Alam,
    Nicholas Cebry, Haris Suhail, Shi Bu, Subramanian S Iyer, Sudhakar Pamarti, Rakesh
    Kumar, and Puneet Gupta. 2021. Designing a 2048-chiplet, 14336-core waferscale
    processor. In 2021 58th ACM/IEEE Design Automation Conference (DAC). IEEE, 1183–1188.'
  - '- <span id="page-14-11"></span>[72] Saptadeep Pal, Daniel Petrisko, Matthew Tomei,
    Puneet Gupta, Subramanian S Iyer, and Rakesh Kumar. 2019. Architecting waferscale
    processors-a GPU case study. In 2019 IEEE International Symposium on High Performance
    Computer Architecture (HPCA). IEEE, 250–263.'
  - '- <span id="page-14-12"></span>[73] Sunyoung Park. 2021. High Bandwidth Interposer
    Switch (HBI-S) Topology in Modular System on Chip. Ph. D. Dissertation.'
  - '- <span id="page-14-3"></span>[74] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit,
    Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. 2018. Image transformer.
    In International conference on machine learning. PMLR, 4055–4064.'
  - '- <span id="page-14-4"></span>[75] Yubin Qin, Yang Wang, Dazheng Deng, Zhiren
    Zhao, Xiaolong Yang, Leibo Liu, Shaojun Wei, Yang Hu, and Shouyi Yin. 2023. Fact:
    Ffn-attention co-optimized transformer architecture with eager correlation prediction.
    In Proceedings of the 50th Annual International Symposium on Computer Architecture.
    1–14.'
  - '- <span id="page-14-7"></span>[76] Colin Raffel, Noam Shazeer, Adam Roberts,
    Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J
    Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text
    transformer. Journal of machine learning research 21, 140 (2020), 1–67.'
  - '- <span id="page-14-32"></span>[77] Saeed Rashidi, Srinivas Sridharan, Sudarshan
    Srinivasan, and Tushar Krishna. 2020. Astra-sim: Enabling sw/hw co-design exploration
    for distributed dl training platforms. In 2020 IEEE International Symposium on
    Performance Analysis of Systems and Software (ISPASS). IEEE, 81–92.'
  - '- <span id="page-14-16"></span>[78] Saeed Rashidi, William Won, Sudarshan Srinivasan,
    Puneet Gupta, and Tushar Krishna. 2024. FRED: Flexible REduction-Distribution
    Interconnect and Communication Implementation for Wafer-Scale Distributed Training
    of DNN Models. arXiv preprint arXiv:2406.19580 (2024).'
  - '- <span id="page-14-30"></span>[79] Peter Sanders, Jochen Speck, and Jesper Larsson
    Träff. 2009. Two-tree algorithms for full bandwidth broadcast, reduction and scan.
    Parallel Comput. 35, 12 (2009), 581–594.'
  - '- <span id="page-14-21"></span>[80] Johannes Schemmel, Daniel Brüderle, Andreas
    Grübl, Matthias Hock, Karlheinz Meier, and Sebastian Millner. 2010. A wafer-scale
    neuromorphic hardware system for large-scale neural modeling. In 2010 IEEE International
    Symposium on Circuits and Systems (ISCAS). IEEE, 1947–1950.'
  - '- <span id="page-14-20"></span>[81] Tom Schram, Surajit Sutar, Iuliana Radu,
    and Inge Asselberghs. 2022. Challenges of wafer-scale integration of 2D semiconductors
    for high-performance transistor circuits. Advanced Materials 34, 48 (2022), 2109796.'
  - '- <span id="page-14-28"></span>[82] Omer S Sella, Andrew W Moore, and Noa Zilberman.
    2018. FEC killed the cutthrough switch. In Proceedings of the 2018 Workshop on
    Networking for Emerging Applications and Technologies. 15–20.'
  - '- <span id="page-14-9"></span>[83] Yakun Sophia Shao, Jason Clemons, Rangharajan
    Venkatesan, Brian Zimmer, Matthew Fojtik, Nan Jiang, Ben Keller, Alicia Klinefelter,
    Nathaniel Pinckney, Priyanka Raina, et al. 2019. Simba: Scaling deep-learning
    inference with multichip-module-based architecture. In Proceedings of the 52nd
    Annual IEEE/ACM International Symposium on Microarchitecture. 14–27.'
  - '- <span id="page-14-29"></span>[84] Debendra Das Sharma, Gerald Pasdast, Zhiguo
    Qian, and Kemal Aygun. 2022. Universal chiplet interconnect express (UCIe): An
    open industry standard for innovations with chiplets at package level. IEEE Transactions
    on Components, Packaging and Manufacturing Technology 12, 9 (2022), 1423–1431.'
  - '- <span id="page-14-33"></span>[85] Mingcong Song, Xinru Tang, Fengfan Hou, Jing
    Li, Wei Wei, Yipeng Ma, Runqiu Xiao, Hongjie Si, Dingcheng Jiang, Shouyi Yin,
    et al. 2024. Tackling the dynamicity in a production llm serving system with sota
    optimizations via hybrid prefill/decode/verify scheduling on efficient meta-kernels.
    arXiv preprint arXiv:2412.18106 (2024).'
  - '- <span id="page-14-5"></span>[86] Mingcong Song, Jiechen Zhao, Yang Hu, Jiaqi
    Zhang, and Tao Li. 2018. Prediction based execution on deep neural networks. In
    2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA).
    IEEE, 752–763.'
  - '- <span id="page-14-6"></span>[87] Mingcong Song, Kan Zhong, Jiaqi Zhang, Yang
    Hu, Duo Liu, Weigong Zhang, Jing Wang, and Tao Li. 2018. In-situ ai: Towards autonomous
    and incremental deep learning for iot systems. In 2018 IEEE International Symposium
    on High Performance Computer Architecture (HPCA). IEEE, 92–103.'
  - '- <span id="page-14-25"></span>[88] Dylan Stow, Itir Akgun, and Yuan Xie. 2019.
    Investigation of cost-optimal network-on-chip for passive and active interposer
    systems. In 2019 ACM/IEEE International Workshop on System Level Interconnect
    Prediction (SLIP). IEEE, 1–8.'
  - '- <span id="page-14-26"></span>[89] Dylan Stow, Yuan Xie, Taniya Siddiqua, and
    Gabriel H Loh. 2017. Cost-effective design of scalable high-performance systems
    using active and passive interposers. In 2017 IEEE/ACM International Conference
    on Computer-Aided Design (ICCAD). IEEE, 728–735.'
  - '- <span id="page-14-31"></span>[90] Emil Talpes, Debjit Das Sarma, Doug Williams,
    Sahil Arora, Thomas Kunjan, Benjamin Floering, Ankit Jalote, Christopher Hsiong,
    Chandrasekhar Poorna, Vaidehi Samant, et al. 2023. The microarchitecture of dojo,
    tesla''s exa-scale computer. IEEE Micro 43, 3 (2023), 31–39.'
  - '- <span id="page-14-13"></span>[91] Emil Talpes, Douglas Williams, and Debjit
    Das Sarma. 2022. Dojo: The microarchitecture of tesla''s exa-scale computer. In
    2022 IEEE Hot Chips 34 Symposium (HCS). IEEE Computer Society, 1–28.'
  - '- <span id="page-15-1"></span><span id="page-15-0"></span>[92] Zhanhong Tan,
    Hongyu Cai, Runpei Dong, and Kaisheng Ma. 2021. Nn-baton: Dnn workload orchestration
    and chiplet granularity exploration for multichip accelerators. In 2021 ACM/IEEE
    48th Annual International Symposium on Computer Architecture (ISCA). IEEE, 1013–1026.'
  - '- <span id="page-15-10"></span>[93] Hugo Touvron, Thibaut Lavril, Gautier Izacard,
    Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman
    Goyal, Eric Hambro, Faisal Azhar, et al. 2023. LLaMA: open and efficient foundation
    language models. arXiv. arXiv preprint arXiv:2302.13971 (2023).'
  - '- <span id="page-15-7"></span>[94] Yuichiro Ueno and Rio Yokota. 2019. Exhaustive
    study of hierarchical allreduce patterns for large messages between GPUs. In 2019
    19th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID).
    IEEE, 430–439.'
  - '- <span id="page-15-15"></span>[95] Yuichiro Ueno and Rio Yokota. 2019. Exhaustive
    study of hierarchical allreduce patterns for large messages between GPUs. In 2019
    19th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID).
    IEEE, 430–439.'
  - '- <span id="page-15-13"></span>[96] Mary K Vernon and Udi Manber. 1988. Distributed
    round-robin and first-come first-serve protocols and their applications to multiprocessor
    bus arbitration. ACM SIGARCH Computer Architecture News 16, 2 (1988), 269–279.'
  - '- <span id="page-15-12"></span>[97] Mark Wade, Erik Anderson, Shahab Ardalan,
    Pavan Bhargava, Sidney Buchbinder, Michael L Davenport, John Fini, Haiwei Lu,
    Chen Li, Roy Meade, et al. 2020. TeraPHY: a chiplet technology for low-power,
    high-bandwidth in-package optical I/O. IEEE Micro 40, 2 (2020), 63–71.'
  - '- <span id="page-15-2"></span>[98] Zhiquan Wan, Zhipeng Cao, Shunbin Li, Peijie
    Li, Qingwen Deng, Weihao Wang, Kun Zhang, Guandong Liu, Ruyun Zhang, and Qinrang
    Liu. 2024. Architectural Exploration for Waferscale Switching System. IEEE Transactions
    on Very Large Scale Integration (VLSI) Systems (2024).'
  - '- <span id="page-15-5"></span>[99] Ronald Williams and Ogden Marsh. 1993. Future
    WSI technology: stacked monolithic WSI. IEEE transactions on components, hybrids,
    and manufacturing technology 16, 7 (1993), 610–614.'
  - '- <span id="page-15-3"></span>[100] William Won, Midhilesh Elavazhagan, Sudarshan
    Srinivasan, Ajaya Durg, Samvit Kaul, Swati Gupta, and Tushar Krishna. 2023. TACOS:
    Topology-Aware Collective Algorithm Synthesizer for Distributed Machine Learning.
    arXiv'
  - preprint arXiv:2304.05301 (2023).
  - '- <span id="page-15-9"></span>[101] William Won, Taekyung Heo, Saeed Rashidi,
    Srinivas Sridharan, Sudarshan Srinivasan, and Tushar Krishna. 2023. Astra-sim2.
    0: Modeling hierarchical networks and disaggregated systems for large-model training
    at scale. In 2023 IEEE International Symposium on Performance Analysis of Systems
    and Software (ISPASS). IEEE, 283–294.'
  - '- <span id="page-15-4"></span>[102] William Won, Saeed Rashidi, Sudarshan Srinivasan,
    and Tushar Krishna. 2024. LIBRA: Enabling Workload-Aware Multi-Dimensional Network
    Topology Optimization for Distributed Training of Large AI Models. In 2024 IEEE
    International Symposium on Performance Analysis of Systems and Software (ISPASS).
    IEEE, 205–216.'
  - '- <span id="page-15-17"></span>[103] Jiantao Wu, Wan-Ping Lee, Alistair Ward,
    Jerilyn A Walker, Miriam K Konkel, Mark A Batzer, and Gabor T Marth. 2014. Tangram:
    a comprehensive toolbox for mobile element insertion detection. BMC genomics 15
    (2014), 1–15.'
  - '- <span id="page-15-6"></span>[104] FENG Yinxiao and MA Kaisheng. 2022. Chiplet
    actuary: A quantitative cost model and multi-chiplet architecture exploration
    [C]. In The 59th ACM/IEEE Design Automation Conference, San Francisco, USA. 121–126.'
  - '- <span id="page-15-11"></span>[105] Fanlong Zeng, Wensheng Gan, Yongheng Wang,
    and S Yu Philip. 2023. Distributed training of large language models. In 2023
    IEEE 29th International Conference on Parallel and Distributed Systems (ICPADS).
    IEEE, 840–847.'
  - '- <span id="page-15-16"></span>[106] Hengrui Zhang, August Ning, Rohan Baskar
    Prabhakar, and David Wentzlaff. 2024. Llmcompass: Enabling efficient hardware
    design for large language model inference. In 2024 ACM/IEEE 51st Annual International
    Symposium on Computer Architecture (ISCA). IEEE, 1080–1096.'
  - '- <span id="page-15-14"></span>[107] Susan Zhang, Stephen Roller, Naman Goyal,
    Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li,
    Xi Victoria Lin, et al. 2023. Opt: Open pre-trained transformer language models,
    2022. URL https://arxiv. org/abs/2205.01068 3 (2023), 19–0.'
  - '- <span id="page-15-8"></span>[108] Jingchen Zhu, Chenhao Xue, Yiqi Chen, Zhao
    Wang, and Guangyu Sun. 2024. Theseus: Exploring Efficient Wafer-Scale Chip Design
    for Large Language Models. arXiv preprint arXiv:2407.02079 (2024).'
- id: forest_access_aware_gpu_uvm_management_mao_lin_https_orcid_org_0000_0002_1460_0766_university_of_california_merced_merced_california_usa_mlin59_ucmerced_edu
  title: 'Forest: Access-aware GPU UVM Management'
  abstract: With GPU unified virtual memory (UVM), CPU and GPU can share a flat virtual
    address space. UVM enables the GPUs to utilize the larger CPU system memory as
    an expanded memory space. However, UVM's on-demand page migration is accompanied
    by expensive page fault handling overhead. To mitigate such overhead, tree-based
    neighboring prefetcher (TBNp) has been used by GPUs. TBNp effectively reduces
    page faults by exploiting locality at multiple levels. However, we observe its
    access-pattern oblivious design leads to excessive page thrashing and unnecessary
    migrations. In this paper, we tackle the inefficiencies with a novel access-aware
    UVM management, Forest. Forest uses a software-hardware codesign to configure
    the optimal tree prefetchers at runtime based on each data object's access patterns.
    With the heterogeneous treebased prefetching, Forest provides 1.86× and 1.39×
    speedups over the baseline TBNp and state-of-the-art optimization solutions, respectively.
    Forest also shows a 1.51× speedup for real-world deep learning models, including
    CNNs and Transformers.
  keywords: Unified Virtual Memory, Prefetching, GPU, Memory Management
  document: '![](_page_0_Picture_0.jpeg)


    # Forest: Access-aware GPU UVM Management


    [Mao Lin](https://orcid.org/0000-0002-1460-0766) University of California, Merced
    Merced, California, USA mlin59@ucmerced.edu


    [Guilherme Cox](https://orcid.org/0000-0001-8292-4554) NVIDIA Santa Clara, California,
    USA gcox@nvidia.com


    ### Abstract


    With GPU unified virtual memory (UVM), CPU and GPU can share a flat virtual address
    space. UVM enables the GPUs to utilize the larger CPU system memory as an expanded
    memory space. However, UVM''s on-demand page migration is accompanied by expensive
    page fault handling overhead. To mitigate such overhead, tree-based neighboring
    prefetcher (TBNp) has been used by GPUs. TBNp effectively reduces page faults
    by exploiting locality at multiple levels. However, we observe its access-pattern
    oblivious design leads to excessive page thrashing and unnecessary migrations.
    In this paper, we tackle the inefficiencies with a novel access-aware UVM management,
    Forest. Forest uses a software-hardware codesign to configure the optimal tree
    prefetchers at runtime based on each data object''s access patterns. With the
    heterogeneous treebased prefetching, Forest provides 1.86× and 1.39× speedups
    over the baseline TBNp and state-of-the-art optimization solutions, respectively.
    Forest also shows a 1.51× speedup for real-world deep learning models, including
    CNNs and Transformers.


    # CCS Concepts


    • Computer systems organization → Parallel architectures; • General and reference
    → Performance; Design.


    # Keywords


    Unified Virtual Memory, Prefetching, GPU, Memory Management


    ### ACM Reference Format:


    Mao Lin, Yuan Feng, Guilherme Cox, and Hyeran Jeon. 2025. Forest: Accessaware
    GPU UVM Management. In Proceedings of the 52nd Annual International Symposium
    on Computer Architecture (ISCA ''25), June 21–25, 2025, Tokyo, Japan. ACM, New
    York, NY, USA, [16](#page-15-0) pages. [https://doi.org/10.1145/](https://doi.org/10.1145/3695053.3731047)
    [3695053.3731047](https://doi.org/10.1145/3695053.3731047)


    # 1 Introduction


    GPU unified virtual memory (UVM) has gained popularity thanks to its hassle-less
    memory expansion capability. With UVM, data objects can be placed anywhere in
    either CPU or GPU memories and can be fetched to the GPU memory when GPU accesses
    them


    [This work is licensed under a Creative Commons Attribution 4.0 International
    License.](https://creativecommons.org/licenses/by/4.0) ISCA ''25, Tokyo, Japan
    © 2025 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-1261-6/25/06
    <https://doi.org/10.1145/3695053.3731047>


    [Yuan Feng](https://orcid.org/0009-0004-2451-9568) University of California, Merced
    Merced, California, USA yfeng44@ucmerced.edu


    [Hyeran Jeon](https://orcid.org/0000-0002-1767-8198) University of California,
    Merced Merced, California, USA hjeon7@ucmerced.edu


    <span id="page-0-0"></span>![](_page_0_Figure_18.jpeg)


    Figure 1: TBNp vs. Forest: TBNp uses homogeneous tree prefetchers for all memory
    blocks while Forest flexibly applies heterogeneous trees to support diverse access
    patterns.


    frequently [\[4,](#page-13-0) [16\]](#page-13-1). In GPU computing, the memory
    capacity is one of the main performance limiters [\[22,](#page-14-0) [23,](#page-14-1)
    [28,](#page-14-2) [47,](#page-14-3) [53\]](#page-14-4). Given the increasing size
    of emerging workloads, UVM is one of the most favorable technologies because it
    allows deploying such big data workloads on GPUs. Alongside the rapid evolution
    of GPU architectures, UVM has been one of the core mechanisms to effectively expand
    GPU memory capacity through interplay with emerging technologies such as cache-coherent
    and access counter-based data sharing [\[17,](#page-13-2) [18\]](#page-13-3).


    To better utilize the limited GPU memory space with proactive page migrations,
    several solutions have been proposed, such as prefetching and pre-eviction methods
    [\[11,](#page-13-4) [26,](#page-14-5) [27,](#page-14-6) [50\]](#page-14-7), address
    translation optimization [\[41](#page-14-8)[–43\]](#page-14-9), page thrashing
    reduction [\[44,](#page-14-10) [45\]](#page-14-11) and multi-path data transfer
    [\[12\]](#page-13-5). One industry solution used by several generations of NVIDIA
    GPUs is tree-based neighboring prefetcher (TBNp) [\[2,](#page-13-6) [26\]](#page-14-5).
    It decides the target pages to prefetch based on access locality and controls
    the prefetch aggressiveness with treebased prediction. TBNp partitions memory
    space into 2MB virtual address blocks (VABlocks) and manages each VABlock with
    a full binary tree structure. A 64KB leaf node is the minimum migration unit.


    Though the TBNp effectively exploits access locality, we observe two limitations:
    1) one tree configuration is used for all applications and all data objects regardless
    of the memory access patterns and 2) the UVM driver decides page eviction based
    on the page fault events, not by the actual page accesses. Both limitations stem
    from the driver-side UVM management. As the GPU driver cannot monitor the memory
    access patterns in the GPU device, TBNp often


    produces suboptimal migration decisions, which result in severe page thrashing
    and unnecessary migrations.


    To resolve the inefficiencies, we propose an access-aware GPU UVM management,
    namely Forest. Forest manages UVM in the unit of individual data objects and configures
    a prefetch tree that works best for each data object. Figure [1](#page-0-0) shows
    the conceptual differences between TBNp and Forest. TBNp manages UVM in fixed-size
    memory units with homogeneous prefetch trees (e.g., one tree manages a 2MB memory
    space with 64KB leaf nodes). Forest on the other hand manages UVM in the unit
    of individual data object''s size and configures one tree for each data object
    that works the best for the data object (i.e., heterogeneous prefetch trees).


    There are four challenges to managing the UVM with accessaware heterogeneous tree
    structures. First, the access pattern of individual data objects should be identified.
    The current UVM is managed purely by the driver located in the host CPU [\[2\]](#page-13-6).
    Therefore, the UVM can detect access patterns only in the granularity of page
    fault, which is too coarse-grained. The page access counters used by some GPUs
    only reflect page hotness, which can not be used for access pattern detection.
    To enable more finegrained and correct access pattern detection, Forest repurposes
    the existing hardware page access counters to record the page access sequence
    within each data object, with a lightweight hardware support. By leveraging the
    existing access counter fetch function, the UVM driver can detect access patterns
    of each object. Second, the prefetcher should be configured at runtime. The TBNp
    information is managed by the UVM driver. We extend the existing UVM driver to
    integrate two 1-bit metadata per non-leaf node, with which Forest dynamically
    merges the leaf nodes or isolates the TBNp tree. Third, optimal tree configuration
    should be identified. From rigorous analysis, we introduce four access patterns
    that significantly influence prefetch performance and propose the best-performing
    tree configuration for each pattern. Fourth, the access pattern detection should
    not cause performance overhead. To reduce the burden of runtime access pattern
    detection, we propose speculative Forest (SpecForest). The SpecForest uses compiler
    analysis and pattern recording to statically detect simple access patterns and
    reuse detected patterns for subsequent kernels without runtime pattern detection.
    For complex access patterns (e.g., indirect accesses), we propose a novel pattern
    similarity detection. At compile time, we group the data objects that use similar
    data indexing. At runtime, we assign the same tree prefetcher to all objects in
    the same group as soon as one data in the group finishes the pattern detection.


    Our evaluations under a 150% memory oversubscription show that Forest and SpecForest
    outperform the Baseline TBNp by 72% and 86%, respectively. Compared to state-of-the-art
    solutions [\[26,](#page-14-5) [27,](#page-14-6) [29\]](#page-14-12), Forest and
    SpecForest respectively achieve 28% and 39% speedups. Additionally, SpecForest
    delivers an average performance improvement of 51% across various real-world deep
    learning workloads, including CNN-based and Transformer-based models.


    Our contributions are as follows:


    • To the best of our knowledge, this is the first study that uncovers the intrinsic
    issue of unnecessary page migrations and thrashings of UVM TBNp.


    <span id="page-1-0"></span>![](_page_1_Figure_7.jpeg)


    Figure 2: UVM walkthrough for far-faults.


    - We propose Forest, an access-aware UVM management. Forest uses software-hardware
    co-design to fundamentally tackle the root cause of the inefficiencies of software-based
    UVM management. Forest introduces a novel heterogeneous prefetching strategy while
    existing studies mostly focused on migration threshold adjustment over the existing
    homogeneous tree prefetcher [\[25](#page-14-13)[–27,](#page-14-6) [29\]](#page-14-12).

    - We introduce compiler-assisted optimized access pattern detection, namely SpecForest,
    which further improves performance with early adoption of optimal prefetcher.

    - We demonstrate the effectiveness of Forest across an extensive number of workloads,
    including general-purpose GPU benchmarks and real-world deep learning (DL) applications
    (e.g., Transformers and CNNs).


    # 2 Background and Related Work


    # 2.1 GPU Unified Virtual Memory System


    UVM allows CPUs and GPUs to share a flat virtual memory system. With UVM, CPUs
    and GPUs can communicate via zero-copy pointer sharing and seamless data transfer
    without explicit memory copies. If a data object is allocated by GPU applications
    (e.g., through cudaMallocManaged), the data pages are copied to the GPU memory
    when the data is accessed by the GPU kernel. The UVM driver handles such implicit
    memory copy through far-fault handling. Far-faults occur when pages are not physically
    present in the GPU device memory [\[2,](#page-13-6) [3,](#page-13-7) [25](#page-14-13)[–27\]](#page-14-6).
    The far-faults trigger on-demand data allocation and migration. When a GPU kernel
    executes a memory operation on the data if the GPU-side MMU (GMMU) finds the data
    page is not in the GPU memory, a far-fault is triggered. Then, the UVM driver
    copies the page from CPU to GPU. Figure [2](#page-1-0) illustrates the end-to-end
    steps of the far-fault handling. Despite the advantages of UVM, the seamless page
    sharing does not directly lead to a performance gain because the long far-fault
    handling latency is added to the critical path of memory operations, as shown
    in the figure [\[25](#page-14-13)[–27,](#page-14-6) [38,](#page-14-14) [46\]](#page-14-15).


    With UVM, the limited GPU memory space can be effectively expanded to the CPU
    system memory and can run an application with a larger memory footprint than the
    GPU memory (i.e., memory oversubscription). To support memory oversubscription,
    UVM also handles runtime page eviction. If GPU doesn''t have space to fetch a


    <span id="page-2-0"></span>![](_page_2_Figure_1.jpeg)


    Figure 3: TBNp operations. For simplicity, this example denotes 512 KB of consecutive
    memory, while each TBN tree represents a 2 MB VABlock in the original TBNp.


    new page, the least recently used (LRU) 2MB virtual address block (VABlock) is
    evicted from GPU to CPU. The UVM driver identifies the LRU pages by monitoring
    the sequence of far-faults. Thus, the pages that are least recently copied to
    the GPU are selected to be evicted [\[2\]](#page-13-6). Because evictions do not
    consider GPU-side access recency, hot pages are often incorrectly victimized [\[27\]](#page-14-6).


    #### <span id="page-2-4"></span>2.2 Tree-based Neighboring Prefetcher (TBNp)


    To mitigate the overhead associated with UVM, NVIDIA GPUs employ TBNp, which uses
    a full binary tree structure [\[2,](#page-13-6) [25,](#page-14-13) [26\]](#page-14-5).
    Each 2MB VABlock is managed with a five-level full binary tree having 32 64KB
    leaf nodes. Each non-leaf node holds two metadata, namely Ntotal and Nmigrated,
    each indicating the node''s total and migrated child node size. TBNp manages migrations
    in the unit of a leaf node. Therefore, once a page encounters a far-fault, the
    entire 64KB in the associated leaf node is migrated. TBNp also does proactive
    prefetching leveraging the tree structure. In each 2MB tree, if more than 50%
    of the leaf nodes are migrated to the GPU memory, the TBNp proactively prefetches
    the remaining leaf nodes to the GPU by assuming a high locality. With these two
    prefetch algorithms, multiple on-demand far-faults can be skipped with one far-fault
    handling.


    Figure [3](#page-2-0) illustrates the operations of the TBNp. Each box attached
    to the leaf node is a 64KB basic block. ○1 A far-fault occurs on the leftmost
    yellow 4KB page within 0 0 . Then, the GPU driver migrates the page to the GPU.
    TBNp then locates the leaf node holding the page. ○2 TBNp prefetches the remaining
    pages (blue) in the same leaf node to the GPU, as the TBNp''s migration unit is
    64KB. This prefetching process is applied to subsequent faults in 1 0 (○3 and
    ○4 ) and 2 0 (○5 and ○6 ). After migrating these three 64KB blocks, the prefetcher
    notices that over 50% of the pages within node 0 2 are in the GPU memory (i.e.,
    Nmigrated is greater than 50% of Ntotal in 0 2 ). ○7 Then, the prefetcher migrates
    all remaining pages (green) of 0 2 to the GPU memory. ○8 The next far-fault occurs
    in 5 0 and ○9 a corresponding 64KB region is also migrated. ○10 UVM observes that
    over 50% of the pages in node 0 3 are in the GPU memory and the prefetcher migrates
    all remaining pages in 0 3 to the GPU memory (i.e., Nmigrated is greater than
    50% of Ntotal in 0 3 ).


    # 2.3 Other UVM Prefetching Solutions


    Besides TBNp, several studies have aimed to reduce the far-fault overhead. Choi
    et al. [\[12\]](#page-13-5) introduced a multi-path migration strategy in multi-GPU
    environments that bypasses eviction during


    <span id="page-2-3"></span><span id="page-2-2"></span><span id="page-2-1"></span>![](_page_2_Figure_10.jpeg)


    Figure 4: Execution time with diverse tree configurations (lower is better).


    critical far-fault handling; Kim et al. [\[38\]](#page-14-14) used a batched far-fault
    handling, thus amortizing the time spent on GPU runtime fault handling. Other
    studies developed smarter algorithms for data prefetching or eviction under memory
    oversubscription, such as DeepUM [\[35\]](#page-14-16), which utilized the repetitive
    nature of kernel executions and memory access patterns in DL training to prefetch
    data based on historical behaviors. Several studies tackled the limitations of
    TBNp. Ganguly et al. proposed a tree-based eviction and accesscounter-based dynamic
    memory management strategies in their series of papers [\[25–](#page-14-13)[27\]](#page-14-6).
    Early-Adaptor [\[29\]](#page-14-12) designed a model that automatically adjusts
    prefetching aggressiveness based on page fault history. These prior works overlooked
    the fundamental limitations of software-only UVM design, which Forest aims to
    address via software-hardware co-design.


    ### 3 Motivation


    # 3.1 One Configuration Doesn''t Fit All


    To understand the effectiveness of the TBNp for improving performance, we measured
    the execution time while varying the tree configurations. We used a cycle-level
    GPU architecture simulator that implements TBNp [\[48\]](#page-14-17). The tested
    applications are selected from various GPGPU benchmark suites. More details are
    in Table [1](#page-8-0) and Table [2.](#page-10-0) Figure [4](#page-2-1) shows
    the normalized execution time when using different tree configurations. Both figures
    use the conventional TBNp size as the baseline, where each tree prefetcher manages
    a 2MB memory space with 64KB leaf nodes. From these results, we couldn''t find
    one optimal configuration that performs the best for all applications. In Figure
    [4a,](#page-2-2) the first eight applications show better performance with larger
    trees, while the remaining applications do not. RA and SSSP show better performance
    with smaller trees, whereas CN, NW and BFS perform better with medium (1MB) trees.
    Likewise, in Figure [4b,](#page-2-3) the first eight applications perform better
    with larger leaf nodes, while SSSP and NW show better performance with smaller
    leaf nodes. We couldn''t find a clear trend for CN, RA and BICG. Even in the first
    eight applications, some applications


    ISCA ''25, June 21–25, 2025, Tokyo, Japan Lin et al.


    <span id="page-3-3"></span><span id="page-3-0"></span>![](_page_3_Figure_1.jpeg)


    Figure 5: Memory access patterns of **BICG** and **RA**.


    (STEN and HS) perform better with a 128KB leaf node, which is not the largest
    leaf node size. This result raises strong doubt about the effectiveness of homogeneous
    TBNp tree configuration. It is noteworthy that none of the 15 applications perform
    the best with the baseline configuration. Given that the baseline tree configuration
    is used as the default setting in the commercialized GPUs as well as academic
    studies, we believe that the UVM setting should be fundamentally revisited.


    We know the application''s memory access pattern. Can we find the best tree configuration
    for the application?: No. In more than half of our tested applications, we observed
    that there is no such tree configuration that is best for all kernels executed
    by the application. Even in each kernel, the access patterns are significantly
    different among data objects. Furthermore, the same data objects are often accessed
    differently in different kernels.


    Figure [5](#page-3-0) shows such a diverse example. Each graph is partitioned
    vertically for multiple kernels (i.e., in BICG, one data object is shared by two
    kernels) and horizontally for multiple objects (i.e., RA has three data objects
    in one kernel). For each kernel and data object, the graphs show the page addresses
    (Y-axis) accessed throughout the execution time (X-axis). In BICG, the graph shows
    how the two kernels access the same data differently. In kernel 1, data has a
    clear linear access pattern. However, in kernel 2, almost all the pages are accessed
    from the beginning without a certain pattern. This is because the data is used
    as output and input in kernels 1 and 2, respectively. Kernel 1 generates the data
    and shares the pointer as an input with kernel 2. Due to the limited coherence
    and synchronization supports, GPU applications typically have many readers and
    a single writer for each data. Each warp executes in an in-order fashion, which
    is naturally synchronized. Thus, the output data tends to be accessed linearly,
    as shown in kernel 1. On the other hand, input data doesn''t need to be synchronously
    accessed. If the data is read by multiple warps in multiple SMs, the access pattern
    can be easily scattered, as shown in kernel 2. In RA, in one kernel, the top two
    data show similarly scattered access patterns while the bottom data has a clear
    linear pattern. Thus, we cannot summarize each application''s access pattern with
    one pattern.


    For the applications having diverse access patterns across kernels and data objects,
    one prefetch configuration cannot produce the best performance. Some studies [\[10,](#page-13-8)
    [27,](#page-14-6) [29\]](#page-14-12) tackled such access pattern diversity by
    adapting migration thresholds. While these solutions can dynamically control the
    migration aggressiveness, they have limited impact because the thresholds are
    not tailored to individual data and the aggressiveness is bounded to the default


    <span id="page-3-1"></span>![](_page_3_Figure_7.jpeg)


    <span id="page-3-2"></span>Figure 6: Side-effects of TBNp


    TBNp tree size. We propose a more fundamental solution, which dynamically configures
    the TBNp for individual data objects.


    # 3.2 TBNp, A Hidden Source of Memory Oversubscription.


    We also examined the effectiveness of TBNp on memory utilization. Two important
    performance metrics of memory prefetchers are accuracy and timeliness. If data
    are incorrectly prefetched or prefetched too early or late, the prefetched data
    will end up unused while unnecessarily occupying memory space. We examined if
    all the migrated data are accessed by the GPU program before being evicted. We
    define unnecessary migrations as the number of migrations that end up having zero
    accesses until eviction.


    Figure [6a](#page-3-1) shows the six applications that encountered a significant
    number of unnecessary migrations; from 5% to 48% of the total migrated pages were
    unnecessarily migrated. We also measured the unnecessary migrations in 64KB basic
    block (leaf node) units. The unnecessary block migrations occur when blocks are
    prefetched alongside their neighboring blocks (e.g., green blocks in Figure [3\)](#page-2-0)
    but remain unaccessed until they are evicted. BICG and SN show a similar ratios
    in block and page units, which indicates that most of the needless migrations
    stem from the unnecessary blocks within the large tree. In other words, many of
    the blocks migrated by the proactive prefetch are not accessed, while the blocks
    that are migrated due to far-fault are well utilized. Conversely, the other applications
    show a significantly higher percentage of unnecessary pages compared to blocks.
    This is because these applications sparsely access only a few pages within each
    migrated block. In other words, while most of the migrated blocks are accessed
    at least once, many pages within each block are never accessed. This is mainly
    caused by the large migration unit; 64KB minimum regardless the number of pages
    accessed within the block. In both cases, the uniform tree configuration is the
    root cause of the unnecessary migrations.


    #### <span id="page-3-4"></span>3.3 Driver-driven Page Eviction and Thrashing


    To evict pages under oversubscription, UVM driver maintains an LRU list. The LRU
    list is updated upon page faults. Thus, the LRU list does not accurately reflect
    the actual access recency on device. Note that even under access-counter-based
    UVM migration, the eviction policy still uses the far-fault-based LRU list. To
    understand the impact of this driver-driven eviction, we measured page thrashing,
    which is defined as the number of pages that are migrated again after eviction.
    Figure [6b](#page-3-2) shows the results when applications use 150% larger memory
    than the GPU memory. In the benchmarks with


    <span id="page-4-0"></span>![](_page_4_Figure_1.jpeg)


    Figure 7: Architectural design of Forest (Newly added or modified components are
    highlighted with red outlines).


    linear access patterns with limited data reuse (e.g., 2DC, 3DC, and AV), there
    was no page thrashing because the migration sequence and LRU order match naturally.
    For the remaining benchmarks, from 9K to over 100K pages were remigrated after
    eviction. For some applications, the total thrashed pages (counting repetitive
    thrashing separately) is worth over 5.7× of their memory footprint! Such severe
    page thrashing occurs due to three reasons: 1) insufficient GPU memory, 2) suboptimal
    prefetcher leading to unnecessary migrations, and 3) driver-driven LRU decision
    evicting data soon to be migrated again. From our experiments, the suboptimal
    prefetcher and driver-driven eviction contribute an average of 27% and 6% (up
    to 46% and 18%) of the thrashings, respectively. This motivates us to reconsider
    the eviction mechanism for better memory utilization.


    # 4 Forest: Access-aware GPU UVM Management


    To tackle the inefficiencies of the access-oblivious homogeneous TBNp, we propose
    an access-aware GPU UVM management, Forest. Forest dynamically detects access
    patterns of individual data objects and configures the prefetch tree for each
    data object.


    # 4.1 Overall Design


    Figure [7](#page-4-0) shows the overall architecture of Forest, which consists
    of three main components: an access time tracker (ATT), an access pattern detector
    (APD), and a prefetch engine (PE). ATT is a newly added on-device page access
    timing tracker, which is integrated in the GMMU. ATT uses the existing hardware
    page access counters to record the sequence of page accesses for each UVM-managed
    data object. APD is a new UVM driver module, which collects the page access timing
    information recorded by ATT once per the pre-defined profiling phase, determines
    the access pattern, and recommends the proper TBNp for each data object. PE in
    the existing UVM driver operates migrations and prefetches. We extend the PE to
    configure the recommended TBNp configuration and operate migrations and prefetches
    accordingly.


    # <span id="page-4-1"></span>4.2 Access Time Tracker (ATT)


    To understand the access patterns of individual data objects, we should be able
    to collect the page access recency. We could leverage existing hardware page access
    counter registers [\[27,](#page-14-6) [56\]](#page-14-18), which monitor the number
    of accesses per page. However, the existing hardware page access counters reflect
    each page''s access intensity rather than access recency. ATT repurposes the access
    counter registers to collect page access recency with the help of an object table,
    as illustrated in Figure [7.](#page-4-0)


    Each entry of the object table is associated with one data object managed by UVM
    and consists of a start and an end virtual page numbers (VPN), an access timer,
    a recency order, and a cease bit. Each entry is reserved when a data is allocated
    with a UVM malloc API (e.g., cudaMallocManaged) and is filled upon kernel invocation.


    The access timer maintains the total page access count of the data object. Upon
    a page access, the access timer value of the associated data is incremented by
    one and the updated value is written to the page''s access counter register. This
    way, the hardware page access counter registers can reflect the access order of
    each page within the data object. For example, suppose that a data object has
    a linear access pattern, so the pages N, N+1, and N+2 have their hardware access
    counter register values C, C+1, and C+2, respectively. This reflects that consecutive
    pages are accessed in subsequent timings. The access timer in ATT has the latest
    access counter register value, C+2. If another page is accessed, the access timer
    will be incremented to C+3 and the new page will have C+3 as its hardware page
    access counter register value.


    The existing page access counters are implemented with 32-bit registers and are
    updated by the GMMU at every local memory access during the TLB lookup [\[27,](#page-14-6)
    [56\]](#page-14-18). We leverage the existing counter architecture and operation,
    except for the value of the access counter registers. The access timer in the
    object table and the page access counter registers provide object-level access
    intensity and page-level spatial locality, which together can support more accurate
    migration decisions.


    The recency order is the LRU list of the objects in the object table, which will
    be used for page replacement (details in Section [4.5\)](#page-7-0). The cease
    bit is a 1-bit indicator that shows whether an access pattern has been detected
    by APD. APD detects the data object''s access pattern by using the access timing
    information collected by ATT (details in Section [4.3\)](#page-5-0). Once the
    APD identifies the access pattern, the cease bit is set to 1 and the ATT stops
    the monitoring.


    The access timer has 32 bits. With a 32-bit access timer, up to 4 billion accesses
    can be recorded, which is rarely overflown according to our experiments. However,
    if the access timer reaches the maximum value, the access timer as well as the
    page access counter registers associated with the data are halved. According to
    our analysis of GPU benchmark applications, each application uses up to 10 UVM
    data objects. Real-world applications (e.g., opensource deep learning and large
    language models) also rarely use more than eight UVM data per kernel [\[9,](#page-13-9)
    [21\]](#page-14-19). Thus, we use a 10 entry object table. If a kernel uses more
    than 10 UVM objects, the GPU driver selects the largest 10 objects. The smaller
    objects with less performance impact are supported by the default tree configuration
    instead of Forest. The recency order has 4 bits to represent the LRU order among
    the 10 objects. The VPN start


    <span id="page-5-4"></span>![](_page_5_Figure_1.jpeg)


    <span id="page-5-3"></span>Figure 8: Memory access patterns of **DWT** and **SSSP**.


    and end fields have 40 bits each [\[24\]](#page-14-20) and the cease bit is 1
    bit. Thus, the object table has a total of 147 bytes. To support multikernel execution,
    we employ as many object tables as the number of maximum concurrent kernels. GPUs
    support 16 to 128 concurrent kernels [\[19\]](#page-13-10) so object tables can
    be 18.375KB at the maximum.


    #### <span id="page-5-0"></span>4.3 Access Pattern Detector (APD)


    APD detects access patterns of individual data objects and identifies the optimal
    TBNp configuration.


    4.3.1 Access timer-based pattern monitoring. To detect access patterns, Forest
    leverages ATT''s page access timing information. To copy the access timer information
    of the target object from GPU to APD, we leverage an existing UVM driver function
    (e.g., fetch\_access\_counter\_buffer\_entries(.)) which copies all access counters
    [\[51\]](#page-14-21). We extend this function to copy the counters of target
    objects. The access counters are copied from the GPU to APD once per a pre-defined
    number of data accesses (10K accesses in our design). If ATT finds that an object''s
    access timer reaches the threshold accesses, it triggers an interrupt. Then, the
    UVM driver executes the aforementioned extended function and reads access counters
    of all pages within the data object.


    The existing UVM driver copies access counters via the access counter buffer data
    structure as illustrated in Figure [7.](#page-4-0) We use the existing architectures.
    Out of the parameters used in the buffer, APD focuses on the VPN and access counter
    of each page, denoted as (= a set of ( , ) pairs, where and are the page''s VPN
    and hardware access counter register value, respecitvely). APD examines of each
    data object and performs a pattern check based on the pre-defined access patterns,
    which will be explained in Section [4.3.2.](#page-5-1) If one of the patterns
    is detected, the profiling for the data object concludes. The APD then instructs
    the PE to update the TBNp configuration for the data object, records the pattern,
    and notifies the ATT to cease triggering interrupts by setting the cease bit in
    the object table. Note that APD is a UVM driver module which can be operated without
    halting GPU execution. During the pattern detection and TBNp configuration, the
    GPU continues the execution with the default TBNp configuration.


    The patterns are recorded in a pattern table, as shown in Figure [7.](#page-4-0)
    Each entry of the pattern table is associated with a UVM-managed object and consists
    of kernel ID, the start and end VPNs, and pattern fields. The pattern field has
    two bits because we use four patterns, which will be described in the next section.
    Similar to the object table in ATT, each entry of the pattern table is reserved
    when a


    <span id="page-5-2"></span>![](_page_5_Figure_9.jpeg)


    Figure 9: Performance impact of tree configurations for different access patterns
    (lower is better).


    data is allocated with a UVM malloc API. If no pattern is detected, the APD repeats
    the procedure after the next monitoring phase. To prevent continuous monitoring
    throughout the entire execution, after a pre-defined (10 in our design) rounds
    of monitoring, the APD designates the data object as having a default access pattern
    and pauses further profiling for that data. The default access pattern will be
    explained shortly.


    <span id="page-5-1"></span>4.3.2 Access pattern classification & tree configuration.
    Access pattern consideration for UVM optimization is not new. Most studies [\[10,](#page-13-8)
    [25,](#page-14-13) [29\]](#page-14-12) classified the access pattern of each application
    into either regular or irregular. However, the access regularity cannot reflect
    access aggressiveness, which is essential for designing a prefetch algorithm.
    Thus, we propose more fine-grained classifications. We identified four access
    patterns that influence the performance of prefetch algorithms the most by analyzing
    91 data objects used by popular GPU benchmark suites (Table [1\)](#page-8-0):
    Linear/Streaming, Non-Linear High-Coverage High-Intensity, Non-Linear High-Coverage
    Low-Intensity, and Non-Linear Low Coverage. Our proposed classification can support
    both access locality and aggressiveness. From the analysis, we also identified
    the best-performing prefetch tree per access pattern. We show sample results (Figure
    [9\)](#page-5-2) while explaining our tree selection below.


    #### Linear/Streaming (LS):


    In LS pattern, the pages of the data object are accessed in either forward or
    inverse order. In GPU, due to concurrent memory accesses by warps and SMs, individual
    pages may not be accessed in clearly linear timings. Thus, we check if the accesses
    follow approximate linearity by using a linear regression algorithm. We first
    perform a linear regression [\[33\]](#page-14-22) on the of each object to model
    the relationship between the pages ( ) and their access timing information ( ).
    We then calculate the coefficient of determination ( 2 ) [\[7\]](#page-13-11),
    a common metric used to evaluate the degree of linearity. If 2 is closer to 1,
    the dataset follows a stronger linear pattern. The access pattern is classified
    as LS if the profiling data satisfies the following condition:


    $$R\_{P\_{S\text{set}}}^2 > R\_{LS}^2 \text{ [7]} \tag{1}$$


    where 2 is the calculated coefficient of profiling data and 2 represents the linearity
    threshold (set to 0.8 in this study). Kernel 1 in Figure [5a](#page-3-3) is an
    example LS pattern. In the LS pattern, the accessed pages are rarely reused within
    the kernel. Therefore, a more aggressive prefetching and eviction is favorable.
    Under TBNp''s 50% proactive prefetching policy, more pages will be proactively
    prefetched if a larger tree is used. As each tree covers consecutive VPNs, the
    proactively prefetched pages will likely be accessed by the LS data objects. The
    results in Figure [9](#page-5-2) align with our design; the objects with LS access
    pattern perform better with larger tree and leaf nodes. In our experiments, we
    use a 4MB tree with 256KB leaf nodes for LS data, which are the maximum sizes
    that we considered. Note that we consider trees between 512KB and 4MB and leaf
    nodes between 16KB and 256KB.


    Non-Linear High-Coverage High-Intensity (HCHI):


    $$p\_n - p\_1 \ge N\_{\mathcal{P}} \times \mathcal{P} \text{ and } N\_a \ge N\_{\mathcal{P}}
    \times \mathcal{A} \tag{2}$$


    where <sup>1</sup> and are the smallest and largest VPNs of the target object
    accessed by the GPU during the profiling phase, represents the total number of
    pages in the data object, denotes the number of accessed pages, P indicates the
    threshold of access coverage (0.6 in our design), and A represents the threshold
    of access intensity (0.4 in our design). This pattern means that the accessed
    VPN range and the accessed page count within the monitoring period are larger
    than the pre-defined portion of the pages in the data. In other words, a large
    number of pages in wide address ranges are accessed for a short time. Kernel 2
    in Figure [5a](#page-3-3) is an example. For such scattered accesses, aggressive
    prefetching easily leads to unnecessary migrations. Because HCHI data accesses
    a large number of pages, while only few data within the pages are actually accessed,
    if we use a large tree, the GPU memory will be quickly filled with a significant
    amount of unused data. Therefore, we configure HCHI to use small trees so that
    prefetches can happen within the small tree boundary. The leaf node is configured
    to be default size because small leaf nodes will lead to frequent page faults
    while too large leaf nodes will make the small trees quickly fill up. The results
    in Figure [9](#page-5-2) support our design; HCHI data show better performance
    with smaller tree and medium size leaf nodes. In our experiments, we use a 512KB
    tree with 64KB leaf nodes. Non-Linear High-Coverage Low-Intensity (HCLI):


    $$p\_n - p\_1 \ge N\_\mathcal{P} \times \mathcal{P} \text{ and } N\_a < N\_\mathcal{P}
    \times \mathcal{A} \tag{3}$$


    This pattern means that a wide address range within the target data object is
    accessed with a small number of pages during a monitoring period. Kernels 1 and
    3 in Figure [8b](#page-5-3) are examples. Similar to the HCHI, the HCLI pattern
    is better to use less aggressive prefetch to reduce unnecessary migrations. Compared
    to HCHI, HCLI shows more severe access sparsity. Therefore, we configure both
    trees and leaf nodes small. The results in Figure [9](#page-5-2) justify our design
    as the HCLI data show better performance with smaller tree and leaf nodes. In
    our experiments, we set a 512KB tree with 16KB leaf nodes.


    Non-Linear Low Coverage (LC):


    $$p\_n - p\_1 < N\_{\mathcal{P}} \times \mathcal{P} \tag{4}$$


    ![](_page_6_Figure_10.jpeg)


    <span id="page-6-0"></span>![](_page_6_Figure_11.jpeg)


    Figure 10: Example of motion bit and isolation bit.


    This class includes objects with small footprints and those not settled to a clear
    access pattern. Figure [8a](#page-5-4) shows an example. For this pattern, we
    use the default tree configuration (2MB tree with 64KB leaf nodes) because LC
    data neither shows a strong access trend nor has clear performance impacts with
    different tree configurations. LC patterns in Figure [9](#page-5-2) also do not
    show a clear pattern. We use LC as a default pattern.


    #### 4.4 Prefetch Engine (PE)


    PE is in the existing UVM driver and performs migrations and prefetches. We extend
    the PE to reconfigure TBNp for individual data objects. As described in Section
    [2.2,](#page-2-4) the baseline TBNp uses metadata, Nmigrated and Ntotal, per non-leaf
    node to control migrations. For dynamic tree configuration, we add two 1-bit metadata
    per non-leaf node; an isolation bit and a motion bit.


    The isolation bit is used to change the tree size. The motion bit controls the
    leaf size. The isolation and motion bits cannot both be set to 1 in the same node.
    16KB is our minimum configurable leaf size. Thus, initially, each UVM data is
    partitioned into 16KB units and the isolation and motion bits are configured to
    represent the default 2MB tree with 64KB leaf nodes. If the isolation bit is set
    to 1, PE isolates the left and right child nodes, preventing them from influencing
    each other''s migration behaviors. Otherwise, all child nodes and the parent node
    itself are considered part of the same TBN tree.


    In Figure [10,](#page-6-0) the isolation bit of 0 4 is 1 to indicate that the
    left and right trees are independent. In other words, the figure represents two
    prefetch trees each covering 128KB (=8 × 16KB) memory space. When the motion bit
    is set to 1, the PE promotes all child nodes to a single basic block. If it is
    not set, PE demotes the child nodes to separate nodes. In Figure [10,](#page-6-0)
    the motion bits of the immediate parent nodes of all leaf nodes ( 0 1 to 7 1 )
    are 1, which indicates that the trees use 32 KB basic blocks (or leaf nodes).


    Once the tree is configured with these motion and isolation bits according to
    APD''s recommended tree architecture, PE performs prefetching based on the values
    of the existing metadata, Nmigrated and Ntotal of the updated tree. For example,
    in Figure [10,](#page-6-0) after all nodes under 0 2 are migrated, a page fault
    on the left child of 2 1 triggers prefetching of the entire 2 1 because motion
    bit of 2 1 is 1 so both its children are considered as one combined leaf node.
    This in turn makes the Nmigrated of 0 3 greater than half of Ntotal of 0 3 and
    causes all nodes under 0 3 to be migrated. On the other hand, when a page fault
    occurs on the left child of 4 1 , only both children of 4 <sup>1</sup> will be
    migrated since 0 3 and 1 3 are


    independent trees as isolation bit of 0 4 is 1 and hence Nmigrated of 0 <sup>4</sup>
    will be ignored by PE.


    This tree configuration implicitly incurs page migrations to abide by the leaf
    node unit migrations. If the leaf size is increased and a subset of the enlarged
    leaf node is already in the GPU memory, we prefetch the remaining pages of the
    leaf node to the GPU memory upon tree configuration.


    #### <span id="page-7-0"></span>4.5 Eviction


    As discussed in Section [3.3,](#page-3-4) the existing UVM page replacement policy
    incurs severe page thrashing. To mitigate the inefficiency, a study [\[27\]](#page-14-6)
    proposed to use the page access counters for page eviction. As the original access
    counters reflect access frequency, they evict the least frequently used (LFU)
    blocks. However, LFU cannot resolve the page thrashing problem completely because
    recently accessed pages could be easily evicted due to limited access frequency
    history though they tend to have higher temporal locality.


    To tackle the eviction inefficiency, we propose to leverage our repurposed access
    counters and implement a pseudo-LRU. As described in Section [4.2,](#page-4-1)
    the access counters in Forest hold the access recency information within each
    data object. Thus, we can identify the LRU page within each data object by finding
    the page with the smallest hardware counter register value.


    To find the global LRU page, we leverage the recency order field in the ATT object
    table. The recency order field maintains the sorted order of access recency among
    the objects in the table. When data is accessed, the recency order field is updated
    such that the accessed data has the highest value of all (10 in our design as
    the object table has 10 entries). Therefore, the object with the smallest recency
    order (which is 1) is the LRU object. By leveraging that, we evict the leaf node
    that has the LRU page of the LRU object.


    When GPU encounters a far-fault under memory oversubscription, ATT checks the
    object table and finds the LRU object. The LRU object information is sent with
    the far-fault interrupt to the UVM driver. Then, the UVM driver reads the page
    access counters of the LRU object (similar to the operations in Section [4.3\)](#page-5-0),
    finds the LRU page, and commands eviction of the leaf node having the LRU page.
    Given that conventional LRU relies on page fault events and searches for the LRU
    page across the entire memory space, Forest confines its search to the LRU object,
    significantly reducing the search space and alleviating the associated overhead.


    #### <span id="page-7-2"></span>4.6 Driver Overhead


    The UVM driver handles the pattern detection and tree configurations in parallel
    while the GPU continues the execution. Therefore, the driver operations do not
    affect the overall performance. The access counter transfer could incur performance
    overhead because it consumes the PCIe bandwidth. In the evaluation, we included
    the times for transferring the access counters for every profiling phase and the
    times for setting the cease bit upon pattern detection for every object. The times
    for tree traversal in Forest are similar or even less than the TBNp because NVIDIA
    driver uses 63 nodes per 2MB tree for TBNp, while four variant trees of Forest
    use 15 to 63 nodes, thus the depth of trees is equal to or shorter than in TBNp.
    The runtime overhead is evaluated in detail in Section [7.7.](#page-13-12)


    <span id="page-7-1"></span>![](_page_7_Figure_11.jpeg)


    Figure 11: Workflow of Forest.


    #### 4.7 Bringing It All Together


    The workflow of Forest is as follows and illustrated in Figure [11.](#page-7-1)


    Detecting access patterns and configuring trees: Upon a kernel launch, the UVM
    driver first initializes the object table in ATT by populating the ATT entries
    for the objects accessed by the kernel. The UVM memory allocation APIs (e.g.,
    cudaMallocManaged) will provide each object''s information such as the VPN ranges
    ( 1 ). During kernel execution, memory accesses update the access timer and recency
    order of the corresponding object in ATT, which is located in GMMU ( 2 ). When
    an object has more than a predefined number of accesses (10K transactions in Forest),
    ATT triggers an interrupt to notify the GPU driver. The driver then retrieves
    the object''s access timer information by using the driver''s existing access
    counter copy function ( 3 ). Then, the driver checks the access pattern via APD.
    Once the pattern is determined, the driver records it to the pattern table, reconfigures
    the object''s tree by setting motion and isolation bits of the tree nodes in PE
    ( 4 ), and notifies ATT to stop monitoring patterns for the object by setting
    the cease bit in the object table ( 5 ). If a pattern is not detected after multiple
    detection attempts (e.g., 10 attempts in Forest), Forest marks the corresponding
    object as default (LC) pattern ( 6 ). Throughout this process, GPU does not need
    to stall its execution.


    Prefetching with newly configured tree & eviction: The reconfigured tree is then
    utilized when a new page fault occurs ( 1 ). Once the fault is resolved, the driver
    prefetches data based on the newly configured tree ( 2 ). These steps are identical
    to the baseline UVM, except for using a new tree configuration. When a page fault
    occurs under oversubscription, eviction is required. Then, Forest identifies the
    LRU object in the ATT''s object table, selects the corresponding leaf node with
    the LRU page (i.e., the page with the smallest LRU timer), and proceeds with eviction
    ( 3 ).


    #### 5 Speculative Forest


    Forest uses profiling to detect the access patterns. To reduce the profiling phase
    and configure the optimal TBNp early, we propose Speculative Forest (SpecForest),
    which uses pattern recording, static analysis, and access similarity detection.


    <span id="page-8-0"></span>


    | Category | App Name       | Abbr. | Kernel                | Data Object (Pattern)      |                                |                             |                                 |                                  |            |

    |----------|----------------|-------|-----------------------|----------------------------|--------------------------------|-----------------------------|---------------------------------|----------------------------------|------------|

    | Linear   | 2D Conv [26]   | 2DC   | Convolution_kernel    | A: LS                      |
    B: LS                          |                             |                                 |                                  |            |

    |          | 3D Conv [30]   | 3DC   | Convolution3D_kernel  | A: LS                      |
    B: LS                          |                             |                                 |                                  |            |

    |          | AddVector [26] | AV    | vecAdd                | A: LS                      |
    B: LS                          |                             |                                 |                                  |            |

    |          | FDTD-2D [26]   | FDTD  | fdtd_step1_kernel     | _fict_: LS                 |
    ey: LS                         | hz: LS                      |                                 |                                  |            |

    |          |                |       | fdtd_step2_kernel     | ex: LS                     |
    hz: LS                         |                             |                                 |                                  |            |

    |          |                |       | fdtd_step3_kernel     | ex: LS                     |
    ey: LS                         | hz: LS                      |                                 |                                  |            |

    |          | srad_v2 [26]   | SRAD  | srad_cuda_1           | E_C: LS                    |
    W_C: LS                        | N_C: LS                     | S_C: LS                         |
    J_cuda: LS                       | C_cuda: LS |

    |          |                |       | srad_cuda_1           | E_C: LS                    |
    W_C: LS                        | N_C: LS                     | S_C: LS                         |
    J_cuda: LS                       | C_cuda: LS |

    |          | stencil [26]   | STEN  | block2D_hybrid_       | A0: LS                     |
    Anext: LS                      |                             |                                 |                                  |            |

    |          | hotspot [26]   | HS    | calculate_temp        | power: LS                  |
    temp_src: LS                   | temp_dst: LS                |                                 |                                  |            |

    |          | dwt2d [30]     | DWT   | c_CopySrcToC          | d_r: LS                    |
    d_g: LS                        | d_b: LS                     | d_src: LS                       |                                  |            |

    |          |                |       | fdwt53Kernel1         | in: HCHI                   |
    out: HCHI                      |                             |                                 |                                  |            |

    |          |                |       | fdwt53Kernel2         | in: LC                     |
    out: LC                        |                             |                                 |                                  |            |

    |          | CifarNet [36]  | CN    | ExecuteThirdLayer     | Layer3_W: LS               |
    Layer3_F: LS                   | Layer2_pool: LS             |                                 |                                  |            |

    | Mixed    |                |       | ExecuteFourthLayer    | Layer4_W: HCHI             |
    Layer4_F: LS                   | Pool3_L: HCHI               |                                 |                                  |            |

    |          | ra [26]        | RA    | kernel                | input: LS                  |
    output: HCHI                   | table: HCHI                 |                                 |                                  |            |

    |          | BICG [30]      | BICG  | bicg_kernel1          | A_gpu: LS                  |
    r_gpu: LS                      | s_gpu: LS                   |                                 |                                  |            |

    |          |                |       | bicg_kernel2          | A_gpu: HCHI                |
    p_gpu: LS                      | q_gpu: LS                   |                                 |                                  |            |

    |          | nw [26]        | NW    | needle_cuda_shared_1  | reference: LC              |
    itemsets: LC                   |                             |                                 |                                  |            |

    |          |                |       | needle_cuda_shared_2  | reference: HCLI            |
    itemsets: HCLI                 |                             |                                 |                                  |            |

    |          | sssp [26]      | SSSP  | initializeArrays      | shortest<br>Distances
    : LS | updating<br>: LS<br>Short      |                             |                                 |                                  |            |

    |          |                |       | Kernel1               | vertexArray: LS            |
    edgeArray: HCLI                | weightArray: HCLI           | shortest<br>: LS<br>Distances   |
    updating<br>: HCLI<br>Short      |            |

    |          |                |       | Kernel2               | shortest<br>Distances
    : LS | updating<br>: LS<br>Short      |                             |                                 |                                  |            |

    |          | bfs [26]       | BFS   | Kernel1               | g_graph<br>_nodes          |
    : LS g_graph_edges: LS         | g_graph<br>: HCHI<br>_mask  | g_updating_<br>graph_mask:
    HCHI | g_graph_visited: LS g_cost: HCHI |            |

    |          |                |       | Kernel2               | g_graph<br>: LS<br>_mask   |
    g_updating_<br>graph_mask : LS | g_graph<br>: LS<br>_visited |                                 |                                  |            |

    |          | SqeezeNet [36] | SN    | ExecuteFirstLayer     | Weights: HCHI              |
    Data__R: LS                    | Data__G: LS                 | Data__B: LS                     |
    Features: HCHI                   |            |

    |          |                |       | pooling1              | Features: HCHI             |
    Pool_layer: HCLI               |                             |                                 |                                  |            |

    |          |                |       | Executefire2expand1x1 | Weights: HCHI              |
    Features: LS                   | Pool_layer: LS              |                                 |                                  |            |

    |          |                |       | Executefire2expand3x3 | Weights: HCHI              |
    fire2_Features: LS             | 1x1_Features: HCHI          |                                 |                                  |            |

    |          |                |       | global_pooling        | Features: LS               |
    output_GPU: LS                 |                             |                                 |                                  |            |


    Table 1: Workloads and Access Patterns of UVM-managed Data Objects.


    ∗ : Repeated kernel invocation instances only present the first one, since all
    the instances exhibit a similar pattern.


    # 5.1 Access Pattern Recording


    We leverage repetitive kernel executions to skip profiling. As described in Section
    [4.3,](#page-5-0) APD records the access patterns in the pattern table. We maintain
    the pattern information throughout the application execution. If an application
    runs a kernel multiple times, the recorded pattern is used without reprofiling
    the kernel. Such repetitive kernel execution can be easily found in large GPU
    applications because large data should be processed over multiple kernel executions
    by utilizing the limited hardware resources. When a kernel is launched, the APD
    checks if the kernel has the record in the pattern table. If so, the cease bits
    of the corresponding entries in the ATT''s object table will be set and ATT will
    not trigger interrupts for profiling. APD will share the optimal tree configuration
    for the recorded pattern with PE and PE will configure the TBNp from the beginning
    of the execution.


    ```

    1 __global__ void vecAdd (int n , float *x , float * y )

    2 {

    3 int index = blockIdx . x * blockDim . x + threadIdx . x ;

    4 int stride = blockDim . x * gridDim . x ;

    5 for ( int i = index ; i < n ; i += stride )

    6 ▶ y [ i ] = x [ i ] + y [ i ];

    7 }

    ```

    Listing 1: A code snippet of **AV**.


    # 5.2 Static Access Pattern Detection


    The pattern recording only works for kernels executing repetitively. For the kernels
    that do not have a recorded pattern, we apply a


    compile-time access pattern analysis. In GPU applications, data access patterns
    can be estimated by checking the data indexes [\[5,](#page-13-13) [60,](#page-15-1)
    [63\]](#page-15-2). Out of the four access patterns, static analysis can detect
    the LS pattern, which typically uses indexing with fixed strides. Listing [1](#page-8-1)
    shows an example. Both x and y are accessed using the same index i (global thread
    id) with a fixed stride. Once the compiler detects LS data objects, the information
    is passed to cudaMallocManaged(.). This one-bit information can be passed in many
    ways but we extend the existing API''s parameter. cudaMallocManaged(.) uses flags
    parameter to indicate the access exclusiveness between CPU and GPU. It uses 1-bit
    information but we extend it to use 2 bits. If the upper bit of the flags is 1,
    it means the data object uses the LS pattern. Note that this pattern bit is set
    by the compiler. Thus, this doesn''t require program modification. If the UVM
    driver finds this bit is set, LS pattern is recorded in the pattern table and
    the profiling is skipped.


    ```

    1 __global__ void Kernel1 (...) {

    2 int tid = blockIdx . x * blockDim . x + threadIdx . x ;

    3 ...

    4 ▶ int edgeStart = vertexArray [ tid ];

    5 ▶ int edgeEnd = vertexArray [ tid + 1];

    6 ...

    7 for ( int edge = edgeStart ; edge < edgeEnd ; edge ++) {

    8 ▶ int nid = edgeArray [ edge ];

    9 ▶ atomicMin (... , weightArray [ edge ]) ;

    10 ... }}

    ```


    ```

    Listing 2: A code snippet of SSSP.

    ```

    # 5.3 Access Pattern Similarity Detection


    The patterns HCHI, HCLI, and LC might include complex indexing (e.g., nested and
    indirect references). Therefore, the static analysis is not effective. To support
    these patterns, we propose a novel access pattern similarity detection. Listing
    [2](#page-8-2) shows an example. In SSSP, both edgeArray and weightArray are indexed
    with the content of another array, vertexArray. Compilers cannot detect access
    patterns for such indirect indexing. Instead, we focus on the indexing similarity.
    As edgeArray and weightArray use the same index, edge, we can expect that they
    will follow the same access pattern. We group the two arrays into one pattern
    group.


    During the index analysis, the compiler identifies UVM data objects that use the
    same index. If the index is an indirect reference or an outcome of a complex stride
    calculation, the compiler creates a similarity group and marks those data with
    the same group ID. We deliver the similarity group information via the same method
    that is used for static access pattern detection. We extend the flags parameter
    to have a few more bits to represent the group IDs. We use three bits because
    our benchmarks have up to eight similarity groups. If the UVM driver finds that
    the similarity bits in the flags parameter are non-zero, it encodes the similarity
    bit value (group ID) to the pattern field in APD''s pattern table. To distinguish
    the similarity groups from the four access patterns, we add three bits to the
    pattern field; the most significant three bits are similarity group ID and the
    remaining two bits indicate the access patterns. If an object has non-zero values
    in the similarity group bits while the remaining two bits are zeros in the pattern
    field, the APD initiates the profiling. When one data''s pattern is determined,
    APD searches the pattern table and marks the same pattern to the objects having
    the same similarity group ID. This way, we can configure TBN tree early and skip
    profiling for all objects in the same similarity group as soon as one object finishes
    the profiling.


    #### 5.4 SpecForest in Forest Workflow


    SpecForest adds two steps on the Forest''s workflow, Figure [11.](#page-7-1) Utilizing
    recorded patterns: When a kernel is launched ( 1 ), the driver checks whether
    the access patterns of each object have been recorded by searching the pattern
    table in the APD with the kernel name and the address range of the object. If
    a pattern is found, Forest configures the object''s tree with the recorded pattern.


    Utilizing similarity: When an object''s pattern is determined ( 4 ), SpecForest
    evaluates its pattern similarity, leveraging similarity detection results. If
    there are other objects in the same similarity group, their trees are reconfigured
    together in PE.


    ### 6 Discussions


    Why Tree? Forest uses TBNp as the baseline prefetcher because it is used by all
    recent generations of NVIDIA GPUs and the tree-based prefetching is more efficient
    for bursty GPU memory behaviors and in controlling prefetch aggressiveness than
    conventional prefetching algorithms [\[1,](#page-13-14) [34,](#page-14-25) [35,](#page-14-16)
    [40,](#page-14-26) [62\]](#page-15-3). Note that the fundamental differences between
    CPU and GPU execution models hinder the adoption of advanced CPU-side prefetch
    algorithms to UVM [\[6,](#page-13-15) [32,](#page-14-27) [34,](#page-14-25) [61\]](#page-15-4).
    To support complicated access patterns, CPU solutions track individual load instruction-triggered
    cache


    misses while each load instruction can cause a storm of cache misses and faults
    in GPU. Detecting strides from such massive memory transactions is challenging.
    Furthermore, GPU hardware should be modified significantly to enable such instruction-level
    fine-grained monitoring. The problem becomes worse for UVM management because
    such fine-grained monitoring information should be delivered via slow PCIe and
    processed by CPU. Even for coarser-grained page faults, due to the storms of memory
    accesses, UVM drivers handle them in batch units to avoid significant CPU-side
    performance overhead [\[2,](#page-13-6) [8,](#page-13-16) [14\]](#page-13-17).
    The hierarchical tree structure can support such a unique GPU computing better
    because batches of leaf nodes can be concurrently handled. Beyond that, Forest
    enables a more flexible and autonomous (without geeky programmer''s efforts) management
    with lightweight object-level access monitoring.


    That being said, Forest can be also applied to non-tree prefetchers used by different
    GPUs. For example, AMD GPUs use range-based prefetching for shared virtual memory
    (SVM) [\[13\]](#page-13-18). SVM migrates memory in large units (Ranges) between
    2MB and 1GB. Such an aggressive prefetching is effective in reducing far-faults,
    while its access-pattern obliviousness often results in significant memory thrashing
    [\[13\]](#page-13-18). Forest could resolve this problem by partitioning each
    Range to smaller migration units based on the access patterns of the associated
    object. Once AMD GPU supports per-page access count, ATT can be implemented with
    negligible overhead.


    Impact on emerging & diverse architectures While we evaluated with discrete GPU
    systems, our idea can be applied to emerging and diverse architectures, such as
    Grace-Hopper (GH) superchips. GH uses cache-line-level communication between a
    Grace CPU and a Hopper GPU via a high-bandwidth interconnect [\[17,](#page-13-2)
    [57\]](#page-14-28). Similar to zero-copy, data can be shared via remote accesses
    rather than migration in GH. However, the impact of efficient prefetch cannot
    be underestimated. Due to the notable bandwidth gap among CPU local memory (500
    GBps), inter-chip interconnect (900 GBps), and GPU local memory (4 TBps), pages
    with intensive GPU accesses are better migrated to GPU memory rather than shared
    remotely, thus needing efficient prefetch. Given that GPU''s SIMD-like execution
    model inherently exhibits high page locality (our benchmarks access each page
    3K times on average up to 14K times), Forest can manage memories of such emerging
    superchips efficiently. If GPU local memory is oversubscribed, the impact will
    be even higher.


    # 7 Evaluation


    # 7.1 Methodology


    We extended a UVM-enabled GPGPU-Sim 4.0 [\[48\]](#page-14-17) to design Forest.
    The simulator is integrated with UVM emulator including TBNp tree traversal function.
    We modified TBNp functions for Forest. The page fault handling latency and page
    table walk latency are set to 45 µs and 100 cycles, respectively [\[26,](#page-14-5)
    [27\]](#page-14-6). Remote memory access latency is set to 200 core cycles. The
    delayed migration threshold is set to 8 [\[27\]](#page-14-6). GPU memory is assumed
    to be oversubscribed by 150% by default. We considered the driver overhead as
    described in Section [4.6.](#page-7-2) Detailed simulator configurations are listed
    in Table [2.](#page-10-0)


    <span id="page-9-0"></span>7.1.1 Compared solutions. We compare Forest with the
    following solutions including three state-of-the-art (SOTA) solutions.


    <span id="page-10-2"></span>10 2


    <span id="page-10-1"></span>![](_page_10_Figure_2.jpeg)


    Figure 13: Number of far-faults (lower is better): Note that Zero-Copy doesn''t
    incur page faults.


    Linear Mixed


    2DC 3DC AV FDTD SRAD STEN HS Geo. DWT CN RA BICG NW SSSP BFS SN Geo. GEO.


    - Baseline [\[2,](#page-13-6) [26\]](#page-14-5) is the TBNp that uses 2MB trees
    and 64KB leaf nodes. The UVM driver selects a 2MB VABlock from the LRU list, which
    is updated during far-faults and migrations.

    - Oracle Homo-TBNp is an oracle application-level TBNp configuration. We assume
    that GPU knows the optimal homogeneous TBN tree for each application without profiling.
    We chose the configuration that performs best in Figure [4.](#page-2-1)

    - Zero-copy uses cache-line-level remote memory accesses instead of page-level
    migration or prefetching [\[17,](#page-13-2) [57\]](#page-14-28). Host data is
    accessed without a page fault. Instead, a 128B cache line is copied from CPU to
    GPU [\[49,](#page-14-29) [54\]](#page-14-30).

    - Delayed Foresttriggers migration or prefetching only when the pages are accessed
    more than a threshold. We set the threshold 8 as in Table [2.](#page-10-0)

    - Range [\[13\]](#page-13-18) is AMD''s SVM approach. Range migrates and evicts
    data in variable units, at least 2MB. We configured to manage each object with
    one to multiple 2MB Ranges.

    - InterplayUVM (SOTA) [\[26\]](#page-14-5) extends TBNp with a tree-based eviction
    policy supporting locality-aware eviction, similar to proactive prefetching. If
    any node has more than half its child nodes on the host, all child nodes are evicted.
    Forest builds on this with a dynamic, tree-aware structure that maintains TBN
    tree semantics.

    - EarlyAdaptor (SOTA) [\[29\]](#page-14-12) uses the baseline TBNp and controls
    the prefetch aggressiveness by adapting the migration threshold. It also develops
    a model that dynamically adjusts prefetching intensity based on far-fault history.

    - AdaptiveThreshold (SOTA) [\[27\]](#page-14-6) adaptively alternates between
    remote zero-copy access to host-pinned memory and on-demand page migration. This
    adaptation leverages the existing hardware access counter.

    - TemporalPrefetcher [\[34\]](#page-14-25) applies temporal correlation-based
    prefetching. We implemented a correlation table tracking faulty block sequences.
    On a page fault, it migrates the faulty block with the next two correlated blocks
    recorded in the


    Table 2: Simulator Configuration.


    <span id="page-10-0"></span>


    | Simulator [48]                  | GPGPU-Sim 4.0 (UVM supported)    |

    |---------------------------------|----------------------------------|

    | GPU Architecture                | NVIDIA Turing architecture       |

    | GPU Cores                       | 30 SMs, 64 cores each @ 1365 MHz |

    | Page Size                       | 4KB                              |

    | Far-Fault Handling Latency [26] | 45 𝜇s                            |

    | Page Table Walk Latency [26]    | 100 core cycles                  |

    | CPU-GPU Interconnect            | PCIe 3.0 x16 (8 GT/s/lane)       |

    | Access Counter Threshold [27]   | 8                                |

    | Remote Access Latency [27]      | 200 core cycles                  |


    table [\[35\]](#page-14-16). We evaluated two faulty block granularities, 64KB
    (Temporal\_64K) and 2MB (Temporal\_2M).


    7.1.2 Workloads. We evaluated Forest with 15 workloads selected from widely used
    UVM benchmarks, UVMBench [\[30\]](#page-14-23) and InterplayUVM [\[26\]](#page-14-5).
    We also tested CNN workloads in Tango [\[36\]](#page-14-24) by replacing cudaMalloc
    API calls with cudaMallocManaged. Table [1](#page-8-0) shows the detailed patterns
    of various data objects across kernels. These workloads use a working set size
    from 19.5MB to 144MB, with an average memory footprint of 63.5 MB. By adjusting
    the memory oversubscription ratio, we manipulate the device memory size instead
    of changing the benchmarks'' working set sizes. In Section [7.6,](#page-12-0)
    we also assessed a variety of real-world deep learning models, AlexNet [\[39\]](#page-14-31),
    ResNet50 [\[31\]](#page-14-32), Google''s BERT [\[20\]](#page-13-19), and OpenAI
    Whisper [\[52\]](#page-14-33), running within PyTorch framework. These models
    have footprints from 226MB to 891MB. For these models, we integrated the UVM-enabled
    GPGPU-Sim with Accel-Sim [\[37\]](#page-14-34).


    # 7.2 Overall Performance & Far-fault Reduction


    Figure [12](#page-10-1) shows the overall speedup over the Baseline. Forestshows
    superior performance with average speedups ranged from 1.07× to 2.21× compared
    with the solutions described in Section [7.1.1](#page-9-0) (1.72× over Baseline,
    up to 1.39× over SOTA, and up to 1.9× over TemporalPrefetcher). SpecForest attains
    over 8% further improvement


    than Forest (1.86× over Baseline) because SpecForest can configure the ideal TBNp
    earlier than Forest.


    To understand the speedup, we measured the number of farfaults as shown in Figure
    [13.](#page-10-2) A strong correlation is observed between the performance and
    the total number of far-faults. Forest especially showed better far-fault reduction
    for linear applications compared to AdaptiveThreshold and EarlyAdaptor because
    the migration aggressiveness of these solutions is limited to the 2MB unit, which
    causes more far-faults when applications access over the 2MB address boundary.
    Forest reduces such far-faults by employing a larger tree. The significant far-fault
    reduction of AdaptiveThreshold for mixed pattern applications didn''t lead to
    better performance because AdaptiveThreshold replaces far-faults with slow remote
    accesses. On the other hand, SpecForest reduces far-faults comparably with AdaptiveThreshold
    for mixed-pattern applications, and such reduction is well reflected in the performance
    because the smaller tree and leaf nodes for HCHI and HCLI objects and the proposed
    pseudo-LRU reduce page thrashing without incurring slow remote accesses. Forest
    outperforms Oracle Homo-TBNp significantly for the mixed-pattern applications
    because Oracle Homo-TBNp uses one tree configuration for all data objects. Though
    the tree might be the best among different configurations, the configuration might
    not be best for some data objects. Zero-Copy consistently exhibits poor performance
    due to the slow remote memory accesses and its inability to utilize the high-bandwidth
    device memory or exploit the locality of GPU applications. Range shows excellent
    performance for LS patterns because of its aggressive prefetching. However, its
    performance significantly degrades in mixed-pattern applications because of severe
    page thrashing, which we believe Forest''s access-aware prefetching and gradual
    migration aggressiveness could solve. Delayed Forest shows almost comparable performance
    with Forest for mixed-pattern workloads because it prevents unnecessary migrations
    of sparsely accessed pages. However, it also delays effective migrations for LS
    workloads, which leads to over 17% worse performance than SpecForest for LS workloads.
    Temporal-Prefetcher prefetches blocks based on faulty block history, making it
    efficient for complex yet repeated accesses. For LS patterns that barely have
    repeated accesses (e.g., streaming), correlation prefetching does not bring significant
    speedup. Instead, the size of each migration unit (2MB in Temporal\_2M and 64KB
    in Temporal\_64K) implicitly supports prefetching. Thus, Temporal\_2M shows a
    good performance for Linear benchmarks since each page fault triggers a migration
    of 2MB, similar to Range. On the other hand, it shows the worst performance of
    all for Mixed benchmarks because the aggressive prefetching causes severe GPU
    memory thrashing, thereby diminishing the impact of correlations. Temporal\_64K
    shows a better performance for the Mixed benchmarks. However, it still lags behind
    other GPU prefetchers because it supports limited locality (within 64KB block)
    for LS patterns and its sequence-based design lacks support for bursty access
    behaviors of GPU computing.


    # 7.3 Page Thrashing and Eviction Performance


    We measured the number of page thrashing events while running SpecForest. To understand
    the impact of optimal tree configuration and on-device eviction separately, Figure
    [14](#page-11-0) shows the reduced page thrashings with each of them separately.
    We enabled only optimal


    <span id="page-11-0"></span>![](_page_11_Figure_5.jpeg)


    Figure 14: Page thrashing normalized by Baseline.


    <span id="page-11-1"></span>![](_page_11_Figure_7.jpeg)


    Figure 15: Speedup breakdown (bar charts) and the profiling steps (scatter plot)
    of SpecForest.


    tree configuration while using default UVM eviction policy in the SpecForest (Far-fault
    LRU) measurements. SpecForest (Access-aware LRU) is the full integration of our
    proposed ideas. On average, the proposed tree configuration reduces page thrashings
    by 25%. The access-aware page eviction further reduces thrashings by 7% thanks
    to a more precise LRU block selection.


    # 7.4 Performance Breakdown of SpecForest


    We investigated the effectiveness of SpecForest on profiling step reduction. As
    shown with the scatter plot in Figure [15,](#page-11-1) SpecForestreduces profiling
    steps from 223 to 10 steps on average. The SpecForest employs pattern recording,
    static analysis, and similarity detection, and the bar charts of Figure [15](#page-11-1)
    show the performance impact of each of them. The results indicate that pattern
    recording (denoted PR) offers more benefits for mixed-pattern benchmarks. This
    is because recording can save tree reconfiguration overhead more in mixed pattern
    applications. Static analysis (SA) and similarity detection (SD) each add 2% further
    speedups on average.


    #### Table 3: GPU configurations


    <span id="page-11-2"></span>


    | GPU              | Arch.  | Mem.   | Cores        | H2D Interconn. |  |

    |------------------|--------|--------|--------------|----------------|--|

    |                  |        |        | 28 SMs       |                |  |

    | GTX 1080Ti [26]  | Pascal | GDDR5X | 128 cores/SM | PCIe 3.0 x16   |  |

    |                  |        |        | 1481 MHz     |                |  |

    |                  |        |        | 80 SMs       |                |  |

    | Quadro V100 [59] | Volta  | HBM2   | 64 cores/SM  | PCIe 3.0 x16   |  |

    |                  |        |        | 1132 MHz     |                |  |

    |                  |        |        | 30 SMs       |                |  |

    | RTX 2060 [55]    | Turing | GDDR6  | 64 cores/SM  | PCIe 3.0 x16   |  |

    |                  |        |        | 1365 MHz     |                |  |

    |                  |        |        | 46 SMs       |                |  |

    | RTX 3070 [58]    | Ampere | GDDR6  | 128 cores/SM | PCIe 4.0 x16   |  |

    |                  |        |        | 1320 MHz     |                |  |

    |                  |        |        | 114 SMs      |                |  |

    | H100 [15]        | Hopper | HBM2e  | 128 cores/SM | PCIe 5.0 x16   |  |

    |                  |        |        | 1095 MHz     |                |  |


    <span id="page-12-1"></span>![](_page_12_Figure_1.jpeg)


    Figure 16: Speedup under diverse oversubscription.


    <span id="page-12-2"></span>![](_page_12_Figure_3.jpeg)


    Figure 17: Speedup of SpecForest on different GPUs.


    # 7.5 Sensitivity Study


    7.5.1 Sensitivity on Oversubscriptions. We evaluated the impact of subscription
    ratios. As shown in Figure [16,](#page-12-1) in all oversubscription ratios, SpecForest
    consistently achieves notable speedups from 1.57× to 1.95× over Baseline. The
    results show that our solution delivers superior performance at higher memory
    oversubscription ratios due to increased device-host data transfers, which exacerbate
    the inefficiencies in sub-optimal tree configurations. SpecForest can effectively
    tackle the problem.


    7.5.2 Sensitivity on GPU Architecture. When testing on five different GPU architectures,
    as listed in Table [3,](#page-11-2) SpecForest consistently shows notable performance
    improvements as shown in Figure [17](#page-12-2) since workloads'' memory access
    pattern is the main optimization indicator of our approach, rather than specific
    instruction set architecture. For the latest H100, while lower core frequency
    and higher memory bandwidth can improve the performance of memory-bound workloads,
    almost tripled SM count increases concurrency, which leads to higher interconnect
    and memory contentions. Consequently, UVM overhead remains as a bottleneck, which
    is effectively resolved by SpecForest.


    7.5.3 Sensitivity on Pattern Classification Interval. We evaluated the impact
    of the pattern classification interval by varying it from 1K to 100K memory transactions
    on Forest. As shown in Figure [18,](#page-12-3) a short interval (<5K) fails to
    detect patterns within 10 classification rounds, often resulting in an undetermined
    classification that defaults to the conventional tree (e.g., 1K for 2DC, 3DC,
    FDTD; 5K for 2DC). Conversely, a long interval (>100K) delays switching to the
    optimal tree, as observed in STEM, CN, and RA. In cases like NW, where kernels
    have limited memory accesses, pattern classification never occur, forcing to use
    default TBN tree.


    7.5.4 Sensitivity on Threshold of Access Coverage & Intensity. We evaluated the
    impact of different access detection thresholds. First, for the access coverage
    thresholds, we tested from 0.2 to 0.8, while we used 0.6 in all other evaluations.
    Figure [19a](#page-12-4) shows that too


    <span id="page-12-3"></span>![](_page_12_Figure_11.jpeg)


    Figure 18: Impact of pattern classification interval lengths (default 10K).


    <span id="page-12-5"></span><span id="page-12-4"></span>![](_page_12_Figure_13.jpeg)


    Figure 19: Impact of thresholds.


    high threshold misclassifies HC as LC (e.g., CN, SSSP, BFS, and SN). This misclassification
    fails to restrict access aggressiveness, leading to performance degradation. Conversely,
    too low threshold misclassifies LC as HC (e.g., DWT and NW), resulting in suboptimal
    tree configuration. We also tested with various access intensity thresholds from
    0.2 to 0.8. We used 0.4 in all other evaluations. Figure [19b](#page-12-5) illustrates
    that too low threshold misclassifies HCLI as HCHI (e.g., SSSP and NW), while too
    high threshold misclassifies HCHI as HCLI (e.g., CN, RA, BFS, SN). These misclassifications
    hinder benchmarks from utilizing the appropriate tree configuration, impacting
    performance. These results verify our proper setting of default thresholds.


    # <span id="page-12-0"></span>7.6 Forest on Real-world DL Workloads


    We integrated Accel-Sim [\[37\]](#page-14-34) with the UVM-enabled GPGPU-SIM to
    evaluate SpecForest for real-world deep-learning (DL) workloads. As shown in Figure
    [20,](#page-13-21) SpecForest achieves an average speedup of 1.51× (up to 1.62×).
    To understand these promising speedups, we measured the breakdown of access patterns,
    analyzing the distribution of various patterns as defined in Section [4.3.2.](#page-5-1)
    While LS pattern is predominant across all models, particularly in CNN models,
    the HCHI pattern constitutes a significant proportion in Transformer models (e.g.,
    BERT and Whisper) due to the irregular dense memory access behaviors in their
    self-attention layers. By supporting flexible prefetching aggressiveness for individual
    tensor''s access patterns, SpecForest is especially effective for the advanced
    ML models having more complex and diverse access patterns (e.g., HCHI), which
    are challenging to be supported by access-pattern oblivious TBNp.


    <span id="page-13-21"></span>![](_page_13_Figure_1.jpeg)


    Figure 20: Speedup and access pattern breakdown of SpecForest on real-world DL
    workloads.


    <span id="page-13-22"></span>![](_page_13_Figure_3.jpeg)


    Figure 21: Runtime overhead breakdown. (Normalized to the Baseline TBNp tree traversal
    time)


    ### <span id="page-13-12"></span>7.7 Runtime Overhead


    We implemented tree traversal, configuration, and pattern classification in the
    UVM driver emulator, which is integrated with the simulator. We measured the driver-side
    execution time when SpecForest is simulated on an Intel Xeon Gold 6330 server.
    Figure [21](#page-13-22) shows the runtime overhead breakdown of SpecForest, normalized
    to the Baseline (TBNp) tree traversal time. Pattern classification occurs only
    at the beginning stage and ceases once the pattern is determined, making its impact
    negligible compared to the repeated tree traversal operations. Similarly, tree
    reconfiguration occurs only once after the pattern detection. Overall, the total
    runtime overhead of SpecForest is 16% less than the tree traversal time of TBNp.
    This is mainly contributed by the significantly reduced (by 54%) tree traversal
    time of SpecForest. As noted in Section [4.6,](#page-7-2) the proposed four trees
    have shorter or equal height with TBNp trees, thereby shorter traversal times
    for each search. Also, the reduced page thrashings and faults fundamentally decrease
    the traversal attempts.


    #### 7.8 Hardware Overhead


    Forest leverages the existing UVM driver and page access counter architecture.
    The only addition to the hardware is ATT, which uses 147-byte per-kernel object
    tables. To update the recency order, it adds 4-bit comparators.


    ### 8 Conclusion


    TBNp plays a significant role in the UVM performance. In this paper, we disclose
    intrinsic inefficiencies of conventional TBNp and propose Forest, which tackles
    the problems by employing accessaware heterogeneous TBNp configurations and a
    lightweight page eviction policy. The proposed software-hardware co-design allows
    dynamic TBNp adjustments, which enhances overall benchmark performance by 86%
    over the conventional TBNp and 39% over SOTA solutions, and 51% on real world
    DNN workloads, compared to the baseline.


    # Acknowledgments


    This work was supported by NSF grants, CCF-2114514 and CAREER-2341039. Part of
    this research was conducted using Pinnacles (NSF MRI, # 2019144) at the Cyber
    Infrastructure and Research Technologies (CIRT) at the University of California
    Merced.


    #### References


    - <span id="page-13-14"></span>[1] Thomas Alexander and Gershon Kedem. 1996. Distributed
    Prefetch-buffer/Cache Design for High Performance Memory Systems. In Proceedings
    of the 2nd IEEE Symposium on High-Performance Computer Architecture (HPCA ''96).
    IEEE Computer Society, USA, 254.

    - <span id="page-13-6"></span>[2] Tyler Allen and Rong Ge. 2021. Demystifying
    GPU UVM Cost with Deep Runtime and Workload Analysis. In 2021 IEEE International
    Parallel and Distributed Processing Symposium (IPDPS). 141–150. [doi:10.1109/IPDPS49936.2021.00023](https://doi.org/10.1109/IPDPS49936.2021.00023)

    - <span id="page-13-7"></span>[3] Tyler Allen and Rong Ge. 2021. In-depth analyses
    of unified virtual memory system for GPU accelerated computing. In Proceedings
    of the International Conference for High Performance Computing, Networking, Storage
    and Analysis (St. Louis, Missouri) (SC ''21). Association for Computing Machinery,
    New York, NY, USA, Article 64, 15 pages. [doi:10.1145/3458817.3480855](https://doi.org/10.1145/3458817.3480855)

    - <span id="page-13-0"></span>[4] AMD Corporation. Accessed June 2024. Radeons
    Next-generation Vega Architecture. [https://en.wikichip.org/w/images/a/a1/vega-whitepaper.pdf.](https://en.wikichip.org/w/images/a/a1/vega-whitepaper.pdf)

    - <span id="page-13-13"></span>[5] Grant Ayers, Heiner Litz, Christos Kozyrakis,
    and Parthasarathy Ranganathan. 2020. Classifying Memory Access Patterns for Prefetching
    (ASPLOS ''20). Association for Computing Machinery, New York, NY, USA, 513–526.
    [doi:10.1145/](https://doi.org/10.1145/3373376.3378498) [3373376.3378498](https://doi.org/10.1145/3373376.3378498)

    - <span id="page-13-15"></span>[6] Mohammad Bakhshalipour, Pejman Lotfi-Kamran,
    and Hamid Sarbazi-Azad. 2018. Domino Temporal Data Prefetcher. In 2018 IEEE International
    Symposium on High Performance Computer Architecture (HPCA). 131–142. [doi:10.1109/HPCA.2018.](https://doi.org/10.1109/HPCA.2018.00021)
    [00021](https://doi.org/10.1109/HPCA.2018.00021)

    - <span id="page-13-11"></span>[7] Gloria B Barrett. 2000. The Coefficient of
    Determination: Understanding r squared and R squared. The Mathematics Teacher
    93, 3 (2000), 230–234.

    - <span id="page-13-16"></span>[8] Arkaprava Basu, Joseph L. Greathouse, Guru
    Venkataramani, and Jan Vesely. 2018. Interference from GPU System Service Requests.
    In 2018 IEEE International Symposium on Workload Characterization (IISWC).

    - <span id="page-13-9"></span>[9] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan
    Mark Liao. 2020. Yolov4: Optimal speed and accuracy of object detection. arXiv
    preprint arXiv:2004.10934 (2020).

    - <span id="page-13-8"></span>[10] Chia-Hao Chang, Adithya Kumar, and Anand Sivasubramaniam.
    2021. To move or not to move?: page migration for irregular applications in over-subscribed
    GPU memory systems with DynaMap. In ACM International Conference on Systems and
    Storage.

    - <span id="page-13-4"></span>[11] Steven Chien, Ivy Peng, and Stefano Markidis.
    2019. Performance Evaluation of Advanced Features in CUDA Unified Memory. In 2019
    IEEE/ACM Workshop on Memory Centric High Performance Computing (MCHPC). 50–57.
    [doi:10.1109/](https://doi.org/10.1109/MCHPC49590.2019.00014) [MCHPC49590.2019.00014](https://doi.org/10.1109/MCHPC49590.2019.00014)

    - <span id="page-13-5"></span>[12] Sangjin Choi, Taeksoo Kim, Jinwoo Jeong, Rachata
    Ausavarungnirun, Myeongjae Jeon, Youngjin Kwon, and Jeongseob Ahn. 2022. Memory
    harvesting in {Multi-GPU} systems with hierarchical unified virtual memory. In
    2022 USENIX Annual Technical Conference (USENIX ATC 22). USENIX Association, Carlsbad,
    CA, 625– 638.<https://www.usenix.org/conference/atc22/presentation/choi-sangjin>

    - <span id="page-13-18"></span>[13] Bennett Cooper, Thomas R. W. Scogland, and
    Rong Ge. 2024. Shared Virtual Memory: Its Design and Performance Implication for
    Diverse Applications. In Proceedings of the International Conference on Supercomputing
    (Kyoto, Japan). 25 – 37.

    - <span id="page-13-17"></span>[14] NVIDIA Corporation. Accessed Feb 2025. UVM
    GPU non-replayable faults. [https://github.com/NVIDIA/open-gpu-kernel-modules/blob/main/kernel](https://github.com/NVIDIA/open-gpu-kernel-modules/blob/main/kernel-open/nvidia-uvm/uvm_gpu_non_replayable_faults.c)[open/nvidia-uvm/uvm\\_gpu\\_non\\_replayable\\_faults.c.](https://github.com/NVIDIA/open-gpu-kernel-modules/blob/main/kernel-open/nvidia-uvm/uvm_gpu_non_replayable_faults.c)

    - <span id="page-13-20"></span>[15] NVIDIA Corporation. Accessed February 2025.
    NVIDIA H100 Tensor Core GPU Architecture. [https://resources.nvidia.com/en-us-data-center-overview/gtc22](https://resources.nvidia.com/en-us-data-center-overview/gtc22-whitepaper-hopper)
    [whitepaper-hopper.](https://resources.nvidia.com/en-us-data-center-overview/gtc22-whitepaper-hopper)

    - <span id="page-13-1"></span>[16] NVIDIA Corporation. Accessed June 2024. NVIDIA
    Pascal Architecture. [https:](https://www.nvidia.com/en-us/data-center/pascal-gpu-architecture/)
    [//www.nvidia.com/en-us/data-center/pascal-gpu-architecture/.](https://www.nvidia.com/en-us/data-center/pascal-gpu-architecture/)

    - <span id="page-13-2"></span>[17] NVIDIA Corporation. Accessed November 2024.
    NVIDIA Grace Hopper Superchip Architecture Whitepaper. [https://resources.nvidia.com/en-us-grace](https://resources.nvidia.com/en-us-grace-cpu/nvidia-grace-hopper)[cpu/nvidia-grace-hopper.](https://resources.nvidia.com/en-us-grace-cpu/nvidia-grace-hopper)

    - <span id="page-13-3"></span>[18] NVIDIA Corporation. Accessed November 2024.
    Tree-based Prefetcher for ATS (Grace Hopper). [https://github.com/NVIDIA/open-gpu-kernel-modules/blob/](https://github.com/NVIDIA/open-gpu-kernel-modules/blob/main/kernel-open/nvidia-uvm/uvm_ats_faults.c#L381-L464)
    [main/kernel-open/nvidia-uvm/uvm\\_ats\\_faults.c#L381-L464.](https://github.com/NVIDIA/open-gpu-kernel-modules/blob/main/kernel-open/nvidia-uvm/uvm_ats_faults.c#L381-L464)

    - <span id="page-13-10"></span>[19] NVIDIA Corporation. Accessed October 2024.
    Table of Technical Specifications per Compute Capability. [https://docs.nvidia.com/cuda/cuda](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#features-and-technical-specifications-technical-specifications-per-compute-capability)[c-programming-guide/index.html#features-and-technical-specifications](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#features-and-technical-specifications-technical-specifications-per-compute-capability)[technical-specifications-per-compute-capability.](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#features-and-technical-specifications-technical-specifications-per-compute-capability)

    - <span id="page-13-19"></span>[20] Jacob Devlin. 2018. Bert: Pre-training of
    deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805
    (2018).


    - <span id="page-14-19"></span>[21] Jacob Devlin, Ming-Wei Chang, Kenton Lee,
    and Kristina Toutanova. 2018. BERT: Pre-training of Deep Bidirectional Transformers
    for Language Understanding. CoRR abs/1810.04805 (2018). arXiv[:1810.04805](https://arxiv.org/abs/1810.04805)<http://arxiv.org/abs/1810.04805>

    - <span id="page-14-0"></span>[22] Xianzhong Ding, Yunkai Zhang, Binbin Chen,
    Donghao Ying, Tieying Zhang, Jianjun Chen, Lei Zhang, Alberto Cerpa, and Wan Du.
    2023. Vmr2l: Virtual machines rescheduling using reinforcement learning in data
    centers.

    - <span id="page-14-1"></span>[23] Xianzhong Ding, Yunkai Zhang, Binbin Chen,
    Donghao Ying, Tieying Zhang, Jianjun Chen, Lei Zhang, Alberto Cerpa, and Wan Du.
    2025. Towards VM Rescheduling Optimization Through Deep Reinforcement Learning.
    In Proceedings of the Twentieth European Conference on Computer Systems (Rotterdam,
    Netherlands) (EuroSys ''25). Association for Computing Machinery, New York, NY,
    USA, 686–701. [doi:10.1145/3689031.3717476](https://doi.org/10.1145/3689031.3717476)

    - <span id="page-14-20"></span>[24] Yuan Feng, Seonjin Na, Hyesoon Kim, and Hyeran
    Jeon. 2024. Barre Chord: Efficient Virtual Memory Translation for Multi-Chip-Module
    GPUs. In 2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture
    (ISCA). 834–847. [doi:10.1109/ISCA59077.2024.00065](https://doi.org/10.1109/ISCA59077.2024.00065)

    - <span id="page-14-13"></span>[25] Debashis Ganguly, Rami Melhem, and Jun Yang.
    2021. An Adaptive Framework for Oversubscription Management in CPU-GPU Unified
    Memory. In 2021 Design, Automation & Test in Europe Conference & Exhibition (DATE).
    1212–1217. [doi:10.](https://doi.org/10.23919/DATE51398.2021.9473982) [23919/DATE51398.2021.9473982](https://doi.org/10.23919/DATE51398.2021.9473982)

    - <span id="page-14-5"></span>[26] Debashis Ganguly, Ziyu Zhang, Jun Yang, and
    Rami Melhem. 2019. Interplay between hardware prefetcher and page eviction policy
    in CPU-GPU unified virtual memory. In Proceedings of the 46th International Symposium
    on Computer Architecture (, Phoenix, Arizona,) (ISCA ''19). Association for Computing
    Machinery, New York, NY, USA, 224–235. [doi:10.1145/3307650.3322224](https://doi.org/10.1145/3307650.3322224)

    - <span id="page-14-6"></span>[27] Debashis Ganguly, Ziyu Zhang, Jun Yang, and
    Rami Melhem. 2020. Adaptive Page Migration for Irregular Data-intensive Applications
    under GPU Memory Oversubscription. In 2020 IEEE International Parallel and Distributed
    Processing Symposium (IPDPS). 451–461. [doi:10.1109/IPDPS47924.2020.00054](https://doi.org/10.1109/IPDPS47924.2020.00054)

    - <span id="page-14-2"></span>[28] Amir Gholami, Zhewei Yao, Sehoon Kim, Coleman
    Hooper, Michael W. Mahoney, and Kurt Keutzer. 2024. AI and Memory Wall. IEEE Micro
    44, 3 (2024), 33–39. [doi:10.1109/MM.2024.3373763](https://doi.org/10.1109/MM.2024.3373763)

    - <span id="page-14-12"></span>[29] Seokjin Go, Hyunwuk Lee, Junsung Kim, Jiwon
    Lee, Myung Kuk Yoon, and Won Woo Ro. 2023. Early-Adaptor: An Adaptive Framework
    forProactive UVM Memory Management. In 2023 IEEE International Symposium on Performance
    Analysis of Systems and Software (ISPASS). 248–258. [doi:10.1109/ISPASS57527.](https://doi.org/10.1109/ISPASS57527.2023.00032)
    [2023.00032](https://doi.org/10.1109/ISPASS57527.2023.00032)

    - <span id="page-14-23"></span>[30] Yongbin Gu, Wenxuan Wu, Yunfan Li, and Lizhong
    Chen. 2020. Uvmbench: A comprehensive benchmark suite for researching unified
    virtual memory in gpus. arXiv preprint arXiv:2007.09822 (2020).

    - <span id="page-14-32"></span>[31] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and
    Jian Sun. 2016. Deep Residual Learning for Image Recognition. In 2016 IEEE Conference
    on Computer Vision and Pattern Recognition (CVPR). 770–778. [doi:10.1109/CVPR.2016.90](https://doi.org/10.1109/CVPR.2016.90)

    - <span id="page-14-27"></span>[32] Akanksha Jain and Calvin Lin. 2013. Linearizing
    irregular memory accesses for improved correlated prefetching. In Proceedings
    of the 46th Annual IEEE/ACM International Symposium on Microarchitecture (Davis,
    California) (MICRO-46). Association for Computing Machinery, New York, NY, USA,
    247–259. [doi:10.](https://doi.org/10.1145/2540708.2540730) [1145/2540708.2540730](https://doi.org/10.1145/2540708.2540730)

    - <span id="page-14-22"></span>[33] Gareth James, Daniela Witten, Trevor Hastie,
    Robert Tibshirani, and Jonathan Taylor. 2023. Linear regression. In An introduction
    to statistical learning: With applications in python. Springer, 69–134.

    - <span id="page-14-25"></span>[34] Doug Joseph and Dirk Grunwald. 1997. Prefetching
    using Markov predictors. SIGARCH Comput. Archit. News 25, 2 (May 1997), 252–263.
    [doi:10.1145/384286.](https://doi.org/10.1145/384286.264207) [264207](https://doi.org/10.1145/384286.264207)

    - <span id="page-14-16"></span>[35] Jaehoon Jung, Jinpyo Kim, and Jaejin Lee.
    2023. DeepUM: Tensor Migration and Prefetching in Unified Memory. In Proceedings
    of the 28th ACM International Conference on Architectural Support for Programming
    Languages and Operating Systems, Volume 2 (Vancouver, BC, Canada) (ASPLOS 2023).
    Association for Computing Machinery, New York, NY, USA, 207–221. [doi:10.1145/3575693.3575736](https://doi.org/10.1145/3575693.3575736)

    - <span id="page-14-24"></span>[36] Aajna Karki, Chethan Palangotu Keshava, Spoorthi
    Mysore Shivakumar, Joshua Skow, Goutam Madhukeshwar Hegde, and Hyeran Jeon. 2019.
    Tango: A Deep Neural Network Benchmark Suite for Various Accelerators. In 2019
    IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS).
    137–138. [doi:10.1109/ISPASS.2019.00021](https://doi.org/10.1109/ISPASS.2019.00021)

    - <span id="page-14-34"></span>[37] Mahmoud Khairy, Zhesheng Shen, Tor M. Aamodt,
    and Timothy G. Rogers. 2020. Accel-Sim: An Extensible Simulation Framework for
    Validated GPU Modeling. In 2020 ACM/IEEE 47th Annual International Symposium on
    Computer Architecture (ISCA). 473–486. [doi:10.1109/ISCA45697.2020.00047](https://doi.org/10.1109/ISCA45697.2020.00047)

    - <span id="page-14-14"></span>[38] Hyojong Kim, Jaewoong Sim, Prasun Gera, Ramyad
    Hadidi, and Hyesoon Kim. 2020. Batch-Aware Unified Memory Management in GPUs for
    Irregular Workloads. In Proceedings of the Twenty-Fifth International Conference
    on Architectural Support for Programming Languages and Operating Systems (Lausanne,
    Switzerland) (ASPLOS ''20). Association for Computing Machinery, New York, NY,
    USA, 1357–1370. [doi:10.1145/3373376.3378529](https://doi.org/10.1145/3373376.3378529)

    - <span id="page-14-31"></span>[39] Alex Krizhevsky, Ilya Sutskever, and Geoffrey
    E. Hinton. 2012. ImageNet classification with deep convolutional neural networks.
    (2012), 1097–1105.

    - <span id="page-14-26"></span>[40] An-Chow Lai, Cem Fide, and Babak Falsafi.
    2001. Dead-block prediction & dead-block correlating prefetchers. (2001), 144–154.
    [doi:10.1145/379240.379259](https://doi.org/10.1145/379240.379259)

    - <span id="page-14-8"></span>[41] Bingyao Li, Yanan Guo, Yueqi Wang, Aamer Jaleel,
    Jun Yang, and Xulong Tang. 2023. IDYLL: Enhancing Page Translation in Multi-GPUs
    via Light Weight PTE Invalidations. In Proceedings of the 56th Annual IEEE/ACM
    International Symposium on Microarchitecture. 1163–1177.

    - [42] Bingyao Li, Jieming Yin, Anup Holey, Youtao Zhang, Jun Yang, and Xulong
    Tang. 2023. Trans-FW: Short Circuiting Page Table Walk in Multi-GPU Systems via
    Remote Forwarding. In 2023 IEEE International Symposium on High-Performance Computer
    Architecture (HPCA). IEEE, 456–470. [doi:10.1109/HPCA56546.2023.](https://doi.org/10.1109/HPCA56546.2023.10071054)
    [10071054](https://doi.org/10.1109/HPCA56546.2023.10071054)

    - <span id="page-14-9"></span>[43] Bingyao Li, Jieming Yin, Youtao Zhang, and
    Xulong Tang. 2021. Improving address translation in multi-gpus via sharing and
    spilling aware tlb design. In MICRO-54: 54th Annual IEEE/ACM International Symposium
    on Microarchitecture. 1154–1168.

    - <span id="page-14-10"></span>[44] Chen Li, Rachata Ausavarungnirun, Christopher
    J. Rossbach, Youtao Zhang, Onur Mutlu, Yang Guo, and Jun Yang. 2019. A Framework
    for Memory Oversubscription Management in Graphics Processing Units. In Proceedings
    of the Twenty-Fourth International Conference on Architectural Support for Programming
    Languages and Operating Systems (Providence, RI, USA) (ASPLOS ''19). Association
    for Computing Machinery, New York, NY, USA, 49–63. [doi:10.1145/3297858.3304044](https://doi.org/10.1145/3297858.3304044)

    - <span id="page-14-11"></span>[45] Lingda Li and Barbara Chapman. 2019. Compiler
    assisted hybrid implicit and explicit GPU memory management under unified address
    space. In Proceedings of the International Conference for High Performance Computing,
    Networking, Storage and Analysis (Denver, Colorado) (SC ''19). Association for
    Computing Machinery, New York, NY, USA, Article 51, 16 pages. [doi:10.1145/3295500.3356141](https://doi.org/10.1145/3295500.3356141)

    - <span id="page-14-15"></span>[46] Mao Lin and Hyeran Jeon. 2025. Understanding
    Oversubscribed Memory Management for Deep Learning Training. In Proceedings of
    the 5th Workshop on Machine Learning and Systems (World Trade Center, Rotterdam,
    Netherlands) (EuroMLSys ''25). Association for Computing Machinery, New York,
    NY, USA, 46–55. [doi:10.1145/3721146.3721955](https://doi.org/10.1145/3721146.3721955)

    - <span id="page-14-3"></span>[47] Mao Lin, Keren Zhou, and Pengfei Su. 2023.
    DrGPUM: Guiding Memory Optimization for GPU-Accelerated Applications. In Proceedings
    of the 28th ACM International Conference on Architectural Support for Programming
    Languages and Operating Systems, Volume 3 (Vancouver, BC, Canada) (ASPLOS 2023).
    Association for Computing Machinery, New York, NY, USA, 164–178. [doi:10.1145/3582016.3582044](https://doi.org/10.1145/3582016.3582044)

    - <span id="page-14-17"></span>[48] Yechen Liu, Timothy Rogers, and Clayton Hughes.
    2022. Unified Memory: GPGPU-Sim/UVM Smart Integration. SANDIA REPORT (2022). [https://www.](https://www.osti.gov/servlets/purl/1844477/)
    [osti.gov/servlets/purl/1844477/](https://www.osti.gov/servlets/purl/1844477/)

    - <span id="page-14-29"></span>[49] Seung Won Min, Kun Wu, Sitao Huang, Mert Hidayetoğlu,
    Jinjun Xiong, Eiman Ebrahimi, Deming Chen, and Wen-mei Hwu. 2021. Large graph
    convolutional network training with GPU-oriented data communication architecture.
    Proc. VLDB Endow. 14, 11 (July 2021), 2087–2100. [doi:10.14778/3476249.3476264](https://doi.org/10.14778/3476249.3476264)

    - <span id="page-14-7"></span>[50] Saba Mostofi, Hajar Falahati, Negin Mahani,
    Pejman Lotfi-Kamran, and Hamid Sarbazi-Azad. 2023. Snake: A Variable-length Chain-based
    Prefetching for GPUs. In Proceedings of the 56th Annual IEEE/ACM International
    Symposium on Microarchitecture (Toronto, ON, Canada) (MICRO ''23). Association
    for Computing Machinery, New York, NY, USA, 728–741. [doi:10.1145/3613424.3623782](https://doi.org/10.1145/3613424.3623782)

    - <span id="page-14-21"></span>[51] NVIDIA Corporation. Accessed June 2024. NVIDIA
    Linux open GPU kernel module source. [https://github.com/NVIDIA/open-gpu-kernel-modules.](https://github.com/NVIDIA/open-gpu-kernel-modules)

    - <span id="page-14-33"></span>[52] Alec Radford, Jong Wook Kim, Tao Xu, Greg
    Brockman, Christine McLeavey, and Ilya Sutskever. 2023. Robust speech recognition
    via large-scale weak supervision. In Proceedings of the 40th International Conference
    on Machine Learning (Honolulu, Hawaii, USA) (ICML''23). JMLR.org, Article 1182,
    27 pages.

    - <span id="page-14-4"></span>[53] Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley,
    Shaden Smith, and Yuxiong He. 2021. ZeRO-infinity: breaking the GPU memory wall
    for extreme scale deep learning. In Proceedings of the International Conference
    for High Performance Computing, Networking, Storage and Analysis (St. Louis, Missouri)
    (SC ''21). Association for Computing Machinery, New York, NY, USA, Article 59,
    14 pages. [doi:10.1145/3458817.3476205](https://doi.org/10.1145/3458817.3476205)

    - <span id="page-14-30"></span>[54] Jie Ren, Dong Xu, Shuangyan Yang, Jiacheng
    Zhao, Zhicheng Li, Christian Navasca, Chenxi Wang, Harry Xu, and Dong Li. 2024.
    Enabling Large Dynamic Neural Network Training with Learning-based Memory Management.
    In 2024 IEEE International Symposium on High-Performance Computer Architecture
    (HPCA). 788–802. [doi:10.1109/HPCA57654.2024.00066](https://doi.org/10.1109/HPCA57654.2024.00066)

    - <span id="page-14-36"></span>[55] Tor Aamodt''s research group. Accessed July
    2024. GPGPU-sim distribution. [https://github.com/gpgpu-sim/gpgpu-sim\\_distribution/blob/master/](https://github.com/gpgpu-sim/gpgpu-sim_distribution/blob/master/configs/tested-cfgs/SM75_RTX2060/gpgpusim.config)
    [configs/tested-cfgs/SM75\\_RTX2060/gpgpusim.config.](https://github.com/gpgpu-sim/gpgpu-sim_distribution/blob/master/configs/tested-cfgs/SM75_RTX2060/gpgpusim.config)

    - <span id="page-14-18"></span>[56] Nikolay Sakharnykh. Accessed June 2024. Unified
    memory on pascal and volta. [https://on-demand.gputechconf.com/gtc/2017/presentation/s7285-nikolay](https://on-demand.gputechconf.com/gtc/2017/presentation/s7285-nikolay-sakharnykh-unified-memory-on-pascal-and-volta.pdf)[sakharnykh-unified-memory-on-pascal-and-volta.pdf.](https://on-demand.gputechconf.com/gtc/2017/presentation/s7285-nikolay-sakharnykh-unified-memory-on-pascal-and-volta.pdf)

    - <span id="page-14-28"></span>[57] Gabin Schieffer, Jacob Wahlgren, Jie Ren,
    Jennifer Faj, and Ivy Peng. 2024. Harnessing Integrated CPU-GPU System Memory
    for HPC: a first look into Grace Hopper. International Conference on Parallel
    Processing (2024).

    - <span id="page-14-37"></span>[58] Accel-Sim team. Accessed October 2024. GPGPU-sim
    distribution. [https://github.com/accel-sim/gpgpu-sim\\_distribution/blob/dev](https://github.com/accel-sim/gpgpu-sim_distribution/blob/dev-uvm/configs/tested-cfgs/SM86_RTX3070/gpgpusim.config)[uvm/configs/tested-cfgs/SM86\\_RTX3070/gpgpusim.config.](https://github.com/accel-sim/gpgpu-sim_distribution/blob/dev-uvm/configs/tested-cfgs/SM86_RTX3070/gpgpusim.config)

    - <span id="page-14-35"></span>[59] Accel-Sim team. Accessed October 2024. GPGPU-sim
    distribution (Accelsim). [https://github.com/accel-sim/gpgpu-sim\\_distribution/blob/dev-uvm/](https://github.com/accel-sim/gpgpu-sim_distribution/blob/dev-uvm/configs/tested-cfgs/SM7_QV100/gpgpusim.config)


    <span id="page-15-0"></span>[configs/tested-cfgs/SM7\\_QV100/gpgpusim.config.](https://github.com/accel-sim/gpgpu-sim_distribution/blob/dev-uvm/configs/tested-cfgs/SM7_QV100/gpgpusim.config)


    - <span id="page-15-1"></span>[60] Mark Weiser. 1984. Program slicing. IEEE Transactions
    on software engineering 4 (1984), 352–357.

    - <span id="page-15-4"></span>[61] Hao Wu, Krishnendra Nathella, Dam Sunwoo, Akanksha
    Jain, and Calvin Lin. 2019. Efficient Metadata Management for Irregular Data Prefetching.
    In 2019 ACM/IEEE 46th Annual International Symposium on Computer Architecture
    (ISCA). 1–13.

    - <span id="page-15-3"></span>[62] Haoyang Zhang, Yirui Zhou, Yuqi Xue, Yiqi Liu,
    and Jian Huang. 2023. G10: Enabling An Efficient Unified GPU Memory and Storage
    Architecture with Smart Tensor Migrations. In Proceedings of the 56th Annual IEEE/ACM
    International


    Symposium on Microarchitecture (Toronto, ON, Canada) (MICRO ''23). Association
    for Computing Machinery, New York, NY, USA, 395–410. [doi:10.1145/3613424.](https://doi.org/10.1145/3613424.3614309)
    [3614309](https://doi.org/10.1145/3613424.3614309)


    <span id="page-15-2"></span>[63] Yuxuan Zhang, Nathan Sobotka, Soyoon Park, Saba
    Jamilan, Tanvir Ahmed Khan, Baris Kasikci, Gilles A Pokam, Heiner Litz, and Joseph
    Devietti. 2024. RPG2: Robust Profile-Guided Runtime Prefetch Generation. In Proceedings
    of the 29th ACM International Conference on Architectural Support for Programming
    Languages and Operating Systems, Volume 2 (La Jolla, CA, USA) (ASPLOS ''24). Association
    for Computing Machinery, New York, NY, USA, 999–1013. [doi:10.](https://doi.org/10.1145/3620665.3640396)
    [1145/3620665.3640396](https://doi.org/10.1145/3620665.3640396)'
  references:
  - '- <span id="page-13-14"></span>[1] Thomas Alexander and Gershon Kedem. 1996.
    Distributed Prefetch-buffer/Cache Design for High Performance Memory Systems.
    In Proceedings of the 2nd IEEE Symposium on High-Performance Computer Architecture
    (HPCA ''96). IEEE Computer Society, USA, 254.'
  - '- <span id="page-13-6"></span>[2] Tyler Allen and Rong Ge. 2021. Demystifying
    GPU UVM Cost with Deep Runtime and Workload Analysis. In 2021 IEEE International
    Parallel and Distributed Processing Symposium (IPDPS). 141–150. [doi:10.1109/IPDPS49936.2021.00023](https://doi.org/10.1109/IPDPS49936.2021.00023)'
  - '- <span id="page-13-7"></span>[3] Tyler Allen and Rong Ge. 2021. In-depth analyses
    of unified virtual memory system for GPU accelerated computing. In Proceedings
    of the International Conference for High Performance Computing, Networking, Storage
    and Analysis (St. Louis, Missouri) (SC ''21). Association for Computing Machinery,
    New York, NY, USA, Article 64, 15 pages. [doi:10.1145/3458817.3480855](https://doi.org/10.1145/3458817.3480855)'
  - '- <span id="page-13-0"></span>[4] AMD Corporation. Accessed June 2024. Radeons
    Next-generation Vega Architecture. [https://en.wikichip.org/w/images/a/a1/vega-whitepaper.pdf.](https://en.wikichip.org/w/images/a/a1/vega-whitepaper.pdf)'
  - '- <span id="page-13-13"></span>[5] Grant Ayers, Heiner Litz, Christos Kozyrakis,
    and Parthasarathy Ranganathan. 2020. Classifying Memory Access Patterns for Prefetching
    (ASPLOS ''20). Association for Computing Machinery, New York, NY, USA, 513–526.
    [doi:10.1145/](https://doi.org/10.1145/3373376.3378498) [3373376.3378498](https://doi.org/10.1145/3373376.3378498)'
  - '- <span id="page-13-15"></span>[6] Mohammad Bakhshalipour, Pejman Lotfi-Kamran,
    and Hamid Sarbazi-Azad. 2018. Domino Temporal Data Prefetcher. In 2018 IEEE International
    Symposium on High Performance Computer Architecture (HPCA). 131–142. [doi:10.1109/HPCA.2018.](https://doi.org/10.1109/HPCA.2018.00021)
    [00021](https://doi.org/10.1109/HPCA.2018.00021)'
  - '- <span id="page-13-11"></span>[7] Gloria B Barrett. 2000. The Coefficient of
    Determination: Understanding r squared and R squared. The Mathematics Teacher
    93, 3 (2000), 230–234.'
  - '- <span id="page-13-16"></span>[8] Arkaprava Basu, Joseph L. Greathouse, Guru
    Venkataramani, and Jan Vesely. 2018. Interference from GPU System Service Requests.
    In 2018 IEEE International Symposium on Workload Characterization (IISWC).'
  - '- <span id="page-13-9"></span>[9] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan
    Mark Liao. 2020. Yolov4: Optimal speed and accuracy of object detection. arXiv
    preprint arXiv:2004.10934 (2020).'
  - '- <span id="page-13-8"></span>[10] Chia-Hao Chang, Adithya Kumar, and Anand Sivasubramaniam.
    2021. To move or not to move?: page migration for irregular applications in over-subscribed
    GPU memory systems with DynaMap. In ACM International Conference on Systems and
    Storage.'
  - '- <span id="page-13-4"></span>[11] Steven Chien, Ivy Peng, and Stefano Markidis.
    2019. Performance Evaluation of Advanced Features in CUDA Unified Memory. In 2019
    IEEE/ACM Workshop on Memory Centric High Performance Computing (MCHPC). 50–57.
    [doi:10.1109/](https://doi.org/10.1109/MCHPC49590.2019.00014) [MCHPC49590.2019.00014](https://doi.org/10.1109/MCHPC49590.2019.00014)'
  - '- <span id="page-13-5"></span>[12] Sangjin Choi, Taeksoo Kim, Jinwoo Jeong, Rachata
    Ausavarungnirun, Myeongjae Jeon, Youngjin Kwon, and Jeongseob Ahn. 2022. Memory
    harvesting in {Multi-GPU} systems with hierarchical unified virtual memory. In
    2022 USENIX Annual Technical Conference (USENIX ATC 22). USENIX Association, Carlsbad,
    CA, 625– 638.<https://www.usenix.org/conference/atc22/presentation/choi-sangjin>'
  - '- <span id="page-13-18"></span>[13] Bennett Cooper, Thomas R. W. Scogland, and
    Rong Ge. 2024. Shared Virtual Memory: Its Design and Performance Implication for
    Diverse Applications. In Proceedings of the International Conference on Supercomputing
    (Kyoto, Japan). 25 – 37.'
  - '- <span id="page-13-17"></span>[14] NVIDIA Corporation. Accessed Feb 2025. UVM
    GPU non-replayable faults. [https://github.com/NVIDIA/open-gpu-kernel-modules/blob/main/kernel](https://github.com/NVIDIA/open-gpu-kernel-modules/blob/main/kernel-open/nvidia-uvm/uvm_gpu_non_replayable_faults.c)[open/nvidia-uvm/uvm\\_gpu\\_non\\_replayable\\_faults.c.](https://github.com/NVIDIA/open-gpu-kernel-modules/blob/main/kernel-open/nvidia-uvm/uvm_gpu_non_replayable_faults.c)'
  - '- <span id="page-13-20"></span>[15] NVIDIA Corporation. Accessed February 2025.
    NVIDIA H100 Tensor Core GPU Architecture. [https://resources.nvidia.com/en-us-data-center-overview/gtc22](https://resources.nvidia.com/en-us-data-center-overview/gtc22-whitepaper-hopper)
    [whitepaper-hopper.](https://resources.nvidia.com/en-us-data-center-overview/gtc22-whitepaper-hopper)'
  - '- <span id="page-13-1"></span>[16] NVIDIA Corporation. Accessed June 2024. NVIDIA
    Pascal Architecture. [https:](https://www.nvidia.com/en-us/data-center/pascal-gpu-architecture/)
    [//www.nvidia.com/en-us/data-center/pascal-gpu-architecture/.](https://www.nvidia.com/en-us/data-center/pascal-gpu-architecture/)'
  - '- <span id="page-13-2"></span>[17] NVIDIA Corporation. Accessed November 2024.
    NVIDIA Grace Hopper Superchip Architecture Whitepaper. [https://resources.nvidia.com/en-us-grace](https://resources.nvidia.com/en-us-grace-cpu/nvidia-grace-hopper)[cpu/nvidia-grace-hopper.](https://resources.nvidia.com/en-us-grace-cpu/nvidia-grace-hopper)'
  - '- <span id="page-13-3"></span>[18] NVIDIA Corporation. Accessed November 2024.
    Tree-based Prefetcher for ATS (Grace Hopper). [https://github.com/NVIDIA/open-gpu-kernel-modules/blob/](https://github.com/NVIDIA/open-gpu-kernel-modules/blob/main/kernel-open/nvidia-uvm/uvm_ats_faults.c#L381-L464)
    [main/kernel-open/nvidia-uvm/uvm\\_ats\\_faults.c#L381-L464.](https://github.com/NVIDIA/open-gpu-kernel-modules/blob/main/kernel-open/nvidia-uvm/uvm_ats_faults.c#L381-L464)'
  - '- <span id="page-13-10"></span>[19] NVIDIA Corporation. Accessed October 2024.
    Table of Technical Specifications per Compute Capability. [https://docs.nvidia.com/cuda/cuda](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#features-and-technical-specifications-technical-specifications-per-compute-capability)[c-programming-guide/index.html#features-and-technical-specifications](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#features-and-technical-specifications-technical-specifications-per-compute-capability)[technical-specifications-per-compute-capability.](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#features-and-technical-specifications-technical-specifications-per-compute-capability)'
  - '- <span id="page-13-19"></span>[20] Jacob Devlin. 2018. Bert: Pre-training of
    deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805
    (2018).'
  - '- <span id="page-14-19"></span>[21] Jacob Devlin, Ming-Wei Chang, Kenton Lee,
    and Kristina Toutanova. 2018. BERT: Pre-training of Deep Bidirectional Transformers
    for Language Understanding. CoRR abs/1810.04805 (2018). arXiv[:1810.04805](https://arxiv.org/abs/1810.04805)<http://arxiv.org/abs/1810.04805>'
  - '- <span id="page-14-0"></span>[22] Xianzhong Ding, Yunkai Zhang, Binbin Chen,
    Donghao Ying, Tieying Zhang, Jianjun Chen, Lei Zhang, Alberto Cerpa, and Wan Du.
    2023. Vmr2l: Virtual machines rescheduling using reinforcement learning in data
    centers.'
  - '- <span id="page-14-1"></span>[23] Xianzhong Ding, Yunkai Zhang, Binbin Chen,
    Donghao Ying, Tieying Zhang, Jianjun Chen, Lei Zhang, Alberto Cerpa, and Wan Du.
    2025. Towards VM Rescheduling Optimization Through Deep Reinforcement Learning.
    In Proceedings of the Twentieth European Conference on Computer Systems (Rotterdam,
    Netherlands) (EuroSys ''25). Association for Computing Machinery, New York, NY,
    USA, 686–701. [doi:10.1145/3689031.3717476](https://doi.org/10.1145/3689031.3717476)'
  - '- <span id="page-14-20"></span>[24] Yuan Feng, Seonjin Na, Hyesoon Kim, and Hyeran
    Jeon. 2024. Barre Chord: Efficient Virtual Memory Translation for Multi-Chip-Module
    GPUs. In 2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture
    (ISCA). 834–847. [doi:10.1109/ISCA59077.2024.00065](https://doi.org/10.1109/ISCA59077.2024.00065)'
  - '- <span id="page-14-13"></span>[25] Debashis Ganguly, Rami Melhem, and Jun Yang.
    2021. An Adaptive Framework for Oversubscription Management in CPU-GPU Unified
    Memory. In 2021 Design, Automation & Test in Europe Conference & Exhibition (DATE).
    1212–1217. [doi:10.](https://doi.org/10.23919/DATE51398.2021.9473982) [23919/DATE51398.2021.9473982](https://doi.org/10.23919/DATE51398.2021.9473982)'
  - '- <span id="page-14-5"></span>[26] Debashis Ganguly, Ziyu Zhang, Jun Yang, and
    Rami Melhem. 2019. Interplay between hardware prefetcher and page eviction policy
    in CPU-GPU unified virtual memory. In Proceedings of the 46th International Symposium
    on Computer Architecture (, Phoenix, Arizona,) (ISCA ''19). Association for Computing
    Machinery, New York, NY, USA, 224–235. [doi:10.1145/3307650.3322224](https://doi.org/10.1145/3307650.3322224)'
  - '- <span id="page-14-6"></span>[27] Debashis Ganguly, Ziyu Zhang, Jun Yang, and
    Rami Melhem. 2020. Adaptive Page Migration for Irregular Data-intensive Applications
    under GPU Memory Oversubscription. In 2020 IEEE International Parallel and Distributed
    Processing Symposium (IPDPS). 451–461. [doi:10.1109/IPDPS47924.2020.00054](https://doi.org/10.1109/IPDPS47924.2020.00054)'
  - '- <span id="page-14-2"></span>[28] Amir Gholami, Zhewei Yao, Sehoon Kim, Coleman
    Hooper, Michael W. Mahoney, and Kurt Keutzer. 2024. AI and Memory Wall. IEEE Micro
    44, 3 (2024), 33–39. [doi:10.1109/MM.2024.3373763](https://doi.org/10.1109/MM.2024.3373763)'
  - '- <span id="page-14-12"></span>[29] Seokjin Go, Hyunwuk Lee, Junsung Kim, Jiwon
    Lee, Myung Kuk Yoon, and Won Woo Ro. 2023. Early-Adaptor: An Adaptive Framework
    forProactive UVM Memory Management. In 2023 IEEE International Symposium on Performance
    Analysis of Systems and Software (ISPASS). 248–258. [doi:10.1109/ISPASS57527.](https://doi.org/10.1109/ISPASS57527.2023.00032)
    [2023.00032](https://doi.org/10.1109/ISPASS57527.2023.00032)'
  - '- <span id="page-14-23"></span>[30] Yongbin Gu, Wenxuan Wu, Yunfan Li, and Lizhong
    Chen. 2020. Uvmbench: A comprehensive benchmark suite for researching unified
    virtual memory in gpus. arXiv preprint arXiv:2007.09822 (2020).'
  - '- <span id="page-14-32"></span>[31] Kaiming He, Xiangyu Zhang, Shaoqing Ren,
    and Jian Sun. 2016. Deep Residual Learning for Image Recognition. In 2016 IEEE
    Conference on Computer Vision and Pattern Recognition (CVPR). 770–778. [doi:10.1109/CVPR.2016.90](https://doi.org/10.1109/CVPR.2016.90)'
  - '- <span id="page-14-27"></span>[32] Akanksha Jain and Calvin Lin. 2013. Linearizing
    irregular memory accesses for improved correlated prefetching. In Proceedings
    of the 46th Annual IEEE/ACM International Symposium on Microarchitecture (Davis,
    California) (MICRO-46). Association for Computing Machinery, New York, NY, USA,
    247–259. [doi:10.](https://doi.org/10.1145/2540708.2540730) [1145/2540708.2540730](https://doi.org/10.1145/2540708.2540730)'
  - '- <span id="page-14-22"></span>[33] Gareth James, Daniela Witten, Trevor Hastie,
    Robert Tibshirani, and Jonathan Taylor. 2023. Linear regression. In An introduction
    to statistical learning: With applications in python. Springer, 69–134.'
  - '- <span id="page-14-25"></span>[34] Doug Joseph and Dirk Grunwald. 1997. Prefetching
    using Markov predictors. SIGARCH Comput. Archit. News 25, 2 (May 1997), 252–263.
    [doi:10.1145/384286.](https://doi.org/10.1145/384286.264207) [264207](https://doi.org/10.1145/384286.264207)'
  - '- <span id="page-14-16"></span>[35] Jaehoon Jung, Jinpyo Kim, and Jaejin Lee.
    2023. DeepUM: Tensor Migration and Prefetching in Unified Memory. In Proceedings
    of the 28th ACM International Conference on Architectural Support for Programming
    Languages and Operating Systems, Volume 2 (Vancouver, BC, Canada) (ASPLOS 2023).
    Association for Computing Machinery, New York, NY, USA, 207–221. [doi:10.1145/3575693.3575736](https://doi.org/10.1145/3575693.3575736)'
  - '- <span id="page-14-24"></span>[36] Aajna Karki, Chethan Palangotu Keshava, Spoorthi
    Mysore Shivakumar, Joshua Skow, Goutam Madhukeshwar Hegde, and Hyeran Jeon. 2019.
    Tango: A Deep Neural Network Benchmark Suite for Various Accelerators. In 2019
    IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS).
    137–138. [doi:10.1109/ISPASS.2019.00021](https://doi.org/10.1109/ISPASS.2019.00021)'
  - '- <span id="page-14-34"></span>[37] Mahmoud Khairy, Zhesheng Shen, Tor M. Aamodt,
    and Timothy G. Rogers. 2020. Accel-Sim: An Extensible Simulation Framework for
    Validated GPU Modeling. In 2020 ACM/IEEE 47th Annual International Symposium on
    Computer Architecture (ISCA). 473–486. [doi:10.1109/ISCA45697.2020.00047](https://doi.org/10.1109/ISCA45697.2020.00047)'
  - '- <span id="page-14-14"></span>[38] Hyojong Kim, Jaewoong Sim, Prasun Gera, Ramyad
    Hadidi, and Hyesoon Kim. 2020. Batch-Aware Unified Memory Management in GPUs for
    Irregular Workloads. In Proceedings of the Twenty-Fifth International Conference
    on Architectural Support for Programming Languages and Operating Systems (Lausanne,
    Switzerland) (ASPLOS ''20). Association for Computing Machinery, New York, NY,
    USA, 1357–1370. [doi:10.1145/3373376.3378529](https://doi.org/10.1145/3373376.3378529)'
  - '- <span id="page-14-31"></span>[39] Alex Krizhevsky, Ilya Sutskever, and Geoffrey
    E. Hinton. 2012. ImageNet classification with deep convolutional neural networks.
    (2012), 1097–1105.'
  - '- <span id="page-14-26"></span>[40] An-Chow Lai, Cem Fide, and Babak Falsafi.
    2001. Dead-block prediction & dead-block correlating prefetchers. (2001), 144–154.
    [doi:10.1145/379240.379259](https://doi.org/10.1145/379240.379259)'
  - '- <span id="page-14-8"></span>[41] Bingyao Li, Yanan Guo, Yueqi Wang, Aamer Jaleel,
    Jun Yang, and Xulong Tang. 2023. IDYLL: Enhancing Page Translation in Multi-GPUs
    via Light Weight PTE Invalidations. In Proceedings of the 56th Annual IEEE/ACM
    International Symposium on Microarchitecture. 1163–1177.'
  - '- [42] Bingyao Li, Jieming Yin, Anup Holey, Youtao Zhang, Jun Yang, and Xulong
    Tang. 2023. Trans-FW: Short Circuiting Page Table Walk in Multi-GPU Systems via
    Remote Forwarding. In 2023 IEEE International Symposium on High-Performance Computer
    Architecture (HPCA). IEEE, 456–470. [doi:10.1109/HPCA56546.2023.](https://doi.org/10.1109/HPCA56546.2023.10071054)
    [10071054](https://doi.org/10.1109/HPCA56546.2023.10071054)'
  - '- <span id="page-14-9"></span>[43] Bingyao Li, Jieming Yin, Youtao Zhang, and
    Xulong Tang. 2021. Improving address translation in multi-gpus via sharing and
    spilling aware tlb design. In MICRO-54: 54th Annual IEEE/ACM International Symposium
    on Microarchitecture. 1154–1168.'
  - '- <span id="page-14-10"></span>[44] Chen Li, Rachata Ausavarungnirun, Christopher
    J. Rossbach, Youtao Zhang, Onur Mutlu, Yang Guo, and Jun Yang. 2019. A Framework
    for Memory Oversubscription Management in Graphics Processing Units. In Proceedings
    of the Twenty-Fourth International Conference on Architectural Support for Programming
    Languages and Operating Systems (Providence, RI, USA) (ASPLOS ''19). Association
    for Computing Machinery, New York, NY, USA, 49–63. [doi:10.1145/3297858.3304044](https://doi.org/10.1145/3297858.3304044)'
  - '- <span id="page-14-11"></span>[45] Lingda Li and Barbara Chapman. 2019. Compiler
    assisted hybrid implicit and explicit GPU memory management under unified address
    space. In Proceedings of the International Conference for High Performance Computing,
    Networking, Storage and Analysis (Denver, Colorado) (SC ''19). Association for
    Computing Machinery, New York, NY, USA, Article 51, 16 pages. [doi:10.1145/3295500.3356141](https://doi.org/10.1145/3295500.3356141)'
  - '- <span id="page-14-15"></span>[46] Mao Lin and Hyeran Jeon. 2025. Understanding
    Oversubscribed Memory Management for Deep Learning Training. In Proceedings of
    the 5th Workshop on Machine Learning and Systems (World Trade Center, Rotterdam,
    Netherlands) (EuroMLSys ''25). Association for Computing Machinery, New York,
    NY, USA, 46–55. [doi:10.1145/3721146.3721955](https://doi.org/10.1145/3721146.3721955)'
  - '- <span id="page-14-3"></span>[47] Mao Lin, Keren Zhou, and Pengfei Su. 2023.
    DrGPUM: Guiding Memory Optimization for GPU-Accelerated Applications. In Proceedings
    of the 28th ACM International Conference on Architectural Support for Programming
    Languages and Operating Systems, Volume 3 (Vancouver, BC, Canada) (ASPLOS 2023).
    Association for Computing Machinery, New York, NY, USA, 164–178. [doi:10.1145/3582016.3582044](https://doi.org/10.1145/3582016.3582044)'
  - '- <span id="page-14-17"></span>[48] Yechen Liu, Timothy Rogers, and Clayton Hughes.
    2022. Unified Memory: GPGPU-Sim/UVM Smart Integration. SANDIA REPORT (2022). [https://www.](https://www.osti.gov/servlets/purl/1844477/)
    [osti.gov/servlets/purl/1844477/](https://www.osti.gov/servlets/purl/1844477/)'
  - '- <span id="page-14-29"></span>[49] Seung Won Min, Kun Wu, Sitao Huang, Mert
    Hidayetoğlu, Jinjun Xiong, Eiman Ebrahimi, Deming Chen, and Wen-mei Hwu. 2021.
    Large graph convolutional network training with GPU-oriented data communication
    architecture. Proc. VLDB Endow. 14, 11 (July 2021), 2087–2100. [doi:10.14778/3476249.3476264](https://doi.org/10.14778/3476249.3476264)'
  - '- <span id="page-14-7"></span>[50] Saba Mostofi, Hajar Falahati, Negin Mahani,
    Pejman Lotfi-Kamran, and Hamid Sarbazi-Azad. 2023. Snake: A Variable-length Chain-based
    Prefetching for GPUs. In Proceedings of the 56th Annual IEEE/ACM International
    Symposium on Microarchitecture (Toronto, ON, Canada) (MICRO ''23). Association
    for Computing Machinery, New York, NY, USA, 728–741. [doi:10.1145/3613424.3623782](https://doi.org/10.1145/3613424.3623782)'
  - '- <span id="page-14-21"></span>[51] NVIDIA Corporation. Accessed June 2024. NVIDIA
    Linux open GPU kernel module source. [https://github.com/NVIDIA/open-gpu-kernel-modules.](https://github.com/NVIDIA/open-gpu-kernel-modules)'
  - '- <span id="page-14-33"></span>[52] Alec Radford, Jong Wook Kim, Tao Xu, Greg
    Brockman, Christine McLeavey, and Ilya Sutskever. 2023. Robust speech recognition
    via large-scale weak supervision. In Proceedings of the 40th International Conference
    on Machine Learning (Honolulu, Hawaii, USA) (ICML''23). JMLR.org, Article 1182,
    27 pages.'
  - '- <span id="page-14-4"></span>[53] Samyam Rajbhandari, Olatunji Ruwase, Jeff
    Rasley, Shaden Smith, and Yuxiong He. 2021. ZeRO-infinity: breaking the GPU memory
    wall for extreme scale deep learning. In Proceedings of the International Conference
    for High Performance Computing, Networking, Storage and Analysis (St. Louis, Missouri)
    (SC ''21). Association for Computing Machinery, New York, NY, USA, Article 59,
    14 pages. [doi:10.1145/3458817.3476205](https://doi.org/10.1145/3458817.3476205)'
  - '- <span id="page-14-30"></span>[54] Jie Ren, Dong Xu, Shuangyan Yang, Jiacheng
    Zhao, Zhicheng Li, Christian Navasca, Chenxi Wang, Harry Xu, and Dong Li. 2024.
    Enabling Large Dynamic Neural Network Training with Learning-based Memory Management.
    In 2024 IEEE International Symposium on High-Performance Computer Architecture
    (HPCA). 788–802. [doi:10.1109/HPCA57654.2024.00066](https://doi.org/10.1109/HPCA57654.2024.00066)'
  - '- <span id="page-14-36"></span>[55] Tor Aamodt''s research group. Accessed July
    2024. GPGPU-sim distribution. [https://github.com/gpgpu-sim/gpgpu-sim\\_distribution/blob/master/](https://github.com/gpgpu-sim/gpgpu-sim_distribution/blob/master/configs/tested-cfgs/SM75_RTX2060/gpgpusim.config)
    [configs/tested-cfgs/SM75\\_RTX2060/gpgpusim.config.](https://github.com/gpgpu-sim/gpgpu-sim_distribution/blob/master/configs/tested-cfgs/SM75_RTX2060/gpgpusim.config)'
  - '- <span id="page-14-18"></span>[56] Nikolay Sakharnykh. Accessed June 2024. Unified
    memory on pascal and volta. [https://on-demand.gputechconf.com/gtc/2017/presentation/s7285-nikolay](https://on-demand.gputechconf.com/gtc/2017/presentation/s7285-nikolay-sakharnykh-unified-memory-on-pascal-and-volta.pdf)[sakharnykh-unified-memory-on-pascal-and-volta.pdf.](https://on-demand.gputechconf.com/gtc/2017/presentation/s7285-nikolay-sakharnykh-unified-memory-on-pascal-and-volta.pdf)'
  - '- <span id="page-14-28"></span>[57] Gabin Schieffer, Jacob Wahlgren, Jie Ren,
    Jennifer Faj, and Ivy Peng. 2024. Harnessing Integrated CPU-GPU System Memory
    for HPC: a first look into Grace Hopper. International Conference on Parallel
    Processing (2024).'
  - '- <span id="page-14-37"></span>[58] Accel-Sim team. Accessed October 2024. GPGPU-sim
    distribution. [https://github.com/accel-sim/gpgpu-sim\\_distribution/blob/dev](https://github.com/accel-sim/gpgpu-sim_distribution/blob/dev-uvm/configs/tested-cfgs/SM86_RTX3070/gpgpusim.config)[uvm/configs/tested-cfgs/SM86\\_RTX3070/gpgpusim.config.](https://github.com/accel-sim/gpgpu-sim_distribution/blob/dev-uvm/configs/tested-cfgs/SM86_RTX3070/gpgpusim.config)'
  - '- <span id="page-14-35"></span>[59] Accel-Sim team. Accessed October 2024. GPGPU-sim
    distribution (Accelsim). [https://github.com/accel-sim/gpgpu-sim\\_distribution/blob/dev-uvm/](https://github.com/accel-sim/gpgpu-sim_distribution/blob/dev-uvm/configs/tested-cfgs/SM7_QV100/gpgpusim.config)'
  - <span id="page-15-0"></span>[configs/tested-cfgs/SM7\\_QV100/gpgpusim.config.](https://github.com/accel-sim/gpgpu-sim_distribution/blob/dev-uvm/configs/tested-cfgs/SM7_QV100/gpgpusim.config)
  - '- <span id="page-15-1"></span>[60] Mark Weiser. 1984. Program slicing. IEEE Transactions
    on software engineering 4 (1984), 352–357.'
  - '- <span id="page-15-4"></span>[61] Hao Wu, Krishnendra Nathella, Dam Sunwoo,
    Akanksha Jain, and Calvin Lin. 2019. Efficient Metadata Management for Irregular
    Data Prefetching. In 2019 ACM/IEEE 46th Annual International Symposium on Computer
    Architecture (ISCA). 1–13.'
  - '- <span id="page-15-3"></span>[62] Haoyang Zhang, Yirui Zhou, Yuqi Xue, Yiqi
    Liu, and Jian Huang. 2023. G10: Enabling An Efficient Unified GPU Memory and Storage
    Architecture with Smart Tensor Migrations. In Proceedings of the 56th Annual IEEE/ACM
    International'
  - Symposium on Microarchitecture (Toronto, ON, Canada) (MICRO '23). Association
    for Computing Machinery, New York, NY, USA, 395–410. [doi:10.1145/3613424.](https://doi.org/10.1145/3613424.3614309)
    [3614309](https://doi.org/10.1145/3613424.3614309)
  - '<span id="page-15-2"></span>[63] Yuxuan Zhang, Nathan Sobotka, Soyoon Park, Saba
    Jamilan, Tanvir Ahmed Khan, Baris Kasikci, Gilles A Pokam, Heiner Litz, and Joseph
    Devietti. 2024. RPG2: Robust Profile-Guided Runtime Prefetch Generation. In Proceedings
    of the 29th ACM International Conference on Architectural Support for Programming
    Languages and Operating Systems, Volume 2 (La Jolla, CA, USA) (ASPLOS ''24). Association
    for Computing Machinery, New York, NY, USA, 999–1013. [doi:10.](https://doi.org/10.1145/3620665.3640396)
    [1145/3620665.3640396](https://doi.org/10.1145/3620665.3640396)'
- id: lightml_a_photonic_accelerator_for_efficient_general_purpose_machine_learning_liang_liu_https_orcid_org_0000_0002_7047_1794_university_of_pittsburgh_pittsburgh_pennsylvania_usa_lil125_pitt_edu
  title: 'LightML: A Photonic Accelerator for Efficient General Purpose Machine Learning'
  abstract: 'The rapid integration of AI technologies into everyday life across sectors
    such as healthcare, autonomous driving, and smart home applications requires extensive
    computational resources, placing strain on server infrastructure and incurring
    significant costs.

    We present LightML, the first system-level photonic crossbar design, optimized
    for high-performance machine learning applications. This work provides the first
    complete memory and buffer architecture carefully designed to support the high-speed
    photonic crossbar, achieving over 80% utilization. LightML also introduces solutions
    for key ML functions, including large-scale matrix multiplication (MMM), element-wise
    operations, non-linear functions, and convolutional layers. Delivering 325 TOP/s
    at only 3 watts, LightML offers significant improvements in speed and power efficiency,
    making it ideal for both edge devices and dense data center workloads.'
  keywords: Optical Computing Crossbar, Coherent Photonic Multiplication, Ultralow-Energy
    Machine Learning
  document: "![](_page_0_Picture_0.jpeg)\n\n# LightML: A Photonic Accelerator for\
    \ Efficient General Purpose Machine Learning\n\n[Liang Liu](https://orcid.org/0000-0002-7047-1794)\
    \ University of Pittsburgh Pittsburgh, Pennsylvania, USA lil125@pitt.edu\n\n[Nathan\
    \ Youngblood](https://orcid.org/0000-0003-2552-9376) University of Pittsburgh\
    \ Pittsburgh, Pennsylvania, USA nathan.youngblood@pitt.edu\n\n[Sadra Rahimi Kari](https://orcid.org/0000-0001-8230-2165)\
    \ University of Pittsburgh Pittsburgh, Pennsylvania, USA sadra.rahimi@pitt.edu\n\
    \n[Youtao Zhang](https://orcid.org/0000-0001-8425-8743) University of Pittsburgh\
    \ Pittsburgh, Pennsylvania, USA zhangyt@cs.pitt.edu\n\n[Xin Xin](https://orcid.org/0000-0003-0952-2115)\
    \ U. of Central Florida Olando, Florida, USA xin.xin@ucf.edu\n\n[Jun Yang](https://orcid.org/0000-0001-8372-6541)\
    \ U. of Pittsburgh Pittsburgh, Pennsylvania, USA juy9@pitt.edu\n\n#### Abstract\n\
    \nThe rapid integration of AI technologies into everyday life across sectors such\
    \ as healthcare, autonomous driving, and smart home applications requires extensive\
    \ computational resources, placing strain on server infrastructure and incurring\
    \ significant costs.\n\nWe present LightML, the first system-level photonic crossbar\
    \ design, optimized for high-performance machine learning applications. This work\
    \ provides the first complete memory and buffer architecture carefully designed\
    \ to support the high-speed photonic crossbar, achieving over 80% utilization.\
    \ LightML also introduces solutions for key ML functions, including large-scale\
    \ matrix multiplication (MMM), element-wise operations, non-linear functions,\
    \ and convolutional layers. Delivering 325 TOP/s at only 3 watts, LightML offers\
    \ significant improvements in speed and power efficiency, making it ideal for\
    \ both edge devices and dense data center workloads.\n\n#### CCS Concepts\n\n\
    • Hardware → Emerging optical and photonic technologies; • Computing methodologies\
    \ → Neural networks.\n\n## Keywords\n\nOptical Computing Crossbar, Coherent Photonic\
    \ Multiplication, Ultralow-Energy Machine Learning\n\n#### ACM Reference Format:\n\
    \nLiang Liu, Sadra Rahimi Kari, Xin Xin, Nathan Youngblood, Youtao Zhang, and\
    \ Jun Yang. 2025. LightML: A Photonic Accelerator for Efficient General Purpose\
    \ Machine Learning. In Proceedings of the 52nd Annual International Symposium\
    \ on Computer Architecture (ISCA '25), June 21–25, 2025, Tokyo, Japan. ACM, New\
    \ York, NY, USA, [16](#page-15-0) pages. [https://doi.org/10.1145/3695053.](https://doi.org/10.1145/3695053.3731053)\
    \ [3731053](https://doi.org/10.1145/3695053.3731053)\n\n#### 1 Introduction\n\n\
    The booming AI market is producing numerous technologies that are quickly adopted\
    \ and deployed in people's daily lives, spanning\n\n[This work is licensed under\
    \ a Creative Commons Attribution 4.0 International License.](https://creativecommons.org/licenses/by/4.0)\
    \ ISCA '25, Tokyo, Japan © 2025 Copyright held by the owner/author(s). ACM ISBN\
    \ 979-8-4007-1261-6/25/06 <https://doi.org/10.1145/3695053.3731053>\n\napplication\
    \ domains ranging from healthcare [\\[2,](#page-13-0) [60,](#page-14-0) [66\\\
    ]](#page-14-1) and autonomous driving [\\[44,](#page-14-2) [48\\]](#page-14-3)\
    \ to smart homes [\\[11,](#page-13-1) [50\\]](#page-14-4). The sizes of these\
    \ CNN, DNN, and Transformer-based AI models are rapidly expanding, demanding immense\
    \ computational resources and pushing the server infrastructure to its limits,\
    \ resulting in significant financial costs [\\[5,](#page-13-2) [20,](#page-13-3)\
    \ [76,](#page-14-5) [88\\]](#page-15-1). Training and inference tasks for large\
    \ language models can easily demand teraflops of computational power per inference\
    \ task or training epoch [\\[31\\]](#page-13-4). Consequently, this \"computational\
    \ obesity\" often restricts their deployment to large servers and data centers\
    \ and frequently impedes user-side training and inference.\n\nComputing with light\
    \ offers a compelling alternative to conventional computing because optical interconnects,\
    \ such as fibers and waveguides, are not constrained by energy efficiency issues\
    \ related to Joule heating, RF crosstalk, and capacitance [\\[33,](#page-13-5)\
    \ [52,](#page-14-6) [65\\]](#page-14-7). This eliminates the energy-bandwidth\
    \ tradeoff that restricts the data transfer rate of electrical interconnects and\
    \ allows an exceptionally high bandwidth density [\\[8\\]](#page-13-6). Various\
    \ architectures have demonstrated the feasibility of photonic computing [\\[3,](#page-13-7)\
    \ [13,](#page-13-8) [18,](#page-13-9) [46,](#page-14-8) [68,](#page-14-9) [92\\\
    ]](#page-15-2).\n\nWhile these properties benefit the efficiency of today's machine\
    \ learning applications, previous work has often focused on isolated physical\
    \ aspects, lacking complete architectural support that integrates all necessary\
    \ components. In particular, a few significant challenges remain to be addressed:\
    \ 1. Existing work lacks a thorough memory solution to fully saturate the high\
    \ data demand. For applications with complex data structures, such as large-scale\
    \ matrix convolution, naive memory fetching schemes limit the performance of the\
    \ crossbar arrays. 2. Current optical crossbar designs support limited operations,\
    \ whereas general-purpose ML applications require a broader range of functionalities,\
    \ including non-linear functions, element-wise operations, and normalization functions.\
    \ 3. Prior designs did not fully exploit circuit-level optimizations, resulting\
    \ in inefficiencies in basic operations.\n\nIn this paper, we introduce LightML,\
    \ an electronic-photonic codesigned architecture that optimizes memory access\
    \ and supports full-spectrum ML-specific operations to address existing gaps.\
    \ The contributions of this work include:\n\n- Pioneering the first system-level\
    \ photonic crossbar architecture with a novel memory and buffer design that sustains\
    \ high-speed data throughput.\n- Integrating a non-linear function unit by utilizing\
    \ phase modulators and achieving non-linear functions via the Fourier Series.\n\
    \n• Efficiently implementing operations such as matrix transpose, batch normalization,\
    \ and ReLU activation functions through simple and easy-to-deploy circuit-level\
    \ designs.\n\nThe advantages of LightML include its computing speed and high energy\
    \ efficiency, offering 325 TOP/s computing power with only 3 watts of power consumption.\
    \ When including HBM, the system achieves a power efficiency of 17.1 TOP/s/W—13.6x\
    \ higher than a GPU, and when excluding HBM (under equivalent configurations),\
    \ it is up to 1.91x more efficient than state-of-the-art NVM-based crossbars.\
    \ Additionally, with the proposed optimization, LightML can rival the NVIDIA A100\
    \ for ML model inference, providing up to 4x shorter latency. Furthermore, LightML\
    \ delivers 5-bit precision while maintaining less than 3% inference accuracy loss\
    \ compared to models in Floating Point. Therefore, LightML is an excellent choice\
    \ for both edge devices, where energy efficiency is crucial, and data centers,\
    \ where it can handle dense computing tasks efficiently.\n\n#### 2 Background\n\
    \n#### 2.1 Photonic MAC Operation\n\nSigned multiplication and addition can be\
    \ performed optically using a simple beam splitter and differential photodetection.\
    \ For multiplication, we make use of homodyne detection, which interferes with\
    \ two optical fields with each other to achieve photo-electric multiplication\
    \ at the quantum limit [\\[27,](#page-13-10) [71\\]](#page-14-10). When two coherent\
    \ optical signals are incident on a 3dB coupler, which outputs two light beams\
    \ with 50% power, in Figure [1a](#page-1-0)), the instantaneous differential output\
    \ intensity will be:\n\n$$I\\_{+} - I\\_{-} = 2|xy|\\sin\\left(\\Delta\\phi\\\
    right).$$\n\nwhere and are the electric field amplitudes and Δ = − is the relative\
    \ phase difference between the two optical signals. Differential detection removes\
    \ the common mode intensities of the input signals (| | 2 and || 2 ), leaving\
    \ only the product of the two field amplitudes, ||. Because the interference also\
    \ depends on the relative phase difference between the two input signals, it is\
    \ possible to encode both positive and negative numbers (e.g., − = || ) and even\
    \ complex numbers as recently demonstrated [\\[38\\]](#page-13-11). Since we encode\
    \ data sequentially in the time domain, addition can be performed by temporal\
    \ integration via charge accumulation on a capacitor, as shown in Figure [1b](#page-1-0)).\
    \ Thus, the dot-product between vectors x and y can be accomplished optically\
    \ on-chip with only a 3dB directional coupler and a few simple electrical components.\
    \ This reduces both the input optical power and the ADC frequency by 1/, where\
    \ is the number of time-multiplexed elements (i.e., length of the input vectors),\
    \ thus significantly improving the scalability and compute efficiency over weight-stationary\
    \ optical approaches [\\[89\\]](#page-15-3).\n\n#### 2.2 Optical Modulator\n\n\
    The Michelson interferometric modulator (MIM) design [\\[62\\]](#page-14-11) is\
    \ comprised of a segmented intensity modulator with binary code weighting [\\\
    [83\\]](#page-14-12) (Figure [1d](#page-1-0)). By applying voltage to the segmented\
    \ yellow cells, we can precisely control both the phase and amplitude of the modulated\
    \ light. A phase shift ( = −) is used to encode the sign of the input signal,\
    \ while the amplitude is modulated linearly in proportion to the cell length.\
    \ Different cell lengths are employed to represent the MSB and LSB. The use of\
    \ segmented modulators\n\n<span id=\"page-1-0\"></span>![](_page_1_Figure_11.jpeg)\n\
    \nFigure 1: a) Photonic MAC uses a beam splitter and differential detection. b)\
    \ Circuit-level implementation of a) using an on-chip beam splitter and integrating\
    \ circuit. c) Photonic crossbar architecture for matrix-matrix multiplication\
    \ comprised of an array of dot-product unit cells (green). d) Illustration of\
    \ optical DAC comprised of a segmented Michelson modulator and phase shifter.\n\
    \nallows for direct electro-optic (E-O) conversion of an -bit binary value to\
    \ an analog amplitude without the need for a dedicated DAC for each modulator.\
    \ This can achieve high E-O conversion efficiency (< 250 fJ/b) and better linearity\
    \ than conventional optical modulators + electronic DAC designs [\\[51,](#page-14-13)\
    \ [83\\]](#page-14-12). Additionally, the folded cavity design reduces footprint\
    \ and capacitance by ∼ 2x [\\[62\\]](#page-14-11).\n\n#### <span id=\"page-1-1\"\
    ></span>2.3 Photonic Crossbar\n\nTo scale this concept beyond single dot-product\
    \ operations, we can create an array of dot-product unit cells, each coupled to\
    \ row and column waveguides via directional couplers with a predetermined splitting\
    \ ratio ( 2 and 2 ) as illustrated in Figure [1c](#page-1-0)). This allows optical\
    \ fan-out in a compact, two-dimensional array and amortizes the energy required\
    \ for modulation. Similar arrays have been experimentally demonstrated with a\
    \ large number of unit cells (e.g., 64 × 64 in [\\[74\\]](#page-14-14) and 128\
    \ × 128 in [\\[93\\]](#page-15-4)) in other contexts such as LI-DAR [\\[64,](#page-14-15)\
    \ [93\\]](#page-15-4), photonic phased arrays [\\[74\\]](#page-14-14), and in-memory\
    \ photonic processors [\\[18\\]](#page-13-9). The crossbar can perform integer\
    \ matrix-matrix multiplication (MMM), C = XY, where ∈ N × and ∈ N × , are the\
    \ number of pulses temporally streamed into the array from memory, and can be\
    \ the dimension of the crossbar array (up to 128). Previous work [\\[89\\]](#page-15-3)\
    \ shows that when the number of pulses, , achieves 1024, the computational efficiency\
    \ (TOPs/s/W) reaches the maximum. Our photonic crossbar architecture allows larger-scale\
    \ MMMs with tiling, as we explore in Section [6.](#page-5-0)\n\nThe remarkable\
    \ speed primarily stems from the ability to perform a dot product at each crosspoint\
    \ of the photonic crossbar at speeds exceeding billions of multiply-accumulate\
    \ (MAC) operations per second per crosspoint. In contrast, in conventional resistive\
    \ memory-based crossbars, a dot product is executed in each column\n\n<span id=\"\
    page-2-0\"></span>![](_page_2_Figure_1.jpeg)\n\nFigure 2: a) Measured dot-product\
    \ between two bipolar vectors (top). Difference signal measured by balanced photodetection\
    \ (middle). Integrated difference signal gives the final result with a 3.6% error\
    \ (bottom). b) Microscope image of 4 × 4 crossbar array (top) and captured NIR\
    \ image of crossbar output (bottom). Reproduced from [\\[38\\]](#page-13-11).\n\
    \nor row at speeds ranging from tens to a few hundred megahertz, limited by interconnect\
    \ capacitance and analog noise.\n\n#### 3 Preliminary Result and Fabrication Feasibility\n\
    \n#### 3.1 Preliminary Result\n\nWe have demonstrated the feasibility of our proposed\
    \ architecture at both the unit cell and crossbar array levels [\\[38\\]](#page-13-11).\
    \ A 4×4 homodyne multiplier prototype was fabricated in the laboratory. The prototype\
    \ utilizes non-resonant thermo-optic Mach-Zehnder interferometer (MZI) modulators\
    \ with a tunable fiber laser source emitting 1550 nm light. Each cell in the array\
    \ consists of two directional couplers to evenly distribute power from the row\
    \ and column waveguides. A compact 3dB directional coupler connects to two output\
    \ ports, which are further coupled to a Near-Infrared (NIR) image sensor via grating\
    \ couplers. Results from a single unit cell are presented in Figure [2a](#page-2-0)).\
    \ In this demonstration, two modulators are used to encode the phase of each signal,\
    \ enabling the computation of the dot product between two 64-element bipolar vectors.\
    \ The differential signal from the two output grating couplers performs multiplication\
    \ (middle panel), while temporal integration achieves accumulation (bottom panel).\
    \ Additionally, we successfully demonstrated optical fan-out and NIR imaging of\
    \ a passive 4x4 photonic crossbar array in Figure [2b](#page-2-0)). Post-processing\
    \ the captured NIR image allows the extraction of MMM results, showcasing the\
    \ scalability and practicality of our approach.\n\n#### <span id=\"page-2-2\"\
    ></span>3.2 Error Modeling\n\nThe precision of the photonic crossbar is sensitive\
    \ to noise and signal degradation. To assess the feasibility of industrial fabrication,\
    \ we model the impact of noise from various fabrication non-idealities based on\
    \ our photonic crossbar design. In Figure [3a](#page-2-1)), we present a preliminary\
    \ error propagation analysis of optical MAC operations in a single unit cell.\
    \ Assuming two digital input signals of dimension , x, y ∈ N , the ideal MAC output\
    \ is <sup>x</sup> <sup>y</sup>. To evaluate the relative error, , between the\
    \ ideal output and the measured result from the\n\n![](_page_2_Figure_9.jpeg)\n\
    \n<span id=\"page-2-1\"></span>![](_page_2_Figure_10.jpeg)\n\nFigure 3: a) Model\
    \ of noise and fabrication error within each unit cell. b) and c) Ablation study\
    \ of splitting ratio and phase noise. d) Overall Noise Modeling.\n\nADC, we use\
    \ the following equation:\n\n$$\\epsilon = \\left(\\mathbf{x}^T \\mathbf{y} -\
    \ (\\mathbf{x}^T \\mathbf{y})\\_{\\text{measure}}\\right) / \\mathbf{x}^T \\mathbf{y}$$\n\
    \nThe measured output, (x <sup>y</sup>)measure, is the voltage measured from the\
    \ accumulated circuit, which is · ∫ 1 2 (+() − <sup>−</sup> ()) d. Here, the integral\
    \ term ∫ (<sup>+</sup> − −) d represents the accumulated voltage over the MAC\
    \ period, and is a calibration coefficient that scales the accumulated current\
    \ to the ADC output. The noise modeling results shown in Figure [3](#page-2-1)\
    \ have been tailored for our architecture, drawing upon seminal theories of homodyne\
    \ detection [\\[58,](#page-14-16) [90\\]](#page-15-5) as well as recent developments\
    \ in coherent photoelectric multiplication for AI processing [\\[26\\]](#page-13-12).\n\
    \n1. Beam Splitters During optical fan-out, 50:50 beam splitters divide the laser\
    \ into 2 beams with equal intensity before fanningout these beams to each unit\
    \ cell in a given row or column via directional couplers [\\[89\\]](#page-15-3).\
    \ However, imperfections in splitting ratios during fan-out and distribution of\
    \ the signals introduce power imbalances as the crossbar size increases. Each\
    \ beam passes through log<sup>2</sup> 2 50:50 beam splitters and up to directional\
    \ couplers, accumulating a fixed power imbalance due to deviation from the ideal\
    \ splitting ratio. We model the power splitting ratio of each 50:50 splitter as\
    \ , = √ 0.5 ± , while the coupling coefficients for each directional coupler in\
    \ the crossbar can be written as 2 = 2 +1 /(1 + 2 +1 ) + 2 where is the fixed\
    \ fabrication error for each splitter/coupler sampled from a Gaussian distribution.\n\
    \n2. Modulation Error: Optical amplitude noise arising from analog electrical\
    \ and optical noise in the modulators, driving circuitry, and the laser itself\
    \ is present when encoding the optical inputs. The light intensity after modulation\
    \ is modeled as + and + , where ∝ and ∝ are the ideal electric field intensities,\
    \ and is Gaussian noise.\n\n3. Phase Shift: Variations in the optical path lengths\
    \ between the two signals, and , introduce fixed phase alignment errors. These\
    \ errors occur due to minor differences in the physical paths traversed by the\
    \ signals and can vary with temperature. Typically, the optical signals travel\
    \ distances ranging from 1 to 10 mm between the modulator and the 3dB coupler.\
    \ We model this phase noise as\n\n , where follows a Gaussian distribution, representing\
    \ the random variations in phase alignment.\n\n4. Photon Detector: Noise also\
    \ occurs during optical-to-electrical conversion. The generated current is = ·\
    \ ℎ || 2 , where ℎ || 2 is the photon energy, and is the photon detector's conversion\
    \ efficiency (typically between 0.5 and 0.8). We model photon detector noise as\
    \ Gaussian noise , affecting the output as <sup>+</sup> + and <sup>−</sup> + .\
    \ 5. Thermal Condition: Photonic devices are sensitive to thermal variations because\
    \ the refractive index of silicon changes slightly with temperature. In an RC\
    \ circuit, thermal (Johnson–Nyquist) noise[\\[41\\]](#page-14-17) induces voltage\
    \ fluctuations across the capacitor, which can be expressed as: rms = √︁ /, where\
    \ is the Boltzmann constant, is the absolute temperature, and is the capacitance.\
    \ For instance, with = 15fF (the same as DRAM) at room temperature (300 K), the\
    \ voltage fluctuation is about 0.5 mV, which has only a minimal effect on our\
    \ error model. Furthermore, our platform minimizes thermal variation by consuming\
    \ less than 20W (including HBM2E), resulting in negligible temperature increases.\n\
    \nOther sources of noise, such as those from ADC readers, are negligible due to\
    \ the maturity of these technologies. In Figures [3b](#page-2-1)) and [3c](#page-2-1)),\
    \ we plot the relative dot-product error due to errors in the splitting ratios\
    \ and phase error, respectively, while holding other noise sources constant. A\
    \ smaller relative error ensures higher bit precision. For instance, a relative\
    \ error below 1/2 <sup>4</sup> = 0.0625 allows for a 5-bit precise MAC operation\
    \ (4 bits for magnitude and 1 bit for the sign). It is important to note that\
    \ errors in both the splitting ratio and phase can be removed if: 1) they are\
    \ constant after fabrication and 2) can be determined through an initial calibration\
    \ step. In Figure [3d](#page-2-1)), we apply the Monte Carlo method to model the\
    \ entire noise system. All noise sources follow Gaussian distributions with mean\
    \ = 0 and standard deviation = 1/2 , where = {1, 2, . . . , 8} represents different\
    \ precision levels for devices (e.g., modulators, photon detectors). The relative\
    \ error is plotted against the MAC operation dimensions, showing that the error\
    \ decreases as the temporal dimension increases. Our previous work [\\[67\\]](#page-14-18)\
    \ analyzed the accuracy degradation of various ML models under different error\
    \ configurations. In this work, we also simulated the error model by injecting\
    \ Gaussian noise during inference. As shown in Table [6,](#page-12-0) the resulting\
    \ accuracy drop remains within approximately 1% to 2%.\n\n#### 3.3 Industrial\
    \ improvement\n\nOur laboratory prototype is limited by the available fabrication\
    \ technologies. However, industries with state-of-the-art fabrication techniques\
    \ [\\[36,](#page-13-13) [81,](#page-14-19) [84\\]](#page-14-20) can achieve higher\
    \ precision, operating frequencies, and address many limitations.\n\nOptical Modulator:\
    \ Existing work [\\[84\\]](#page-14-20) has demonstrated a 20 GHz optical modulator\
    \ fabricated using 40 nm technology, achieving both sign encoding and 32-level\
    \ magnitude resolution. This corresponds to a 6-bit precision modulator.\n\nPhase\
    \ Alignment: For optical devices operating at a 1550 nm wavelength, the relative\
    \ optical path length must be controlled to within 1550/( · 2 4 ) ≈ 50 nm to maintain\
    \ 4-bit amplitude and 1-bit sign. Achieving this requires laser trimming individual\
    \ unit cells in the photonic crossbar array, as discussed extensively in [\\[89\\\
    ]](#page-15-3). Alternatively, localized thermal tuning has been demonstrated\
    \ for\n\npost-fabrication trimming of silicon photonic devices [\\[36\\]](#page-13-13).\
    \ Our lab is currently exploring using low-loss phase-change materials for laser\
    \ trimming, building on methods shown in [\\[81\\]](#page-14-19).\n\nSplitting\
    \ Ratio: To address optical losses and imperfections in splitting or coupling\
    \ ratios and achieve 5-bit precision, the splitting ratio should maintain 2 <\
    \ 0.5/2 5 . A one-time calibration of the crossbar array can be performed. This\
    \ calibration applies a scaling factor to each unit cell, compensating for fabrication\
    \ variations and enabling the scalability of larger crossbar arrays.\n\nUsing\
    \ SOTA technology, the precision of non-ideal components can exceed 5 bits, ensuring\
    \ < 1/2 5 . In Figure [3d](#page-2-1)), the relative error under 5-bit precision\
    \ approaches 10−<sup>2</sup> ≈ 2 −6 , supporting results with approximately 6\
    \ bits of magnitude and 1 bit for the sign. Our design employs a conservative\
    \ strategy, targeting 4 bits for magnitude and 1 bit for the sign, with potential\
    \ scalability to 8 bits in future iterations. For instance, [\\[92\\]](#page-15-2)\
    \ experimentally demonstrated 8-bit precision across 1,000 optical MAC operations\
    \ using high-performance foundry-fabricated modulators. Additionally, [\\[13\\\
    ]](#page-13-8) adopts the Residue Number System (RNS) to address precision limitations\
    \ in photonic computing, enabling the system to operate reliably at lower precision.\n\
    \n#### 4 Advantages of Photonic Computing Core\n\nAs noted earlier, the feasibility\
    \ of fabricating optical crossbars has been demonstrated by our lab prototypes\
    \ and refined by the broader research community. Building on this foundation,\
    \ we will develop a fully functional accelerator for machine learning, called\
    \ \"LightML\". Key advantages over prior accelerators include:\n\n1. Significant\
    \ Speed Boost over Resistive Crossbars. LightMLoffers significant advantages over\
    \ resistive memory crossbars for matrix multiplication, including orders-of-magnitude\
    \ improvements in speed and energy efficiency. This speed stems from the photonic\
    \ crossbar's ability to perform <sup>2</sup> dot-products simultaneously at each\
    \ crosspoint, enabling true MMM, unlike resistive memory crossbars that handle\
    \ only dot-products at a time for matrix-vector multiplication (MVM). LightMLcomputes\
    \ using electro-optical modulators that operate at the GHz range, while resistive\
    \ memories require multiple ADC-intensive iterations for full matrix operations.\
    \ Further, LightML does not require high-current and slow reprogramming of weights\
    \ for ML models, which is much more efficient and low energy than resistive memory\
    \ crossbars.\n\n2. Support for Various Non-Linear Functions. There have been prior\
    \ studies on implementing complete ML models, such as MNIST [\\[81\\]](#page-14-19),\
    \ using optical crossbars specifically for MMM or MVM [\\[12,](#page-13-14) [24,](#page-13-15)\
    \ [86\\]](#page-15-6). In these works, operations beyond MMM are still handled\
    \ by a host processor. However, optical signals inherently encode both amplitude\
    \ and phase information, and this phase information enables straightforward computation\
    \ of functions like sin and cos. These functions can be used to derive non-linear\
    \ functions such as Exponential, Sigmoid, and tanh, which are critical for ML\
    \ applications (details are provided in Section [6.2\\)](#page-5-1). Existing\
    \ accelerators require additional functional units to compute these non-linear\
    \ operations, leading to significant data transfer overheads. In this paper, we\
    \ demonstrate how both MMM and non-MMM operations can be executed entirely within\
    \ the optical crossbar. To the best of our\n\nDesign Overview\n\n<span id=\"page-4-0\"\
    ></span>![](_page_4_Figure_1.jpeg)\n\nFigure 4: Overview of LightML.\n\nknowledge,\
    \ this is the first work to establish a photonic computing unit capable of general-purpose\
    \ ML acceleration.\n\n3. High Energy Efficiency. Although the modulators can operate\
    \ at tens of GHz, our prior studies have shown that the optical crossbar, which\
    \ includes the laser source, modulators, and readout circuits, consumes less than\
    \ 1W [\\[89\\]](#page-15-3). As demonstrated in the evaluations section, these\
    \ combined advantages enable LightML to rival the NVIDIA A100 GPU in computing\
    \ power while consuming <sup>1</sup> 13 of the energy, and achieve 1.91x power\
    \ efficiency compared to the SOTA photonic and PCM-based crossbars [\\[19,](#page-13-16)\
    \ [47,](#page-14-21) [53\\]](#page-14-22).\n\n#### 5 Overview\n\n#### 5.1 LightML\
    \ Architecture\n\nIn Figure [4,](#page-4-0) we show the architectural overview\
    \ of LightML. The proposed architecture comprises the core compute unit, an optical\
    \ crossbar, supporting analog circuitry, memories and buffers, and control units,\
    \ as illustrated in Figure [4.](#page-4-0) The compute unit performs MMM, MVM,\
    \ nonlinear functions, and element-wise operations. The memories and buffers are\
    \ designed to sustain the high input/output data rates required by the compute\
    \ unit, while the control unit orchestrates complex functions composed of multiple\
    \ primitive operations, as listed in Table [1.](#page-4-1)\n\nComputing Unit:\
    \ The crossbar is equipped with 2×128 modulators for amplitude modulation (in\
    \ both dimensions) and 1×128 phase modulation (in the horizontal dimension), with\
    \ the latter primarily used for computing non-linear functions. Each crosspoint\
    \ includes an accumulation capacitor connected to analog-to-digital converters\
    \ (ADCs) for processing readouts, as shown in Figure [1b](#page-1-0)). Column\
    \ and row selectors are employed to reduce the number of ADCs by time-multiplexing\
    \ them, allowing us to explore trade-offs between area, power, and latency. The\
    \ ADCs and supporting circuitry will be further detailed to handle frequent operations\
    \ such as divisions and transpositions (See Section [6.4\\)](#page-6-0).\n\nTable\
    \ 1: AI Functions Supported by LightML.\n\n<span id=\"page-4-1\"></span>\n\n|\
    \ Class             | Function Name                |\n|-------------------|------------------------------|\n\
    | Element-Wise Ops  | Add, Sub, Mul, Scaling       |\n| Matrix/Vector Ops | Sum,\
    \ Mean, Dot, MVM, MMM     |\n| Nonlinear Ops     | Sin, Log, Exp, Tanh, Sigmoid\
    \ |\n| Normalization Ops | BatchNorm, LayerNorm         |\n| Systolic Ops    \
    \  | Linear, Conv2d               |\n\n<span id=\"page-4-2\"></span>![](_page_4_Figure_11.jpeg)\n\
    \nMUX\n\nFigure 5: Design Scheme of On-chip Input Buffers.\n\nDEMUX\n\nControl\
    \ Unit: The control unit consists of several components: a memory controller for\
    \ address translation, a buffer controller to manage load/store routers and pipelining\
    \ buffers, a nonlinear controller equipped with internal registers for storing\
    \ coefficients and parameters required for non-linear functions, and an analog\
    \ controller responsible for overseeing the circuits around the ADCs to perform\
    \ regular, transpose, and scaling operations.\n\nMemory and On-chip Buffer: The\
    \ memories and buffers are designed to meet the high bandwidth requirements of\
    \ the compute unit, for a scalable crossbar, such as a 128×128 array with 2×128\
    \ modulators operating at 12 GHz, requiring 2 · 128 · 12G= 3TB data per second\
    \ to fully utilize the crossbar at the peak performance.\n\nIn Figure [5,](#page-4-2)\
    \ we illustrate the data flow from memory to the modulator. LightML adopts a 2-stack\
    \ High-Bandwidth Memory (HBM2E) configuration, providing up to 920 GB/s of memory\
    \ bandwidth. For each memory cycle, HBM2E fetches 1KB of data for the chip. We\
    \ implement a 1KB buffer row to load this data on the chip.\n\nWe implement three\
    \ on-chip buffers: input, weight, and output. To support continuous data flow\
    \ to the optical modulators, the input buffer uses a double-buffer scheme—one\
    \ buffer loads from HBM while the other drives the modulators, alternating each\
    \ cycle. This ensures data delivery within a single dot-product duration. As shown\
    \ in Figure [5,](#page-4-2) top blocks receive data, and bottom blocks feed the\
    \ modulators. DEMUXs and MUXs control write and read paths, respectively. The\
    \ input buffer consists of 128 lines for 128 modulators, each with 1KB of SRAM,\
    \ totaling 128 KB.\n\nData from the loaded row (1KB) is distributively written\
    \ into all 128 lines of the input buffer, with each line sequentially receiving\
    \ 8B data. We implement a load router to control the data writing addresses, ensuring\
    \ consistent addressing logic across all 128 lines. The weight buffer operates\
    \ similarly to the input buffer but uses a single buffer architecture. When fetching\
    \ the weights, the modulator operation is halted until the process is complete.\n\
    \n#### 5.2 Flexibility in General AI Functions Support\n\nAI models typically\
    \ consist of complex modules—including Multi-Layer Perceptrons, convolutional\
    \ layers, attention mechanisms, and various nonlinear activations. However, many\
    \ of these modules can be decomposed into a small set of fundamental operations,\
    \ such as MMM, MVM, element-wise operations, and matrix transpositions. For instance,\
    \ the Transformer's attention mechanism involves MMM, softmax (a nonlinear function),\
    \ element-wise scaling, addition, and matrix transposition [\\[79\\]](#page-14-23).\
    \ We conduct a comprehensive analysis of these complex operations and identify\
    \ common primitive components. The supported functions are summarized in Table\
    \ [1.](#page-4-1) A majority of those primitive components can be efficiently\
    \ implemented on our LightML, while the remaining operations are\n\nHS MUX\n\n\
    primarily memory-intensive. Implementing these operations enables acceleration\
    \ for a wide range of AI models.\n\n#### <span id=\"page-5-0\"></span>6 Common\
    \ Modules for ML Applications\n\n#### 6.1 Quantization\n\nTo address the precision\
    \ constraint, this work leverages the standard integer quantization machinist\
    \ [\\[21,](#page-13-17) [82\\]](#page-14-24). The integer quantization utilizes\
    \ batch-wise scaling factor, , to map the real value ∈ [−, ] into its 5-bit integer\
    \ format, ˆ ∈ [−16, 16]:\n\n$$\\mathsf{quant}\\,\\mathsf{t}\\colon\\,\\hat{\\\
    mathsf{x}} = \\mathsf{l}\\,\\mathsf{x}\\,\\mathsf{l}\\,\\mathsf{/s}\\,\\,\\mathsf{s}\\\
    ,\\,\\mathsf{/s}\\,\\,\\mathsf{/s}\\,\\,\\mathsf{/s}\\,\\,\\mathsf{/s}\\,\\,\\\
    mathsf{/s}\\,\\,\\mathsf{/s}$$\n\nThe scaling factor is computed by = /16, where\
    \ is the realvalued range of the input, and 16 is the integer range limit. During\
    \ the MAC operation, all elements within each input matrix/vector xˆ and wˆ share\
    \ the same scaling factors, and . The output ˆ is\n\n $\\hat{y} = \\sum\\_{i}^{N}\
    \ \\hat{\\mathbf{w}}\\_{i}$   $\\hat{\\mathbf{x}}\\_{i} = \\sum\\_{i}^{N} \\mathbf{s}\\\
    _{\\mathbf{w}}$   $\\mathbf{w}\\_{i} \\cdot \\mathbf{s}\\_{\\mathbf{x}}$   $\\\
    mathbf{x}\\_{i} = \\mathbf{s}\\_{\\mathbf{w}}$   $\\mathbf{s}\\_{\\mathbf{x}}\
    \ \\cdot \\mathbf{y}$ \n\nHence, the integer output ˆ has the combined scaling\
    \ factor . In the worst-case scenario, <sup>ˆ</sup> may require <sup>2</sup> <sup>×</sup>\
    \ <sup>5</sup> <sup>+</sup> log<sup>2</sup> () − 1 bits to maintain accuracy.\
    \ In conventional digital computing, quantized outputs are typically stored in\
    \ higher-precision formats, a new scaling factor is derived, and the data is then\
    \ downscaled to 5 bits, which is deployed in the cuBLAS library [\\[56\\]](#page-14-25).\n\
    \nIn LightML, downscaling is efficiently handled by our ADC scaling unit (see\
    \ Section [6.4\\)](#page-6-0). First, we perform a one-time calibration of the\
    \ laser power so that the maximum MAC output max obtained when 1,024 inputs are\
    \ each set to the maximum value (+16)—corresponds to 1V in the ADC readout. For\
    \ a sequence of outputs with an individual scale, ′ , we will adjust the ADC's\
    \ reference voltage to ′ /MAXV, fitting the outputs within the appropriate range.\n\
    \n#### <span id=\"page-5-1\"></span>6.2 Non-Linear Function\n\nNonlinear functions,\
    \ typically serving as activation functions in neural networks, vary by model.\
    \ For instance, Long-Short-Term Memory (LSTM) [\\[30\\]](#page-13-18) and Recurrent\
    \ Neural Networks (RNN) [\\[63\\]](#page-14-26) use ℎ, while transformers [\\\
    [79\\]](#page-14-23) use the function (via SoftMax). Traditionally, they are implemented\
    \ using math libraries with parallelism support, when available, on CPUs and GPUs.\
    \ In ML accelerators, these functions are typically executed on dedicated hardware,\
    \ which lacks the flexibility to easily adapt to new functions and algorithms.\
    \ There is also prior work [\\[73\\]](#page-14-27) that uses a diode array to\
    \ simulate nonlinear functions; however, it supports only a single function and\
    \ requires a carefully tailored sequence, leading to increased latency.\n\nIn\
    \ our design, we implement nonlinear functions via the Fourier Series, and we\
    \ use the phase modulator to generate sin or cos function outputs. The phase modulator\
    \ calibrates the waveguide to ensure the phase difference between two light beams\
    \ is 0 or . To compute the nonlinear function, we couple two light beams: one\
    \ containing only phase information and the other containing only amplitude information.\
    \ The phase input, , is represented as 5-bit data, dividing the phase into 32\
    \ intervals between − and . The amplitude input is encoded as 5-bit data, . After\
    \ coupling, the current readout will be proportional to sin(). For any nonlinear\n\
    \n<span id=\"page-5-2\"></span>![](_page_5_Figure_14.jpeg)\n\nFigure 6: Implementation\
    \ for Nonlinear Functions.\n\nfunction, (), we can decompose it as a Fourier Series:\n\
    \n$$f(\\mathbf{x}) = \\sum\\_{k=1}^{N} a\\_k \\cdot \\sin\\left(2\\pi k \\mathbf{x}/L\\\
    right) + b\\_k \\cdot \\cos\\left(2\\pi k \\mathbf{x}/L\\right).$$\n\nwhere the\
    \ input, , is within the interval [−, ], and is the degree of the series (number\
    \ of terms). and , are invariant to the input, , and can be computed by:\n\n$$a\\\
    _k = \\int\\_{-L}^{L} f(t) \\sin \\left(2\\pi \\frac{kt}{L}\\right) dt, \\quad\
    \ b\\_k = \\int\\_{-L}^{L} f(t) \\cos \\left(2\\pi \\frac{kt}{L}\\right) dt$$\n\
    \nThese coefficients are unique for each nonlinear function. For an odd function\
    \ (), the symmetric integral yields = 0, and for an even function, = 0. We preload\
    \ the coefficients into the controller's register file. For a 5-bit precision\
    \ digital input, the maximum degree for the Fourier Series is 32, and the register\
    \ stores up to 64 5-bit coefficients for each nonlinear function.\n\nSince the\
    \ Fourier Series is periodic, it regulates the input within a specific interval.\
    \ For each nonlinear function, we define a corresponding effective interval. The\
    \ computation allows the input to be within this interval, − ≤ ≤ (e.g., using\
    \ = 4 for the Sigmoid function), and all > will be capped to . The quantized input\
    \ is a 5-bit data using ˆ = 16 · . For all multipliers of ˆ, e.g., 2,ˆ 3,ˆ · ·\
    \ · , larger than 16, we modulate them by 32, which can be achieved by reading\
    \ the last 5 bits of the 8-bit readouts.\n\nIn Figure [6,](#page-5-2) we illustrate\
    \ the flow of nonlinear functions in three steps: 1. compute the multiplier: The\
    \ input is modulated through an amplitude modulator and multiplied by the factors\
    \ 1, 2, 3, · · · , ; 2. intermediate result measurement: The output is measured\
    \ using 8-bit ADCs, with the last 5 bits extracted as the intermediate result\
    \ and stored in the output buffer; 3. phase encoding and final multiplication:\
    \ All multipliers ,ˆ 2,ˆ · · · ˆ are encoded as phase information and multiplied\
    \ by the corresponding coefficients 1, 2, · · · , . LightML is capable of batch\
    \ computing up to 128 inputs simultaneously, significantly enhancing throughput\
    \ for nonlinear operations. At the top of Figure [6,](#page-5-2) we use the 5-bit\
    \ precision error model from Section [3.2](#page-2-2) to simulate three non-linear\
    \ function: Sigmoid, tanh, and − 2 (Gaussian). The blue curve represents the ideal\
    \ 5-bit output, while the red error bars indicate the 3-sigma error range of our\
    \ implementation. The average error rate is 4.2%, which is generally tolerable\
    \ in most machine learning applications.\n\n### <span id=\"page-5-3\"></span>6.3\
    \ Element-Wise Operation\n\nElement-wise operations, such as addition and multiplication,\
    \ apply arithmetic operations to corresponding elements of two or more vectors\
    \ or matrices. They are not only common but also essential in\n\n<span id=\"page-6-1\"\
    ></span>Element-wise Operation\n\nFigure 7: Implementation for Element-Wise Operations.\n\
    \nmany AI models, particularly in skip connections, backpropagation, normalization,\
    \ and attention mechanisms. Note that element-wise arithmetic is a special case\
    \ of the dot product. These operations can be implemented straightforwardly, but\
    \ efficiency and accuracy must be carefully considered, as illustrated below.\n\
    \nAddition: On the left of Figure [7,](#page-6-1) we illustrate the addition of\
    \ two vectors, and . The inputs and are encoded as two consecutive pulses from\
    \ a vertical or horizontal modulator, respectively. A modulator in the perpendicular\
    \ direction generates constant light with unit amplitude, resulting in the output\
    \ 1 · + 1 · . However, computing + at a single crosspoint does not generate sufficient\
    \ charge accumulation in the capacitor. To rapidly accumulate enough current,\
    \ the unit light is duplicated <sup>1</sup> times, ensuring that all <sup>1</sup>\
    \ cross-points (indicated by the colored blocks with blue arrows in Figure [7\\\
    )](#page-6-1) produce the same output. This configuration allows for the computation\
    \ of 2(128 − 1) elements parallelly. Finally, we employ the circuit shown on the\
    \ right of Figure [7,](#page-6-1) which redirects and accumulates the currents\
    \ from all <sup>1</sup> cells into a single capacitor, producing the final output.\n\
    \nMultiplication: In the middle of Figure [7,](#page-6-1) we show an example of\
    \ element-wise multiplication between two vectors, and . The vector is modulated\
    \ from the horizontal direction, while is modulated from the vertical direction.\
    \ For each input pair and , <sup>2</sup> modulators are used repeatedly to accumulate\
    \ the current, and the final readout is the summation from 2 2 unit cells.\n\n\
    Scaling: Scaling involves multiplying an input matrix or vector, , by a constant\
    \ factor, . As shown on the right of Figure [7,](#page-6-1) this operation resembles\
    \ element-wise addition. We encode the coefficient <sup>3</sup> times in both\
    \ directions and use the remaining 128 − <sup>3</sup> modulators to encode the\
    \ input , as illustrated in the figure. The results are obtained by summing the\
    \ currents from <sup>3</sup> unit cells.\n\nThe duplicate numbers 1, 2, and <sup>3</sup>\
    \ are determined using an optimal strategy to balance charge accumulation and\
    \ parallelism. For a single cell, the charging duration is approximately 100 ns.\
    \ For duplicated cells, this duration is linearly reduced to <sup>100</sup> ns\
    \ (In the real-world scenario, it might need > 100 ns due to the imperfection\
    \ of the circuit, requiring a further recalibration). As the duplicate number\
    \ increases, parallelism is compromised. To maximize the number of element-wise\
    \ operations within a fixed duration, e.g., 100 ns, the number of operations can\
    \ be expressed as:\n\n$$\\text{Add:} (128-N\\_1) \\cdot N\\_1, \\quad \\text{Multi:}\
    \ (128/N\\_2) \\cdot N\\_2^2, \\quad \\text{Scal:} (128-N\\_3) \\cdot N\\_3.$$\n\
    \nwhere 128−<sup>1</sup> is the number of parallel inputs, and <sup>1</sup> =\
    \ 100/( <sup>100</sup> 1 ) is the number of operations for each cell that can\
    \ be conducted in 100ns. We solve the optimal duplicate numbers to be 1, <sup>3</sup>\
    \ = 64. This configuration enables 64 parallel inputs to be processed simultaneously,\
    \ with a charging duration of 100/64 ns per cell. For multiplication, larger values\
    \ of <sup>2</sup> produce better results. To\n\n<span id=\"page-6-2\"></span>![](_page_6_Figure_10.jpeg)\n\
    \nFigure 8: a) Circuit Design for Transposable Readout Module. b) Implementation\
    \ for Scaling Unit Via ADCs.\n\nensure consistency with addition and compatibility\
    \ with the same accumulation circuit, we select 2 2 = 64, resulting in <sup>2</sup>\
    \ = 8.\n\n#### <span id=\"page-6-0\"></span>6.4 Analog-Implemented Functions\n\
    \nReLU Unit: The Rectified Linear Unit (ReLU) is a widely used activation function\
    \ in machine learning, defined as ReLU() = max(0, ), which introduces non-linearity\
    \ into models. It can be efficiently implemented in an analog circuit, as a diode-based\
    \ analog filter can nullify negative values. In LightML, the ReLU function is\
    \ applied before the ADC readout. As the analog signal is transferred from the\
    \ capacitor to the ADC, a rectifier diode blocks any negative current, ensuring\
    \ that only positive values are passed.\n\nTransposable Readout: Matrix transpose\
    \ is frequently used in AI models such as transformers, e.g., BERT, GPT, natural\
    \ language processing, and recommendation systems. Supporting efficient transposition\
    \ is challenging for DRAM-based memory, as it requires O ( 2 ) non-sequential\
    \ read/write operations, disrupting data locality [\\[85\\]](#page-14-28). We\
    \ propose performing transposition at the ADC readout circuit, offering a faster\
    \ and more cost-effective solution compared to existing methods like the transpose\
    \ unit [\\[25\\]](#page-13-19) or in-plane transpose [\\[24\\]](#page-13-15).\n\
    \nAfter the optical crossbar completes the MAC operation, the results are stored\
    \ in capacitors as analog signals, and our approach transposes the intermediate\
    \ output when needed for the next layer. We illustrate the readout process in\
    \ Figure [8](#page-6-2) a). Each of the 128×128 results is connected to a port\
    \ that carries the analog voltage from the capacitors. Each crosspoint result\
    \ is connected to row and column selectors. When transposition is needed, the\
    \ selectors route the inputs: row selectors to column ADCs and column selectors\
    \ to row ADCs, effectively performing the transposition. While an ideal setup\
    \ would use one ADC per crosspoint, this approach can cause significant power\
    \ consumption, especially with high-resolution and fast-sampling ADCs. LightML\
    \ features 8×128 ADCs operating at 0.9 GHz, and this process repeats for 16 rounds\
    \ until all results are sent to the ADCs. The choice and number of ADCs are optimized\
    \ to align with the throughput of the crossbar, output buffer, input buffer, and\
    \ HBMs, details are in Section [8.4.](#page-10-0)\n\nScaling via ADCs: Section\
    \ [6.3](#page-5-3) introduces a general scaling method using the optical crossbar.\
    \ We propose another cost-effective alternative using ADCs for scaling with a\
    \ relatively small factor\n\nFully Connected Layer\n\n<span id=\"page-7-0\"></span>![](_page_7_Figure_1.jpeg)\n\
    \nFigure 9: Implementation of Linear Transformation.\n\n ∈ [0.25, 2], which is\
    \ useful for frequently applied operations like batch normalization. An ADC converts\
    \ continuous analog signals into discrete digital values, with the reference voltage\
    \ determining the range of signals it can convert. This reference voltage, , typically\
    \ represents the maximum voltage the ADC can measure and can be supplied externally\
    \ or generated internally. The input voltage is mapped to a digital value based\
    \ on its proportion to , following the formula: ˆ = × 2 (2 is the maximum digital\
    \ value for an n-bit ADC). Thus, the digital output is essentially divided by\
    \ and digitized. This relationship can be used to implement a division by a coefficient\
    \ by setting = , while the input is = . Note that functions effectively within\
    \ the range ∈ [0.25, 2], considering the error-tolerant range of an 8-bit ADC\
    \ [\\[57\\]](#page-14-29). Figure [8b](#page-6-2)) illustrates an example of scaling,\
    \ along with the corresponding PSpice[\\[54\\]](#page-14-30) simulation result.\
    \ Here, the input ranges from 0 to 1 V, with = 0.5 V. The ADC output accurately\
    \ reflects the input scaled by a factor of 2, demonstrating the precision of this\
    \ approach.\n\n#### 6.5 Linear Transformation\n\nA linear transformation is a\
    \ fundamental operation in AI models, involving the multiplication of an input\
    \ feature map with a weight matrix. Linear layers form the backbone of machine\
    \ learning models, and when combined with batch normalization and ReLU activations,\
    \ they become core building blocks of deep learning and neural networks. A general\
    \ linear layer applies a transformation to the input data and typically includes\
    \ MMM/MVM, Batch Normalization, and Activation Function, which can be expressed\
    \ as:\n\n#### = ReLU ( / − ′ )\n\nwhere ∈ R in × is the input matrix/vector, e.g.\
    \ feature map, with dimension in and is the number of batches, ∈ R in×out is the\
    \ weight matrix, and and are the mean and standard deviation for batch normalization.\
    \ Typically, the batch means and standard deviations are fixed during the inference\
    \ [\\[35\\]](#page-13-20) and are referred to as running\\_mean and running\\\
    _var in Pytorch Platform [\\[61\\]](#page-14-31).\n\nMatrix Tiling: Building on\
    \ existing MMM in photonic devices [\\[26,](#page-13-12) [71,](#page-14-10) [89\\\
    ]](#page-15-3), LightML also introduces matrix tiling to handle largescale MMM.\
    \ The optical crossbar supports MMMs by, at maximum, two 128 × 1024 matrices (See\
    \ Section [2.3\\)](#page-1-1), and tiling is necessary to accommodate MMMs with\
    \ larger scales. Since LightML can\n\nISCA '25, June 21–25, 2025, Tokyo, Japan\
    \ Liang Liu, Sadra Rahimi Kari, Xin Xin, Nathan Youngblood, Youtao Zhang, and\
    \ Jun Yang\n\nperform MMM for two 128 × 1024 matrices at one time, MMM for larger\
    \ matrices would need tiling, as depicted in Figure [9.](#page-7-0) We tile the\
    \ feature map into sub-matrices with 128 batches and 1024 infeatures and the weights\
    \ into sub-matrices with 1024 in-features and 128 out-features. In each iteration,\
    \ we compute the multiplication between these two tiled sub-matrices. We iterate\
    \ the computation in three loops: ① iterates over the batches. We sequentially\
    \ select 128 batches from the feature map and multiply them with the same weight\
    \ sub-matrix; ② iterates over the input features. The weight matrix slides 1,024\
    \ dimensions from left to right, while the feature map slides from top to bottom;\
    \ ③ iterates over the output features. Dataflow: In Figure [9,](#page-7-0) both\
    \ the input feature map and weights are tiled into 128 × 1024 submatrices to fit\
    \ into the crossbar. This process has two stages: In the MAC stage, the sub-matrices\
    \ , , , and undergo the MAC operation with the sub-matrices and in the optical\
    \ crossbar. The intermediate results, , ,, and of 128×128 matrices, are stored\
    \ in the output buffer. In the addition stage, we perform an element-wise addition\
    \ of the intermediate matrices. This involves summing the intermediate results\
    \ to obtain + and + .\n\nReLU and BatchNorm: The Batch Normalization consists\
    \ of two operations: dividing and subtracting , which can be performed through\
    \ ADCs scaling, and then the element-wise addition. and are initially stored in\
    \ the coefficient registers of the control unit. During the MAC stages, is converted\
    \ into voltages as the reference voltages to read the MAC output. During the addition\
    \ stage, we append the negative of the batch mean, − ′ 1 , − ′ 2 , · · · , − ′\
    \ out , as an extra term in the element-wise addition. Last, we use the analog\
    \ ReLU unit to filter out negative values before ADC Readout.\n\n#### 6.6 Convolutional\
    \ Operation\n\nConvolution in machine learning is a critical component primarily\
    \ for extracting features. The convolutional operation can be defined as = ⊛ ,\
    \ where ∈ R ××in is the input feature map with width and height both , and in\
    \ is the number of input channels. ∈ R ××in×out is a four-dimensional tensor representing\
    \ the weights, where is the kernel size andout is the number of output channels.\
    \ ∈ R ××out is the output feature map.\n\nChallenges: Implementing the convolutional\
    \ layer can be memory intensive due to the non-sequential distribution of convolution\
    \ windows in memory. A common solution is the im2col operation, which unfolds\
    \ the input feature maps into a matrix such that elements in each row correspond\
    \ to one convolutional window. In classical computing, im2col is typically performed\
    \ separately for each input channel, creating a 2 × <sup>2</sup> matrix for each\
    \ channel and then concatenating the in channels. This process requires loading\
    \ each feature map into the on-chip cache and performing at least 2 · 2 · in cache\
    \ read/write operations.\n\nOptimized Memory Structure: The proposed memory structure\
    \ eliminates the cache-wise im2col operation. Instead, we utilize the transposable\
    \ readout module, an on-chip address router, ensuring the computation unit and\
    \ memory are efficiently utilized in most scenarios. A two-stack HBM2E contains\
    \ 32 pseudo-channels and 8 banks for each, which can arbitrarily read 32KB of\
    \ data within each bank. We distribute the × × in feature map across 32 pseudo-channels,\
    \ with each channel storing ⌈in/32⌉ of × matrices.\n\nConv2d\n\n<span id=\"page-8-0\"\
    ></span>![](_page_8_Figure_2.jpeg)\n\nFigure 10: Implementation of Convolutional\
    \ Operation.\n\nWe group the elements in the same location for channels into a\
    \ line, and the im2col operation applies the same operation to the line rather\
    \ than a single element. Our approach transposes the feature map and the weight\
    \ into a matrix, where we span the × feature maps into 2 columns, and all columns\
    \ are concatenated into a matrix (it can be achieved by reading out from the row\
    \ selector). The weight is also converted into a 2 × matrix (prepared during the\
    \ offline period).\n\nFigure [10](#page-8-0) shows an example of a 4 × 4 × 3 feature\
    \ map being convolved with 2 × 2 × 3 × 3 weights. We use digits to annotate the\
    \ locations of elements in the feature map and different colors to represent different\
    \ channels. In the example, the three-channel feature map is distributed across\
    \ three pseudo-channels in the HBM, with their first bank, Bnk1, stores a 2 ×\
    \ 4 matrix, i.e., 1234-5678. During memory loading, each channel loads a 1×4 matrix\
    \ to the load row on the chip, forming a data line as 1234-1234-1234. Next, we\
    \ use the load router to reorder the data line into three lines: 111-222, 222-333,\
    \ and 333-444 and write these lines into the input buffer. By repeating the memory\
    \ loading process twice, we prepare the data for convolution. We then feed the\
    \ unfolded feature map and weights (weights are preprocessed and sequentially\
    \ read into the weights buffer) into the optical crossbar for the MAC operation.\
    \ The MAC result is a 3 × 3 matrix. We use the transposable readout to read the\
    \ mesh in a column-wise order, distributing results for different output channels\
    \ into different buffer lines. The same data structure stores the results in the\
    \ HBM, which will serve as the input feature map for the next layer.\n\nTiling:\
    \ The tiling for the convolutional layer is similar to the linear layer, involving\
    \ three loops: ①: As the optical crossbar has 128 ports, we tile a × feature map\
    \ by selecting = ⌊128/⌋ rows to form sub-matrices of no more than 128 elements.\
    \ If exceeds 128, we split each row into ⌈/128⌉ 128-dimension chunks. For each\
    \ iteration, besides the target rows, we fetch ( −1) additional rows to the input\
    \ buffer as the top and bottom rows required for the convolution. This loop repeats\
    \ ⌈/⌉ times. ②: Given the limit of 1,024 MAC operations, each iteration accommodates\
    \ up to ⌈1024/ 2 ⌉ input channels, forming no more than 1,024 MAC operations.\
    \ This loop repeats ⌈in × 2 /1024⌉ times. ③: The final loop iterates over the\
    \ output channels, selecting 128 output channels each time and repeating for ⌈out/128⌉\
    \ rounds.\n\n### <span id=\"page-8-2\"></span>7 Pipelining and Scheduling\n\n\
    To ensure all hardware units remain fully utilized, the computing speed of the\
    \ crossbar must be aligned with the data transfer rates to\n\n<span id=\"page-8-1\"\
    ></span>![](_page_8_Figure_9.jpeg)\n\nFigure 11: Example of Scheduling for Linear\
    \ Transformation.\n\nand from the input and output buffers. A pipeline illustrating\
    \ memory loads, stores, computation, analog circuit, and ADC readout is shown\
    \ in Figure [11,](#page-8-1) where we capture a scheduling example of a MMM operation\
    \ tiled into 4 submatrices. In the computing unit, the crossbar's latency is proportional\
    \ to the length of the dot-product operation, determined by the number of pulses\
    \ modulated at each crosspoint. For a modulator operating at 12 GHz and a crossbar\
    \ performing 1,024 continuous pulses, each dot-product operation takes approximately\
    \ 85 ns. The analog circuit, including the ReLU unit, requires 5 ns to stabilize\
    \ its output (Simulated Via PSpice). Additionally, the system incorporates 8×128\
    \ ADCs operating at 0.9 GHz, which need 16 rounds (17.7 ns) to complete the readout.\
    \ On the memory side, feature maps are transmitted from HBM modules to the input\
    \ buffers. A double-buffer scheme allows one buffer to receive data from the HBM\
    \ while the other feeds the modulator. Since SRAM buffers are significantly faster\
    \ than HBM, the bottleneck lies in the HBM read latency. For a sequential read\
    \ of a 128 × 1024 matrix, the latency is approximately 97 ns. Subsequently, the\
    \ results from the output buffer, a 128 × 128 matrix, are written back to the\
    \ HBMs, requiring around 12 ns.\n\n#### 8 Evaluations\n\n### 8.1 Experiment Setup\n\
    \nModel and Dataset: In this work, we evaluate the performance of LightML across\
    \ a diverse set of CNNs and datasets. For the dataset, we use CIFAR-10 [\\[40\\\
    ]](#page-13-21), CIFAR-100 [\\[40\\]](#page-13-21), and ImageNet [\\[14\\]](#page-13-22).\
    \ The evaluated models cover a spectrum of sizes, we use ResNet-18 (11M parameters),\
    \ ResNet-50 (25M), ResNet-101 (45M) [\\[29\\]](#page-13-23), VGG-11 (133M), VGG-16\
    \ (138M), VGG-19 (144M) [\\[70\\]](#page-14-32), MobileNet-V2 (3.5M), MobileNet-V3-Large\
    \ (5.4M) [\\[32\\]](#page-13-24). We choose 32 as the batch size for the inference\
    \ images, where the A100 GPU reaches its maximum efficiency.\n\nSimulation and\
    \ Platform: We implement LightML in a cycleaccurate simulator from scratch. We\
    \ implement the modules for\n\n9\n\n<span id=\"page-9-0\"></span>\n\n| Component\
    \       | Size / #     | Freq.(Hz)<br>Power |        | Area(mm2<br>) |\n|-----------------|--------------|--------------------|--------|---------------|\n\
    | On-Chip Buffer  | 256/128/64KB | 2G                 | 84mW   | 0.68        \
    \  |\n| ADCs [57]       | 128x8        | 0.9G               | 1.93W  | 5.73  \
    \        |\n| Capacitors      | 128x128x2    | -                  | -      | -\
    \             |\n| Laser Source[7] | 1            | -                  | 120mW\
    \  | -             |\n| Modulators [84] | 128x2+128    | 12G                |\
    \ 810mW  | 69.1          |\n| Detectors[55]   | 128x128x2    | 12G           \
    \     | 21.8mW | 235           |\n| On-Chip Total   | -            | -       \
    \           | 2.97W  | 310           |\n| HBM [42]        | 2x16GB       | 1.6G\
    \               | ∼2x8W  | ∼2x80         |\n| Total           | -            |\
    \ -                  | ∼19W   | ∼470          |\n\neach proposed built-in function\
    \ unit in the simulator. The simulator includes the memory module, the on-chip\
    \ buffer, and the optical crossbar. The analog circuit, comprising the RC charging\
    \ circuit and ADC readout, is simulated using PSpice [\\[54\\]](#page-14-30),\
    \ while the power and area of the on-chip buffers are estimated with CACTI [\\\
    [4\\]](#page-13-26).\n\nFor the baseline comparison, latency results are measured\
    \ on a real machine running the PyTorch platform [\\[61\\]](#page-14-31). The\
    \ hardware setup includes an Intel i9-13900K for CPU, an NVIDIA A100 [\\[9,](#page-13-27)\
    \ [10\\]](#page-13-28) for GPU, and a Google TPU V3 [\\[37\\]](#page-13-29).GPU\
    \ and TPU measurements use FP16 data precision, and CPU uses Int8 (Int8 and Int4\
    \ are not widely adopted in CUDA or XLA environments. The precision configuration\
    \ is aligned with existing crossbar work [\\[28,](#page-13-30) [34,](#page-13-31)\
    \ [72\\]](#page-14-35)).\n\nThe implementation and experimental setup are available\
    \ at: [https://github.com/Liang78825/LightML.git.](https://github.com/Liang78825/LightML.git)\n\
    \n#### 8.2 Specification Detail\n\nOptical Layout: In our layout, we have assumed\
    \ that each crossbar unit cell is 120m x 120m [\\[89\\]](#page-15-3). This space\
    \ includes the two directional couplers, 3 dB splitter, and on-chip balanced Ge\
    \ photodetectors. A recent work by Rogers et al. [\\[64\\]](#page-14-15) demonstrated\
    \ a functional 512-pixel LiDAR receiver array on a silicon photonic platform that\
    \ contained one directional coupler, 3 dB splitter, balanced Ge photodetectors,\
    \ and an optical antenna in an 80m x 100m unit cell. Notably, in this example,\
    \ the optical antenna (excluded from our platform) comprised the majority of the\
    \ layout space rather than the other optical components.\n\nElectrical Devices:\
    \ The RC charging circuit was designed using PSpice simulations to minimize charging\
    \ duration while accumulating optical current to 1V with minimal noise. The circuit\
    \ employs two optical-electrical transceivers [\\[60\\]](#page-14-0), producing\
    \ currents from -600 nA to 600 nA. It incorporates a 15 fF capacitor (similar\
    \ to DDR4 technology), a 500 Ω resistor, and an 8-bit SAR-ADC operating at 0.9\
    \ GHz[\\[57\\]](#page-14-29). Through the simulation result of the RC circuit,\
    \ the\n\ncircuit's error distribution is characterized by N (0.00037, 0.0024)\
    \ after 1,024 pulse accumulations.\n\nTable [3](#page-9-0) summarizes the hardware\
    \ configuration of LightML. The on-chip buffer consists of three double buffers:\
    \ an input buffer (256KB), a weight buffer (128KB), and an output buffer (64KB),\
    \ operating at a frequency of 2 GHz. LightML adopts 8 sets of 128 ADCs. Each crosspoint\
    \ in the 128×128 array contains 2 capacitors, and its area is included in the\
    \ Detector modules. The laser source[\\[7\\]](#page-13-25) is integrated externally,\
    \ operating at a wavelength of 1550nm, with adjustable power consumption up to\
    \ 150mW. The modulators[\\[84\\]](#page-14-20) are arranged in 128×2 for amplitude\
    \ and 128 for phase. The modulator frequency can be scaled up to 50GHz, and we\
    \ choose 12GHz to match the memory throughput. The homodyne detection scheme requires\
    \ 2 photodetectors in every crosspoint of the 128×128 crossbar. Overall, LightML\
    \ has a total power consumption of 2.97W and occupies an area of 310 mm<sup>2</sup>\
    \ . In addition to considering on-chip integrated HBM, we also investigate Micron's\
    \ HBM2E [\\[75\\]](#page-14-36). The power consumption of HBM2E varies with temperature,\
    \ supply voltage, and workload. For our analysis, we assume a supply voltage of\
    \ 1V under full load at room temperature, which results in a power consumption\
    \ of 8W per stack as reported in the technical documentation[\\[42\\]](#page-14-34).\
    \ Moreover, based on data from the NVIDIA A100 [\\[9\\]](#page-13-27), which utilizes\
    \ an 8-stack HBM2E configuration—we estimate the package area to be approximately\
    \ 80 mm<sup>2</sup> .\n\n#### 8.3 Comparison with Existing Accelerators:\n\nTo\
    \ the best of our knowledge, this is the first study to provide architectural\
    \ support for photonic crossbars. Some previous studies have performed high-level\
    \ evaluations but with limited architectural design (details in Section [10\\\
    )](#page-12-1). In this work, we deliver a comparison with general-purpose ML\
    \ processing via CPU, GPU, and TPU, and classical PE-based accelerator (ThinkFast[\\\
    [1\\]](#page-13-32)), PE-based PIM (SP-PIM [\\[39\\]](#page-13-33)), and ReRAM-based\
    \ crossbar RRAM-CIM[\\[80\\]](#page-14-37) and RRAM-CIM2 [\\[87\\]](#page-15-7).\
    \ The comparison includes performance efficiency (PE) and computational efficiency\
    \ (CE).\n\nPower consumption is a critical factor for scalability and deployment\
    \ in edge devices. GPUs, TPUs, and ThinkFast consume 210W, 185W, and 300W, whereas\
    \ LightML operates at only 2.97W. Note that, because the memory architectures\
    \ differ across platforms, the reported power values exclude the memory subsystem.\
    \ PE and CE highlight the benefits of LightML. With a PE of 109 TOP/s/W and a\
    \ CE of 1.04 TOP/s/mm<sup>2</sup> , LightML outperforms GPU by 109/1.48 = 73.6×\
    \ and the state-of-the-art NVM-based accelerators by more than 105/57 = 1.91×\
    \ in terms of efficiency. The comparison indicates that LightML offers a compelling\
    \ combination of\n\n| Platform           | GPU        | TPU        | ThinkFast[1]\
    \ | SP-PIM[39] | RRAM-CIM[80] | RRAM-CIM2[87] | LightML  |\n|--------------------|------------|------------|--------------|------------|--------------|---------------|----------|\n\
    | Technology         | 7nm        | 12nm       | 14nm         | 28nm       | 130nm\
    \        | 40nm          | 28nm     |\n| Frequency          | 765MHz     | 940MHz\
    \     | 0.9GHz       | 450MHz     | -            | 100MHz        | 12GHz    |\n\
    | Precision          | FP16       | FP16       | Int8         | FP16       | Int2/4\
    \       | Int2/4        | Int5     |\n| Peak Perf.         | 312TFlOP/s | 180TFlOP/s\
    \ | 820TOP/s     | 3.2TOP/s   | 1.8TOP/s     | 0.63TOP/s     | 325TOP/s |\n| Area\
    \               | 826mm2     | 648mm2     | 725mm2       | 5.75mm2    | 159mm2\
    \       | 0.4mm2        | 310mm2   |\n| Power              | ∼210W      | ∼185W\
    \      | 300W         | 0.14W      | 45mW         | 11mW          | 2.97W    |\n\
    | PE (TOP/s/W)       | 1.48       | 0.97       | 2.73         | 22.4       | 40\
    \           | 57            | 109      |\n| CE (TOP/s/mm2<br>) | 0.37       |\
    \ 0.27       | 1.13         | 0.55       | 0.011        | 1.57          | 1.04\
    \     |\n\nTable 2: Comparison of Various AI Accelerators Across Different Specs.\n\
    \n<span id=\"page-10-2\"></span>![](_page_10_Figure_2.jpeg)\n\nFigure 12: (a),\
    \ (b), and (c) show the normalized inference latency for ResNet18, ResNet50, ResNet101\
    \ (R18, R50, R101), VGG11, VGG16, VGG19 (V11, V16, V19), and MobileNet-V2 (M-V2)\
    \ with 32-batch input images from ImageNet, CIFAR-100, and CIFAR-10; (e) present\
    \ the latency for a × × × 32 feature map convolving with a 3 × 3 × × kernel; (d),\
    \ (f), (g), and (h) show the latency results for × × 32 inputs.\n\nhigh performance,\
    \ low power consumption, and efficient utilization of area, making it a suitable\
    \ candidate for various ML applications, particularly in power-constrained environments.\
    \ If the power consumption of memory components (i.e., HBMs) is taken into account,\
    \ then LightML outperforms the GPU by a factor of (325/19W)/(312/250W)=13.6x.\n\
    \n#### <span id=\"page-10-0\"></span>8.4 Sensitive Study – Number of ADCs\n\n\
    An ADC typically has long latency and consumes a non-negligible amount of power.\
    \ If every crosspoint in the crossbar is equipped with its own ADC, the total\
    \ ADC power would dominate the overall power consumption, thereby reducing the\
    \ power efficiency (PE). As an alternative, a single ADC can be multiplexed across\
    \ multiple crosspoints to lower the total ADC power—for example, a single row\
    \ of ADCs can serve several rows of the crossbar. Thus, the ADC configuration\
    \ plays a critical role in balancing PE and performance.\n\nTo evaluate this tradeoff,\
    \ we vary the number of ADC rows multiplexed across the entire crossbar: 1x128,\
    \ 2x128, 4x128, 8x128, and 16x128. The results are summarized in Table [4.](#page-10-1)\
    \ As expected, power consumption increases with the number of ADCs, ranging from\
    \ 1.27W to 4.88W. Peak performance also improves with more ADCs, increasing from\
    \ 147TOP/s to 356TOP/s. PE reaches its maximum at the 4x128 configuration.\n\n\
    |  |  |  | Table 4: Sensitive Study of ADC Configuration. |\n|--|--|--|------------------------------------------------|\n\
    |--|--|--|------------------------------------------------|\n\n<span id=\"page-10-1\"\
    ></span>\n\n| ADC Conf.     | 1x128 | 2x128 | 4x128 | 8x128 | 16x128 |  |\n|---------------|-------|-------|-------|-------|--------|--|\n\
    | Power (W)     | 1.27  | 1.51  | 1.99  | 2.97  | 4.88   |  |\n| Perf. (TOP/s)\
    \ | 147   | 214   | 277   | 325   | 356    |  |\n| PE (TOP/s/W)  | 115   | 141\
    \   | 138   | 109   | 72     |  |\n| Latency       | 9.4ms | 6.3ms | 4.8ms | 4.0ms\
    \ | 3.7ms  |  |\n| Image/W/s     | 2686  | 3351  | 3348  | 2670  | 1792   |  |\n\
    | Latency       | 39ms  | 23ms  | 16ms  | 13ms  | 11ms   |  |\n| Image/W/s   \
    \  | 641   | 889   | 975   | 822   | 563    |  |\n| Latency       | 63ms  | 39ms\
    \  | 27ms  | 23ms  | 20ms   |  |\n| Image/W/s     | 397   | 538   | 577   | 466\
    \   | 314    |  |\n|               |       |       |       |       |        |\
    \  |\n\nAdditionally, we evaluate the latency of model inferences with respect\
    \ to the ADC configuration, specifically for ResNet18, 50, and 101 running on\
    \ ImageNet with a batch size of 32. The latency and energy efficiency (measured\
    \ as the number of images processed per Watt per second) are detailed in Table\
    \ [4.](#page-10-1) The results show that the 4x128 configuration not only provides\
    \ the best PE but also demonstrates optimal efficiency for processing images.\
    \ Overall, these findings highlight the importance of selecting an appropriate\
    \ number of ADCs to optimize the balance between power consumption and performance\
    \ in LightML. Notably, the 8x128 setup offers an 17% performance gain at the cost\
    \ of only 1W additional power, making it a worthwhile tradeoff.\n\n#### 8.5 Latency\
    \ Comparison across ML Models/Ops\n\nIn this section, we compare the latency of\
    \ LightML with that of CPU, GPU, and TPU. Latency data for CPU, GPU, and TPU is\
    \ collected from real machines, the latency for the baseline machine is measured\
    \ after the model and data are loaded to the device memory (model.to(device)),\
    \ and we repeat the inference for 150 rounds and average the results for the last\
    \ 100 rounds. GPU and TPU devices are synchronized before the start of each round.\
    \ We use a batch size of 32 for all latency evaluations. We evaluate the normalized\
    \ Latency by dividing the actual latency of each device by the latency of LightML.\n\
    \nFigures [12](#page-10-2) f), [12](#page-10-2) g), and [12](#page-10-2) h) assess\
    \ the normalized latency of Addition, Multiplication, and Scaling. The results\
    \ show that LightML is not optimal for element-wise operations due to poor crossbar\
    \ utilization, with at most 1/64 of the crossbar being used. Specifically, LightML\
    \ is 8.2x to 9.7x slower than the NVIDIA A100 for Multiplication and 1.9x to 2.1x\
    \ slower for Scaling.\n\nFigures [12d](#page-10-2)) and [12e](#page-10-2)) evaluate\
    \ the latency of the Linear and Convolutional (Conv) layers. The evaluation for\
    \ convolution and linear layers integrates ReLU and Batch Normalization. The results\
    \ show that LightML outperforms GPU by more than 1.4x for the linear layer, with\
    \ a performance gain of up to 3.2x for smaller feature\n\n<span id=\"page-11-0\"\
    ></span>Table 5: Comparison of Existing Non-Linear Function Units. Utilization\n\
    \n|                | Bits | Delay  | Extra | Extra   | Supported |\n|----------------|------|--------|-------|---------|-----------|\n\
    | NFU            |      |        | Power | Area    | Functions |\n| Zamanlogy\
    \ [91] | 6    | 2.12ns | 64mW  | 0.27mm2 | \U0001D70E         |\n| PLAN[59]  \
    \     | 8    | 3.71ns | 57mW  | -       | \U0001D70E         |\n| RALUT [43] \
    \    | 8    | 2.12ns | -     | 1.06mm2 | \U0001D70E, tanh   |\n| Optim-LUT [49]\
    \ | 8    | 2.82ns | 590mW | 0.78mm2 | \U0001D70E, tanh   |\n| PWL LUT [78]   |\
    \ 8    | 0.95ns | 466mW | 0.22mm2 | \U0001D70E, tanh   |\n| LightML        | 5\
    \    | 3.1ns  | N/A   | 0       | Any       |\n\nmaps. For the convolutional layer,\
    \ LightML slightly outperforms the GPU for 64 × 64 and 32 × 32 inputs, with performance\
    \ gains ranging from 0.7x to 1.6x.\n\nFigures [12](#page-10-2) a), b), and c)\
    \ present the normalized latency results for ImageNet, CIFAR-100, and CIFAR-10,\
    \ respectively. For ResNet models, the latency of LightML is approximately 0.9x\
    \ to 1.3x compared to GPU. For VGG models, LightML outperforms the GPU with performance\
    \ gains of 1.3x to 4.0x.\n\nAs shown in the results, LightML provides promising\
    \ performance compared to GPU and TPU devices, operating at a power consumption\
    \ of only 3W. However, there are two main drawbacks: 1. The crossbar dimension\
    \ (128×128) is small compared to the GPU and TPU, where the A100 has 108×256 tensor\
    \ cores, each delivering 4 FP16 operations per core [\\[9,](#page-13-27) [10\\\
    ]](#page-13-28). TPU v3 comprises four Matrix-Multiplication Units, each performing\
    \ MMM for 128×128×2 input [\\[37\\]](#page-13-29). 2. LightML is not well-suited\
    \ for element-wise operations due to low crosspoint utilization, a limitation\
    \ shared by NVM-based crossbars. Future optimizations could consider incorporating\
    \ a dedicated optical unit specifically designed for element-wise operations.\n\
    \n#### 8.6 Comparison of Non-Linear Function Units\n\nPrior Non-linear Function\
    \ Unit (NFU) in ML accelerators generally falls into two categories. The first\
    \ uses modular approximations (e.g., PieceWise Linear (PWL)) [\\[59,](#page-14-38)\
    \ [91\\]](#page-15-8), which segment nonlinear functions into linear pieces but\
    \ have limited applicability (e.g., sigmoid only). The second uses Look-Up Tables\
    \ (LUTs) to support a broader range of functions at the cost of higher area and\
    \ power. Other methods, such as Taylor expansions, are generally too costly. In\
    \ contrast, LightML computes nonlinear functions using two rounds of ADC readouts.\
    \ Similar to element-wise operations, its NFU supports (128 − 20) × 2 = 216 inputs\
    \ concurrently: 108 vertical modulators generate the phase and 20 horizontal modulators\
    \ for amplitude, with a similar configuration in the other direction. No extra\
    \ hardware is required—our optical modulators naturally generate sine and cosine\
    \ functions, enabling arbitrary nonlinear function implementation.\n\nIn Table\
    \ [5,](#page-11-0) we compare the NFU of LightML against prior methods. Since\
    \ the NFU in LightML can process 216 inputs per round, we use the same number\
    \ of NFU units in the baseline for a fair comparison. Compared to existing approaches,\
    \ LightML offers two key advantages: 1. It leverages existing hardware without\
    \ requiring additional circuits, resulting in negligible power and area overhead.\
    \ This simplifies fabrication and eliminates the need for data movement across\
    \ different function units. 2. The optical NFU can implement a broad range of\
    \ nonlinear functions using the same hardware, which is hard to achieve in prior\
    \ methods.\n\n<span id=\"page-11-1\"></span>![](_page_11_Figure_10.jpeg)\n\nFigure\
    \ 13: Utilization of Memory / Computing Unit.\n\n#### 8.7 Utilization Analysis\
    \ of LightML\n\nIn this section, we analyze the utilization of three key components\
    \ of LightML: memory load/store, optical MAC, and ADC readout. As discussed in\
    \ Section [7,](#page-8-2) the utilization encompasses two parallel processes:\
    \ (1) involves the memory process and (2) involves the computing unit.\n\nThe\
    \ utilization results are depicted in Figure [13,](#page-11-1) and the notations\
    \ and configuration are the same as Figure [12.](#page-10-2) The color bar shows\
    \ the breakdown latency of each operation. For example, when we measure the individual\
    \ latency of each process, and the utilization is calculated by dividing the component's\
    \ latency by the overall latency. For memory operations, we display only the load\
    \ and store latency, excluding the latency due to page faults and row activation,\
    \ which are included in the overall latency but not shown in the figure. Figure\
    \ [13](#page-11-1) a) illustrates the utilization when computing the convolutional\
    \ layer. For the convolution layer, the memory process shows under-saturation,\
    \ with utilization rates between 40% and 60%. Conversely, the computing unit achieves\
    \ more than 90% utilization.\n\n#### 8.8 Impact of Precision on Model Accuracy\n\
    \nIn this section, we evaluate the accuracy of ML models using quantization sampled\
    \ from crossbar devices. We simulate = 5 and = 8 modes by quantizing the inputs\
    \ and outputs of each MAC operation to 5 and 8 bits, respectively, and applying\
    \ the same Gaussian noise as described in Section [3.2.](#page-2-2) In 5-bit mode,\
    \ the error rate is low (0.0034), while in 8-bit mode, the error rate reaches\
    \ 0.632; however, the mean error distance is lower in the 8-bit mode. For simulating\
    \ the NVM-based crossbar, we use 4-bit precision to encode the intermediate map\
    \ and ( = 2 or = 3)-bits for the weights. We assume separate functional units\
    \ for addition and non-linear functions, providing 8-bit precision. All noise\
    \ and precision loss in the simulation are generated using Gaussian noise.\n\n\
    The results, presented in Table [6,](#page-12-0) show the accuracy of different\
    \ models under varying precision levels. The simulations were conducted on the\
    \ PyTorch platform using a three-layer convolutional network for MNIST [\\[15\\\
    ]](#page-13-34), ResNet-18 for CIFAR-10, and MobileNetV2 for ImageNet. For MNIST,\
    \ LightML matches the GPU/TPU performance, reaching 99.2% accuracy. For CIFAR-10\
    \ and ImageNet, LightML is slightly lower than the accuracy of GPU/TPU but significantly\
    \ higher than the NVM-based crossbars.\n\n<span id=\"page-12-0\"></span>Table\
    \ 6: Accuracy under Different Precision Models.\n\n| Dataset            | MNIST\
    \ | CIFAR-10 | ImageNet |\n|--------------------|-------|----------|----------|\n\
    | NVM (\U0001D441\U0001D44F<br>= 2)    | 98.1% | 84.8%    | 60.3%    |\n| NVM\
    \ (\U0001D441\U0001D44F<br>= 3)    | 99.0% | 86.4%    | 63.5%    |\n| LightML(\U0001D441\
    \U0001D44F<br>= 5) | 99.2% | 90.6%    | 66.1%    |\n| LightML(\U0001D441\U0001D44F\
    <br>= 8) | 99.1% | 91.1%    | 66.5%    |\n| GPU/TPU (FP16)     | 99.2% | 92.4%\
    \    | 69.8%    |\n\n#### 9 Feasibility for Large Language Models\n\nLarge Language\
    \ Models (LLMs) [\\[6,](#page-13-35) [16,](#page-13-36) [79\\]](#page-14-23) are\
    \ widely used in machine learning but demand substantial computation and energy\
    \ for training and inference. Recent studies show they tolerate lower precision,\
    \ with only a 1–2% accuracy drop using FP8 and FP4 [\\[23\\]](#page-13-37), making\
    \ them viable for LightML deployment.\n\nAn algorithm example of an attention\
    \ block is shown below. This block utilizes two LightML functions: LightML.Linear,\
    \ which performs the linear transformation to compute Q, K, and V matrices; and\
    \ LightML.NonLinear, which computes the softmax via a nonlinear function. Additionally,\
    \ memory operations, such as head splitting and token padding, are simulated using\
    \ a basic memory read/write strategy.\n\n|    | Algorithm 1 Implementation for\
    \ Attention Block                                                            \
    \                                             |\n|----|--------------------------------------------------------------------------------------------------------------------------------------------------------|\n\
    |    | 1: ⊲ x = Input Token<br>2: xq = LightML.Linear(x, W_q)                \
    \                                                                            \
    \     |\n|    | 3: xk = LightML.Linear(x, W_k, transpose=True)<br>4: xv = LightML.Linear(x,\
    \ W_v)                                                                       |\n\
    |    | 5: queries[:] = Memory.Split_Head(xq, num_head)<br>6: keys[:] = Memory.Split_Head(xk,\
    \ num_head)                                                        |\n|    | 7:\
    \ for i in range(num_head) do<br>√                                           \
    \                                                                         |\n\
    | 8: | scores[i] = LightML.Linear(queries[i], keys[i], scale =<br>head_dim)<br>9:\
    \ end for                                                                    \
    \ |\n|    | 10: scores = LightML.NonLinear(scores, 'softmax')<br>11: scores =\
    \ Memory.Combine_head(scores, num_head)<br>12: output = LightML.Linear(scores,\
    \ values) |\n\nWe present a preliminary implementation of BERT[\\[16\\]](#page-13-36),\
    \ Llama 3.1 8B[\\[77\\]](#page-14-42), and ViT/B-16[\\[17\\]](#page-13-38), normalized\
    \ to the performance of LightML as shown in Figure [14.](#page-12-2) We compare\
    \ against two baselines—Google TPU and NVIDIA A100 GPU—both tested on real machines,\
    \ with the GPU using INT8 quantization. For BERT and Llama, tokenization is assumed\
    \ to run on the CPU, processing input sequences of class digits. Sequences start\
    \ with eight tokens, with model completion up to 128 tokens. For the ViT, inputs\
    \ are sampled from ImageNet with 384 × 384 pixel images and 1,000 classes.\n\n\
    Our current implementations lack proper optimizations for LLM models, resulting\
    \ in performance disadvantages compared to the two baselines. In the Llama model,\
    \ an NVIDIA A100 GPU is about 2.2x faster than LightML, but LightML achieves 6x\
    \ higher energy efficiency per token. A detailed analysis of the attention block\
    \ reveals two key inefficiencies. First, attention inputs, represented by the\
    \ and matrices of size tokens × , often have fewer token counts (tokens) than\
    \ the crossbar dimension (128) for short sequences, leading to underutilization.\
    \ Second, element-wise operations (e.g., addition) on large matrices are computationally\
    \ inefficient on the crossbar, contributing 20% to attention overhead. Overall,\
    \ while our results highlight the potential of photonic circuits for LLMs, further\
    \ optimizations are needed for future work.\n\n<span id=\"page-12-2\"></span>![](_page_12_Figure_10.jpeg)\n\
    \nFigure 14: LLM Inference Latency.\n\n#### <span id=\"page-12-1\"></span>10 Related\
    \ Work\n\nHamerly et al. [\\[26\\]](#page-13-12) propose an optical architecture\
    \ using homodyne detection to perform MAC operations. The system relies on an\
    \ electronic controller, such as a CPU, to generate input signals and handle nonlinear\
    \ functions. By reformulating convolutions as matrix-matrix multiplications, they\
    \ demonstrate support for both fully connected and convolutional layers. The design\
    \ achieves 100aJ per MAC (50mW) on a 100×100 crossbar, excluding power for memory,\
    \ ADCs, and the laser source.\n\nSludds et al. [\\[71\\]](#page-14-10) present\
    \ Netcast, an optical edge-computing architecture where model weights are encoded\
    \ via wavelengthdivision multiplexing (WDM) and transmitted as light from the\
    \ cloud to an on-device crossbar for processing. Demonstrated on a two-layer MLP\
    \ for MNIST, Netcast achieves energy consumption as low as 40aJ per operation\
    \ (40mW).\n\nLi et al. [\\[45\\]](#page-14-43) present a scalable accelerator\
    \ that leverages a modular design where each chiplet contains processing elements\
    \ interconnected using optical waveguides.\n\nShiflett et al. [\\[69\\]](#page-14-44)\
    \ propose Flumen, a dual-function photonic interconnect that enables both data\
    \ communication and in-network computation. The architecture utilizes a Mach-Zehnder\
    \ interferometer mesh to perform MMM directly on optical signals through an SVD-based\
    \ transformation. This design allows dynamic switching between communication and\
    \ computation.\n\nGiamougiannis et al. [\\[22\\]](#page-13-39) design a coherent\
    \ photonic crossbar (Xbar) that maps matrix elements to dedicated optical nodes.\
    \ Rather than decomposing a matrix via SVD, the Xbar modulates the input signal\
    \ with programmable amplitude and phase at each node, enabling scalable linear\
    \ transformations with linear loss dependence.\n\nTo the best of our knowledge,\
    \ LightML is the first architecture to support an optical computing device, efficiently\
    \ handling highbandwidth optical signals with pipelined model execution and optimized\
    \ power and compute efficiency. It also pioneers the implementation of general\
    \ nonlinear functions within the optical crossbar—a key capability unmet by prior\
    \ opto-electronic accelerators and crucial for modern AI models.\n\n#### 11 Conclusion\n\
    \nWe presented LightML, a high-performance photonic computing architecture optimized\
    \ for ML applications. Supporting diverse ML operations, LightML is ideal for\
    \ edge devices and data centers. It offers superior computing speed and energy\
    \ efficiency, outperforming state-of-the-art solutions.\n\n#### Acknowledgments\n\
    \nThe authors would like to thank the anonymous reviewers for their valuable feedback\
    \ and suggestions. This work is supported by NSF grants #2334628, #2337674, #2154973,\
    \ as well as AFOSR grant FA9550-24-1-0064.\n\nISCA '25, June 21–25, 2025, Tokyo,\
    \ Japan Liang Liu, Sadra Rahimi Kari, Xin Xin, Nathan Youngblood, Youtao Zhang,\
    \ and Jun Yang\n\n#### References\n\n- <span id=\"page-13-32\"></span>[1] Dennis\
    \ Abts, Jonathan Ross, Jonathan Sparling, Mark Wong-VanHaren, Max Baker, Tom Hawkins,\
    \ Andrew Bell, John Thompson, Temesghen Kahsai, Garrin Kimmell, et al. 2020. Think\
    \ fast: A tensor streaming processor (TSP) for accelerating deep learning workloads.\
    \ In 2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture\
    \ (ISCA). IEEE, 145–158.\n- <span id=\"page-13-0\"></span>[2] Shuroug A Alowais,\
    \ Sahar S Alghamdi, Nada Alsuhebany, Tariq Alqahtani, Abdulrahman I Alshaya, Sumaya\
    \ N Almohareb, Atheer Aldairem, Mohammed Alrashed, Khalid Bin Saleh, Hisham A\
    \ Badreldin, et al. 2023. Revolutionizing healthcare: the role of artificial intelligence\
    \ in clinical practice. BMC Medical Education 23, 1 (2023), 689.\n- <span id=\"\
    page-13-7\"></span>[3] Farshid Ashtiani, Alexander J. Geers, and Firooz Aflatouni.\
    \ 2022. An on-chip photonic deep neural network for image classification. Nature\
    \ 606 (6 2022), 501–506. Issue 7914.<https://doi.org/10.1038/s41586-022-04714-0>\n\
    - <span id=\"page-13-26\"></span>[4] Rajeev Balasubramonian, Andrew B Kahng, Naveen\
    \ Muralimanohar, Ali Shafiee, and Vaishnav Srinivas. 2017. CACTI 7: New tools\
    \ for interconnect exploration in innovative off-chip memories. ACM Transactions\
    \ on Architecture and Code Optimization (TACO) 14, 2 (2017), 1–25.\n- <span id=\"\
    page-13-2\"></span>[5] Colby R Banbury, Vijay Janapa Reddi, Max Lam, William Fu,\
    \ Amin Fazel, Jeremy Holleman, Xinyuan Huang, Robert Hurtado, David Kanter, Anton\
    \ Lokhmotov, et al. 2020. Benchmarking tinyml systems: Challenges and direction.\
    \ arXiv preprint arXiv:2003.04821 (2020).\n- <span id=\"page-13-35\"></span>[6]\
    \ Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla\
    \ Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et\
    \ al. 2020. Language Models are Few-Shot Learners. arXiv preprint arXiv:2005.14165\
    \ (2020).\n- <span id=\"page-13-25\"></span>[7] Bigeng Chen, Xingshi Yu, Xia Chen,\
    \ Milan M Milosevic, David J Thomson, Ali Z Khokhar, Shinichi Saito, Otto L Muskens,\
    \ and Graham T Reed. 2018. Real-time monitoring and gradient feedback enable accurate\
    \ trimming of ion-implanted silicon photonic devices. Optics express 26, 19 (2018),\
    \ 24953–24963.\n- <span id=\"page-13-6\"></span>[8] Bill Corcoran, Mengxi Tan,\
    \ Xingyuan Xu, Andreas Boes, Jiayang Wu, Thach G. Nguyen, Sai T. Chu, Brent E.\
    \ Little, Roberto Morandotti, Arnan Mitchell, and David J. Moss. 2020. Ultra-dense\
    \ optical data transmission over standard fibre with a single chip source. Nature\
    \ Communications 11 (5 2020), 2568. Issue 1. <https://doi.org/10.1038/s41467-020-16265-x>\n\
    - <span id=\"page-13-27\"></span>[9] NVIDIA Corporation. 2020. NVIDIA A100 Tensor\
    \ Core GPU Architecture Whitepaper. [https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/](https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf)\
    \ [nvidia-ampere-architecture-whitepaper.pdf](https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf)\
    \ Accessed: 2024-06-11.\n- <span id=\"page-13-28\"></span>[10] NVIDIA Corporation.\
    \ 2020. NVIDIA A100 Tensor Core GPU Datasheet. [https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/](https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf)\
    \ [nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf](https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf)\
    \ Accessed: 2024-06-11.\n- <span id=\"page-13-1\"></span>[11] Ivan Cvitić, Dragan\
    \ Peraković, Marko Periša, and Brij Gupta. 2021. Ensemble machine learning approach\
    \ for classification of IoT devices in smart home. International Journal of Machine\
    \ Learning and Cybernetics 12, 11 (2021), 3179–3202.\n- <span id=\"page-13-14\"\
    ></span>[12] Cansu Demirkiran, Furkan Eris, Gongyu Wang, Jonathan Elmhurst, Nick\
    \ Moore, Nicholas C Harris, Ayon Basumallik, Vijay Janapa Reddi, Ajay Joshi, and\
    \ Darius Bunandar. 2023. An electro-photonic system for accelerating deep neural\
    \ networks. ACM Journal on Emerging Technologies in Computing Systems 19, 4 (2023),\
    \ 1–31.\n- <span id=\"page-13-8\"></span>[13] Cansu Demirkiran, Guowei Yang, Darius\
    \ Bunandar, and Ajay Joshi. 2024. Mirage: An RNS-based photonic accelerator for\
    \ DNN training. In 2024 ACM/IEEE 51st Annual International Symposium on Computer\
    \ Architecture (ISCA). IEEE, 73–87.\n- <span id=\"page-13-22\"></span>[14] Jia\
    \ Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet:\
    \ A large-scale hierarchical image database. In 2009 IEEE conference on computer\
    \ vision and pattern recognition. Ieee, 248–255.\n- <span id=\"page-13-34\"></span>[15]\
    \ Li Deng. 2012. The mnist database of handwritten digit images for machine learning\
    \ research. IEEE Signal Processing Magazine 29, 6 (2012), 141–142.\n- <span id=\"\
    page-13-36\"></span>[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\
    \ Toutanova. 2018. BERT: Pre-training of Deep Bidirectional Transformers for Language\
    \ Understanding. arXiv preprint arXiv:1810.04805 (2018).\n- <span id=\"page-13-38\"\
    ></span>[17] Alexey Dosovitskiy. 2020. An image is worth 16x16 words: Transformers\
    \ for image recognition at scale. arXiv preprint arXiv:2010.11929 (2020).\n- <span\
    \ id=\"page-13-9\"></span>[18] Johannes Feldmann, Nathan Youngblood, Maxim Karpov,\
    \ Helge Gehring, Xuan Li, M. Stappers, M. Le Gallo, Xin Fu, Anton Lukashchuk,\
    \ Arslan S. Raja, Junqiu Liu, C. D. Wright, Abu Sebastian, Tobias J. Kippenberg,\
    \ W. H. P. Pernice, and Harish Bhaskaran. 2021. Parallel convolutional processing\
    \ using an integrated photonic tensor core. Nature 589 (1 2021), 52–58. Issue\
    \ 7840. [https://doi.org/10.1038/s41586-](https://doi.org/10.1038/s41586-020-03070-1)\
    \ [020-03070-1](https://doi.org/10.1038/s41586-020-03070-1)\n- <span id=\"page-13-16\"\
    ></span>[19] Manuel Le Gallo, Riduan Khaddam-Aljameh, Milos Stanisavljevic, Athanasios\
    \ Vasilopoulos, Benedikt Kersting, Martino Dazzi, Geethan Karunaratne, Matthias\
    \ Brändli, Abhairaj Singh, Silvia M. Müller, Julian Büchel, Xavier Timoneda, Vinay\
    \ Joshi, Malte J. Rasch, Urs Egger, Angelo Garofalo, Anastasios Petropoulos, Theodore\
    \ Antonakopoulos, Kevin Brew, Samuel Choi, Injo Ok, Timothy Philip, Victor Chan,\
    \ Claire Silvestre, Ishtiaq Ahsan, Nicole Saulnier, Vijay Narayanan, Pier Andrea\
    \ Francese, Evangelos Eleftheriou, and Abu Sebastian. 2023. A 64 core mixed-signal\
    \ in-memory compute chip based on phase-change memory for deep neural network\
    \ inference. Nature Electronics 6 (8 2023), 680–693. Issue 9.\n\n<https://doi.org/10.1038/s41928-023-01010-1>\n\
    \n- <span id=\"page-13-3\"></span>[20] Eva García-Martín, Crefeda Faviola Rodrigues,\
    \ Graham Riley, and Håkan Grahn. 2019. Estimation of energy consumption in machine\
    \ learning. J. Parallel and Distrib. Comput. 134 (2019), 75–88.\n- <span id=\"\
    page-13-17\"></span>[21] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael\
    \ W Mahoney, and Kurt Keutzer. 2022. A survey of quantization methods for efficient\
    \ neural network inference. In Low-Power Computer Vision. Chapman and Hall/CRC,\
    \ 291–326.\n- <span id=\"page-13-39\"></span>[22] George Giamougiannis, Apostolos\
    \ Tsakyridis, Yangjin Ma, Angelina Totović, Miltiadis Moralis-Pegios, David Lazovsky,\
    \ and Nikos Pleros. 2023. A coherent photonic crossbar for scalable universal\
    \ linear optics. Journal of Lightwave Technology 41, 8 (2023), 2425–2442.\n- <span\
    \ id=\"page-13-37\"></span>[23] Zhuocheng Gong, Jiahao Liu, Jingang Wang, Xunliang\
    \ Cai, Dongyan Zhao, and Rui Yan. 2024. What Makes Quantization for Large Language\
    \ Model Hard? An Empirical Study from the Lens of Perturbation. In Proceedings\
    \ of the AAAI Conference on Artificial Intelligence, Vol. 38. 18082–18089.\n-\
    \ <span id=\"page-13-15\"></span>[24] Fred G Gustavson and Tadeusz Swirszcz. 2007.\
    \ In-place transposition of rectangular matrices. In Applied Parallel Computing.\
    \ State of the Art in Scientific Computing: 8th International Workshop, PARA 2006,\
    \ Umeå, Sweden, June 18-21, 2006, Revised Selected Papers 8. Springer, 560–569.\n\
    - <span id=\"page-13-19\"></span>[25] Nastaran Hajinazar, Geraldo F Oliveira,\
    \ Sven Gregorio, João Ferreira, Nika Mansouri Ghiasi, Minesh Patel, Mohammed Alser,\
    \ Saugata Ghose, Juan Gómez Luna, and Onur Mutlu. 2021. SIMDRAM: An end-to-end\
    \ framework for bit-serial SIMD computing in DRAM. arXiv preprint arXiv:2105.12839\
    \ (2021).\n- <span id=\"page-13-12\"></span>[26] Ryan Hamerly, Liane Bernstein,\
    \ Alexander Sludds, Marin Soljačić, and Dirk Englund. 2019. Large-scale optical\
    \ neural networks based on photoelectric multiplication. Physical Review X 9,\
    \ 2 (2019), 021032.\n- <span id=\"page-13-10\"></span>[27] Ryan Hamerly, Liane\
    \ Bernstein, Alexander Sludds, Marin Soljačić, and Dirk Englund. 2019. Large-Scale\
    \ Optical Neural Networks Based on Photoelectric Multiplication. Physical Review\
    \ X 9 (5 2019), 021032. Issue 2. [https://doi.org/10.](https://doi.org/10.1103/PhysRevX.9.021032)\
    \ [1103/PhysRevX.9.021032](https://doi.org/10.1103/PhysRevX.9.021032)\n- <span\
    \ id=\"page-13-30\"></span>[28] Muhammad Abdullah Hanif, Aditya Manglik, and Muhammad\
    \ Shafique. 2020. Resistive crossbar-aware neural network design and optimization.\
    \ IEEE Access 8 (2020), 229066–229085.\n- <span id=\"page-13-23\"></span>[29]\
    \ Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning\
    \ for image recognition. In Proceedings of the IEEE conference on computer vision\
    \ and pattern recognition. 770–778.\n- <span id=\"page-13-18\"></span>[30] S Hochreiter.\
    \ 1997. Long Short-term Memory. Neural Computation MIT-Press (1997).\n- <span\
    \ id=\"page-13-4\"></span>[31] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\
    \ Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne\
    \ Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican,\
    \ George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan,\
    \ Erich Elsen, Oriol Vinyals, Jack W. Rae, and Laurent Sifre. 2024. Training computeoptimal\
    \ large language models. In Proceedings of the 36th International Conference on\
    \ Neural Information Processing Systems (, New Orleans, LA, USA,) (NIPS '22).\
    \ Curran Associates Inc., Red Hook, NY, USA, Article 2176, 15 pages.\n- <span\
    \ id=\"page-13-24\"></span>[32] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry\
    \ Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam.\
    \ 2017. Mobilenets: Efficient convolutional neural networks for mobile vision\
    \ applications. arXiv preprint arXiv:1704.04861 (2017).\n- <span id=\"page-13-5\"\
    ></span>[33] Junying Huang, Rongliang Fu, Xiaochun Ye, and Dongrui Fan. 2022.\
    \ A survey on superconducting computing technology: circuits, architectures and\
    \ design tools. CCF Transactions on High Performance Computing 4, 1 (2022), 1–22.\n\
    - <span id=\"page-13-31\"></span>[34] Ruirong Huang, Zichao Yue, Caroline Huang,\
    \ Janarbek Matai, and Zhiru Zhang. 2023. Comprehensive Benchmarking of Binary\
    \ Neural Networks on NVM Crossbar Architectures. arXiv preprint arXiv:2308.06227\
    \ (2023).\n- <span id=\"page-13-20\"></span>[35] Sergey Ioffe and Christian Szegedy.\
    \ 2015. Batch normalization: Accelerating deep network training by reducing internal\
    \ covariate shift. In International conference on machine learning. pmlr, 448–456.\n\
    - <span id=\"page-13-13\"></span>[36] Hasitha Jayatilleka, Harel Frish, Ranjeet\
    \ Kumar, John Heck, Chaoxuan Ma, Meer N Sakib, Duanni Huang, and Haisheng Rong.\
    \ 2021. Post-fabrication trimming of silicon photonic ring resonators at wafer-scale.\
    \ Journal of Lightwave Technology 39, 15 (2021), 5083–5088.\n- <span id=\"page-13-29\"\
    ></span>[37] Norman P Jouppi, Doe Hyun Yoon, George Kurian, Sheng Li, Nishant\
    \ Patil, James Laudon, Cliff Young, and David Patterson. 2020. A domain-specific\
    \ supercomputer for training deep neural networks. Commun. ACM 63, 7 (2020), 67–78.\n\
    - <span id=\"page-13-11\"></span>[38] Sadra Rahimi Kari, Nicholas A. Nobile, Dominique\
    \ Pantin, Vivswan Shah, and Nathan Youngblood. 2024. Realization of an integrated\
    \ coherent photonic platform for scalable matrix operations. Optica 11 (4 2024),\
    \ 542. Issue 4. <https://doi.org/10.1364/OPTICA.507525>\n- <span id=\"page-13-33\"\
    ></span>[39] Jung-Hoon Kim, Jaehoon Heo, Wontak Han, Jaeuk Kim, and Joo-Young\
    \ Kim. 2023. SP-PIM: A 22.41 TFLOPS/W, 8.81 Epochs/Sec Super-Pipelined Processing-In-Memory\
    \ Accelerator with Local Error Prediction for On-Device Learning. In 2023 IEEE\
    \ Symposium on VLSI Technology and Circuits (VLSI Technology and Circuits). IEEE,\
    \ 1–2.\n- <span id=\"page-13-21\"></span>[40] Alex Krizhevsky, Geoffrey Hinton,\
    \ et al. 2009. Learning multiple layers of features from tiny images. (2009).\n\
    \n- <span id=\"page-14-17\"></span>[41] Rolf Landauer. 1989. Johnson-Nyquist noise\
    \ derived from quantum mechanical transmission. Physica D: Nonlinear Phenomena\
    \ 38, 1-3 (1989), 226–229.\n- <span id=\"page-14-34\"></span>[42] Seyed Saber\
    \ Nabavi Larimi, Behzad Salami, Osman S Unsal, Adrián Cristal Kestelman, Hamid\
    \ Sarbazi-Azad, and Onur Mutlu. 2021. Understanding power consumption and reliability\
    \ of high-bandwidth memory with voltage underscaling. In 2021 Design, Automation\
    \ & Test in Europe Conference & Exhibition (DATE). IEEE, 517–522.\n- <span id=\"\
    page-14-39\"></span>[43] Karl Leboeuf, Ashkan Hosseinzadeh Namin, Roberto Muscedere,\
    \ Huapeng Wu, and Majid Ahmadi. 2008. High speed VLSI implementation of the hyperbolic\
    \ tangent sigmoid function. In 2008 Third international conference on convergence\
    \ and hybrid information technology, Vol. 1. IEEE, 1070–1073.\n- <span id=\"page-14-2\"\
    ></span>[44] John Leonard, Jonathan How, Seth Teller, Mitch Berger, Stefan Campbell,\
    \ Gaston Fiore, Luke Fletcher, Emilio Frazzoli, Albert Huang, Sertac Karaman,\
    \ et al. 2008. A perception-driven autonomous urban vehicle. Journal of Field\
    \ Robotics 25, 10 (2008), 727–774.\n- <span id=\"page-14-43\"></span>[45] Yuan\
    \ Li, Ahmed Louri, and Avinash Karanth. 2022. SPACX: Silicon photonicsbased scalable\
    \ chiplet accelerator for DNN inference. In 2022 IEEE International Symposium\
    \ on High-Performance Computer Architecture (HPCA). IEEE, 831–845.\n- <span id=\"\
    page-14-8\"></span>[46] Xing Lin, Yair Rivenson, Nezih T. Yardimci, Muhammed Veli,\
    \ Yi Luo, Mona Jarrahi, and Aydogan Ozcan. 2018. All-optical machine learning\
    \ using diffractive deep neural networks. Science 361 (9 2018), 1004–1008. Issue\
    \ 6406. [https:](https://doi.org/10.1126/science.aat8084) [//doi.org/10.1126/science.aat8084](https://doi.org/10.1126/science.aat8084)\n\
    - <span id=\"page-14-21\"></span>[47] Xiaoxiao Liu, Mengjie Mao, Beiye Liu, Hai\
    \ Li, Yiran Chen, Boxun Li, Yu Wang, Hao Jiang, Mark Barnell, Qing Wu, et al.\
    \ 2015. RENO: A high-efficient reconfigurable neuromorphic computing accelerator\
    \ design. In Proceedings of the 52nd Annual Design Automation Conference. 1–6.\n\
    - <span id=\"page-14-3\"></span>[48] RT McAllister, Yarin Gal, Alex Kendall, Mark\
    \ Van Der Wilk, Amar Shah, Roberto Cipolla, and Adrian Weller. 2017. Concrete\
    \ problems for autonomous vehicle safety: Advantages of Bayesian deep learning.\
    \ International Joint Conferences on Artificial Intelligence, Inc.\n- <span id=\"\
    page-14-40\"></span>[49] Pramod Kumar Meher. 2010. An optimized lookup-table for\
    \ the evaluation of sigmoid function for artificial neural networks. In 2010 18th\
    \ IEEE/IFIP International Conference on VLSI and System-on-Chip. IEEE, 91–95.\n\
    - <span id=\"page-14-4\"></span>[50] Sakorn Mekruksavanich and Anuchit Jitpattanakul.\
    \ 2021. Lstm networks using smartphone data for sensor-based human activity recognition\
    \ in smart homes. Sensors 21, 5 (2021), 1636.\n- <span id=\"page-14-13\"></span>[51]\
    \ Sajjad Moazeni, Sen Lin, Mark Wade, Luca Alloatti, Rajeev J. Ram, Milos Popovic,\
    \ and Vladimir Stojanovic. 2017. A 40-Gb/s PAM-4 Transmitter Based on a Ring-Resonator\
    \ Optical DAC in 45-nm SOI CMOS. IEEE Journal of Solid-State Circuits 52 (12 2017),\
    \ 3503–3516. Issue 12.<https://doi.org/10.1109/JSSC.2017.2748620>\n- <span id=\"\
    page-14-6\"></span>[52] Sina Najmaei, Andreu L Glasmann, Marshall A Schroeder,\
    \ Wendy L Sarney, Matthew L Chin, and Daniel M Potrepka. 2022. Advancements in\
    \ materials, devices, and integration schemes for a new generation of neuromorphic\
    \ computers. Materials Today 59 (2022), 80–106.\n- <span id=\"page-14-22\"></span>[53]\
    \ Pritish Narayanan, Stefano Ambrogio, Atsuya Okazaki, Kohji Hosokawa, Hsinyu\
    \ Tsai, Akiyo Nomura, Takeo Yasuda, Charles Mackin, Scott C Lewis, Alexander Friz,\
    \ et al. 2021. Fully on-chip MAC at 14 nm enabled by accurate row-wise programming\
    \ of PCM-based weights and parallel vector-transport in durationformat. IEEE Transactions\
    \ on Electron Devices 68, 12 (2021), 6629–6636.\n- <span id=\"page-14-30\"></span>[54]\
    \ James William Nilsson. 2000. Introduction to PSpice® Manual, Electric Circuits:\
    \ Using ORCad® Release 9.1. Prentice Hall, Upper Saddle River, NJ.\n- <span id=\"\
    page-14-33\"></span>[55] Kengo Nozaki, Shinji Matsuo, Takuro Fujii, Koji Takeda,\
    \ Masaaki Ono, Abdul Shakoor, Eiichi Kuramochi, and Masaya Notomi. 2016. Photonic-crystal\
    \ nanophotodetector with ultrasmall capacitance for on-chip light-to-voltage conversion\
    \ without an amplifier. Optica 3, 5 (May 2016), 483–492. [https://doi.org/10.1364/](https://doi.org/10.1364/OPTICA.3.000483)\
    \ [OPTICA.3.000483](https://doi.org/10.1364/OPTICA.3.000483)\n- <span id=\"page-14-25\"\
    ></span>[56] Cedric Nugteren. 2018. CLBlast: A tuned OpenCL BLAS library. In Proceedings\
    \ of the International Workshop on OpenCL. 1–10.\n- <span id=\"page-14-29\"></span>[57]\
    \ Dong-Ryeol Oh, Kyoung-Jun Moon, Won-Mook Lim, Ye-Dam Kim, Eun-Ji An, and Seung-Tak\
    \ Ryu. 2020. An 8-bit 1-GS/s asynchronous loop-unrolled SAR-flash ADC with complementary\
    \ dynamic amplifiers in 28-nm CMOS. IEEE Journal of Solid-State Circuits 56, 4\
    \ (2020), 1216–1226.\n- <span id=\"page-14-16\"></span>[58] BM Oliver. 1961. Signal\
    \ to noise ratios in photoelectric mixing. Proc. IRE 49, 12 (1961), 1960.\n- <span\
    \ id=\"page-14-38\"></span>[59] Zhe Pan, Zonghua Gu, Xiaohong Jiang, Guoquan Zhu,\
    \ and De Ma. 2022. A modular approximation methodology for efficient fixed-point\
    \ hardware implementation of the sigmoid function. IEEE Transactions on Industrial\
    \ Electronics 69, 10 (2022), 10694–10703.\n- <span id=\"page-14-0\"></span>[60]\
    \ Trishan Panch, Peter Szolovits, and Rifat Atun. 2018. Artificial intelligence,\
    \ machine learning and health systems. Journal of global health 8, 2 (2018).\n\
    - <span id=\"page-14-31\"></span>[61] Adam Paszke, Sam Gross, Francisco Massa,\
    \ Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia\
    \ Gimelshein, Luca Antiga, et al. 2019. Pytorch: An imperative style, high-performance\
    \ deep learning library. Advances in neural information processing systems 32\
    \ (2019).\n- <span id=\"page-14-11\"></span>[62] David Patel, Venkat Veerasubramanian,\
    \ Samir Ghosh, Alireza Samani, Qiuhang Zhong, and David V. Plant. 2014. High-speed\
    \ compact silicon photonic Michelson interferometric modulator. Optics Express\
    \ 22 (11 2014), 26788. Issue 22. [https:](https://doi.org/10.1364/OE.22.026788)\n\
    \n[//doi.org/10.1364/OE.22.026788](https://doi.org/10.1364/OE.22.026788)\n\n-\
    \ <span id=\"page-14-26\"></span>[63] Marc'Aurelio Ranzato, Sumit Chopra, Michael\
    \ Auli, and Wojciech Zaremba. 2015. Sequence level training with recurrent neural\
    \ networks. arXiv preprint arXiv:1511.06732 (2015).\n- <span id=\"page-14-15\"\
    ></span>[64] Christopher Rogers, Alexander Y. Piggott, David J. Thomson, Robert\
    \ F. Wiser, Ion E. Opris, Steven A. Fortune, Andrew J. Compston, Alexander Gondarenko,\
    \ Fanfan Meng, Xia Chen, Graham T. Reed, and Remus Nicolaescu. 2021. A universal\
    \ 3D imaging sensor on a silicon photonics platform. Nature 590 (2 2021), 256–261.\
    \ Issue 7845.<https://doi.org/10.1038/s41586-021-03259-y>\n- <span id=\"page-14-7\"\
    ></span>[65] R Saligram, A Raychowdhury, and Suman Datta. 2024. The future is\
    \ frozen: cryogenic CMOS for high-performance computing. Chip 3, 1 (2024), 100082.\n\
    - <span id=\"page-14-1\"></span>[66] Emre Sezgin. 2023. Artificial intelligence\
    \ in healthcare: Complementing, not replacing, doctors and healthcare providers.\
    \ Digital health 9 (2023), 20552076231186520.\n- <span id=\"page-14-18\"></span>[67]\
    \ Vivswan Shah and Nathan Youngblood. 2023. AnalogVNN: A fully modular framework\
    \ for modeling and optimizing photonic neural networks. APL Machine Learning 1\
    \ (6 2023). Issue 2.<https://doi.org/10.1063/5.0134156>\n- <span id=\"page-14-9\"\
    ></span>[68] Yichen Shen, Nicholas C. Harris, Scott Skirlo, Mihika Prabhu, Tom\
    \ Baehr-Jones, Michael Hochberg, Xin Sun, Shijie Zhao, Hugo Larochelle, Dirk Englund,\
    \ and Marin Soljačić. 2017. Deep learning with coherent nanophotonic circuits.\
    \ Nature Photonics 11 (6 2017), 441–446. Issue 7.<https://doi.org/10.1038/nphoton.2017.93>\n\
    - <span id=\"page-14-44\"></span>[69] Kyle Shiflett, Avinash Karanth, Razvan Bunescu,\
    \ and Ahmed Louri. 2023. Flumen: Dynamic processing in the photonic interconnect.\
    \ In Proceedings of the 50th Annual International Symposium on Computer Architecture.\
    \ 1–13.\n- <span id=\"page-14-32\"></span>[70] Karen Simonyan and Andrew Zisserman.\
    \ 2014. Very deep convolutional networks for large-scale image recognition. arXiv\
    \ preprint arXiv:1409.1556 (2014).\n- <span id=\"page-14-10\"></span>[71] Alexander\
    \ Sludds, Saumil Bandyopadhyay, Zaijun Chen, Zhizhen Zhong, Jared Cochrane, Liane\
    \ Bernstein, Darius Bunandar, P Ben Dixon, Scott A Hamilton, Matthew Streshinsky,\
    \ et al. 2022. Delocalized photonic deep learning on the internet's edge. Science\
    \ 378, 6617 (2022), 270–276.\n- <span id=\"page-14-35\"></span>[72] Shihao Song,\
    \ Adarsha Balaji, Anup Das, and Nagarajan Kandasamy. 2022. Designtechnology co-optimization\
    \ for NVM-based neuromorphic processing elements. ACM Transactions on Embedded\
    \ Computing Systems 21, 6 (2022), 1–27.\n- <span id=\"page-14-27\"></span>[73]\
    \ Nikita Stroev and Natalia G Berloff. 2023. Analog photonics computing for information\
    \ processing, inference, and optimization. Advanced Quantum Technologies 6, 9\
    \ (2023), 2300055.\n- <span id=\"page-14-14\"></span>[74] Jie Sun, Erman Timurdogan,\
    \ Ami Yaacobi, Ehsan Shah Hosseini, and Michael R. Watts. 2013. Large-scale nanophotonic\
    \ phased array. Nature 493 (1 2013), 195–199. Issue 7431.<https://doi.org/10.1038/nature11727>\n\
    - <span id=\"page-14-36\"></span>[75] Micron Technology. [n. d.]. HBM2e DRAM.\
    \ [https://www.micron.com/products/](https://www.micron.com/products/dram/hbm/hbm2e)\
    \ [dram/hbm/hbm2e.](https://www.micron.com/products/dram/hbm/hbm2e) Accessed:\
    \ 2025-02-16.\n- <span id=\"page-14-5\"></span>[76] Neil C Thompson, Kristjan\
    \ Greenewald, Keeheon Lee, and Gabriel F Manso. 2020. The computational limits\
    \ of deep learning. arXiv preprint arXiv:2007.05558 (2020).\n- <span id=\"page-14-42\"\
    ></span>[77] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\
    \ Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal\
    \ Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv\
    \ preprint arXiv:2302.13971 (2023).\n- <span id=\"page-14-41\"></span>[78] Ivan\
    \ Tsmots, Oleksa Skorokhoda, and Vasyl Rabyk. 2019. Hardware implementation of\
    \ sigmoid activation functions using FPGA. In 2019 IEEE 15th International Conference\
    \ on the Experience of Designing and Application of CAD Systems (CADSM). IEEE,\
    \ 34–38.\n- <span id=\"page-14-23\"></span>[79] Ashish Vaswani, Noam Shazeer,\
    \ Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and\
    \ Illia Polosukhin. 2017. Attention is all you need. Advances in neural information\
    \ processing systems 30 (2017).\n- <span id=\"page-14-37\"></span>[80] Weier Wan,\
    \ Rajkumar Kubendran, Clemens Schaefer, Sukru Burc Eryilmaz, Wenqiang Zhang, Dabin\
    \ Wu, Stephen Deiss, Priyanka Raina, He Qian, Bin Gao, et al. 2022. A compute-in-memory\
    \ chip based on resistive random-access memory. Nature 608, 7923 (2022), 504–512.\n\
    - <span id=\"page-14-19\"></span>[81] Changming Wu, Haoqin Deng, Yi-Siou Huang,\
    \ Heshan Yu, Ichiro Takeuchi, Carlos A Ríos Ocampo, and Mo Li. 2024. Freeform\
    \ direct-write and rewritable photonic integrated circuits in phase-change thin\
    \ films. Science Advances 10, 1 (2024), eadk1361.\n- <span id=\"page-14-24\"></span>[82]\
    \ Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev, and Paulius Micikevicius.\
    \ 2020. Integer quantization for deep learning inference: Principles and empirical\
    \ evaluation. arXiv preprint arXiv:2004.09602 (2020).\n- <span id=\"page-14-12\"\
    ></span>[83] Xiaotie Wu, Bipin Dama, Prakash Gothoskar, Peter Metz, Kal Shastri,\
    \ Sanjay Sunder, Jan Van der Spiegel, Yifan Wang, Mark Webster, and Will Wilson.\
    \ 2013. A 20Gb/s NRZ/PAM-4 1V transmitter in 40nm CMOS driving a Si-photonic modulator\
    \ in 0.13µm CMOS. 2013 IEEE International Solid-State Circuits Conference Digest\
    \ of Technical Papers, 128–129.<https://doi.org/10.1109/ISSCC.2013.6487667>\n\
    - <span id=\"page-14-20\"></span>[84] Xiaotie Wu, Bipin Dama, Prakash Gothoskar,\
    \ Peter Metz, Kal Shastri, Sanjay Sunder, Jan Van der Spiegel, Yifan Wang, Mark\
    \ Webster, and Will Wilson. 2013. A 20Gb/s NRZ/PAM-4 1V transmitter in 40nm CMOS\
    \ driving a Si-photonic modulator in 0.13 m CMOS. In 2013 IEEE International Solid-State\
    \ Circuits Conference Digest of Technical Papers. IEEE, 128–129.\n- <span id=\"\
    page-14-28\"></span>[85] Xin Xin, Yanan Guo, Youtao Zhang, and Jun Yang. 2021.\
    \ SAM: Accelerating Strided Memory Accesses. In MICRO-54: 54th Annual IEEE/ACM\
    \ International Symposium on Microarchitecture (Virtual Event, Greece) (MICRO\
    \ '21). Association\n\n<span id=\"page-15-0\"></span>ISCA '25, June 21–25, 2025,\
    \ Tokyo, Japan Liang Liu, Sadra Rahimi Kari, Xin Xin, Nathan Youngblood, Youtao\
    \ Zhang, and Jun Yang\n\nfor Computing Machinery, New York, NY, USA, 324–336.\
    \ [https://doi.org/10.](https://doi.org/10.1145/3466752.3480091) [1145/3466752.3480091](https://doi.org/10.1145/3466752.3480091)\n\
    \n- <span id=\"page-15-6\"></span>[86] Guowei Yang, Cansu Demirkiran, Zeynep Ece\
    \ Kizilates, Carlos A Ríos Ocampo, Ayse K Coskun, and Ajay Joshi. 2023. Processing-in-memory\
    \ using opticallyaddressed phase change memory. In 2023 IEEE/ACM International\
    \ Symposium on Low Power Electronics and Design (ISLPED). IEEE, 1–6.\n- <span\
    \ id=\"page-15-7\"></span>[87] Jong-Hyeok Yoon, Muya Chang, Win-San Khwa, Yu-Der\
    \ Chih, Meng-Fan Chang, and Arijit Raychowdhury. 2021. 29.1 A 40nm 64Kb 56.67\
    \ TOPS/W read-disturbtolerant compute-in-memory/digital RRAM macro with active-feedback-based\
    \ read and in-situ write verification. In 2021 IEEE International Solid-State\
    \ Circuits Conference (ISSCC), Vol. 64. IEEE, 404–406.\n- <span id=\"page-15-1\"\
    ></span>[88] Jie You, Jae-Won Chung, and Mosharaf Chowdhury. 2023. Zeus: Understanding\
    \ and Optimizing {GPU} Energy Consumption of {DNN} Training. In 20th USENIX Symposium\
    \ on Networked Systems Design and Implementation (NSDI 23). 119–139.\n- <span\
    \ id=\"page-15-3\"></span>[89] Nathan Youngblood. 2023. Coherent Photonic Crossbar\
    \ Arrays for Large-Scale Matrix-Matrix Multiplication. IEEE Journal of Selected\
    \ Topics in Quantum Electronics 29, 2: Optical Computing (2023), 1–11. [https://doi.org/10.1109/JSTQE.](https://doi.org/10.1109/JSTQE.2022.3171167)\n\
    \n[2022.3171167](https://doi.org/10.1109/JSTQE.2022.3171167)\n\n- <span id=\"\
    page-15-5\"></span>[90] Horace P Yuen and Vincent WS Chan. 1983. Noise in homodyne\
    \ and heterodyne detection. Optics letters 8, 3 (1983), 177–179.\n- <span id=\"\
    page-15-8\"></span>[91] Babak Zamanlooy and Mitra Mirhassani. 2016. An analog\
    \ CVNS-based sigmoid neuron for precise neurochips. IEEE Transactions on Very\
    \ Large Scale Integration (VLSI) Systems 25, 3 (2016), 894–906.\n- <span id=\"\
    page-15-2\"></span>[92] H. Zhang, M. Gu, X. D. Jiang, J. Thompson, H. Cai, S.\
    \ Paesani, R. Santagati, A. Laing, Y. Zhang, M. H. Yung, Y. Z. Shi, F. K. Muhammad,\
    \ G. Q. Lo, X. S. Luo, B. Dong, D. L. Kwong, L. C. Kwek, and A. Q. Liu. 2021.\
    \ An optical neural chip for implementing complex-valued neural network. Nature\
    \ Communications 12 (12 2021), 457. Issue 1.<https://doi.org/10.1038/s41467-020-20719-7>\n\
    - <span id=\"page-15-4\"></span>[93] Xiaosheng Zhang, Kyungmok Kwon, Johannes\
    \ Henriksson, Jianheng Luo, and Ming C. Wu. 2022. A large-scale microelectromechanical-systems-based\
    \ silicon photonics LiDAR. Nature 603 (3 2022), 253–258. Issue 7900. [https://doi.org/10.](https://doi.org/10.1038/s41586-022-04415-8)\
    \ [1038/s41586-022-04415-8](https://doi.org/10.1038/s41586-022-04415-8)"
  references:
  - '- <span id="page-13-32"></span>[1] Dennis Abts, Jonathan Ross, Jonathan Sparling,
    Mark Wong-VanHaren, Max Baker, Tom Hawkins, Andrew Bell, John Thompson, Temesghen
    Kahsai, Garrin Kimmell, et al. 2020. Think fast: A tensor streaming processor
    (TSP) for accelerating deep learning workloads. In 2020 ACM/IEEE 47th Annual International
    Symposium on Computer Architecture (ISCA). IEEE, 145–158.'
  - '- <span id="page-13-0"></span>[2] Shuroug A Alowais, Sahar S Alghamdi, Nada Alsuhebany,
    Tariq Alqahtani, Abdulrahman I Alshaya, Sumaya N Almohareb, Atheer Aldairem, Mohammed
    Alrashed, Khalid Bin Saleh, Hisham A Badreldin, et al. 2023. Revolutionizing healthcare:
    the role of artificial intelligence in clinical practice. BMC Medical Education
    23, 1 (2023), 689.'
  - '- <span id="page-13-7"></span>[3] Farshid Ashtiani, Alexander J. Geers, and Firooz
    Aflatouni. 2022. An on-chip photonic deep neural network for image classification.
    Nature 606 (6 2022), 501–506. Issue 7914.<https://doi.org/10.1038/s41586-022-04714-0>'
  - '- <span id="page-13-26"></span>[4] Rajeev Balasubramonian, Andrew B Kahng, Naveen
    Muralimanohar, Ali Shafiee, and Vaishnav Srinivas. 2017. CACTI 7: New tools for
    interconnect exploration in innovative off-chip memories. ACM Transactions on
    Architecture and Code Optimization (TACO) 14, 2 (2017), 1–25.'
  - '- <span id="page-13-2"></span>[5] Colby R Banbury, Vijay Janapa Reddi, Max Lam,
    William Fu, Amin Fazel, Jeremy Holleman, Xinyuan Huang, Robert Hurtado, David
    Kanter, Anton Lokhmotov, et al. 2020. Benchmarking tinyml systems: Challenges
    and direction. arXiv preprint arXiv:2003.04821 (2020).'
  - '- <span id="page-13-35"></span>[6] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
    Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish
    Sastry, Amanda Askell, et al. 2020. Language Models are Few-Shot Learners. arXiv
    preprint arXiv:2005.14165 (2020).'
  - '- <span id="page-13-25"></span>[7] Bigeng Chen, Xingshi Yu, Xia Chen, Milan M
    Milosevic, David J Thomson, Ali Z Khokhar, Shinichi Saito, Otto L Muskens, and
    Graham T Reed. 2018. Real-time monitoring and gradient feedback enable accurate
    trimming of ion-implanted silicon photonic devices. Optics express 26, 19 (2018),
    24953–24963.'
  - '- <span id="page-13-6"></span>[8] Bill Corcoran, Mengxi Tan, Xingyuan Xu, Andreas
    Boes, Jiayang Wu, Thach G. Nguyen, Sai T. Chu, Brent E. Little, Roberto Morandotti,
    Arnan Mitchell, and David J. Moss. 2020. Ultra-dense optical data transmission
    over standard fibre with a single chip source. Nature Communications 11 (5 2020),
    2568. Issue 1. <https://doi.org/10.1038/s41467-020-16265-x>'
  - '- <span id="page-13-27"></span>[9] NVIDIA Corporation. 2020. NVIDIA A100 Tensor
    Core GPU Architecture Whitepaper. [https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/](https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf)
    [nvidia-ampere-architecture-whitepaper.pdf](https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf)
    Accessed: 2024-06-11.'
  - '- <span id="page-13-28"></span>[10] NVIDIA Corporation. 2020. NVIDIA A100 Tensor
    Core GPU Datasheet. [https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/](https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf)
    [nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf](https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf)
    Accessed: 2024-06-11.'
  - '- <span id="page-13-1"></span>[11] Ivan Cvitić, Dragan Peraković, Marko Periša,
    and Brij Gupta. 2021. Ensemble machine learning approach for classification of
    IoT devices in smart home. International Journal of Machine Learning and Cybernetics
    12, 11 (2021), 3179–3202.'
  - '- <span id="page-13-14"></span>[12] Cansu Demirkiran, Furkan Eris, Gongyu Wang,
    Jonathan Elmhurst, Nick Moore, Nicholas C Harris, Ayon Basumallik, Vijay Janapa
    Reddi, Ajay Joshi, and Darius Bunandar. 2023. An electro-photonic system for accelerating
    deep neural networks. ACM Journal on Emerging Technologies in Computing Systems
    19, 4 (2023), 1–31.'
  - '- <span id="page-13-8"></span>[13] Cansu Demirkiran, Guowei Yang, Darius Bunandar,
    and Ajay Joshi. 2024. Mirage: An RNS-based photonic accelerator for DNN training.
    In 2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture
    (ISCA). IEEE, 73–87.'
  - '- <span id="page-13-22"></span>[14] Jia Deng, Wei Dong, Richard Socher, Li-Jia
    Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: A large-scale hierarchical image database.
    In 2009 IEEE conference on computer vision and pattern recognition. Ieee, 248–255.'
  - '- <span id="page-13-34"></span>[15] Li Deng. 2012. The mnist database of handwritten
    digit images for machine learning research. IEEE Signal Processing Magazine 29,
    6 (2012), 141–142.'
  - '- <span id="page-13-36"></span>[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee,
    and Kristina Toutanova. 2018. BERT: Pre-training of Deep Bidirectional Transformers
    for Language Understanding. arXiv preprint arXiv:1810.04805 (2018).'
  - '- <span id="page-13-38"></span>[17] Alexey Dosovitskiy. 2020. An image is worth
    16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929
    (2020).'
  - '- <span id="page-13-9"></span>[18] Johannes Feldmann, Nathan Youngblood, Maxim
    Karpov, Helge Gehring, Xuan Li, M. Stappers, M. Le Gallo, Xin Fu, Anton Lukashchuk,
    Arslan S. Raja, Junqiu Liu, C. D. Wright, Abu Sebastian, Tobias J. Kippenberg,
    W. H. P. Pernice, and Harish Bhaskaran. 2021. Parallel convolutional processing
    using an integrated photonic tensor core. Nature 589 (1 2021), 52–58. Issue 7840.
    [https://doi.org/10.1038/s41586-](https://doi.org/10.1038/s41586-020-03070-1)
    [020-03070-1](https://doi.org/10.1038/s41586-020-03070-1)'
  - '- <span id="page-13-16"></span>[19] Manuel Le Gallo, Riduan Khaddam-Aljameh,
    Milos Stanisavljevic, Athanasios Vasilopoulos, Benedikt Kersting, Martino Dazzi,
    Geethan Karunaratne, Matthias Brändli, Abhairaj Singh, Silvia M. Müller, Julian
    Büchel, Xavier Timoneda, Vinay Joshi, Malte J. Rasch, Urs Egger, Angelo Garofalo,
    Anastasios Petropoulos, Theodore Antonakopoulos, Kevin Brew, Samuel Choi, Injo
    Ok, Timothy Philip, Victor Chan, Claire Silvestre, Ishtiaq Ahsan, Nicole Saulnier,
    Vijay Narayanan, Pier Andrea Francese, Evangelos Eleftheriou, and Abu Sebastian.
    2023. A 64 core mixed-signal in-memory compute chip based on phase-change memory
    for deep neural network inference. Nature Electronics 6 (8 2023), 680–693. Issue
    9.'
  - <https://doi.org/10.1038/s41928-023-01010-1>
  - '- <span id="page-13-3"></span>[20] Eva García-Martín, Crefeda Faviola Rodrigues,
    Graham Riley, and Håkan Grahn. 2019. Estimation of energy consumption in machine
    learning. J. Parallel and Distrib. Comput. 134 (2019), 75–88.'
  - '- <span id="page-13-17"></span>[21] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei
    Yao, Michael W Mahoney, and Kurt Keutzer. 2022. A survey of quantization methods
    for efficient neural network inference. In Low-Power Computer Vision. Chapman
    and Hall/CRC, 291–326.'
  - '- <span id="page-13-39"></span>[22] George Giamougiannis, Apostolos Tsakyridis,
    Yangjin Ma, Angelina Totović, Miltiadis Moralis-Pegios, David Lazovsky, and Nikos
    Pleros. 2023. A coherent photonic crossbar for scalable universal linear optics.
    Journal of Lightwave Technology 41, 8 (2023), 2425–2442.'
  - '- <span id="page-13-37"></span>[23] Zhuocheng Gong, Jiahao Liu, Jingang Wang,
    Xunliang Cai, Dongyan Zhao, and Rui Yan. 2024. What Makes Quantization for Large
    Language Model Hard? An Empirical Study from the Lens of Perturbation. In Proceedings
    of the AAAI Conference on Artificial Intelligence, Vol. 38. 18082–18089.'
  - '- <span id="page-13-15"></span>[24] Fred G Gustavson and Tadeusz Swirszcz. 2007.
    In-place transposition of rectangular matrices. In Applied Parallel Computing.
    State of the Art in Scientific Computing: 8th International Workshop, PARA 2006,
    Umeå, Sweden, June 18-21, 2006, Revised Selected Papers 8. Springer, 560–569.'
  - '- <span id="page-13-19"></span>[25] Nastaran Hajinazar, Geraldo F Oliveira, Sven
    Gregorio, João Ferreira, Nika Mansouri Ghiasi, Minesh Patel, Mohammed Alser, Saugata
    Ghose, Juan Gómez Luna, and Onur Mutlu. 2021. SIMDRAM: An end-to-end framework
    for bit-serial SIMD computing in DRAM. arXiv preprint arXiv:2105.12839 (2021).'
  - '- <span id="page-13-12"></span>[26] Ryan Hamerly, Liane Bernstein, Alexander
    Sludds, Marin Soljačić, and Dirk Englund. 2019. Large-scale optical neural networks
    based on photoelectric multiplication. Physical Review X 9, 2 (2019), 021032.'
  - '- <span id="page-13-10"></span>[27] Ryan Hamerly, Liane Bernstein, Alexander
    Sludds, Marin Soljačić, and Dirk Englund. 2019. Large-Scale Optical Neural Networks
    Based on Photoelectric Multiplication. Physical Review X 9 (5 2019), 021032. Issue
    2. [https://doi.org/10.](https://doi.org/10.1103/PhysRevX.9.021032) [1103/PhysRevX.9.021032](https://doi.org/10.1103/PhysRevX.9.021032)'
  - '- <span id="page-13-30"></span>[28] Muhammad Abdullah Hanif, Aditya Manglik,
    and Muhammad Shafique. 2020. Resistive crossbar-aware neural network design and
    optimization. IEEE Access 8 (2020), 229066–229085.'
  - '- <span id="page-13-23"></span>[29] Kaiming He, Xiangyu Zhang, Shaoqing Ren,
    and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings
    of the IEEE conference on computer vision and pattern recognition. 770–778.'
  - '- <span id="page-13-18"></span>[30] S Hochreiter. 1997. Long Short-term Memory.
    Neural Computation MIT-Press (1997).'
  - '- <span id="page-13-4"></span>[31] Jordan Hoffmann, Sebastian Borgeaud, Arthur
    Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa
    Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie
    Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero,
    Karen Simonyan, Erich Elsen, Oriol Vinyals, Jack W. Rae, and Laurent Sifre. 2024.
    Training computeoptimal large language models. In Proceedings of the 36th International
    Conference on Neural Information Processing Systems (, New Orleans, LA, USA,)
    (NIPS ''22). Curran Associates Inc., Red Hook, NY, USA, Article 2176, 15 pages.'
  - '- <span id="page-13-24"></span>[32] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry
    Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. 2017.
    Mobilenets: Efficient convolutional neural networks for mobile vision applications.
    arXiv preprint arXiv:1704.04861 (2017).'
  - '- <span id="page-13-5"></span>[33] Junying Huang, Rongliang Fu, Xiaochun Ye,
    and Dongrui Fan. 2022. A survey on superconducting computing technology: circuits,
    architectures and design tools. CCF Transactions on High Performance Computing
    4, 1 (2022), 1–22.'
  - '- <span id="page-13-31"></span>[34] Ruirong Huang, Zichao Yue, Caroline Huang,
    Janarbek Matai, and Zhiru Zhang. 2023. Comprehensive Benchmarking of Binary Neural
    Networks on NVM Crossbar Architectures. arXiv preprint arXiv:2308.06227 (2023).'
  - '- <span id="page-13-20"></span>[35] Sergey Ioffe and Christian Szegedy. 2015.
    Batch normalization: Accelerating deep network training by reducing internal covariate
    shift. In International conference on machine learning. pmlr, 448–456.'
  - '- <span id="page-13-13"></span>[36] Hasitha Jayatilleka, Harel Frish, Ranjeet
    Kumar, John Heck, Chaoxuan Ma, Meer N Sakib, Duanni Huang, and Haisheng Rong.
    2021. Post-fabrication trimming of silicon photonic ring resonators at wafer-scale.
    Journal of Lightwave Technology 39, 15 (2021), 5083–5088.'
  - '- <span id="page-13-29"></span>[37] Norman P Jouppi, Doe Hyun Yoon, George Kurian,
    Sheng Li, Nishant Patil, James Laudon, Cliff Young, and David Patterson. 2020.
    A domain-specific supercomputer for training deep neural networks. Commun. ACM
    63, 7 (2020), 67–78.'
  - '- <span id="page-13-11"></span>[38] Sadra Rahimi Kari, Nicholas A. Nobile, Dominique
    Pantin, Vivswan Shah, and Nathan Youngblood. 2024. Realization of an integrated
    coherent photonic platform for scalable matrix operations. Optica 11 (4 2024),
    542. Issue 4. <https://doi.org/10.1364/OPTICA.507525>'
  - '- <span id="page-13-33"></span>[39] Jung-Hoon Kim, Jaehoon Heo, Wontak Han, Jaeuk
    Kim, and Joo-Young Kim. 2023. SP-PIM: A 22.41 TFLOPS/W, 8.81 Epochs/Sec Super-Pipelined
    Processing-In-Memory Accelerator with Local Error Prediction for On-Device Learning.
    In 2023 IEEE Symposium on VLSI Technology and Circuits (VLSI Technology and Circuits).
    IEEE, 1–2.'
  - '- <span id="page-13-21"></span>[40] Alex Krizhevsky, Geoffrey Hinton, et al.
    2009. Learning multiple layers of features from tiny images. (2009).'
  - '- <span id="page-14-17"></span>[41] Rolf Landauer. 1989. Johnson-Nyquist noise
    derived from quantum mechanical transmission. Physica D: Nonlinear Phenomena 38,
    1-3 (1989), 226–229.'
  - '- <span id="page-14-34"></span>[42] Seyed Saber Nabavi Larimi, Behzad Salami,
    Osman S Unsal, Adrián Cristal Kestelman, Hamid Sarbazi-Azad, and Onur Mutlu. 2021.
    Understanding power consumption and reliability of high-bandwidth memory with
    voltage underscaling. In 2021 Design, Automation & Test in Europe Conference &
    Exhibition (DATE). IEEE, 517–522.'
  - '- <span id="page-14-39"></span>[43] Karl Leboeuf, Ashkan Hosseinzadeh Namin,
    Roberto Muscedere, Huapeng Wu, and Majid Ahmadi. 2008. High speed VLSI implementation
    of the hyperbolic tangent sigmoid function. In 2008 Third international conference
    on convergence and hybrid information technology, Vol. 1. IEEE, 1070–1073.'
  - '- <span id="page-14-2"></span>[44] John Leonard, Jonathan How, Seth Teller, Mitch
    Berger, Stefan Campbell, Gaston Fiore, Luke Fletcher, Emilio Frazzoli, Albert
    Huang, Sertac Karaman, et al. 2008. A perception-driven autonomous urban vehicle.
    Journal of Field Robotics 25, 10 (2008), 727–774.'
  - '- <span id="page-14-43"></span>[45] Yuan Li, Ahmed Louri, and Avinash Karanth.
    2022. SPACX: Silicon photonicsbased scalable chiplet accelerator for DNN inference.
    In 2022 IEEE International Symposium on High-Performance Computer Architecture
    (HPCA). IEEE, 831–845.'
  - '- <span id="page-14-8"></span>[46] Xing Lin, Yair Rivenson, Nezih T. Yardimci,
    Muhammed Veli, Yi Luo, Mona Jarrahi, and Aydogan Ozcan. 2018. All-optical machine
    learning using diffractive deep neural networks. Science 361 (9 2018), 1004–1008.
    Issue 6406. [https:](https://doi.org/10.1126/science.aat8084) [//doi.org/10.1126/science.aat8084](https://doi.org/10.1126/science.aat8084)'
  - '- <span id="page-14-21"></span>[47] Xiaoxiao Liu, Mengjie Mao, Beiye Liu, Hai
    Li, Yiran Chen, Boxun Li, Yu Wang, Hao Jiang, Mark Barnell, Qing Wu, et al. 2015.
    RENO: A high-efficient reconfigurable neuromorphic computing accelerator design.
    In Proceedings of the 52nd Annual Design Automation Conference. 1–6.'
  - '- <span id="page-14-3"></span>[48] RT McAllister, Yarin Gal, Alex Kendall, Mark
    Van Der Wilk, Amar Shah, Roberto Cipolla, and Adrian Weller. 2017. Concrete problems
    for autonomous vehicle safety: Advantages of Bayesian deep learning. International
    Joint Conferences on Artificial Intelligence, Inc.'
  - '- <span id="page-14-40"></span>[49] Pramod Kumar Meher. 2010. An optimized lookup-table
    for the evaluation of sigmoid function for artificial neural networks. In 2010
    18th IEEE/IFIP International Conference on VLSI and System-on-Chip. IEEE, 91–95.'
  - '- <span id="page-14-4"></span>[50] Sakorn Mekruksavanich and Anuchit Jitpattanakul.
    2021. Lstm networks using smartphone data for sensor-based human activity recognition
    in smart homes. Sensors 21, 5 (2021), 1636.'
  - '- <span id="page-14-13"></span>[51] Sajjad Moazeni, Sen Lin, Mark Wade, Luca
    Alloatti, Rajeev J. Ram, Milos Popovic, and Vladimir Stojanovic. 2017. A 40-Gb/s
    PAM-4 Transmitter Based on a Ring-Resonator Optical DAC in 45-nm SOI CMOS. IEEE
    Journal of Solid-State Circuits 52 (12 2017), 3503–3516. Issue 12.<https://doi.org/10.1109/JSSC.2017.2748620>'
  - '- <span id="page-14-6"></span>[52] Sina Najmaei, Andreu L Glasmann, Marshall
    A Schroeder, Wendy L Sarney, Matthew L Chin, and Daniel M Potrepka. 2022. Advancements
    in materials, devices, and integration schemes for a new generation of neuromorphic
    computers. Materials Today 59 (2022), 80–106.'
  - '- <span id="page-14-22"></span>[53] Pritish Narayanan, Stefano Ambrogio, Atsuya
    Okazaki, Kohji Hosokawa, Hsinyu Tsai, Akiyo Nomura, Takeo Yasuda, Charles Mackin,
    Scott C Lewis, Alexander Friz, et al. 2021. Fully on-chip MAC at 14 nm enabled
    by accurate row-wise programming of PCM-based weights and parallel vector-transport
    in durationformat. IEEE Transactions on Electron Devices 68, 12 (2021), 6629–6636.'
  - '- <span id="page-14-30"></span>[54] James William Nilsson. 2000. Introduction
    to PSpice® Manual, Electric Circuits: Using ORCad® Release 9.1. Prentice Hall,
    Upper Saddle River, NJ.'
  - '- <span id="page-14-33"></span>[55] Kengo Nozaki, Shinji Matsuo, Takuro Fujii,
    Koji Takeda, Masaaki Ono, Abdul Shakoor, Eiichi Kuramochi, and Masaya Notomi.
    2016. Photonic-crystal nanophotodetector with ultrasmall capacitance for on-chip
    light-to-voltage conversion without an amplifier. Optica 3, 5 (May 2016), 483–492.
    [https://doi.org/10.1364/](https://doi.org/10.1364/OPTICA.3.000483) [OPTICA.3.000483](https://doi.org/10.1364/OPTICA.3.000483)'
  - '- <span id="page-14-25"></span>[56] Cedric Nugteren. 2018. CLBlast: A tuned OpenCL
    BLAS library. In Proceedings of the International Workshop on OpenCL. 1–10.'
  - '- <span id="page-14-29"></span>[57] Dong-Ryeol Oh, Kyoung-Jun Moon, Won-Mook
    Lim, Ye-Dam Kim, Eun-Ji An, and Seung-Tak Ryu. 2020. An 8-bit 1-GS/s asynchronous
    loop-unrolled SAR-flash ADC with complementary dynamic amplifiers in 28-nm CMOS.
    IEEE Journal of Solid-State Circuits 56, 4 (2020), 1216–1226.'
  - '- <span id="page-14-16"></span>[58] BM Oliver. 1961. Signal to noise ratios in
    photoelectric mixing. Proc. IRE 49, 12 (1961), 1960.'
  - '- <span id="page-14-38"></span>[59] Zhe Pan, Zonghua Gu, Xiaohong Jiang, Guoquan
    Zhu, and De Ma. 2022. A modular approximation methodology for efficient fixed-point
    hardware implementation of the sigmoid function. IEEE Transactions on Industrial
    Electronics 69, 10 (2022), 10694–10703.'
  - '- <span id="page-14-0"></span>[60] Trishan Panch, Peter Szolovits, and Rifat
    Atun. 2018. Artificial intelligence, machine learning and health systems. Journal
    of global health 8, 2 (2018).'
  - '- <span id="page-14-31"></span>[61] Adam Paszke, Sam Gross, Francisco Massa,
    Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia
    Gimelshein, Luca Antiga, et al. 2019. Pytorch: An imperative style, high-performance
    deep learning library. Advances in neural information processing systems 32 (2019).'
  - '- <span id="page-14-11"></span>[62] David Patel, Venkat Veerasubramanian, Samir
    Ghosh, Alireza Samani, Qiuhang Zhong, and David V. Plant. 2014. High-speed compact
    silicon photonic Michelson interferometric modulator. Optics Express 22 (11 2014),
    26788. Issue 22. [https:](https://doi.org/10.1364/OE.22.026788)'
  - '[//doi.org/10.1364/OE.22.026788](https://doi.org/10.1364/OE.22.026788)'
  - '- <span id="page-14-26"></span>[63] Marc''Aurelio Ranzato, Sumit Chopra, Michael
    Auli, and Wojciech Zaremba. 2015. Sequence level training with recurrent neural
    networks. arXiv preprint arXiv:1511.06732 (2015).'
  - '- <span id="page-14-15"></span>[64] Christopher Rogers, Alexander Y. Piggott,
    David J. Thomson, Robert F. Wiser, Ion E. Opris, Steven A. Fortune, Andrew J.
    Compston, Alexander Gondarenko, Fanfan Meng, Xia Chen, Graham T. Reed, and Remus
    Nicolaescu. 2021. A universal 3D imaging sensor on a silicon photonics platform.
    Nature 590 (2 2021), 256–261. Issue 7845.<https://doi.org/10.1038/s41586-021-03259-y>'
  - '- <span id="page-14-7"></span>[65] R Saligram, A Raychowdhury, and Suman Datta.
    2024. The future is frozen: cryogenic CMOS for high-performance computing. Chip
    3, 1 (2024), 100082.'
  - '- <span id="page-14-1"></span>[66] Emre Sezgin. 2023. Artificial intelligence
    in healthcare: Complementing, not replacing, doctors and healthcare providers.
    Digital health 9 (2023), 20552076231186520.'
  - '- <span id="page-14-18"></span>[67] Vivswan Shah and Nathan Youngblood. 2023.
    AnalogVNN: A fully modular framework for modeling and optimizing photonic neural
    networks. APL Machine Learning 1 (6 2023). Issue 2.<https://doi.org/10.1063/5.0134156>'
  - '- <span id="page-14-9"></span>[68] Yichen Shen, Nicholas C. Harris, Scott Skirlo,
    Mihika Prabhu, Tom Baehr-Jones, Michael Hochberg, Xin Sun, Shijie Zhao, Hugo Larochelle,
    Dirk Englund, and Marin Soljačić. 2017. Deep learning with coherent nanophotonic
    circuits. Nature Photonics 11 (6 2017), 441–446. Issue 7.<https://doi.org/10.1038/nphoton.2017.93>'
  - '- <span id="page-14-44"></span>[69] Kyle Shiflett, Avinash Karanth, Razvan Bunescu,
    and Ahmed Louri. 2023. Flumen: Dynamic processing in the photonic interconnect.
    In Proceedings of the 50th Annual International Symposium on Computer Architecture.
    1–13.'
  - '- <span id="page-14-32"></span>[70] Karen Simonyan and Andrew Zisserman. 2014.
    Very deep convolutional networks for large-scale image recognition. arXiv preprint
    arXiv:1409.1556 (2014).'
  - '- <span id="page-14-10"></span>[71] Alexander Sludds, Saumil Bandyopadhyay, Zaijun
    Chen, Zhizhen Zhong, Jared Cochrane, Liane Bernstein, Darius Bunandar, P Ben Dixon,
    Scott A Hamilton, Matthew Streshinsky, et al. 2022. Delocalized photonic deep
    learning on the internet''s edge. Science 378, 6617 (2022), 270–276.'
  - '- <span id="page-14-35"></span>[72] Shihao Song, Adarsha Balaji, Anup Das, and
    Nagarajan Kandasamy. 2022. Designtechnology co-optimization for NVM-based neuromorphic
    processing elements. ACM Transactions on Embedded Computing Systems 21, 6 (2022),
    1–27.'
  - '- <span id="page-14-27"></span>[73] Nikita Stroev and Natalia G Berloff. 2023.
    Analog photonics computing for information processing, inference, and optimization.
    Advanced Quantum Technologies 6, 9 (2023), 2300055.'
  - '- <span id="page-14-14"></span>[74] Jie Sun, Erman Timurdogan, Ami Yaacobi, Ehsan
    Shah Hosseini, and Michael R. Watts. 2013. Large-scale nanophotonic phased array.
    Nature 493 (1 2013), 195–199. Issue 7431.<https://doi.org/10.1038/nature11727>'
  - '- <span id="page-14-36"></span>[75] Micron Technology. [n. d.]. HBM2e DRAM. [https://www.micron.com/products/](https://www.micron.com/products/dram/hbm/hbm2e)
    [dram/hbm/hbm2e.](https://www.micron.com/products/dram/hbm/hbm2e) Accessed: 2025-02-16.'
  - '- <span id="page-14-5"></span>[76] Neil C Thompson, Kristjan Greenewald, Keeheon
    Lee, and Gabriel F Manso. 2020. The computational limits of deep learning. arXiv
    preprint arXiv:2007.05558 (2020).'
  - '- <span id="page-14-42"></span>[77] Hugo Touvron, Thibaut Lavril, Gautier Izacard,
    Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman
    Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation
    language models. arXiv preprint arXiv:2302.13971 (2023).'
  - '- <span id="page-14-41"></span>[78] Ivan Tsmots, Oleksa Skorokhoda, and Vasyl
    Rabyk. 2019. Hardware implementation of sigmoid activation functions using FPGA.
    In 2019 IEEE 15th International Conference on the Experience of Designing and
    Application of CAD Systems (CADSM). IEEE, 34–38.'
  - '- <span id="page-14-23"></span>[79] Ashish Vaswani, Noam Shazeer, Niki Parmar,
    Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin.
    2017. Attention is all you need. Advances in neural information processing systems
    30 (2017).'
  - '- <span id="page-14-37"></span>[80] Weier Wan, Rajkumar Kubendran, Clemens Schaefer,
    Sukru Burc Eryilmaz, Wenqiang Zhang, Dabin Wu, Stephen Deiss, Priyanka Raina,
    He Qian, Bin Gao, et al. 2022. A compute-in-memory chip based on resistive random-access
    memory. Nature 608, 7923 (2022), 504–512.'
  - '- <span id="page-14-19"></span>[81] Changming Wu, Haoqin Deng, Yi-Siou Huang,
    Heshan Yu, Ichiro Takeuchi, Carlos A Ríos Ocampo, and Mo Li. 2024. Freeform direct-write
    and rewritable photonic integrated circuits in phase-change thin films. Science
    Advances 10, 1 (2024), eadk1361.'
  - '- <span id="page-14-24"></span>[82] Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail
    Isaev, and Paulius Micikevicius. 2020. Integer quantization for deep learning
    inference: Principles and empirical evaluation. arXiv preprint arXiv:2004.09602
    (2020).'
  - '- <span id="page-14-12"></span>[83] Xiaotie Wu, Bipin Dama, Prakash Gothoskar,
    Peter Metz, Kal Shastri, Sanjay Sunder, Jan Van der Spiegel, Yifan Wang, Mark
    Webster, and Will Wilson. 2013. A 20Gb/s NRZ/PAM-4 1V transmitter in 40nm CMOS
    driving a Si-photonic modulator in 0.13µm CMOS. 2013 IEEE International Solid-State
    Circuits Conference Digest of Technical Papers, 128–129.<https://doi.org/10.1109/ISSCC.2013.6487667>'
  - '- <span id="page-14-20"></span>[84] Xiaotie Wu, Bipin Dama, Prakash Gothoskar,
    Peter Metz, Kal Shastri, Sanjay Sunder, Jan Van der Spiegel, Yifan Wang, Mark
    Webster, and Will Wilson. 2013. A 20Gb/s NRZ/PAM-4 1V transmitter in 40nm CMOS
    driving a Si-photonic modulator in 0.13 m CMOS. In 2013 IEEE International Solid-State
    Circuits Conference Digest of Technical Papers. IEEE, 128–129.'
  - '- <span id="page-14-28"></span>[85] Xin Xin, Yanan Guo, Youtao Zhang, and Jun
    Yang. 2021. SAM: Accelerating Strided Memory Accesses. In MICRO-54: 54th Annual
    IEEE/ACM International Symposium on Microarchitecture (Virtual Event, Greece)
    (MICRO ''21). Association'
  - <span id="page-15-0"></span>ISCA '25, June 21–25, 2025, Tokyo, Japan Liang Liu,
    Sadra Rahimi Kari, Xin Xin, Nathan Youngblood, Youtao Zhang, and Jun Yang
  - for Computing Machinery, New York, NY, USA, 324–336. [https://doi.org/10.](https://doi.org/10.1145/3466752.3480091)
    [1145/3466752.3480091](https://doi.org/10.1145/3466752.3480091)
  - '- <span id="page-15-6"></span>[86] Guowei Yang, Cansu Demirkiran, Zeynep Ece
    Kizilates, Carlos A Ríos Ocampo, Ayse K Coskun, and Ajay Joshi. 2023. Processing-in-memory
    using opticallyaddressed phase change memory. In 2023 IEEE/ACM International Symposium
    on Low Power Electronics and Design (ISLPED). IEEE, 1–6.'
  - '- <span id="page-15-7"></span>[87] Jong-Hyeok Yoon, Muya Chang, Win-San Khwa,
    Yu-Der Chih, Meng-Fan Chang, and Arijit Raychowdhury. 2021. 29.1 A 40nm 64Kb 56.67
    TOPS/W read-disturbtolerant compute-in-memory/digital RRAM macro with active-feedback-based
    read and in-situ write verification. In 2021 IEEE International Solid-State Circuits
    Conference (ISSCC), Vol. 64. IEEE, 404–406.'
  - '- <span id="page-15-1"></span>[88] Jie You, Jae-Won Chung, and Mosharaf Chowdhury.
    2023. Zeus: Understanding and Optimizing {GPU} Energy Consumption of {DNN} Training.
    In 20th USENIX Symposium on Networked Systems Design and Implementation (NSDI
    23). 119–139.'
  - '- <span id="page-15-3"></span>[89] Nathan Youngblood. 2023. Coherent Photonic
    Crossbar Arrays for Large-Scale Matrix-Matrix Multiplication. IEEE Journal of
    Selected Topics in Quantum Electronics 29, 2: Optical Computing (2023), 1–11.
    [https://doi.org/10.1109/JSTQE.](https://doi.org/10.1109/JSTQE.2022.3171167)'
  - '[2022.3171167](https://doi.org/10.1109/JSTQE.2022.3171167)'
  - '- <span id="page-15-5"></span>[90] Horace P Yuen and Vincent WS Chan. 1983. Noise
    in homodyne and heterodyne detection. Optics letters 8, 3 (1983), 177–179.'
  - '- <span id="page-15-8"></span>[91] Babak Zamanlooy and Mitra Mirhassani. 2016.
    An analog CVNS-based sigmoid neuron for precise neurochips. IEEE Transactions
    on Very Large Scale Integration (VLSI) Systems 25, 3 (2016), 894–906.'
  - '- <span id="page-15-2"></span>[92] H. Zhang, M. Gu, X. D. Jiang, J. Thompson,
    H. Cai, S. Paesani, R. Santagati, A. Laing, Y. Zhang, M. H. Yung, Y. Z. Shi, F.
    K. Muhammad, G. Q. Lo, X. S. Luo, B. Dong, D. L. Kwong, L. C. Kwek, and A. Q.
    Liu. 2021. An optical neural chip for implementing complex-valued neural network.
    Nature Communications 12 (12 2021), 457. Issue 1.<https://doi.org/10.1038/s41467-020-20719-7>'
  - '- <span id="page-15-4"></span>[93] Xiaosheng Zhang, Kyungmok Kwon, Johannes Henriksson,
    Jianheng Luo, and Ming C. Wu. 2022. A large-scale microelectromechanical-systems-based
    silicon photonics LiDAR. Nature 603 (3 2022), 253–258. Issue 7900. [https://doi.org/10.](https://doi.org/10.1038/s41586-022-04415-8)
    [1038/s41586-022-04415-8](https://doi.org/10.1038/s41586-022-04415-8)'
- id: fred_a_wafer_scale_fabric_for_3d_parallel_dnn_training_saeed_rashidi_https_orcid_org_0000_0002_6472_9920_georgia_institute_of_technology_atlanta_ga_usa_saeed_rashidi_gatech_edu
  title: 'FRED: A Wafer-scale Fabric for 3D Parallel DNN Training'
  abstract: Wafer-scale systems are an emerging technology that tightly integrates
    high-end accelerator chiplets with high-speed wafer-scale interconnects, enabling
    low-latency and high-bandwidth connectivity. This makes them a promising platform
    for deep neural network (DNN) training. However, current network-on-wafer topologies,
    such as 2D Meshes, lack the flexibility needed to support various parallelization
    strategies effectively. In this paper, we propose Fred, a wafer-scale fabric architecture
    tailored to the unique communication needs of DNN training. Fred creates a distributed
    on-wafer topology with tiny microswitches, providing nonblocking connectivity
    for collective communications between arbitrary groups of accelerators and enabling
    in-switch collective support. Our results show that for sample parallelization
    strategies, Fred can improve the average end-to-end training time of ResNet-152,
    Transformer-17B, GPT-3, and Transformer-1T by 1.76×, 1.87×, 1.34×, and 1.4×, respectively,
    compared to a baseline wafer-scale Mesh.
  keywords: distributed training, wafer-scale platforms
  document: '![](_page_0_Picture_0.jpeg)


    # FRED: A Wafer-scale Fabric for 3D Parallel DNN Training


    [Saeed Rashidi](https://orcid.org/0000-0002-6472-9920)<sup>∗</sup> Georgia Institute
    of Technology Atlanta, GA, USA saeed.rashidi@gatech.edu


    [William Won](https://orcid.org/0000-0002-1715-9144) Georgia Institute of Technology
    Atlanta, GA, USA william.won@gatech.edu


    [Sudarshan Srinivasan](https://orcid.org/0009-0002-8662-5820) Intel Bangalore,
    Karnataka, India sudarshan.srinivasan@intel.com


    [Puneet Gupta](https://orcid.org/0000-0002-6188-1134) UCLA Los Angeles, CA, USA
    puneetg@ucla.edu


    [Tushar Krishna](https://orcid.org/0000-0001-5738-6942) Georgia Tech Atlanta,
    GA, USA tushar@ece.gatech.edu


    ## Abstract


    Wafer-scale systems are an emerging technology that tightly integrates high-end
    accelerator chiplets with high-speed wafer-scale interconnects, enabling low-latency
    and high-bandwidth connectivity. This makes them a promising platform for deep
    neural network (DNN) training. However, current network-on-wafer topologies, such
    as 2D Meshes, lack the flexibility needed to support various parallelization strategies
    effectively. In this paper, we propose Fred, a wafer-scale fabric architecture
    tailored to the unique communication needs of DNN training. Fred creates a distributed
    on-wafer topology with tiny microswitches, providing nonblocking connectivity
    for collective communications between arbitrary groups of accelerators and enabling
    in-switch collective support. Our results show that for sample parallelization
    strategies, Fred can improve the average end-to-end training time of ResNet-152,
    Transformer-17B, GPT-3, and Transformer-1T by 1.76×, 1.87×, 1.34×, and 1.4×, respectively,
    compared to a baseline wafer-scale Mesh.


    ### CCS Concepts


    • Hardware →Emerging technologies;• Networks →Network architectures; • Computer
    systems organization → Architectures.


    # Keywords


    distributed training, wafer-scale platforms


    #### ACM Reference Format:


    Saeed Rashidi, William Won, Sudarshan Srinivasan, Puneet Gupta, and Tushar Krishna.
    2025. FRED: A Wafer-scale Fabric for 3D Parallel DNN Training. In Proceedings
    of the 52nd Annual International Symposium on Computer Architecture (ISCA ''25),
    June 21–25, 2025, Tokyo, Japan. ACM, New York, NY, USA, [15](#page-14-0) pages.<https://doi.org/10.1145/3695053.3731055>


    [This work is licensed under a Creative Commons Attribution-NonCommercial 4.0](https://creativecommons.org/licenses/by-nc/4.0)
    [International License.](https://creativecommons.org/licenses/by-nc/4.0) ISCA
    ''25, Tokyo, Japan © 2025 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-1261-6/25/06
    <https://doi.org/10.1145/3695053.3731055>


    # 1 Introduction


    DNN models are on an exponential growth curve. A recent study shows that in less
    than two years, the compute and memory requirements for DNN training have increased
    by 1,800× and 1,500×, respectively [\[58\]](#page-14-1). Distributing the training
    across multiple accelerators or neural processing units (NPUs) is a common practice
    today to reduce the training time. However, one critical side effect of distributed
    training is the communication overhead between NPUs to synchronize model gradients
    and/or activations, depending on the parallelization strategy. As the number of
    NPUs scales, communication overhead increases, up to a point where it becomes
    the dominant factor in distributed training latency [\[26,](#page-13-0) [27,](#page-13-1)
    [54,](#page-14-2) [55\]](#page-14-3).


    There are fundamental limits to the bandwidth that can be provided even by high-speed
    rack-scale fabrics (such as NVLink [\[33\]](#page-13-2)), and thus there has been
    a growing interest in platforms that integrate multiple NPUs together in the same
    package. Cerebras [\[10\]](#page-13-3) demonstrated one extreme incarnation of
    this idea in the form of a monolithic wafer with NPUs connected to one another.
    More costand yield-effective approaches include silicon/organic interposerbased
    approaches [\[24,](#page-13-4) [59\]](#page-14-4) or using Silicon Interconnect
    Fabric (Si-IF), which bonds chiplets directly onto a full thickness silicon wafer
    without needing a package [\[14,](#page-13-5) [47,](#page-14-5) [48\]](#page-14-6).
    In this work, we assume a passive, interconnect-only wafer-scale substrate onto
    which chiplets are bonded at fine pitch similar to Si-IF or TSMC-SoW [\[61\]](#page-14-7).
    This allows for heterogeneous integration of compute, memory, and network chiplets
    from disparate technologies, unlike the monolithic Cerebras approach.


    While there is broad agreement on the scalability and bandwidth benefits that
    wafer-scale substrates can provide, the architecture of the fabric connecting
    the NPUs remains an open question. All wafer-scale accelerator proposals to date
    (e.g., Cerebras CS2 [\[10\]](#page-13-3), NVIDIA''s SIMBA [\[59\]](#page-14-4),
    UCLA''s waferscale GPU [\[48\]](#page-14-6), Chiplet Cloud [\[49\]](#page-14-8),
    Chen et al., TTO [\[31\]](#page-13-6)) have implemented a 2D Mesh topology for
    the fabric. The choice of a Mesh is understandable. It is the most pervasive topology
    in many-core chips given its ease of place-and-route and scalability and is the
    natural choice on a wafer-scale substrate as well. However, we demonstrate that
    the inherent blocking nature of the 2D Mesh topology is extremely inefficient
    for DNN training communication use cases.


    Communication in DNN training depends inherently on the parallelism strategy being
    employed. Data-Parallel (DP) [\[27,](#page-13-1) [34\]](#page-13-7), Model-Parallel
    (MP) [\[26,](#page-13-0) [32\]](#page-13-8), and Pipeline-Parallel (PP) [\[23,](#page-13-9)
    [40\]](#page-13-10) are the building blocks of any parallelism strategy. In DP,
    the DNN


    <sup>∗</sup>The author is now at Meta: rashidi1saeed@meta.com


    <span id="page-1-0"></span>![](_page_1_Figure_1.jpeg)


    Figure 1: HW-SW Co-Design Stack for Optimizing DNN training communication. This
    work addresses the three phases highlighted in red. The left part of the figure
    shows a sample logical view of training workers in 3D parallelism. The parallelization
    strategy is of size MP(4)-DP(3)-PP(2), meaning that there are 4/3/2 peer workers
    for the MP/DP/PP dimension. Each worker is named with 3 digits, representing the
    ID of the worker in the MP, DP, and PP dimensions, respectively. Workers that
    are aligned along each dimension should communicate for that respective dimension''s
    parallelization type. For example, workers 000, 100, 200, and 300 should communicate
    for MP type (i.e., activation/input gradient sync during forwardpass/back-propagation),
    while workers 300, 310, 320 should communicate for DP type (weight gradient sync
    during backpropagation).


    model is replicated across NPUs and each NPU works on a different set of training
    samples (i.e., minibatch). In MP, each DNN layer is sharded across NPUs while
    they work on the same training samples. In PP, each NPU hosts a subset of DNN
    layers, and training samples flow through the NPUs in a pipeline manner. 3D parallelism
    [\[41\]](#page-13-11) utilizes all the aforementioned strategies by creating different
    MP/DP/PP groups between NPUs. The optimal balance between DP, MP, and PP is heavily
    dependent on the workload and underlying platform and can significantly vary for
    different workload/platform configurations [\[26,](#page-13-0) [50\]](#page-14-9).
    [Figure 1](#page-1-0) shows an example of a 3D parallelism strategy.


    <span id="page-1-2"></span>![](_page_1_Figure_4.jpeg)


    **Parallelization Strategy**


    Figure 2: The normalized computation/communication overhead for various parallelization
    strategies (explained in [Section 2\)](#page-1-1) of Transformer-17B, when running
    on top of the 2D Mesh topology (as described in [Section 6\)](#page-7-0) that
    connects 20 NPUs on the wafer.


    From a communication perspective, 3D parallelism requires executing multiple concurrent
    communication operations between NPUs within the same MP/DP/PP group at different
    stages of distributed training. Moreover, different parallelism strategies stress
    the compute and communication differently. This is quantified in [Figure 2.](#page-1-2)
    As the figure shows, high communication overhead can result in the total training
    overhead of compute-efficient strategies being greater than that of less compute-efficient
    strategies (e.g., MP(20)- DP(1)-PP(1) vs. MP(5)-DP(4)-PP(1)). Other than the communication


    volume which is determined by the workload, the main purpose of such high network
    overhead in [Figure 2](#page-1-2) is the inefficient use of network resources
    in the baseline topology. This is mainly because: (i) for the majority of the
    comm operations, only half or less than half of the NPU links get activated (discussed
    in detail in [Section 3.2.4\)](#page-4-0), (ii) network contention between MP/DP/PP
    parallel groups (discussed in detail in [Section 3.2.2\)](#page-3-0). The full
    list of baseline


    topology challenges is discussed in [Section 3.2.](#page-3-1) In summary, an optimal
    wafer-scale fabric for distributed DNN training should meet the following three
    needs:


    - (1) Handle multiple non-blocking collective communications with minimum congestion.

    - (2) Be efficient for all 3D parallelism configurations.

    - (3) Provide high-BW connectivity between NPUs.


    In this work, we propose Fred, a wafer-scale fabric with Flexible REduction-Distribution
    feature for supporting arbitrary 3D parallelism. Fred includes: (i) a novel topology
    with switches that provide native support for reduction and broadcast for bandwidth
    amplification, (ii) a collective routing algorithm with non-blocking support,
    and (iii) a device placement algorithm to minimize congestion. We deploy Fred
    over a wafer-scale substrate [\[47\]](#page-14-5). Each NPU in our architecture
    is a hybrid integration of high-end compute chiplets and 3D-stack DRAM chiplets
    (analogous to H100 [\[45\]](#page-14-10)). We also discuss solutions to physically
    layout and scale the Fred topology over a wafer substrate.


    To the best of our knowledge, Fred is the first wafer-scale fabric proposal tailored
    for DNN training, that can efficiently support multiple concurrent collectives
    for hybrid parallelization strategies (e.g., 3D parallelism). Hence, Fred enables
    the compiler to consider any type of parallelization strategy without any concern
    about how efficiently they can be executed on the network. To summarize:


    - We motivate the challenges with designing a wafer-scale fabric for 3D parallelism
    [\(Section 3\)](#page-3-2).

    - We propose Fred, a novel network fabric that includes several innovative features:
    a switch fabric with flexible reductiondistribution trees connected via a scalable
    topology [\(Sec](#page-4-1)[tion 4\)](#page-4-1), and a novel routing algorithm
    to route multiple collectives concurrently, along with a congestion-aware device
    placement policy for 3D parallelism [\(Section 5\)](#page-5-0).

    - We demonstrate how Fred can be implemented as a waferscale fabric [\(Section
    6\)](#page-7-0).

    - We compare Fred with baseline fabrics for some sample workloads and parallelization
    strategies [\(Section 8\)](#page-10-0).


    Our results show that Fred can improve the average end-toend training time of
    ResNet-152, Transformer-17B, GPT-3, and Transformer-1T by 1.76×, 1.87×, 1.34×,
    and 1.4×, respectively, when compared to the baseline 2D Mesh.


    # <span id="page-1-1"></span>2 Background


    # <span id="page-1-3"></span>2.1 Collective Communication Patterns


    Although DNN models can be highly diverse, most of their communication during
    distributed training can be handled through collective patterns [\[27\]](#page-13-1).
    Depending on the model type and parallelization strategy, different types of collectives
    may be needed to synchronize on activations/gradients during forward-pass/backpropagation
    [\[55\]](#page-14-3). [Figure 3](#page-2-0) shows the mathematical implication
    of


    the most common collective patterns between three workers. During Reduce-Scatter,
    workers communicate in such a way that, at the end, each worker has a portion
    of globally reduced data. In All-Gather, each worker broadcasts its local data
    to all other workers. All-Reduce is the most common pattern in distributed training
    [\[27\]](#page-13-1) and can be thought of as a Reduce-Scatter followed by an
    All-Gather. In Reduce, multiple NPUs participate in reducing data, and the result
    is stored only on one NPU, while Gather collects the data from all NPUs and stores
    them on a single NPU. Multicast means a single NPU sends its data to multiple
    NPUs. In All-to-All, each worker sends a portion of its local data to each worker.


    <span id="page-2-0"></span>![](_page_2_Figure_2.jpeg)


    Figure 3: Collective communication patterns among three workers.


    #### <span id="page-2-2"></span>2.2 Collective Communication Algorithms


    The patterns described in [Section 2.1](#page-1-3) can be handled through different
    collective algorithms. In general, there are two distinct way to implement such
    algorithms:


    1) Endpoint-based. NPUs communicate in a peer-to-peer distributed manner through
    explicit send/recv of messages with themselves and without requiring central coordination.
    In this case, the optimal algorithm is usually dependent on the physical network
    topology and collective size. For example, ring-based All-Reduce is optimal when
    the physical topology is a ring, while tree-based All-Reduce is optimal for tree-based
    topologies, or when the message size is small [\[64\]](#page-14-11).


    One drawback of the NPU-to-NPU-based approach is the amount of traffic it generates.
    For example, the most BW-optimal NPU-to-NPU algorithms require each NPU to send/receive
    nearly <sup>2</sup>( <sup>−</sup>1) bytes of data to execute an All-Reduce of
    bytes among NPUs [\[27\]](#page-13-1), which is almost 2× of the All-Reduce size
    ( bytes) [\[11,](#page-13-12) [12,](#page-13-13) [64\]](#page-14-11). This is
    because all endpoint-based algorithms must perform reduction and gather phases
    separately, resulting in ( <sup>−</sup>1) send/recv per NPU to accomplish each
    phase, respectively [\[11,](#page-13-12) [12,](#page-13-13) [64\]](#page-14-11).


    2) In-Network Collective Execution. To alleviate the extra traffic of endpoint-based
    approach, recent proposals have introduced innetwork collective algorithms by
    adding compute capability to the switches [\[27,](#page-13-1) [35,](#page-13-14)
    [57\]](#page-14-12) to perform both reduction and gather at the same time. For
    example, an All-Reduce of bytes only requires each NPU to send/receive bytes to
    the switch/switch-hierarchy. The switch/switch-hierarchy receives bytes from each
    NPU, performs reduction across all received data from all NPUs, and broadcasts
    bytes back to all NPUs. Therefore, compared to the endpointbased approach, each
    NPU sends/receives almost half the traffic ( bytes vs. <sup>2</sup>( <sup>−</sup>1)
    bytes.) [\[35\]](#page-13-14). Additionally, In-Network collective execution allows
    the endpoint resources to be allocated for training compute tasks, while the network
    switches handle the collectives efficiently.


    #### 2.3 Communication in 3D Parallelism


    There are multiple ways of distributing the distributed training tasks across
    multiple NPUs (a.k.a. the parallelization strategy): MP (a.k.a. Tensor-parallelism)
    [\[62\]](#page-14-13), DP [\[34\]](#page-13-7), and PP [\[23,](#page-13-9) [40\]](#page-13-10).
    The combination of these strategies can be generalized in the form of 3Dparallelism
    [\[41\]](#page-13-11). [Figure 1](#page-1-0) shows the concept of 3D-parallelism.
    In this case, each training worker is part of one MP, DP, and PP group, where
    the ID (offset) of each NPU within its MP/DP/PP group is determined using the
    first/second/third digits of a 3-digit worker ID. Therefore, the NPUs that have
    the same DP & PP digits are within the same MP group (e.g., 000, 100, 200, and
    300).


    The NPUs within the same DP group should communicate through the All-Reduce collective
    pattern during back-propagation to sync their locally computed model gradients
    and update the model before starting the next training iteration [\[27\]](#page-13-1).
    For the MP group case, NPUs need to communicate during forward-pass/back-propagation
    to synchronize on output-activations/input-gradients. The communication pattern,
    however, depends on the layer type and the way it is sharded. The usually observed
    patterns are: All-Reduce [\[62\]](#page-14-13), All-to-All [\[42\]](#page-13-15),
    Reduce-Scatter [\[32\]](#page-13-8), and All-Gather [\[32\]](#page-13-8). For
    the PP group, the NPUs need to transfer the output-activations/inputgradients
    during forward-pass/back-propagation on the borderline layers and pass the data
    to the NPU(s) hosting the next set of layers. [Table 1](#page-2-1) represents
    collective patterns incurred by each parallelization strategy.


    [Figure 1](#page-1-0) also shows the necessity to handle multiple collectives
    at the same time. For e.g., there are eight different DP groups, meaning that
    up to eight concurrent All-Reduces should be handled for the DP communications
    (similarly, there are six/twelve concurrent communication operations for MP/PP
    communications). Moreover, the communication type and peer workers differ across
    MP, DP, and PP groups. Thus, it is crucial for the underlying network fabric to
    be flexible for concurrent and different collective patterns.


    <span id="page-2-1"></span>Table 1: Collective patterns incurred by distinct parallelizations.


    | Parallelism | Reduce<br>Scatter | All<br>Gather | All<br>Reduce | All-to-All
    | Point-to<br>Point |

    |-------------|-------------------|---------------|---------------|------------|-------------------|

    | Model       | ✓                 | ✓             | ✓             | ✓          |                   |

    | Data        |                   |               | ✓             |            |                   |

    | Pipeline    |                   |               |               |            |
    ✓                 |

    | 3D          | ✓                 | ✓             | ✓             | ✓          |
    ✓                 |


    # 2.4 Multi-chiplet Integration


    In chiplet-based integration, NPU chips are fabricated and then bonded to a package
    interconnect (e.g., Si-IF) [\[47,](#page-14-5) [59,](#page-14-4) [61\]](#page-14-7).
    In this approach, components from different technologies (e.g., even DRAM) can
    be integrated on the package. Additionally, since the chiplets can be tested before
    integration, this approach has a better yield, supports heterogeneity, and requires
    less redundancy compared to fully monolithic approaches such as Cerebras [\[10\]](#page-13-3).


    Multi-chiplet Fabric Topologies. Recent products and research in multi-chiplet
    platforms are all based on interconnecting NPUs through a 2D-mesh topology [\[10,](#page-13-3)
    [31,](#page-13-6) [47–](#page-14-5)[49,](#page-14-8) [59\]](#page-14-4). Among
    the main reasons for choosing 2D-mesh for on-package/on-wafer is ease of place
    & route and area optimality over a 2D substrate [\[48\]](#page-14-6). Thus, in
    this paper, we choose 2D-mesh as the main baseline topology and compare our proposal
    against it.


    # <span id="page-3-2"></span>3 Desired Metrics for a Wafer-scale Fabric


    ### <span id="page-3-6"></span>3.1 Communication Demands


    First, we discuss two execution modes for running DNN training over a wafer-scale
    substrate.


    3.1.1 Weight Stationary. When DNN models can fit entirely in the available on-chip
    memory within a wafer, loading the entire model parameters and collecting the
    training result to/from the package is a one-time overhead.[1](#page-3-3) The
    cost of loading the pre-trained and storing the trained model is amortized over
    thousands of training iterations. The input samples, however, need to be loaded
    at the beginning of each training iteration. Such I/O operations have minimal
    impact on the overall training performance since the samples are much smaller
    than the model size. Therefore, in this mode, the main performance factor is the
    efficiency of compute cores and the NPU-to-NPU communication performance. A non-optimized
    interconnect can result in poor NPU-to-NPU communication performance for certain
    parallelization strategies, forcing the compiler to discard some strategies despite
    their better compute and on-chip memory utilization, solely because of their poor
    communication performance [\(Section 3.2\)](#page-3-1).


    3.1.2 Weight Streaming. When the available on-chip memory is insufficient to fit
    the model, the execution model shifts to weight streaming [\[10,](#page-13-3)
    [58\]](#page-14-1). In this scenario, only a subset of DNN layers is loaded onto
    the package at any given time. After processing these layers, the on-chip storage
    is reclaimed for the next set of layers. Consequently, the entire model must be
    loaded onto the chip multiple times during the model training (at least once during
    the forward pass and once during back-propagation). Additionally, as NPUs compute
    model gradients, they push this data to off-chip storage, and a lightweight on-storage
    compute core updates the model for the next iteration[2](#page-3-4) [\[10\]](#page-13-3).
    This approach makes the performance I/O bound, meaning that the upper-bound training
    performance scales as ∝ \_ /\_ . Therefore, in addition to compute efficiency
    and NPU-to-NPU communication performance, maintaining maximum I/O bandwidth is
    crucial. A rigid topology can create hotspots when distributing/collecting the
    model/gradients to/from the I/O channels, which limits the I/O data rate [\(Section
    3.2\)](#page-3-1) and directly impacts training performance.


    # <span id="page-3-1"></span>3.2 Challenges with 2D Mesh


    Next, we discuss specific challenges in a 2D Mesh for supporting the communication
    needs of DNN training.


    3.2.1 Efficient I/O. As mentioned earlier, maintaining high I/O bandwidth is critical
    for achieving optimal performance in the weight streaming execution model. However,
    the 2D mesh often falls short of delivering maximum I/O performance. [Figure 4](#page-3-5)
    illustrates this problem using a 4 × 4 mesh topology with a pure DP parallelization
    strategy. In this scenario, each weight fetched from an off-chip memory channel
    must be broadcast to all NPUs. [Figure 4\(](#page-3-5)A) shows a broadcast algorithm,
    based on the MPI implementation of one-to-many pattern on 2D mesh [\[1\]](#page-13-16),
    when reading from two different memory channels (shown as the red and blue flows).


    Ideally, all memory channels should stream (different) weights simultaneously
    and with the line-rate to maximize the I/O BW. However, the shape of 2D Mesh topology
    inherently generates hotspots and negatively affects the I/O BW. [Figure 4\(](#page-3-5)B)
    shows the maximum channel load, for one hotspot link, when all memory channels
    are fetching the weights at the same time. If the BW of each memory channel is
    bytes/s, then the hotspot link should have the capacity (BW) of 7 bytes/s to allow
    the maximum I/O BW on a 4 × 4 mesh.


    <span id="page-3-5"></span>![](_page_3_Figure_14.jpeg)


    Figure 4: (A) The broadcast communication pattern when reading from two different
    I/O channels (shown in red and blue arrows). The number associated with each arrow
    shows the timestamp when data crosses that link for one packet. In practice, multiple
    packets are pipelined across each path. In this example, the parallelization strategy
    is MP(1)-DP(16)-PP(1), and the model weights are broadcast among all NPUs for
    the weight streaming execution model. Note that the reverse order is used to sum
    the weight gradients during the back-propagation and write the final results into
    the remote storage. (B) The maximum channel load analysis corresponding to [Figure
    4.](#page-3-5)a, when all of the I/O channels are used simultaneously.


    In general, for an × mesh and 4 × external I/O channels, the wafer-scale fabric
    links should have a bandwidth of (2N − 1)P bytes/s to fully utilize the I/O bandwidth
    in all parallelization strategies, assuming each I/O channel has a bandwidth of
    bytes/s. As the formula indicates, the required link bandwidth grows () with the
    mesh width. For larger packages, the technology might not support such high-bandwidth
    requirements on the package. In such cases, the I/O channel rate must be scaled
    down proportionally to accommodate the maximum link bandwidth, i.e., = \_ (2 −1)
    .


    Fred''s Solution. Fred prevents network hotspots by adaptively routing the traffic
    through all of its links equally, enabling further scalability of the wafer-scale
    systems.


    <span id="page-3-0"></span>3.2.2 Ease of Device Placement. Device placement involves
    assigning each logical training worker to a physical NPU. With NPUs, there are
    ! possible device placement mappings. This becomes critical in 3D parallelism,
    as each training worker may have different communication volumes and patterns
    with other workers across distinct parallelization groups (refer to [Figure 1\)](#page-1-0).
    Therefore, finding a device placement that minimizes network contention is essential.


    However, this is challenging with rigid topologies, especially 2D Mesh, where
    certain communication patterns are inherently prioritized over others. [Figure
    5](#page-4-2) illustrates two different mappings for a given MP(2)-DP(4)-PP(2)
    strategy. In [Figure 5\(](#page-4-2)A), the MP and


    <span id="page-3-3"></span><sup>1</sup>All model updates over different training
    iterations happen on-chip.


    <span id="page-3-4"></span><sup>2</sup>Model updates involve low operational intensity.
    Hence, performing these updates off-chip prevents wasting I/O bandwidth by avoiding
    loading optimizer states onto the chip for lightweight operations.


    <span id="page-4-2"></span>FRED: A Wafer-scale Fabric for 3D Parallel DNN Training
    ISCA ''25, June 21–25, 2025, Tokyo, Japan


    ![](_page_4_Figure_1.jpeg)


    Figure 5: Two different device placement mappings for an MP(2)- DP(4)-PP(2) strategy.
    (A) A device placement that favors MP and DP communications but causes congestion
    for PP communications. (B) A device placement that favors DP and PP communications
    but causes congestion for MP communications.


    DP communications are free of congestion, but PP communications cause congestion
    between different PP groups. Conversely, in [Figure 5\(](#page-4-2)B), DP and
    PP communications are optimized, but MP communications face congestion between
    MP groups. Ultimately, as 2D mesh offers two logically disjoint dimensions ( and
    ), it is mathematically impossible for all 3D parallelism dimensions to be optimally
    mapped onto a 2D Mesh. This is trivial by observing the four corner NPUs, where
    each NPU offers two outgoing links. Consequently, due to the limited path diversity,
    one out of the three parallelization groups must experience network congestion
    and reduced communication performance. Determining which communication patterns
    to prioritize, unavoidable on 2D Mesh, requires a thorough analysis of the end-to-end
    workload and understanding the impact of different communication operations.


    Fred''s Solution. Fred supports congestion-free routing for all communication
    patterns simultaneously.


    <span id="page-4-3"></span>![](_page_4_Figure_5.jpeg)


    Figure 6: Network communications on a 4 × 4 mesh topology for a non-aligned MP(5)-DP(3)-PP(1)
    parallelization strategy. (A) Nonoptimized execution of communication patterns
    (e.g., All-Reduce) within the NPUs of the same MP group. (B) Traffic congestion
    between two different DP groups, shown in red and blue, assuming X-Y routing (other
    group traffic is not shown for clarity).


    3.2.3 Non-Aligned Parallelization Strategies. When searching for the best parallelization
    strategy itself, there are many possible configurations where the size of MP/DP/PP
    is not aligned with the physical topology dimensions. Such configurations create
    extra challenges on a 2D Mesh, due to the limited path diversity with distinct
    NPU-to-NPU distances.


    [Figure 6](#page-4-3) illustrates the communication issues within a 4 × 4 2Dmesh
    topology for an MP(5)-DP(3)-PP(1) strategy. [Figure 6\(](#page-4-3)A) demonstrates
    how NPUs in the same MP group need to communicate. Collective communications are
    often optimized for well-structured


    topologies (e.g., rings, trees, switches). However, as shown in [Fig](#page-4-3)[ure
    6\(](#page-4-3)A), the MP groups form non-standard shapes, making it challenging
    to identify the most optimized collective algorithm for each shape. For example,
    the distance between NPU 420 and 020 is two hops, due to the rigid shape of 2D
    Mesh, making it impossible to construct a well-constructed ring, even without
    considering network congestion. [Figure 6\(](#page-4-3)B) depicts the extra traffic
    congestion between two different DP groups, marked in red and blue, caused by
    nonaligned dimensions.


    Fred''s Solution. Fred provides congestion-free topology and routing mechanisms
    for any size/placement of MP/DP/PP.


    <span id="page-4-0"></span>3.2.4 Network BW Utilization. Maintaining high bandwidth
    utilization is challenging for a 2D Mesh. For instance, MP communications are
    required during both forward-pass and back-propagation phases, while DP communications
    occur only during back-propagation. However, these links cannot be utilized by
    MP communications due to the limited paths and lack of optimal routing. Consequently,
    the links used for DP communication during back-propagation remain underutilized
    during the forward-pass phase, detrimenting full bandwidth utilization for many
    strategies on a 2D Mesh. Fred''s Solution. Fred can utilize the full bandwidth
    of each NPU for every communication phase.


    3.2.5 In-Network Collective Execution. Supporting in-network collectives can significantly
    reduce network traffic and improve execution performance as described in [Section
    2.2.](#page-2-2) This feature, currently employed in off-chip switches [\[27,](#page-13-1)
    [44\]](#page-14-14), requires centralized or hierarchical switches which can perform
    the collection, reduction, and broadcast of multiple data. A 2D Mesh with distributed
    NPUs and without a shared central entity, however, impedes the adaptation of the
    in-network collective support.


    Fred''s Solution. Fred employs a switch-based topology that supports in-network
    collective execution.


    3.2.6 Takeaway. Ideally, a fabric for DNN training should enable each NPU to fully
    utilize its network bandwidth for any communication phase of 3D-parallel training
    without congestion and with support for in-network collectives. These requirements
    cannot be met via a 2D Mesh, due to their natural shape and rigidity. This underscores
    the need for the adaptation of new topology and routing mechanisms, such as Fred.


    # <span id="page-4-1"></span>4 Fred Network Fabric Architecture


    A Fred switch forms the backbone of the fabric. Hierarchical connections of the
    Fred switches form the full Fred fabric, which is described in [Section 6.1.](#page-7-1)
    The key idea behind a Fred switch is simple: break the switch into the most fundamental
    components, and add small compute capability to each component. The fine-grained
    distribution of compute enables supporting flexible and concurrent in-switch collective
    execution for 3D parallelism communication patterns. In addition, distributed
    computation of collectives is more scalable to map over the high-BW wafer-scale
    links than having centralized compute and memory entities.


    [Figure 7\(](#page-5-1)a) shows a Fred switch, which consists of a control unit,
    input port buffers, and the Fred interconnect. The control unit performs routing
    between the input ports and the output ports.


    <span id="page-5-1"></span>![](_page_5_Figure_2.jpeg)


    Figure 7: (a) An overview of the Fred switch with P ports. (b) Fred interconnect
    (recursively constructed) when the number of ports is even (2) or odd (2 + 1).
    (c) Fred(2) switch. (d) Fred(3) switch. (e) R-Switch. (f) D-Switch. (g) RD-Switch.
    (h) An example of a Fred2(8) interconnect implementation and two routed All-Reduce
    communication patterns (green and orange). (i) Routing Algorithm for three All-Reduce
    comm flows on Fred2(8) with conflict graph. (j) Example of Routing conflict.


    The Fred interconnect, shown in [Figure 7\(](#page-5-1)b), is inspired by Clos
    networks [\[17\]](#page-13-17). Clos networks are identified through the tuple
    (, , ), where ≥ 2 is the number of middle stage switches, is the number of input/output
    ports per each input/output microswitch (Switch), and is the number of input/output
    Switches. Fred''s connectivity is similar to the (, = 2, ) Clos network, which
    is denoted as Fred(). denotes to the number of middlestage switches, and identifies
    the number of input(output) ports. Fred can be designed for an arbitrary number
    of ports by building on top of the previous works [\[13\]](#page-13-18). is <sup>2</sup>
    <sup>2</sup>+<sup>1</sup> when is an even odd number. Similar to the Clos network,
    Fred interconnect is constructed recursively, where the middle stage switches
    are the ×Fred() ×Fred( + 1) switches for the even odd number of ports, as shown
    in [Figure 7\(](#page-5-1)b). The recursive design of Fred ends when encountering
    the base Fred(2) or Fred(3) Switches, which are depicted in [Figure 7\(](#page-5-1)c)
    and [Figure 7\(](#page-5-1)d), respectively.


    The main difference of Fred, compared to a baseline Clos, is adding the reduction
    and/or distribution (broadcast) support to the baseline Switches. This creates
    three types of Switches depending on which of these two features is present in
    the Switch. [Figure 7\(](#page-5-1)e) shows the R-Switch structure that has the
    reduction feature, i.e., reducing data on the two input ports and routing to one
    of the output ports. [Figure 7\(](#page-5-1)f) shows the D-Switch, which is able
    to perform distribution by broadcasting one of the input data to both output ports.
    RD-Switch is a 2 × 2 Switch and can perform both reduction and distribution, as
    shown in [Figure 7\(](#page-5-1)g). The entire Fred switch is built using these
    three Switch types (plus Muxes and Demuxes to connect the last port to all intermediate
    stage switches when is odd) through the recursive process explained earlier.


    [Figure 7\(](#page-5-1)h) shows the complete structure of a Fred2(8) switch with
    two concurrent All-Reduce operations (green and orange). The highlighted // means
    that the reduction/distribution/reductiondistribution features of the corresponding
    Switch are activated. For instance, the input Switch connecting the input ports
    4, 5 performs the reduction and routes the result to one of its output ports.
    Other non-highlighted Switches operate like Clos Switches.


    ### <span id="page-5-0"></span>5 Conflict-free Collective Routing


    ### 5.1 Communication Patterns on Fred


    The fine-grained reduction and broadcast features enable Fred Switches to perform
    all different types of collective communication patterns observed in distributed
    training. Collective implementation on Fred, however, can be abstracted through
    the notation of communication flow (or flow in short).


    A flow on Fred() includes a set of input ports ()={ip1, ip2, ...., ip } and output
    ports ()={op1, op2, ...., op }, where || ≤ and || ≤ . The flow results in reducing
    the data across the input ports determined in and broadcasting the final result
    to the output ports identified in . The port numbers and cardinality of and can
    be set independently, depending on the communication pattern. Each communication
    algorithm can be expressed in terms of performing one or more flows. For example,
    the orange All-Reduce pattern in [Figure 7\(](#page-5-1)h) is a single flow with
    = {3, 4, 5} and = {3, 4, 5}.


    Simple Communication Algorithms. Simple communication algorithms refer to communication
    patterns that can be realized on Fred by performing only one flow. [Table 2](#page-6-0)
    summarizes different simple communication patterns on Fred and the number of involved
    input/output ports.


    Compound Communication Algorithms. Compound communication algorithms realize the
    communication patterns through multiple flows on Fred. [Table 2](#page-6-0) summarizes
    different compound communication patterns on Fred. For example, Reduce-Scatter
    among inputs is broken into serial steps of the reduce flow, and during step 1
    ≤ ≤ , the reduce operation corresponding to the result of the is done. The process
    is similar for other compound communication algorithms.


    #### 5.2 Routing Protocol


    Fred considers a flow as a unit of routing, and supports concurrent routing of
    multiple flows. Similar to the previous methods [\[43\]](#page-14-15), Fred routing
    protocol is also recursive, meaning that first the status of outermost Switch
    levels (i.e., input/output Switches)


    are determined, and then routing is recursively called on the middle stage switches.
    The difference is, however, supporting reduction/distribution features on the
    Fred switches, and the dependency between the input/output ports of a flow, which
    requires a new routing algorithm to realize these differences. Fred''s routing
    protocol is built upon the following intuitions:


    - If two flows share the same input or output Switch, they should be routed through
    different middle-stage switches (subnetworks).

    - If both input ports of an R-Switch or RD-Switch belong to the same flow, the
    reduction feature is activated.

    - If both output ports of a D-Switch or RD-Switch belong to the same flow, the
    distribution (broadcast) feature of the Switch is activated.


    The latter two points are easy to realize. To satisfy the first point, Fred routing
    protocol creates a conflict graph. [Figure 7\(](#page-5-1)i) shows the first step
    of a routing example for a Fred2(8) interconnect with the associated conflict
    graph for this step.


    In the conflict graph, each node represents a flow and the edges between the nodes
    represent a conflict (i.e., sharing an input or output Switch) between the two
    nodes (flows). Fred routing applies the graph coloring on the conflict graph to
    find the routing of each flow. The number of colors is the number of intermediate
    stage switches (i.e., ). [Figure 7\(](#page-5-1)i) also shows the results of the
    graph coloring. Here, there are only two colors since = 2. Two flows are routed
    to the up subnetwork (blue), and one to the down subnetwork (red). After this
    step, the routing protocol and the conflict graph generation are recursively called
    on the middle blue and red Fred2(4) switches. Note that a desired property of
    DL training is the deterministic and repetitive nature of its communication patterns
    that can be inferred at compile time. Therefore, the routing algorithm for different
    comm phases of the training workload can be executed at compile time and then
    saved at the control unit of the Fred switches and used during the training to
    minimize the routing overhead.


    #### 5.3 Routing Conflicts and Methods to Resolve


    There are certain cases where not all flows can be routed at the same time, causing
    routing conflict. The routing conflict is identified when the graph coloring fails
    to color all of the nodes within the conflict graph. [Figure 7\(](#page-5-1)j)
    shows an example of a routing conflict when there are four flows to be routed
    on a Fred2(8) and the resulting conflict graph. The conflict graph cannot be colored
    using only two colors due to the circular dependencies between flows: 0, 1, 2.
    Note that the routing conflict may happen during any recursive call to the routing
    algorithm (for routing the subnetworks). If the routing conflict is identified,
    the entire routing is marked to have a conflict.


    We now discuss ways to address such conflicts. (1) Blocking the Conflicting Flows.
    The first trivial way is to block some of the conflicting flows and run them after
    the other flows are finished. This translates to removing some of the nodes in
    the conflict graph. For example, in [Figure 7\(](#page-5-1)j), if any of the flows
    1, 2, or 3 is blocked, the routing can proceed to the next step (i.e., subnetworks).
    This option is, however, costly in terms of performance since it blocks some of
    the flows.


    <span id="page-6-0"></span>Table 2: Simple (shaded) and Compound collective algorithms.


    | Pattern        | IPs | OPs | Comment                       |

    |----------------|-----|-----|-------------------------------|

    | Unicast        | 1   | 1   |                               |

    | Multicast      | 1   | >1  |                               |

    | Reduce         | >1  | 1   |                               |

    |                |     |     | Input ports and output        |

    | All-Reduce     | i>1 | i>1 | ports are the same            |

    |                |     |     | Broken into multiple serial   |

    | Reduce-Scatter | i>1 | i>1 | Reduce collectives, each      |

    |                |     |     | on a different output port    |

    |                |     |     | Broken into multiple serial   |

    | All-Gather     | i>1 | i>1 | Multicast collectives, each   |

    |                |     |     | on a different input port     |

    |                |     |     | Broken into multiple serial   |

    | Scatter        | 1   | i>1 | Unicast operations, each      |

    |                |     |     | on a different output port    |

    |                |     |     | Broken into multiple serial   |

    | Gather         | i>1 | 1   | Unicast operations, each      |

    |                |     |     | on a different input port     |

    |                |     |     | Broken into i serial steps of |

    |                |     |     | Unicast operations. In step   |

    | All-To-All     | i>1 | i>1 | 1 ≤ j ≤ i, each input port    |

    |                |     |     | unicasts to the output port   |

    |                |     |     | with distance j in the OPs    |


    (2) Increasing the Number of Middle Stages. Another method is to design Fred switches
    with more intermediate stage switches (i.e., increase ). This method increases
    the number of colors for the graph coloring algorithm. Therefore, more conflicting
    flows can be routed simultaneously.[3](#page-6-1) However, this comes at the expense
    of more HW overhead.


    (3) Decomposing the Communication Algorithms. For the unicast-only traffic, Fred
    interconnect is rearrangeably nonblocking when = 2 and strict-sense nonblocking
    when ≥ 3. This fact can be leveraged to decompose some of the communication algorithms
    into multiple steps and break the dependency among input/output ports in each
    step (i.e., making them unicast traffic). In the worst case, any collective algorithm
    can be decomposed into complete unicast traffic. For example, All-Reduce can be
    handled through a ring-based algorithm at the endpoints (NPUs), rather than innetwork
    execution, which is complete unicast traffic. As a result, flows 0, 1, and 2 in
    [Figure 7\(](#page-5-1)j) can switch to ring-based All-Reduce at the endpoint,
    while flow 3 uses an in-network All-Reduce algorithm. This method solves the routing
    by degrading the communication performance of the conflicting flows (but it does
    not block any flow). (4) Intelligent Device Placement. Another method to prevent
    conflicts is through intelligent device placement (mapping) of the training workers
    to the physical NPUs at the start time. For example, if in [Figure 7\(](#page-5-1)j)
    the workers mapped to NPUs of ports 1 and 4 swap their locations, the conflict
    does not happen.


    In Fred, we prioritize the communication performance and do not use options (1)
    and (3). We use option (2) to simplify the device placement algorithm by only
    using Fred3() switches, ensuring that we have three colors in our routing algorithm
    protocol. Then, for the device placement algorithm, we map the training workers
    within the same MP group on consecutive physical NPUs, followed by iterating over
    workers within PP and DP, respectively. This is sufficient to prevent routing
    conflicts for 3D-Parallelism communication patterns.


    <span id="page-6-1"></span><sup>3</sup> For example, Fred3(8) can route all the
    flows in [Figure 7\(](#page-5-1)j).


    # 5.4 Handling Overlapping Communications


    In training, the workload at a given time may require multiple communication operations.
    For example, while handling the DP communication in backpropagation, the workload
    may initiate the PP communication to exchange the next microbatch between the
    workers. However, FRED''s circuit switch configuration may handle one communication
    phase at a given time. Additionally, different NPUs might issue communication
    at different times, due to variations in the compute latencies. Hence, there should
    be a mechanism to safely preempt the current executing communication operation
    and execute the new communication, with minimal effects to the in-flight packets,
    if the latter has a higher priority.


    We address this issue by allocating multiple Virtual Circuits (VCs) per port,
    each dedicated to a specific communication group (e.g., MP), and the FRED''s interconnect
    to be reconfigured between different overlapping communication operations. While
    it is possible to frequently reconfigure FRED''s interconnect in short intervals
    to handle overlapping communication operations concurrently, we choose to reconfigure
    FRED to execute the highest priority communication operation among the currently
    pending operations (and preempt the current communication if a new higher priority
    communication is issued). This decision simplifies the design and minimizes the
    FRED''s reconfiguration overhead, and is in line with the training workload requirements,
    since the workload is usually blocking on one communication operation (highest
    priority) at any given point in time. In our 3D-parallel case, the priority of
    communication operations in descending order is: MP, PP, and DP. More discussion
    on FRED''s buffer management and flow control is described in [Section 6.2.3.](#page-8-0)


    #### <span id="page-7-0"></span>6 Wafer Scale Architecture


    We present an instance of a wafer-scale NPU system connected using Fred, for evaluation
    purposes. We note that alternate configurations are also feasible.


    #### <span id="page-7-1"></span>6.1 Layout of Fred Fabric


    A Fred switch builds a foundation to connect multiple wafer-scale NPUs. However,
    for large wafer-scale systems, due to physical limitations such as wiring, area,
    etc., it is not feasible to connect all of the NPUs through a single Fred switch.
    Hence, the Fred fabric provides a hierarchical design for the scalable connection
    of large wafer-scale systems. [Figure 8](#page-7-2) shows an example of the Fred
    fabric that shows a 2-level tree connection of the Fred switches and the NPUs
    connected to the leaf (L1) switches[4](#page-7-3) . In general, tree height and
    the BW across different levels are determined by the system size and physical
    constraints (see [Section 6.2\)](#page-7-4).


    When there are multiple levels of Fred switches, the communication algorithms
    might need to cross several switches and hence, need to be optimized accordingly.
    For example, [Figure 8\(](#page-7-2)a) shows the flow path for an All-Reduce between
    NPUs 1, 5, and 6. In this


    case, the data of NPUs 1 and 5 are reduced on their local L1 switch (to reduce
    the traffic going to the L2 switch), and the result along with the data of NPU
    6 are reduced on the L2 switch. The final result is sent back to the corresponding
    L1 switches. The L1 switch attached to NPUs 1 and 5 also multicasts the result
    to the NPUs.


    <span id="page-7-2"></span>![](_page_7_Figure_12.jpeg)


    Figure 8: Physical and Logical Views of 2-level Fred Topologies.


    ### <span id="page-7-4"></span>6.2 Wafer-scale Architecture Configuration


    We assume a standard 300 wafer diameter, similar to the prior works [\[3,](#page-13-21)
    [48\]](#page-14-6), resulting in a 70000 <sup>2</sup> wafer area.


    6.2.1 Constraints. Fundamentally, there are two physical limitations that limit
    the amount of compute and other resources on the wafer: (i) Thermal constraints,
    and (ii) Power delivery network [\[3,](#page-13-21) [47,](#page-14-5) [48\]](#page-14-6).
    Thermal constraints limit the amount of power that can be delivered to the wafer,
    depending on the cooling mechanism. Previous works report the maximum power limit
    within the 9.6 [\[48\]](#page-14-6) to 15 [\[10\]](#page-13-3) range. In this
    paper, we assume 15 power is available for the wafer-scale system. The other limitation
    is the power delivery network, which might necessitate using big on-wafer voltage
    regulator modules (VRMs), limiting the available area for NPUs [\[48\]](#page-14-6).
    However, alternative solutions can eliminate the need for on-wafer VRMs by either
    supplying the voltage from the top of the wafer [\[10\]](#page-13-3), or delivering
    the power from the back of the wafer by using the through-wafer-vias (TWVs) [\[36\]](#page-13-22).
    In this paper, we assume the on-wafer VRMs are not used by using any of the solutions
    described earlier.


    <span id="page-7-5"></span>6.2.2 Physical System Parameters. [Table 3](#page-8-1)
    shows the other set of physical parameters. We assume that the NPU chiplets are
    tested before bonding. If Known Good Die testing is difficult, larger chiplets
    such as NPU Compute may need to be broken into smaller constituents. Recent work
    [\[20\]](#page-13-23) has suggested that these chiplets actually need to be moderately
    large (40<sup>2</sup> -400<sup>2</sup> ) in size for cost-optimality. For the
    purposes of our evaluation, we assume an H100 GPU-like NPU compute chiplet, each
    equipped with five stacks of HBM3 chiplet memories, resulting in combined power
    consumption of 700 and an area of 1314 <sup>2</sup> [\[45\]](#page-14-10).


    The NPU compute chiplet perimeter can support up to 12 TBps wafer-scale BW, where
    6 TBps of it is allocated to support the 3 TBs local HBM memory BW (3 TBps for
    read + 3 TBps for write), and the other 6 TBps is allocated to support 3 TBps
    bi-directional total NPU-to-NPU BW (3 TBps for send + 3 TBps for receive).


    The 15 power budget limits the total amount of NPUs on the wafer to 15 /700 ≈
    21, excluding other component power overheads (e.g., I/O controller, wafer-scale
    wires). This anticipated power density of 22W/cm<sup>2</sup> is well within the
    projection of cooling capability in heterogeneous integration roadmaps [\[5\]](#page-13-24).
    In this paper,


    <span id="page-7-3"></span><sup>4</sup>We note that Fred layout shown is not tiled.
    This means that the substrate (where chiplets are bonded) may not be able to use
    stepper-based lithography. But directwritten maskless lithography is not uncommon
    for substrate patterning. This was used in a commercial packaging provider ThinkDeca
    [\[25\]](#page-13-19). Such patterning has no symmetry requirement, albeit it
    has a lower throughput. Also, note that using maskless lithography increases the
    substrate manufacturing moderately [\[7\]](#page-13-20), but substrate manufacturing
    is a small fraction of the total system cost [\[47\]](#page-14-5).


    Table 3: Physical system parameters.


    <span id="page-8-1"></span>


    | Component                   | Model                   | Area           | Power                                     |
    Characteristics                                                                                                                                       |

    |-----------------------------|-------------------------|----------------|-------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------|

    | NPU<br>Compute              | GPU-like<br>[45]        | 814 𝑚𝑚2        | 525
    𝑊                                     | • FP16:<br>1,000 TFLOPS                                                                                                                               |

    | NPU<br>Memory               | 5 ×<br>HBM3<br>[45, 48] | 5 ×<br>100 𝑚𝑚2 | 5 ×<br>35
    𝑊                               | • Total Capacity:<br>80 GB<br>• Total BW:<br>3
    TBps                                                                                                   |

    | Wafer-Scale<br>Interconenct | SI-IF<br>[3, 48]        | 4 𝑢𝑚<br>pitch  | 0.063<br>pJ/bit
    [46]<br>Baseline:<br>100𝑊 | • # of<br>Metal Layers: 2<br>• Freq: 1.74 GHz<br>•
    BW: 53.7 GB<br>/mm/metal layer<br>• BW/NPU<br>Compute:<br>6 TBps<br>/metal layer<br>Latency:
    20 𝑛𝑠 |

    | I/O                         | CXL 3                   | 18 ×<br>20 𝑚𝑚2 | 18
    ×<br>5 𝑊                               | • BW: 128                                                                                                                                             |


    <span id="page-8-2"></span>Table 4: HW overhead of Fred implementation of [Figure
    8\(](#page-7-2)b).


    | Component                     | Area         | Power      |

    |-------------------------------|--------------|------------|

    | Fred3(12) L1 Switch           | 15 × 685 𝑚𝑚2 | 15× 3.75 𝑊 |

    | Fred3(11) L1 Switch           | 10 × 678 𝑚𝑚2 | 10× 3.40 𝑊 |

    | Fred3(10) L2 Switch           | 10 × 814 𝑚𝑚2 | 10× 3.11 𝑊 |

    | Additional Wafer-Scale Wiring | N/A          | 58𝑊        |

    | Total                         | 25195 𝑚𝑚2    | 179.35 𝑊   |


    we consider a 20-NPU wafer-scale system to make room for other component power
    overheads. Additionally, 18×I/O Controllers are used to connect the wafer to the
    external memory. Hence, the total NPU + I/O Controller area overhead is 26640
    <sup>2</sup> .


    Similar to [\[48\]](#page-14-6), we assume in the baseline, the NPU chips are
    placed with a 100 distance from each other. Combined with the I/O controllers,
    the entire baseline can be fit within a rectangle with the size of 190.8 × 150.4
    in the center of the wafer, leaving the rest of the wafer area unclaimed.


    <span id="page-8-0"></span>6.2.3 Fred Topology and Parameters. To motivate Fred,
    we leverage the fact that the combination of a constrained power budget and high-end
    NPUs results in utilizing 26640<sup>2</sup> out of 70000<sup>2</sup> area, making
    room to utilize otherwise unclaimed area for flexible fabrics like Fred. However,
    any fabric proposal must have low power consumption since most of the power budget
    is allocated to the NPUs. We demonstrate that Fred meets these properties.


    Our target Fred topology is similar to [Figure 8\(](#page-7-2)a), where 20 NPUs
    and I/O controllers are connected through a 2-level (almost) fat-tree topology.
    Similar to the baseline, the BW/NPU is still 3 TBps, but the bisection BW is increased
    to 30 TBps. It is almost fat-tree since the L1-to-L2 BW is the summation of attached
    NPU BW only (and not NPU + I/O Controller). The reason is that if one participant
    of any flow (e.g., Reduce) is an I/O controller, then the entire flow''s BW requirement
    is determined by the I/O controller''s BW (e.g., 128 GBps), which is significantly
    less than NPU-to-NPU BW. Hence, an almost fat-tree gives the same performance
    as the full fat-tree.


    Looking at the BW requirements of Fred L1/L2 switches in [Fig](#page-7-2)[ure
    8\(](#page-7-2)a), it is clear that each switch chiplet requires a perimeter,
    to connect the wafer-scale network wires, that is not feasible to build. Hence,
    in reality, each of the Fred switches in [Figure 8\(](#page-7-2)a) is decomposed
    into multiple lower-BW Fred chiplets. [Figure 8\(](#page-7-2)b) shows a logical
    view of implementing the (almost) fat-tree based topology of [Figure 8\(](#page-7-2)a)
    using feasible Fred chiplets. As [Figure 8\(](#page-7-2)b) shows, each switch
    of [Figure 8\(](#page-7-2)a) is implemented by decomposing it into multiple smaller,
    but feasible, Fred switches (enclosed in the strip line). For our evaluations,
    we use Fred3() switches.


    As [Figure 8\(](#page-7-2)b) shows, in Fred fabric, L1 switches have hybrid BW
    downstream links to connect to the NPUs and I/O controllers. This requires Fred
    L1 switches to use different interface circuitry for NPU vs. I/O controller links,
    which is accounted for in the overhead numbers in [Table 4.](#page-8-2) In general,
    hybrid on-chip interconnects are widely used in many designs (e.g., to connect
    on-chip routers vs. memory controllers in multi-core processors) [\[18\]](#page-13-26).


    Flow Control. We assume a Virtual Cut-Through flow control with a credit-based
    backpressure mechanism to guarantee the switch buffer as packets flow through
    FRED''s fabric. To enable preemptive communication execution, we consider four
    VCs per port: three data VCs dedicated to MP, DP, and PP packets and one control
    VC for the ACK/NACK and other control messages. The data/control packet size is
    4KB/512B, with each flit size set to be 512B. The packet header size is 6B to
    allow for large sequence numbers. Each packet header also has the index to the
    Switch configuration bits, stored in the control unit for a specific communication
    phase[5](#page-8-3) . If all ports receive a packet belonging to a higher priority
    phase, Fred changes its Switch configuration to that phase and starts forwarding
    the packets from that phase. Additionally, there is a default header index, which
    refers to a phase where all flows are unicast and Fred falls back to the online
    routing to determine the Switch configs. While not present in our workloads, this
    mode is useful when dealing with communication patterns such as alltoallv where
    different src/dst pairs have different size flows that are changing dynamically.


    The retransmission protocol is set to be simple Go-Back-N, with an accumulative
    ack per every 16 data packets to reduce the ack overhead to less than 1% of the
    network BW. If a switch receives a NACK from an NPU, it forwards it to all input
    ports participating in that flow, which is then propagated to all NPUs serving
    as the source of the flow, and retransmission starts from the NACKed packet.


    Additionally, each input port has a 24KB buffer per data VC and a 2KB buffer for
    the control VC. These policies ensure that in the case of communication preemption,
    there are enough buffers available (i.e., \_ × = 24KB) for the new communication
    operation to send at the full link BW.


    HW Overhead. [Table 4](#page-8-2) shows the overheads of our proposed Fred implementation
    shown in [Figure 8.](#page-7-2) We assume 1.5KB SRAM per FRED switch to store
    the Switch configurations for different communication operations. The numbers
    are obtained post layout using 15nm NanGate PDK. The total power overhead is 179.35
    , which is about 1.2% of the total power budget. The total area overhead is 25195
    <sup>2</sup> , which can be accommodated by using the unclaimed


    <span id="page-8-3"></span><sup>5</sup>Compound collectives have multiple phases


    Table 5: Target configurations.


    <span id="page-9-0"></span>


    | Configs  | Comment                                                     |  |  |

    |----------|-------------------------------------------------------------|--|--|

    |          | The baseline topology where 20 NPUs are formed using a      |  |  |

    |          | 4×5 2D-mesh topology (3.75 TBps bisection). Each border     |  |  |

    | Baseline | NPU is attached to an I/O controller, except for the corner |  |  |

    |          | NPUs that are connected to two I/O controllers, summing     |  |  |

    |          | the total I/O controllers to 18.                            |  |  |

    |          | Fred topology where 20 NPUs and 18 I/O controllers are      |  |  |

    |          | connected similar to Figure 8, but with the                 |  |  |

    | Fred-A   | same bisection BW as the baseline (3.75 TBps).              |  |  |

    |          | This is achieved by downscaling L1-L2 links from            |  |  |

    |          | 12 TBps, in Figure 8(a), to 1.5 TBps. Additionally, it      |  |  |

    |          | does not support in-switch collective execution.            |  |  |

    |          | Thus, collective algorithms are handled at the endpoints.   |  |  |

    |          | Similar to Fred-A, but with in-network collective           |  |  |

    | Fred-B   | execution.                                                  |  |  |

    |          | Fred topology where 20 NPUs and 18 I/O                      |  |  |

    | Fred-C   | controllers are connected similar to Figure 8               |  |  |

    |          | (30 TBps bisection), but without any in-switch              |  |  |

    |          | collective execution (handled at the endpoint).             |  |  |

    |          | Similar to Fred-C, but with in-network                      |  |  |

    | Fred-D   | collective execution.                                       |  |  |


    Table 6: Target workloads.


    <span id="page-9-1"></span>


    | Workload             | Parallelization<br>Strategy | Execution Model   |

    |----------------------|-----------------------------|-------------------|

    | ResNet-152 [21]      | MP(1)_DP(20)_PP(1)          | Weight Stationary |

    | Transformer-17B [38] | MP(3)_DP(3)_PP(2)           | Weight Stationary |

    | GPT-3 [9]            | MP(2)_DP(5)_PP(2)           | Weight Streaming  |

    | Transformer-1T [19]  | MP(1)_DP(20)_PP(1)          | Weight Streaming  |


    area available on the wafer. Note that, as discussed in [Section 6.2.3,](#page-8-0)
    the main area overhead of the Fred chiplets is due to I/O for supporting high-BW
    wafer-scale interconnects, and not because of the switch logic overhead.


    Discussion: Fred Area Overhead. As we discussed earlier, the unclaimed area on
    the wafer allows for designing large (but low power) Fred switches to deliver
    high I/O BW requirements for our topology. In fact, Fred''s internal logic occupies
    less than 5% of the chip area. Hence, the area overhead of Fred can be significantly
    reduced if the I/O density increases.


    In our design, we conservatively assume the switch chips use the same interconnect
    technology as the NPUs (e.g., pitch, frequency, etc.). However, switch area can
    be further reduced by applying more aggressive network bandwidth technologies.
    Next generation of I/O technology is expected to deliver up to 250 GBps/mm (compared
    to 107.4 GBps/mm in our design) [\[6\]](#page-13-31). This results in designing
    Fred switch chips with only 18.4% of current area with the same I/O BW.


    The other I/O technology alternative is using the serialized highspeed links such
    as UCIe Advanced [\[60\]](#page-14-17), which can deliver up to 1 TBps/. This
    results in designing Fred switch chips with only 5% of the current area. Note
    that even with the high area assumption of Fred, we don''t expect the yield issue
    to be a practical problem since compared to the compute NPUs, Fred switches have
    much less internal logic and hence encounter fewer defects.


    #### 7 Evaluation Methodology


    #### 7.1 Baseline and Fred Configurations


    Baseline. The baseline topology is a 5 × 4 2D-mesh with I/O controllers attached
    to the edge NPUs, similar to prior multi-chiplet wafer-scale prototypes [\[31,](#page-13-6)
    [47](#page-14-5)[–49,](#page-14-8) [59\]](#page-14-4). Since each NPU has 3 TBps
    bandwidth [\(Section 6.2.2\)](#page-7-5), each NPU-to-NPU link in the 2D-Mesh
    is equal to 750 GBps, resulting in the bisection BW of 3.75 TBps. The I/O Controller-to-NPU
    is 128 GBps.


    Fred. We test four different variations of Fred to show how different features
    of Fred contribute to the overall performance. [Table 5](#page-9-0) shows the
    target configurations. Fred-A shows the effect of going from mesh to switch-based
    topology with the same bisection and without in-network collective execution.
    Fred-B builds on top of Fred-A and adds the in-network collective execution feature.
    Fred-C increases the bisection BW without in-network collective execution. Finally,
    Fred-D is the most optimal variant of Fred by adding the in-network collective
    execution to the previous variant.


    ### 7.2 Collective Algorithm


    For the baseline 2D mesh and when there is a wafer-wide collective, we use the
    hierarchical 2D algorithm with two concurrent chunks (in reverse direction) to
    enhance utilization [\[28,](#page-13-32) [31\]](#page-13-6). For collectives between
    arbitrary NPUs, we build logical rings between involved NPUs and perform the ring
    algorithm. We also use X-Y routing, which is common in real systems [\[28\]](#page-13-32).
    For Fred-A and Fred-C, we use the hierarchical 2-D ring algorithm to reduce the
    traffic of L1-L2 links, similar to [\[16\]](#page-13-33). Fred-B and Fred-D use
    the in-network capability and use the hierarchical Fred switch topology to perform
    the collective, as explained in [Section 6.1.](#page-7-1)


    #### 7.3 Target Workloads and Execution modes


    In the interest of space, we evaluate four training workloads, ranging from 60M
    to 1T parameters to be the representative for a broad range of ML workloads. [Table
    6](#page-9-1) shows the target workloads and their corresponding parallelization
    strategy and execution models studied in [Section 8.2.](#page-11-0) ResNet-152
    and Transformer-17B (Transformer model with 17 billion parameters) can fit on
    the on-wafer memory and hence, use the weight stationary execution mode [\(Sec](#page-3-6)[tion
    3.1\)](#page-3-6). In contrast, GPT-3 and Transformer-1T (Transformer model with
    1 trillion parameters) use the weight streaming execution mode [\(Section 3.1\)](#page-3-6).
    Workers within the same DP group perform All-Reduce together during the back-propagation
    to sync on weight gradients. In weight stationary mode, the workers use the Microsoft
    ZeRO optimizer stage 2 [\[52\]](#page-14-18) along the DP dimension to reduce
    the memory footprint. Note that in weight streaming mode, the DP groups should
    reduce the gradients as they stream them out to the external memory through the
    I/O controller. The pattern is the reverse communication direction of [Figure
    4.](#page-3-5) For Transformer-17B, GPT-3 and Transformer-1T, the model split
    is based on the Megatron-LM method [\[62\]](#page-14-13), which requires two All-Reduces
    (along the MP dimension) for each transformer layer stack during forwardpass &
    back-propagation. For the PP split on Transformer-17B, we assume the minibatch
    is divided into 8 microbatches to hide the effect of pipeline bubbles [\[23\]](#page-13-9).
    For GPT-3, however, pipelining works differently since it is combined with the
    weight streaming. In this case, = 2 indicates that each time 2 consecutive layers
    are


    brought to the wafer and distributed among different NPUs along the PP dimension.
    Thus, splitting the minibatch into two microbatches is enough to hide the pipeline
    latency. In [Section 8.1](#page-10-1) and [Section 8.2,](#page-11-0) the minibatch
    size for all workloads is set to DP\_size×16, while in [Section 8.3](#page-12-0)
    (and also [Figure 2\)](#page-1-2) the minibatch size is increased to DP\_size×40
    to allow for finer-grain pipelining when PP\_size increases[6](#page-10-2) . All
    workloads use FP16 gradient precision.


    ### <span id="page-10-6"></span>7.4 Simulation Framework


    We use ASTRA-SIM [\[2,](#page-13-34) [55\]](#page-14-3), which is an open-source
    simulation methodology for modeling distributed training systems. ASTRA-SIM enables
    the profiling of compute and communication performance of distinct wafer-scale
    fabrics, including Fred. It can model various parallelization strategies and the
    overlapping of compute with comm kernels. Additionally, its network back-end can
    simulate the comm operations in detail. We extend ASTRA-SIM to model the I/O-to-wafer
    transfers for both the weight stationary and weight streaming scenarios. For each
    workload, we run the simulation for two training iterations (i.e., two forward
    + two backward-pass).


    Previous works have shown that endpoint-based collective execution (our baseline)
    puts more pressure on the endpoint''s compute and memory BW resources, hindering
    the compute kernel efficiency [\[53\]](#page-14-19). To favor the baseline and
    only focus on the network characteristics, we omit such effects in our baseline
    system and assume the compute kernels can run as efficient as the in-network collective
    execution systems such as Fred.


    Metric of Evaluation. In [Section 8,](#page-10-0) we report the end-to-end training
    times and their breakdowns into total compute time and different exposed communication
    times. Since the minibatch size per training iteration may be different depending
    on the parallelization strategy, we normalize the reported times by dividing the
    latencies by the minibatch size when comparing the different parallelization strategies
    of the same workload (e.g., [Figure 2\)](#page-1-2). The exposed communication
    time refers to the amount of time that is not overlapped with the compute time
    and the workload is waiting for the communication to be finished. Depending on
    the parallelization strategy and execution model, there might be multiple sources
    of exposed communication times—load, DP, MP, PP, and/or weight streaming.


    ### <span id="page-10-0"></span>8 Results


    ### <span id="page-10-1"></span>8.1 Microbenchmark Results.


    [Figure 9](#page-10-3) presents the communication breakdown across 3D parallelism
    phases for two parallelization strategies for Transformer-17B. For the MP(20)-DP(1)-PP(1)
    strategy, there are only wafer-wide All-Reduce operations for the MP communication.
    The baseline effective BW utilization is bounded by the corner NPUs since they
    have only 2 links to other NPUs. This limits the average network BW utilization
    of each NPU to be around 2 × 750GBps = 1500GBps. In Fred-A, each NPU-L1 BW is
    3 TBps, but NPU-L2 BW is 375GBps.[7](#page-10-4) Using a similar analysis as [\[56\]](#page-14-20),
    we see that hierarchical collectives result in NPU-L2 BW being the bottleneck
    and the effective NPU BW utilization is 375GBps + 4 × 375GBps = 1850GBps. In Fred-B,
    the L1 switches first perform the All-Reduce and then use the entire L1-L2 BW
    to forward the data to the L2 switches for the second All-Reduce. Therefore, each
    NPU can send the data to L2 switch at the speed of 1500GBps (L1-L2 BW). However,
    since it is an in-network collective execution, the amount of traffic each NPU
    sends out is almost half of the traffic in the endpoint-based collective. Fred-C
    has much more L1-L2 BW and therefore each NPU can drive the BW utilization to
    3TBps. In Fred-D, an additional in-network collective execution reduces the traffic
    by half in addition to the 3TBps NPU BW utilization.


    <span id="page-10-3"></span>![](_page_10_Figure_13.jpeg)


    Figure 9: Communication microbenchmark results for comparing only communication
    performance at different phases of 3D-parallelism, for two different parallelization
    strategies of Transformer-17B from [Figure 2.](#page-1-2)


    The MP(2)-DP(5)-PP(2) case has all MP (All-Reduce), DP (All-Reduce), and PP (multicast)
    communications. For the MP communications, the baseline NPU can only utilize 1
    link (out of its up to 4 links), resulting in only 750GBps BW utilization. Since
    all the communicating NPUs are below the same L1 switch in Fred topologies, they
    can use the entire 3TBps of NPU-L1 BW to communicate. Additionally, in the special
    case when the number of peer NPUs is two, the amount of traffic for endpoint-based
    vs. in-network execution is the same. Hence, all Fred variants have the same performance
    for MP communication.


    Again, the baseline is limited by the corner NPUs, which can utilize only one
    of their links for DP communication. Hence, the baseline NPU BW is 750GBps. In
    Fred, and for the DP communication, each NPU should communicate with four other
    NPUs under different L1 switches. Therefore, in Fred variants the L1-L2 BW should
    be shared across four collective flows. Therefore, L1-L2 BW plays a significant
    role in the performance of this collective. In Fred-A, each NPU has an average
    NPU-L2 BW of 375GBps, and hence, the NPU BW utilization is only 375GBps, which
    is worse than the baseline. In Fred-B, however, the L2 switch is used to perform
    All-Reduce for each flow. This reduces the traffic generated by each NPU roughly
    by 37.5%, which makes its overall performance closer to the baseline. In Fred-C,
    however, the NPU-L2 BW is increased to 3TBps. Finally, Fred-D Improves the Fred-C
    by performing in-network collective and reducing the traffic by 37.5%.


    For the PP comm, the baseline NPU can utilize one of its links to forward data
    to the next pipeline stage and hence, its BW utilization is 750GBps. Note that
    this is possible since in the case of language models such as Transformer-17B,
    one NPU within the mp group is sufficient to multicast the output to all NPUs
    at the next stage,[8](#page-10-5) and hence, there is no contention between NPUs
    of the same MP group at the same stage. In Fred, all peer NPUs are below the same
    L1 switch and can utilize the entire 3 BW for the PP comm.


    <span id="page-10-2"></span><sup>6</sup> For these results, we assume the number
    of microbatches is 1, 10, 20, 20, 20, 40 for the Transformer-17B with size of
    1, 2, 4, 5, 10, 20, respectively. For Transformer-1T, the number of microbatches
    is equal to the size.


    <span id="page-10-4"></span><sup>7</sup>Assuming the L1-L2 BW is equally shared
    among all NPUs.


    <span id="page-10-5"></span><sup>8</sup>All NPUs within the same MP group produce
    the same output in this case


    Discussion: Fred''s NPU to L1 Topology Logic. Now that we have presented the microbenchmark
    results, we can discuss why we preferred to choose a tree-based topology to connect
    every four NPUs to the L1 switches. An alternative solution can be a fully-connected
    topology to connect every four NPUs and then use only one switch level. However,
    this design choice still suffers from the endpoint-based effects (i.e., increased
    use of compute and memory BW at the endpoint) discussed in [Section 7.4.](#page-10-6)
    Furthermore, as explained in [Section 2.2,](#page-2-2) endpoint-based methods
    produce more communication traffic. For example, in the case of four NPUs, the
    most endpoint-based BW optimal algorithms produce 1.5D traffic per NPU to perform
    an All-Reduce of size D [\[56,](#page-14-20) [64\]](#page-14-11), while the in-network
    collective execution produces only D traffic per NPU [\[57\]](#page-14-12), 50%
    lower than the fully connected topology.


    <span id="page-11-1"></span>![](_page_11_Figure_2.jpeg)


    Figure 10: End-to-end training times are decomposed into compute times and different
    communication times. The runtime of each workload is normalized to its corresponding
    baseline.


    ### <span id="page-11-0"></span>8.2 Full Workload Results: Deep Dive


    [Figure 10](#page-11-1) shows the end-to-end runtimes of the training workloads
    for the baseline vs. Fred. Due to space limitations, we only show the Fred-C and
    Fred-D in comparison with the baseline. However, we note that Fred-A and Fred-B
    results are between the baseline and Fred-C, in terms of performance. In general,
    input activations, compared to the model parameters, are relatively small in size
    and hence, do not have significant overhead on the total iteration time. Additionally,
    the input activations of the next iteration can be prefetched to the wafer whenever
    the wafer-scale interconnect is idle. Hence, we observe no initial\_input\_load
    exposed comm for any of our target workloads, except for the Transformer-1T.


    ResNet-152 uses pure DP with a weight stationary model. Hence, the only communication
    costs that repeat on each training iteration are the input minibatch loading and
    DP communication. As explained earlier, in wafer-wide All-Reduce collective, the
    baseline is able to utilize 1.5 TBps of NPU BW. Fred-C and Fred-D can achieve
    3 TBps NPU BW but Fred-D can further reduce the network traffic by ≈ 2×, resulting
    in a significant reduction of DP exposed comm. Thus, Fred-C and Fred-D can improve
    the end-to-end training runtime by 1.41× and 1.76×, respectively, for ResNet-152.


    Transformer-17B uses all dimensions of the 3D-parallelism and therefore, has all
    DP, MP, and PP communication overheads. The baseline device placement favors MP,
    but compromises the PP and DP comms, especially due to the non-aligned parallelization
    strategy dimensions as explained in [Section 3.](#page-3-2) Another drawback of
    the baseline is the underutilized links due to the non-overlapping


    <span id="page-11-2"></span>![](_page_11_Figure_9.jpeg)


    Figure 11: Baseline vs. Fred-D for various parallelization strategies of Transformer-1T
    and Transformer-17B


    nature of MP/DP/PP comms (see [Section 3\)](#page-3-2). Fred-C, on the other hand,
    does not have the problem of underutilized links and nonaligned parallelization
    strategies. It also does not require favoring any of DP, MP, or PP over the other
    strategies. Fred-D can further improve the MP and DP collectives'' performance
    due to in-switch collective execution capability. As a result, Fred-C and Fred-D
    can improve the overall end-to-end training performance by 1.75× and 1.87×, respectively.


    GPT-3 combines weight streaming with 3D-parallelism. Using the analysis of [Section
    3,](#page-3-2) the baseline topology is unable to stream weights with the full
    line-rate of I/O controllers. The reason is that the hotspot link requires (2
    × 5 − 1) × 128 GBps = 1152 GBps, while link capacity is only 750 GBps. Therefore,
    the I/O channels should work with <sup>750</sup> <sup>1152</sup> <sup>=</sup>
    <sup>0</sup>.65<sup>×</sup> of the line-rate. The MP/PP comm performance of Fred-C
    and Fred-D is ≈ 4× better than the baseline, due to the underutilized links in
    the baseline. Note that the reason why Fred-C and Fred-D have the same performance
    for MP collective comm is because dim(MP)=2. In this special case, as explained
    earlier, end-to-end and in-switch collective execution have the same amount of
    networking traffic and hence, have the same performance. In total, Fred-D and
    Fred-C outperform the baseline by 1.34× in terms of overall training time for
    GPT-3.


    Transformer-1T is another weight streaming workload, but with only DP parallelism.
    As a result, the weight streaming delay is the only communication overhead in
    addition to the initial input load. The high-performance compute NPUs and limited
    off-chip I/O BW puts the weight streaming performance directly on the critical
    path. This means that the NPUs can work with the line-rate of the weight being
    streamed, and the main limiting factor is how fast all the weights can be streamed.
    In this case, both Fred-C and Fred-D can leverage the full I/O BW, while the baseline
    topology can only work with 0.65× of the total I/O BW as explained earlier. Additionally,
    since I/O controllers are always being utilized for weight streaming, there is
    no idle time to prefetch the input minibatch of the next iteration during the
    current training iteration. Hence, the initial input load cannot be hidden, although
    its overhead is very negligible. In total, using Fred-C/Fred-D improves the training
    time by 1.4×. FRED: A Wafer-scale Fabric for 3D Parallel DNN Training ISCA ''25,
    June 21–25, 2025, Tokyo, Japan


    # <span id="page-12-0"></span>8.3 Various Parallelization Strategies


    To test the efficiency of Fred for different parallelization strategies, we pick
    two workloads, Transformer-17B and Transformer-1T, and compare the baseline performance
    vs. Fred-D in [Figure 11a](#page-11-2) and [Figure 11b,](#page-11-2) respectively.
    The Avg. bars are obtained across all parallelization strategies similar to [Figure
    2,](#page-1-2) however, not all individual parallelization strategies of [Figure
    2](#page-1-2) are shown in [Fig](#page-11-2)[ure 11a](#page-11-2) and [Figure
    11b](#page-11-2) due to lack of space. As can be observed from both figures, Fred-D
    can significantly improve communication performance and reduce the total exposed
    communication in all parallelization strategies.


    Such improvements make the most compute-efficient (i.e., least compute time) parallelization
    strategy also to be the be the best parallelization strategy overall. For example,
    for Transformer-17B the most compute efficient strategy is MP(20)-DP(1)-PP(1).
    However, this configuration does not have the lowest overall training time in
    the baseline system due to its huge exposed communication overheads. Thanks to
    the benefits of Fred-D in reducing the share of exposed communication overheads,
    this configuration is now the most optimal compared to other parallelization strategies.
    This is also true for Transformer-1T, where the most compute-efficient strategy
    (i.e., MP(5)-DP(1)-PP(4)) is now the most optimal strategy.


    Overall, when averaged across all parallelization strategies, Fred-D can improve
    the exposed communication time by 4.22× and 3.92×, resulting in training speedup
    by 1.63× and 1.44× for Transformer-17B and Transformer-1T, respectively.


    Discussion: going beyond a single wafer. While the main focus of this paper is
    on providing flexible on-wafer interconnects to allow for more flexible parallelization
    strategies, here we discuss the possible scenarios when the model cannot fit on
    a single wafer. The first method is to pyramidically load and unload parts of
    the model (i.e., weight streaming) as we considered and evaluated in the paper.
    However, in some cases more than one wafer is needed for training to reduce the
    training time. In that case, the optimal inter-wafer topology is an open question.
    Some methods use reduction trees to accumulate the gradients obtained from different
    wafers [\[10\]](#page-13-3). This method, although efficient for data-parallel
    strategy across the wafers, is not flexible if we consider other parallelization
    strategies across wafers. A Fred-like inter-wafer interconnect can be constructed
    to allow for more flexibility across the wafers. In any case, on-wafer Fred topology
    can work in tandem with the inter-wafer interconnect to form efficient hierarchical
    collectives. For example, a global all-reduce can be broken into: i) a special
    intra-wafer reduce scatter performed by Fred where only the boundary NPUs with
    access to the I/O maintain the results, followed by ii) an All-Reduce facilitated
    by the inter-wafer interconnect where boundary NPUs reduce the data across different
    wafers, iii) followed by the final intra-wafer special All-Gather done by Fred
    where the boundary NPUs broadcast the final result to all NPUs within the same
    wafer. Discussion: going beyond 3D Parallelism. While the main focus of this paper
    was on the MP/DP/PP parallelism, recently, more parallelization strategies have
    been proposed. Examples include Expert-Parallelism (EP) [\[63\]](#page-14-21),
    Context Parallelism (CP) [\[37\]](#page-13-35), and more customized and non-homogeneous
    strategies where the parallelization strategy might change layer by layer [\[26\]](#page-13-0).
    While not quantitatively studied in this paper, we expect that increasing the


    parallelization strategy dimensions further increases the network congestion and
    reduces the effective network BW for each parallelism dimension on the baseline
    2D Mesh. This highlights the need to have a flexible network fabric such as Fred.


    # 9 Related Works


    Accelerator Fabrics. Prior works on flexible DNN Accelerators [\[15,](#page-13-36)
    [22,](#page-13-37) [29,](#page-13-38) [39,](#page-13-39) [51\]](#page-14-22) have
    explored indirect topologies such as Benes/Fattree/Clos for efficiently distributing
    operands and reducing partial sums. This work leverages this concept to build
    a topology optimized for collectives.


    In-switch Collectives. The idea of in-switch collective execution has been proposed
    in many previous works for different network levels. The P4 language [\[8\]](#page-13-40)
    allows for offloading application-specific tasks to network switches that support
    the P4 abstract architecture. [\[30,](#page-13-41) [57\]](#page-14-12) proposed
    programming datacenter Ethernet switches for offloading the All-Reduce collective
    for data-parallel training. iSwitch [\[35\]](#page-13-14) utilizes FPGA logic
    within switches to offload the All-Reduce functionality for the distributed training
    of reinforcement learning. Mellanox SHARP [\[44\]](#page-14-14) is an Infiniband
    switch architecture for performing collectives. Klenk et al. [\[27\]](#page-13-1)
    propose a method to offload collectives to the scale-up (e.g., NVlink[\[33\]](#page-13-2))
    NPU fabric. Clos topologies have also been explored in prior works [\[22,](#page-13-37)
    [30\]](#page-13-41). A fundamental difference between these works and Fred is
    that they are proposed for off-chip networks, which have significantly less BW
    compared to on-package networks. In many of these solutions, the internal switch
    BW should be at least 2× and × the link BW to be efficient (i.e., line-rate) for
    All-Reduce and Reduce between ports, respectively. This is due to the switch architecture
    that performs the reductions only after the routing and on the output port. While
    the difference between off-chip links and on-chip switch architectures allows
    for provisioning such BW differences, it is not applicable for on-package/on-wafer
    platforms where the links are on-chip and can have the same BW as the switches.
    In contrast, Fred performs the reduction operations in multiple steps (Switches)
    during the routing on the Fred interconnect. Hence, the Fred switch works with
    the same BW as the links and can provide line-rate throughput.


    # 10 Conclusion & Future Works


    We propose Fred, a high-BW wafer-scale fabric that is flexible for different configurations
    of the 3D parallelization strategies of distributed training workloads. Fred is
    able to support concurrent in-network collective execution efficiently, enabling
    the upper-level compiler to further optimize the parallelization strategy for
    compute and memory utilization. We plan to study Fred for distributed inference
    as a part of our future work.


    # Acknowledgments


    This work was supported by awards from Intel and the Semiconductor Research Corporation
    (SRC). Also, this research is supported by the ACE Center for Evolvable Computing,
    one of the seven SRC JUMP 2.0 centers. We would like to thank the reviewers for
    their insightful comments. We also thank Jinsun Yoo for his help in revising the
    paper.


    ISCA ''25, June 21–25, 2025, Tokyo, Japan Rashidi et al.


    ### References


    - <span id="page-13-16"></span>[1] 2015. MPI: A Message-Passing Interface Standard.
    [https://www.mpi-forum.org/](https://www.mpi-forum.org/docs/mpi-3.1/mpi31-report.pdf)
    [docs/mpi-3.1/mpi31-report.pdf.](https://www.mpi-forum.org/docs/mpi-3.1/mpi31-report.pdf)

    - <span id="page-13-34"></span>[2] 2020. ASTRA-SIM: Enabling SW/HW Co-Design Exploration
    for Distributed DL Training Platforms. [https://github.com/astra-sim/astra-sim.git.](https://github.com/astra-sim/astra-sim.git)

    - <span id="page-13-21"></span>[3] 2021. Scale-Out Packageless Processing. [https://nanocad.ee.ucla.edu/wp-content/](https://nanocad.ee.ucla.edu/wp-content/papercite-data/pdf/phdth11.pdf)
    [papercite-data/pdf/phdth11.pdf.](https://nanocad.ee.ucla.edu/wp-content/papercite-data/pdf/phdth11.pdf)

    - <span id="page-13-25"></span>[4] 2022. Compute Express Link 3.0. [https://www.computeexpresslink.org/\\_files/](https://www.computeexpresslink.org/_files/ugd/0c1418_a8713008916044ae9604405d10a7773b.pdf)
    [ugd/0c1418\\_a8713008916044ae9604405d10a7773b.pdf.](https://www.computeexpresslink.org/_files/ugd/0c1418_a8713008916044ae9604405d10a7773b.pdf)

    - <span id="page-13-24"></span>[5] 2023. IEEE Heterogeneous Integration Roadmap.
    [https://eps.ieee.org/images/](https://eps.ieee.org/images/files/HIR_2023/ch20_thermalfinal.pdf)
    [files/HIR\\_2023/ch20\\_thermalfinal.pdf.](https://eps.ieee.org/images/files/HIR_2023/ch20_thermalfinal.pdf)

    - <span id="page-13-31"></span>[6] 2024. Heterogeneous Integration Roadmap. [https://eps.ieee.org/images/files/](https://eps.ieee.org/images/files/HIR_2024/HIR_2024_ch22_2D-3D.pdf)
    [HIR\\_2024/HIR\\_2024\\_ch22\\_2D-3D.pdf.](https://eps.ieee.org/images/files/HIR_2024/HIR_2024_ch22_2D-3D.pdf)

    - <span id="page-13-20"></span>[7] C. Neil Berglund. 2010. Cost/benefit assessment
    of maskless lithography. In Photomask Technology 2010, M. Warren Montgomery and
    Wilhelm Maurer (Eds.), Vol. 7823. International Society for Optics and Photonics,
    SPIE, 782318. [https:](https://doi.org/10.1117/12.868482) [//doi.org/10.1117/12.868482](https://doi.org/10.1117/12.868482)

    - <span id="page-13-40"></span>[8] Pat Bosshart, Dan Daly, Glen Gibb, Martin Izzard,
    Nick McKeown, Jennifer Rexford, Cole Schlesinger, Dan Talayco, Amin Vahdat, George
    Varghese, and David Walker. 2014. P4: Programming Protocol-Independent Packet
    Processors. SIGCOMM Comput. Commun. Rev. 44, 3 (jul 2014), 87–95. [https://doi.org/10.](https://doi.org/10.1145/2656877.2656890)
    [1145/2656877.2656890](https://doi.org/10.1145/2656877.2656890)

    - <span id="page-13-29"></span>[9] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
    Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish
    Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger,
    Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens
    Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,
    Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,
    Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners.
    arXiv[:2005.14165](https://arxiv.org/abs/2005.14165) [cs.CL]

    - <span id="page-13-3"></span>[10] Cerebras. 2021. Cerebras Systems: Achieving
    Industry Best AI Performance Through A Systems Approach. [https://f.hubspotusercontent30.net/hubfs/](https://f.hubspotusercontent30.net/hubfs/8968533/Cerebras-CS-2-Whitepaper.pdf)
    [8968533/Cerebras-CS-2-Whitepaper.pdf.](https://f.hubspotusercontent30.net/hubfs/8968533/Cerebras-CS-2-Whitepaper.pdf)

    - <span id="page-13-12"></span>[11] Ernie Chan, Marcel Heimlich, Avi Purkayastha,
    and Robert van de Geijn. 2007. Collective Communication: Theory, Practice, and
    Experience: Research Articles. Concurrency and Computation: Practice and Experience
    19, 13 (Sep. 2007), 1749–1783.

    - <span id="page-13-13"></span>[12] Ernie Chan, Robert van de Geijn, William Gropp,
    and Rajeev Thakur. 2006. Collective Communication on Architectures That Support
    Simultaneous Communication over Multiple Links. In 11th ACM SIGPLAN Symposium
    on Principles and Practice of Parallel Programming (PPoPP). 2–11. [https://doi.org/10.1145/1122971.](https://doi.org/10.1145/1122971.1122975)
    [1122975](https://doi.org/10.1145/1122971.1122975)

    - <span id="page-13-18"></span>[13] Chihming Chang and Rami Melhem. 1997. Arbitrary
    Size Benes Networks. Parallel Processing Letters 07, 03 (1997), 279–284. [https://doi.org/10.1142/](https://doi.org/10.1142/S0129626497000292)
    [S0129626497000292](https://doi.org/10.1142/S0129626497000292) arXiv[:https://doi.org/10.1142/S0129626497000292](https://arxiv.org/abs/https://doi.org/10.1142/S0129626497000292)

    - <span id="page-13-5"></span>[14] Shuangliang Chen, Saptadeep Pal, and Rakesh
    Kumar. 2024. Wafterscale Network Switches. In Proceedings of the 51th Annual International
    Symposium on Computer Architecture (ISCA ''24). Association for Computing Machinery.

    - <span id="page-13-36"></span>[15] Yu-Hsin Chen, Tien-Ju Yang, Joel Emer, and
    Vivienne Sze. 2019. Eyeriss v2: A flexible accelerator for emerging deep neural
    networks on mobile devices. IEEE Journal on Emerging and Selected Topics in Circuits
    and Systems 9, 2 (2019), 292–308.

    - <span id="page-13-33"></span>[16] M. Cho, U. Finkler, M. Serrano, D. Kung, and
    H. Hunter. 2019. BlueConnect: Decomposing all-reduce for deep learning on heterogeneous
    network hierarchy. IBM Journal of Research and Development 63, 6 (Oct. 2019),
    1:1–11. [https://doi.](https://doi.org/10.1147/JRD.2019.2947013) [org/10.1147/JRD.2019.2947013](https://doi.org/10.1147/JRD.2019.2947013)

    - <span id="page-13-17"></span>[17] Charles Clos. 1953. A study of non-blocking
    switching networks. Bell System Technical Journal 32, 2 (1953), 406–424.

    - <span id="page-13-26"></span>[18] William James Dally and Brian Patrick Towles.
    2004. Principles and Practices of Interconnection Networks. Morgan Kaufmann Publishers
    Inc., San Francisco, CA, USA.

    - <span id="page-13-30"></span>[19] Google. 2021. Google Open-Sources Trillion-Parameter
    AI Language Model Switch Transformer. [https://www.infoq.com/news/2021/02/google-trillion](https://www.infoq.com/news/2021/02/google-trillion-parameter-ai/)[parameter-ai/.](https://www.infoq.com/news/2021/02/google-trillion-parameter-ai/)

    - <span id="page-13-23"></span>[20] Alexander Graening, Saptadeep Pal, and Puneet
    Gupta. 2023. Chiplets: How Small is too Small?. In Proc. ACM/IEEE Design Automation
    Conference (DAC). 6.

    - <span id="page-13-27"></span>[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and
    Jian Sun. 2015. Deep Residual Learning for Image Recognition. arXiv[:1512.03385](https://arxiv.org/abs/1512.03385)
    [cs.CV]

    - <span id="page-13-37"></span>[22] Reza Hojabr, Mehdi Modarressi, Masoud Daneshtalab,
    Ali Yasoubi, and Ahmad Khonsari. 2017. Customizing clos network-on-chip for neural
    networks. IEEE Trans. Comput. 66, 11 (2017), 1865–1877.

    - <span id="page-13-9"></span>[23] Yanping Huang, Youlong Cheng, Ankur Bapna,
    Orhan Firat, Mia Xu Chen, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le,
    Yonghui Wu, and Zhifeng Chen. 2018. GPipe: Efficient Training of Giant Neural
    Networks using Pipeline Parallelism.<https://doi.org/10.48550/ARXIV.1811.06965>

    - <span id="page-13-4"></span>[24] Ranggi Hwang, Taehun Kim, Youngeun Kwon, and
    Minsoo Rhu. 2020. Centaur: A chiplet-based, hybrid sparse-dense accelerator for
    personalized recommendations.


    In 2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture
    (ISCA). IEEE, 968–981.


    - <span id="page-13-19"></span>[25] Deca Technologies Inc. 2024. Adaptive Patterning.
    [https://thinkdeca.com/](https://thinkdeca.com/adaptive-patterning/) [adaptive-patterning/](https://thinkdeca.com/adaptive-patterning/)

    - <span id="page-13-0"></span>[26] Zhihao Jia, Matei Zaharia, and Alex Aiken.
    2018. Beyond Data and Model Parallelism for Deep Neural Networks. CoRR abs/1807.05358
    (2018). arXiv[:1807.05358](https://arxiv.org/abs/1807.05358) <http://arxiv.org/abs/1807.05358>

    - <span id="page-13-1"></span>[27] Benjamin Klenk, Nan Jiang, Greg Thorson, and
    Larry Dennison. 2020. An In-Network Architecture for Accelerating Shared-Memory
    Multiprocessor Collectives. In 47th Annual International Symposium on Computer
    Architecture (ISCA). 996–1009.<https://doi.org/10.1109/ISCA45697.2020.00085>

    - <span id="page-13-32"></span>[28] Sameer Kumar and Norm Jouppi. 2020. Highly
    Available Data Parallel ML training on Mesh Networks. arXiv[:2011.03605](https://arxiv.org/abs/2011.03605)
    [cs.LG]<https://arxiv.org/abs/2011.03605>

    - <span id="page-13-38"></span>[29] Hyoukjun Kwon, Ananda Samajdar, and Tushar
    Krishna. 2018. MAERI: Enabling Flexible Dataflow Mapping over DNN Accelerators
    via Reconfigurable Interconnects. In Proceedings of the 23rd International Conference
    on Architectural Support for Programming Languages and Operating Systems (ASPLOS).

    - <span id="page-13-41"></span>[30] ChonLam Lao, Yanfang Le, Kshiteej Mahajan,
    Yixi Chen, Wenfei Wu, Aditya Akella, and Michael Swift. 2021. ATP: In-network
    Aggregation for Multi-tenant Learning. In 18th USENIX Symposium on Networked Systems
    Design and Implementation (NSDI 21). USENIX Association, 741–761. [https://www.usenix.org/](https://www.usenix.org/conference/nsdi21/presentation/lao)
    [conference/nsdi21/presentation/lao](https://www.usenix.org/conference/nsdi21/presentation/lao)

    - <span id="page-13-6"></span>[31] Sabuj Laskar, Pranati Majhi, Sungkeun Kim,
    Farabi Mahmud, Abdullah Muzahid, and Eun Jung Kim. 2024. Enhancing Collective
    Communication in MCM Accelerators for Deep Learning Training. In 2024 IEEE International
    Symposium on High-Performance Computer Architecture (HPCA). 1–16. [https://doi.org/10.](https://doi.org/10.1109/HPCA57654.2024.00069)
    [1109/HPCA57654.2024.00069](https://doi.org/10.1109/HPCA57654.2024.00069)

    - <span id="page-13-8"></span>[32] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong
    Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng
    Chen. 2020. GShard: Scaling Giant Models with Conditional Computation and Automatic
    Sharding. <https://doi.org/10.48550/ARXIV.2006.16668>

    - <span id="page-13-2"></span>[33] Ang Li, Shuaiwen Leon Song, Jieyang Chen, Jiajia
    Li, Xu Liu, Nathan R. Tallent, and Kevin J. Barker. 2019. Evaluating Modern GPU
    Interconnect: PCIe, NVLink, NV-SLI, NVSwitch and GPUDirect. arXiv[:1903.04611](https://arxiv.org/abs/1903.04611)
    [cs.AR]

    - <span id="page-13-7"></span>[34] Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar,
    Pieter Noordhuis, Teng Li, Adam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania,
    and Soumith Chintala. 2020. PyTorch Distributed: Experiences on Accelerating Data
    Parallel Training. <https://doi.org/10.48550/ARXIV.2006.15704>

    - <span id="page-13-14"></span>[35] Youjie Li, Iou-Jen Liu, Yifan Yuan, Deming
    Chen, Alexander Schwing, and Jian Huang. 2019. Accelerating Distributed Reinforcement
    Learning with In-Switch Computing. In 46th International Symposium on Computer
    Architecture (ISCA). 279–291.<https://doi.org/10.1145/3307650.3322259>

    - <span id="page-13-22"></span>[36] Meng-Hsiang Liu, Boris Vaisband, Amir Hanna,
    Yandong Luo, Zhe Wan, and Subramanian S. Iyer. 2019. Process Development of Power
    Delivery Through Wafer Vias for Silicon Interconnect Fabric. In 2019 IEEE 69th
    Electronic Components and Technology Conference (ECTC). 579–586. [https://doi.org/10.1109/ECTC.2019.](https://doi.org/10.1109/ECTC.2019.00093)
    [00093](https://doi.org/10.1109/ECTC.2019.00093)

    - <span id="page-13-35"></span>[37] Meta. 2025. [Distributed w/ TorchTitan] Breaking
    Barriers: Training Long Context LLMs with 1M Sequence Length in PyTorch Using
    Context Parallel. [https://discuss.pytorch.org/t/distributed-w-torchtitan-breaking-barriers](https://discuss.pytorch.org/t/distributed-w-torchtitan-breaking-barriers-training-long-context-llms-with-1m-sequence-length-in-pytorch-using-context-parallel/215082)[training-long-context-llms-with-1m-sequence-length-in-pytorch-using](https://discuss.pytorch.org/t/distributed-w-torchtitan-breaking-barriers-training-long-context-llms-with-1m-sequence-length-in-pytorch-using-context-parallel/215082)[context-parallel/215082.](https://discuss.pytorch.org/t/distributed-w-torchtitan-breaking-barriers-training-long-context-llms-with-1m-sequence-length-in-pytorch-using-context-parallel/215082)

    - <span id="page-13-28"></span>[38] Microsoft. 2020. Turing-NLG: A 17-billion-parameter
    language model by Microsoft. [https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17](https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/)
    [billion-parameter-language-model-by-microsoft/.](https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/)

    - <span id="page-13-39"></span>[39] Francisco Muñoz-Martínez, Raveesh Garg, Michael
    Pellauer, José L Abellán, Manuel E Acacio, and Tushar Krishna. 2023. Flexagon:
    A multi-dataflow sparsesparse matrix multiplication accelerator for efficient
    dnn processing. In Proceedings of the 28th ACM International Conference on Architectural
    Support for Programming Languages and Operating Systems, Volume 3. 252–265.

    - <span id="page-13-10"></span>[40] Deepak Narayanan, Aaron Harlap, Amar Phanishayee,
    Vivek Seshadri, Nikhil R. Devanur, Gregory R. Ganger, Phillip B. Gibbons, and
    Matei Zaharia. 2019. PipeDream: Generalized Pipeline Parallelism for DNN Training
    (SOSP ''19). Association for Computing Machinery, New York, NY, USA, 1–15. [https:](https://doi.org/10.1145/3341301.3359646)
    [//doi.org/10.1145/3341301.3359646](https://doi.org/10.1145/3341301.3359646)

    - <span id="page-13-11"></span>[41] Deepak Narayanan, Mohammad Shoeybi, Jared
    Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand,
    Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, Amar Phanishayee, and Matei
    Zaharia. 2021. Efficient Large-Scale Language Model Training on GPU Clusters Using
    Megatron-LM. In Proceedings of the International Conference for High Performance
    Computing, Networking, Storage and Analysis (St. Louis, Missouri) (SC ''21). Association
    for Computing Machinery, New York, NY, USA, Article 58, 15 pages. <https://doi.org/10.1145/3458817.3476209>

    - <span id="page-13-15"></span>[42] Maxim Naumov, Dheevatsa Mudigere, Hao-Jun
    Michael Shi, Jianyu Huang, Narayanan Sundaraman, Jongsoo Park, Xiaodong Wang,
    Udit Gupta, Carole-Jean Wu, Alisson G. Azzolini, Dmytro Dzhulgakov, Andrey Mallevich,
    Ilia Cherniavskii, Yinghai Lu, Raghuraman Krishnamoorthi, Ansha Yu, Volodymyr
    Kondratenko, Stephanie Pereira, Xianjie Chen, Wenlin Chen, Vijay Rao, Bill Jia,
    Liang


    <span id="page-14-0"></span>FRED: A Wafer-scale Fabric for 3D Parallel DNN Training
    ISCA ''25, June 21–25, 2025, Tokyo, Japan


    Xiong, and Misha Smelyanskiy. 2019. Deep Learning Recommendation Model for Personalization
    and Recommendation Systems. arXiv[:1906.00091](https://arxiv.org/abs/1906.00091)
    [cs.IR]


    - <span id="page-14-15"></span>[43] Dimitris Nikolaidis, Panos Groumas, Christos
    Kouloumentas, and Hercules Avramopoulos. 2022. Novel Benes Network Routing Algorithm
    and Hardware Implementation. Technologies 10, 1 (2022). [https://www.mdpi.com/2227-](https://www.mdpi.com/2227-7080/10/1/16)
    [7080/10/1/16](https://www.mdpi.com/2227-7080/10/1/16)

    - <span id="page-14-14"></span><span id="page-14-10"></span>[44] NVIDIA. 2020.
    Mellanox SHARP. [https://docs.mellanox.com/display/sharpv214.](https://docs.mellanox.com/display/sharpv214)
    [45] NVIDIA. 2022. NVIDIA H100 Tensor Core GPU. [https://www.nvidia.com/en-](https://www.nvidia.com/en-us/data-center/h100/)

    - <span id="page-14-16"></span>[us/data-center/h100/.](https://www.nvidia.com/en-us/data-center/h100/)
    [46] Saptadeep Pal and Puneet Gupta. 2020. Pathfinding for 2.5 D interconnect
    technologies. In Proceedings of the Workshop on System-Level Interconnect: Problems
    and Pathfinding Workshop. 1–8.

    - <span id="page-14-5"></span>[47] Saptadeep Pal, Jingyang Liu, Irina Alam, Nicholas
    Cebry, Haris Suhail, Shi Bu, Subramanian S. Iyer, Sudhakar Pamarti, Rakesh Kumar,
    and Puneet Gupta. 2021. Designing a 2048-Chiplet, 14336-Core Waferscale Processor.
    In 2021 58th ACM/IEEE Design Automation Conference (DAC). 1183–1188. [https://doi.org/10.](https://doi.org/10.1109/DAC18074.2021.9586194)
    [1109/DAC18074.2021.9586194](https://doi.org/10.1109/DAC18074.2021.9586194)

    - <span id="page-14-6"></span>[48] Saptadeep Pal, Daniel Petrisko, Matthew Tomei,
    Puneet Gupta, Subramanian S. Iyer, and Rakesh Kumar. 2019. Architecting Waferscale
    Processors - A GPU Case Study. In 2019 IEEE International Symposium on High Performance
    Computer Architecture (HPCA). 250–263.<https://doi.org/10.1109/HPCA.2019.00042>

    - <span id="page-14-8"></span>[49] Huwan Peng, Scott Davidson, Richard Shi, Shuaiwen
    Leon Song, and Michael Taylor. 2024. Chiplet Cloud: Building AI Supercomputers
    for Serving Large Generative Language Models. arXiv[:2307.02666](https://arxiv.org/abs/2307.02666)
    [cs.AR] [https://arxiv.org/abs/](https://arxiv.org/abs/2307.02666) [2307.02666](https://arxiv.org/abs/2307.02666)

    - <span id="page-14-9"></span>[50] Kun Qian, Yongqing Xi, Jiamin Cao, Jiaqi Gao,
    Yichi Xu, Yu Guan, Binzhang Fu, Xuemei Shi, Fangbo Zhu, Rui Miao, Chao Wang, Peng
    Wang, Pengcheng Zhang, Xianlong Zeng, Eddie Ruan, Zhiping Yao, Ennan Zhai, and
    Dennis Cai. 2024. Alibaba HPN: A Data Center Network for Large Language Model
    Training. In Proceedings of the ACM SIGCOMM 2024 Conference (Sydney, NSW, Australia)
    (ACM SIGCOMM ''24). Association for Computing Machinery, New York, NY, USA, 691–706.<https://doi.org/10.1145/3651890.3672265>

    - <span id="page-14-22"></span>[51] Eric Qin, Ananda Samajdar, Hyoukjun Kwon,
    Vineet Nadella, Sudarshan Srinivasan, Dipankar Das, Bharat Kaul, and Tushar Krishna.
    2020. Sigma: A sparse and irregular gemm accelerator with flexible interconnects
    for dnn training. In 2020 IEEE International Symposium on High Performance Computer
    Architecture (HPCA). IEEE, 58–70.

    - <span id="page-14-18"></span>[52] Samyam Rajbhandari, Jeff Rasley, Olatunji
    Ruwase, and Yuxiong He. 2020. ZeRO: Memory Optimizations Toward Training Trillion
    Parameter Models. arXiv[:1910.02054](https://arxiv.org/abs/1910.02054) [cs.LG]

    - <span id="page-14-19"></span>[53] Saeed Rashidi, Matthew Denton, Srinivas Sridharan,
    Sudarshan Srinivasan, Amoghavarsha Suresh, Jade Nie, and Tushar Krishna. 2021.
    Enabling Compute-Communication Overlap in Distributed Deep Learning Training Platforms.
    In 2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture
    (ISCA). 540–553.<https://doi.org/10.1109/ISCA52012.2021.00049>

    - <span id="page-14-2"></span>[54] Saeed Rashidi, Pallavi Shurpali, Srinivas Sridharan,
    Naader Hassani, Dheevatsa Mudigere, Krishnakumar Nair, Misha Smelyanski, and Tushar
    Krishna. 2020. Scalable Distributed Training of Recommendation Models: An ASTRA-SIM
    + NS3 case-study with TCP/IP transport. In 2020 IEEE Symposium on High-Performance
    Interconnects (HOTI). 33–42.<https://doi.org/10.1109/HOTI51249.2020.00020>

    - <span id="page-14-3"></span>[55] Saeed Rashidi, Srinivas Sridharan, Sudarshan
    Srinivasan, and Tushar Krishna. 2020. ASTRA-SIM: Enabling SW/HW Co-Design Exploration
    for Distributed DL Training Platforms. In 2020 IEEE International Symposium on
    Performance Analysis of Systems and Software (ISPASS). 81–92. [https://doi.org/10.1109/ISPASS48437.](https://doi.org/10.1109/ISPASS48437.2020.00018)
    [2020.00018](https://doi.org/10.1109/ISPASS48437.2020.00018)

    - <span id="page-14-20"></span>[56] Saeed Rashidi, William Won, Sudarshan Srinivasan,
    Srinivas Sridharan, and Tushar Krishna. 2022. Themis: A Network Bandwidth-Aware
    Collective Scheduling Policy for Distributed Training of DL Models. In Proceedings
    of the 49th Annual International Symposium on Computer Architecture (New York,
    New York) (ISCA ''22). Association for Computing Machinery, New York, NY, USA,
    581–596. <https://doi.org/10.1145/3470496.3527382>

    - <span id="page-14-12"></span>[57] Amedeo Sapio, Marco Canini, Chen-Yu Ho, Jacob
    Nelson, Panos Kalnis, Changhoon Kim, Arvind Krishnamurthy, Masoud Moshref, Dan
    Ports, and Peter Richtarik. 2021. Scaling Distributed Machine Learning with In-Network
    Aggregation. In 18th USENIX Symposium on Networked Systems Design and Implementation
    (NSDI 21). USENIX Association, 785–808. [https://www.usenix.org/conference/](https://www.usenix.org/conference/nsdi21/presentation/sapio)
    [nsdi21/presentation/sapio](https://www.usenix.org/conference/nsdi21/presentation/sapio)

    - <span id="page-14-1"></span>[58] Sean Lie. 2021. Thinking Outside the Die: Architecting
    the ML Accelerator of the Future. [https://www.microarch.org/micro54/media/lie-keynote.pdf.](https://www.microarch.org/micro54/media/lie-keynote.pdf)

    - <span id="page-14-4"></span>[59] Yakun Sophia Shao, Jason Clemons, Rangharajan
    Venkatesan, Brian Zimmer, Matthew Fojtik, Nan Jiang, Ben Keller, Alicia Klinefelter,
    Nathaniel Pinckney, Priyanka Raina, Stephen G. Tell, Yanqing Zhang, William J.
    Dally, Joel Emer, C. Thomas Gray, Brucek Khailany, and Stephen W. Keckler. 2019.
    Simba: Scaling Deep-Learning Inference with Multi-Chip-Module-Based Architecture.
    In 52nd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO).
    14–27. <https://doi.org/10.1145/3352460.3358302>

    - <span id="page-14-17"></span>[60] Debendra Das Sharma and Thomas Coughlin. 2024.
    Universal Chiplet Interconnect Express: An Open Industry Standard for Memory and
    Storage Applications .


    Computer 57, 01 (Jan. 2024), 75–81.<https://doi.org/10.1109/MC.2023.3318769>


    - <span id="page-14-7"></span>[61] Anton Shilov. 2024. TSMC''s System-on-Wafer
    Platform Goes 3D: CoW-SoW Stacks Up the Chips. [https://www.anandtech.com/show/21372/tsmcs-system](https://www.anandtech.com/show/21372/tsmcs-system-on-wafer-platform-goes-3d-cow-sow)[on-wafer-platform-goes-3d-cow-sow.](https://www.anandtech.com/show/21372/tsmcs-system-on-wafer-platform-goes-3d-cow-sow)

    - <span id="page-14-13"></span>[62] Mohammad Shoeybi, Mostofa Patwary, Raul Puri,
    Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2020. Megatron-LM: Training
    Multi-Billion Parameter Language Models Using Model Parallelism. arXiv[:1909.08053](https://arxiv.org/abs/1909.08053)
    [cs.CL]

    - <span id="page-14-21"></span>[63] Siddharth Singh, Olatunji Ruwase, Ammar Ahmad
    Awan, Samyam Rajbhandari, Yuxiong He, and Abhinav Bhatele. 2023. A Hybrid Tensor-Expert-Data
    Parallelism Approach to Optimize Mixture-of-Experts Training. In Proceedings of
    the 37th International Conference on Supercomputing (ICS ''23). ACM, 203–214.
    [https:](https://doi.org/10.1145/3577193.3593704) [//doi.org/10.1145/3577193.3593704](https://doi.org/10.1145/3577193.3593704)

    - <span id="page-14-11"></span>[64] Rajeev Thakur, Rolf Rabenseifner, and William
    Gropp. 2005. Optimization of Collective Communication Operations in MPICH. International
    Journal of High Performance Computing Applications 19, 1 (Feb. 2005), 49–66. [https://doi.org/10.](https://doi.org/10.1177/1094342005051521)
    [1177/1094342005051521](https://doi.org/10.1177/1094342005051521)'
  references:
  - '- <span id="page-13-16"></span>[1] 2015. MPI: A Message-Passing Interface Standard.
    [https://www.mpi-forum.org/](https://www.mpi-forum.org/docs/mpi-3.1/mpi31-report.pdf)
    [docs/mpi-3.1/mpi31-report.pdf.](https://www.mpi-forum.org/docs/mpi-3.1/mpi31-report.pdf)'
  - '- <span id="page-13-34"></span>[2] 2020. ASTRA-SIM: Enabling SW/HW Co-Design
    Exploration for Distributed DL Training Platforms. [https://github.com/astra-sim/astra-sim.git.](https://github.com/astra-sim/astra-sim.git)'
  - '- <span id="page-13-21"></span>[3] 2021. Scale-Out Packageless Processing. [https://nanocad.ee.ucla.edu/wp-content/](https://nanocad.ee.ucla.edu/wp-content/papercite-data/pdf/phdth11.pdf)
    [papercite-data/pdf/phdth11.pdf.](https://nanocad.ee.ucla.edu/wp-content/papercite-data/pdf/phdth11.pdf)'
  - '- <span id="page-13-25"></span>[4] 2022. Compute Express Link 3.0. [https://www.computeexpresslink.org/\\_files/](https://www.computeexpresslink.org/_files/ugd/0c1418_a8713008916044ae9604405d10a7773b.pdf)
    [ugd/0c1418\\_a8713008916044ae9604405d10a7773b.pdf.](https://www.computeexpresslink.org/_files/ugd/0c1418_a8713008916044ae9604405d10a7773b.pdf)'
  - '- <span id="page-13-24"></span>[5] 2023. IEEE Heterogeneous Integration Roadmap.
    [https://eps.ieee.org/images/](https://eps.ieee.org/images/files/HIR_2023/ch20_thermalfinal.pdf)
    [files/HIR\\_2023/ch20\\_thermalfinal.pdf.](https://eps.ieee.org/images/files/HIR_2023/ch20_thermalfinal.pdf)'
  - '- <span id="page-13-31"></span>[6] 2024. Heterogeneous Integration Roadmap. [https://eps.ieee.org/images/files/](https://eps.ieee.org/images/files/HIR_2024/HIR_2024_ch22_2D-3D.pdf)
    [HIR\\_2024/HIR\\_2024\\_ch22\\_2D-3D.pdf.](https://eps.ieee.org/images/files/HIR_2024/HIR_2024_ch22_2D-3D.pdf)'
  - '- <span id="page-13-20"></span>[7] C. Neil Berglund. 2010. Cost/benefit assessment
    of maskless lithography. In Photomask Technology 2010, M. Warren Montgomery and
    Wilhelm Maurer (Eds.), Vol. 7823. International Society for Optics and Photonics,
    SPIE, 782318. [https:](https://doi.org/10.1117/12.868482) [//doi.org/10.1117/12.868482](https://doi.org/10.1117/12.868482)'
  - '- <span id="page-13-40"></span>[8] Pat Bosshart, Dan Daly, Glen Gibb, Martin
    Izzard, Nick McKeown, Jennifer Rexford, Cole Schlesinger, Dan Talayco, Amin Vahdat,
    George Varghese, and David Walker. 2014. P4: Programming Protocol-Independent
    Packet Processors. SIGCOMM Comput. Commun. Rev. 44, 3 (jul 2014), 87–95. [https://doi.org/10.](https://doi.org/10.1145/2656877.2656890)
    [1145/2656877.2656890](https://doi.org/10.1145/2656877.2656890)'
  - '- <span id="page-13-29"></span>[9] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
    Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish
    Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger,
    Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens
    Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,
    Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,
    Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners.
    arXiv[:2005.14165](https://arxiv.org/abs/2005.14165) [cs.CL]'
  - '- <span id="page-13-3"></span>[10] Cerebras. 2021. Cerebras Systems: Achieving
    Industry Best AI Performance Through A Systems Approach. [https://f.hubspotusercontent30.net/hubfs/](https://f.hubspotusercontent30.net/hubfs/8968533/Cerebras-CS-2-Whitepaper.pdf)
    [8968533/Cerebras-CS-2-Whitepaper.pdf.](https://f.hubspotusercontent30.net/hubfs/8968533/Cerebras-CS-2-Whitepaper.pdf)'
  - '- <span id="page-13-12"></span>[11] Ernie Chan, Marcel Heimlich, Avi Purkayastha,
    and Robert van de Geijn. 2007. Collective Communication: Theory, Practice, and
    Experience: Research Articles. Concurrency and Computation: Practice and Experience
    19, 13 (Sep. 2007), 1749–1783.'
  - '- <span id="page-13-13"></span>[12] Ernie Chan, Robert van de Geijn, William
    Gropp, and Rajeev Thakur. 2006. Collective Communication on Architectures That
    Support Simultaneous Communication over Multiple Links. In 11th ACM SIGPLAN Symposium
    on Principles and Practice of Parallel Programming (PPoPP). 2–11. [https://doi.org/10.1145/1122971.](https://doi.org/10.1145/1122971.1122975)
    [1122975](https://doi.org/10.1145/1122971.1122975)'
  - '- <span id="page-13-18"></span>[13] Chihming Chang and Rami Melhem. 1997. Arbitrary
    Size Benes Networks. Parallel Processing Letters 07, 03 (1997), 279–284. [https://doi.org/10.1142/](https://doi.org/10.1142/S0129626497000292)
    [S0129626497000292](https://doi.org/10.1142/S0129626497000292) arXiv[:https://doi.org/10.1142/S0129626497000292](https://arxiv.org/abs/https://doi.org/10.1142/S0129626497000292)'
  - '- <span id="page-13-5"></span>[14] Shuangliang Chen, Saptadeep Pal, and Rakesh
    Kumar. 2024. Wafterscale Network Switches. In Proceedings of the 51th Annual International
    Symposium on Computer Architecture (ISCA ''24). Association for Computing Machinery.'
  - '- <span id="page-13-36"></span>[15] Yu-Hsin Chen, Tien-Ju Yang, Joel Emer, and
    Vivienne Sze. 2019. Eyeriss v2: A flexible accelerator for emerging deep neural
    networks on mobile devices. IEEE Journal on Emerging and Selected Topics in Circuits
    and Systems 9, 2 (2019), 292–308.'
  - '- <span id="page-13-33"></span>[16] M. Cho, U. Finkler, M. Serrano, D. Kung,
    and H. Hunter. 2019. BlueConnect: Decomposing all-reduce for deep learning on
    heterogeneous network hierarchy. IBM Journal of Research and Development 63, 6
    (Oct. 2019), 1:1–11. [https://doi.](https://doi.org/10.1147/JRD.2019.2947013)
    [org/10.1147/JRD.2019.2947013](https://doi.org/10.1147/JRD.2019.2947013)'
  - '- <span id="page-13-17"></span>[17] Charles Clos. 1953. A study of non-blocking
    switching networks. Bell System Technical Journal 32, 2 (1953), 406–424.'
  - '- <span id="page-13-26"></span>[18] William James Dally and Brian Patrick Towles.
    2004. Principles and Practices of Interconnection Networks. Morgan Kaufmann Publishers
    Inc., San Francisco, CA, USA.'
  - '- <span id="page-13-30"></span>[19] Google. 2021. Google Open-Sources Trillion-Parameter
    AI Language Model Switch Transformer. [https://www.infoq.com/news/2021/02/google-trillion](https://www.infoq.com/news/2021/02/google-trillion-parameter-ai/)[parameter-ai/.](https://www.infoq.com/news/2021/02/google-trillion-parameter-ai/)'
  - '- <span id="page-13-23"></span>[20] Alexander Graening, Saptadeep Pal, and Puneet
    Gupta. 2023. Chiplets: How Small is too Small?. In Proc. ACM/IEEE Design Automation
    Conference (DAC). 6.'
  - '- <span id="page-13-27"></span>[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren,
    and Jian Sun. 2015. Deep Residual Learning for Image Recognition. arXiv[:1512.03385](https://arxiv.org/abs/1512.03385)
    [cs.CV]'
  - '- <span id="page-13-37"></span>[22] Reza Hojabr, Mehdi Modarressi, Masoud Daneshtalab,
    Ali Yasoubi, and Ahmad Khonsari. 2017. Customizing clos network-on-chip for neural
    networks. IEEE Trans. Comput. 66, 11 (2017), 1865–1877.'
  - '- <span id="page-13-9"></span>[23] Yanping Huang, Youlong Cheng, Ankur Bapna,
    Orhan Firat, Mia Xu Chen, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le,
    Yonghui Wu, and Zhifeng Chen. 2018. GPipe: Efficient Training of Giant Neural
    Networks using Pipeline Parallelism.<https://doi.org/10.48550/ARXIV.1811.06965>'
  - '- <span id="page-13-4"></span>[24] Ranggi Hwang, Taehun Kim, Youngeun Kwon, and
    Minsoo Rhu. 2020. Centaur: A chiplet-based, hybrid sparse-dense accelerator for
    personalized recommendations.'
  - In 2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture
    (ISCA). IEEE, 968–981.
  - '- <span id="page-13-19"></span>[25] Deca Technologies Inc. 2024. Adaptive Patterning.
    [https://thinkdeca.com/](https://thinkdeca.com/adaptive-patterning/) [adaptive-patterning/](https://thinkdeca.com/adaptive-patterning/)'
  - '- <span id="page-13-0"></span>[26] Zhihao Jia, Matei Zaharia, and Alex Aiken.
    2018. Beyond Data and Model Parallelism for Deep Neural Networks. CoRR abs/1807.05358
    (2018). arXiv[:1807.05358](https://arxiv.org/abs/1807.05358) <http://arxiv.org/abs/1807.05358>'
  - '- <span id="page-13-1"></span>[27] Benjamin Klenk, Nan Jiang, Greg Thorson, and
    Larry Dennison. 2020. An In-Network Architecture for Accelerating Shared-Memory
    Multiprocessor Collectives. In 47th Annual International Symposium on Computer
    Architecture (ISCA). 996–1009.<https://doi.org/10.1109/ISCA45697.2020.00085>'
  - '- <span id="page-13-32"></span>[28] Sameer Kumar and Norm Jouppi. 2020. Highly
    Available Data Parallel ML training on Mesh Networks. arXiv[:2011.03605](https://arxiv.org/abs/2011.03605)
    [cs.LG]<https://arxiv.org/abs/2011.03605>'
  - '- <span id="page-13-38"></span>[29] Hyoukjun Kwon, Ananda Samajdar, and Tushar
    Krishna. 2018. MAERI: Enabling Flexible Dataflow Mapping over DNN Accelerators
    via Reconfigurable Interconnects. In Proceedings of the 23rd International Conference
    on Architectural Support for Programming Languages and Operating Systems (ASPLOS).'
  - '- <span id="page-13-41"></span>[30] ChonLam Lao, Yanfang Le, Kshiteej Mahajan,
    Yixi Chen, Wenfei Wu, Aditya Akella, and Michael Swift. 2021. ATP: In-network
    Aggregation for Multi-tenant Learning. In 18th USENIX Symposium on Networked Systems
    Design and Implementation (NSDI 21). USENIX Association, 741–761. [https://www.usenix.org/](https://www.usenix.org/conference/nsdi21/presentation/lao)
    [conference/nsdi21/presentation/lao](https://www.usenix.org/conference/nsdi21/presentation/lao)'
  - '- <span id="page-13-6"></span>[31] Sabuj Laskar, Pranati Majhi, Sungkeun Kim,
    Farabi Mahmud, Abdullah Muzahid, and Eun Jung Kim. 2024. Enhancing Collective
    Communication in MCM Accelerators for Deep Learning Training. In 2024 IEEE International
    Symposium on High-Performance Computer Architecture (HPCA). 1–16. [https://doi.org/10.](https://doi.org/10.1109/HPCA57654.2024.00069)
    [1109/HPCA57654.2024.00069](https://doi.org/10.1109/HPCA57654.2024.00069)'
  - '- <span id="page-13-8"></span>[32] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong
    Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng
    Chen. 2020. GShard: Scaling Giant Models with Conditional Computation and Automatic
    Sharding. <https://doi.org/10.48550/ARXIV.2006.16668>'
  - '- <span id="page-13-2"></span>[33] Ang Li, Shuaiwen Leon Song, Jieyang Chen,
    Jiajia Li, Xu Liu, Nathan R. Tallent, and Kevin J. Barker. 2019. Evaluating Modern
    GPU Interconnect: PCIe, NVLink, NV-SLI, NVSwitch and GPUDirect. arXiv[:1903.04611](https://arxiv.org/abs/1903.04611)
    [cs.AR]'
  - '- <span id="page-13-7"></span>[34] Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar,
    Pieter Noordhuis, Teng Li, Adam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania,
    and Soumith Chintala. 2020. PyTorch Distributed: Experiences on Accelerating Data
    Parallel Training. <https://doi.org/10.48550/ARXIV.2006.15704>'
  - '- <span id="page-13-14"></span>[35] Youjie Li, Iou-Jen Liu, Yifan Yuan, Deming
    Chen, Alexander Schwing, and Jian Huang. 2019. Accelerating Distributed Reinforcement
    Learning with In-Switch Computing. In 46th International Symposium on Computer
    Architecture (ISCA). 279–291.<https://doi.org/10.1145/3307650.3322259>'
  - '- <span id="page-13-22"></span>[36] Meng-Hsiang Liu, Boris Vaisband, Amir Hanna,
    Yandong Luo, Zhe Wan, and Subramanian S. Iyer. 2019. Process Development of Power
    Delivery Through Wafer Vias for Silicon Interconnect Fabric. In 2019 IEEE 69th
    Electronic Components and Technology Conference (ECTC). 579–586. [https://doi.org/10.1109/ECTC.2019.](https://doi.org/10.1109/ECTC.2019.00093)
    [00093](https://doi.org/10.1109/ECTC.2019.00093)'
  - '- <span id="page-13-35"></span>[37] Meta. 2025. [Distributed w/ TorchTitan] Breaking
    Barriers: Training Long Context LLMs with 1M Sequence Length in PyTorch Using
    Context Parallel. [https://discuss.pytorch.org/t/distributed-w-torchtitan-breaking-barriers](https://discuss.pytorch.org/t/distributed-w-torchtitan-breaking-barriers-training-long-context-llms-with-1m-sequence-length-in-pytorch-using-context-parallel/215082)[training-long-context-llms-with-1m-sequence-length-in-pytorch-using](https://discuss.pytorch.org/t/distributed-w-torchtitan-breaking-barriers-training-long-context-llms-with-1m-sequence-length-in-pytorch-using-context-parallel/215082)[context-parallel/215082.](https://discuss.pytorch.org/t/distributed-w-torchtitan-breaking-barriers-training-long-context-llms-with-1m-sequence-length-in-pytorch-using-context-parallel/215082)'
  - '- <span id="page-13-28"></span>[38] Microsoft. 2020. Turing-NLG: A 17-billion-parameter
    language model by Microsoft. [https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17](https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/)
    [billion-parameter-language-model-by-microsoft/.](https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/)'
  - '- <span id="page-13-39"></span>[39] Francisco Muñoz-Martínez, Raveesh Garg, Michael
    Pellauer, José L Abellán, Manuel E Acacio, and Tushar Krishna. 2023. Flexagon:
    A multi-dataflow sparsesparse matrix multiplication accelerator for efficient
    dnn processing. In Proceedings of the 28th ACM International Conference on Architectural
    Support for Programming Languages and Operating Systems, Volume 3. 252–265.'
  - '- <span id="page-13-10"></span>[40] Deepak Narayanan, Aaron Harlap, Amar Phanishayee,
    Vivek Seshadri, Nikhil R. Devanur, Gregory R. Ganger, Phillip B. Gibbons, and
    Matei Zaharia. 2019. PipeDream: Generalized Pipeline Parallelism for DNN Training
    (SOSP ''19). Association for Computing Machinery, New York, NY, USA, 1–15. [https:](https://doi.org/10.1145/3341301.3359646)
    [//doi.org/10.1145/3341301.3359646](https://doi.org/10.1145/3341301.3359646)'
  - '- <span id="page-13-11"></span>[41] Deepak Narayanan, Mohammad Shoeybi, Jared
    Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand,
    Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, Amar Phanishayee, and Matei
    Zaharia. 2021. Efficient Large-Scale Language Model Training on GPU Clusters Using
    Megatron-LM. In Proceedings of the International Conference for High Performance
    Computing, Networking, Storage and Analysis (St. Louis, Missouri) (SC ''21). Association
    for Computing Machinery, New York, NY, USA, Article 58, 15 pages. <https://doi.org/10.1145/3458817.3476209>'
  - '- <span id="page-13-15"></span>[42] Maxim Naumov, Dheevatsa Mudigere, Hao-Jun
    Michael Shi, Jianyu Huang, Narayanan Sundaraman, Jongsoo Park, Xiaodong Wang,
    Udit Gupta, Carole-Jean Wu, Alisson G. Azzolini, Dmytro Dzhulgakov, Andrey Mallevich,
    Ilia Cherniavskii, Yinghai Lu, Raghuraman Krishnamoorthi, Ansha Yu, Volodymyr
    Kondratenko, Stephanie Pereira, Xianjie Chen, Wenlin Chen, Vijay Rao, Bill Jia,
    Liang'
  - '<span id="page-14-0"></span>FRED: A Wafer-scale Fabric for 3D Parallel DNN Training
    ISCA ''25, June 21–25, 2025, Tokyo, Japan'
  - Xiong, and Misha Smelyanskiy. 2019. Deep Learning Recommendation Model for Personalization
    and Recommendation Systems. arXiv[:1906.00091](https://arxiv.org/abs/1906.00091)
    [cs.IR]
  - '- <span id="page-14-15"></span>[43] Dimitris Nikolaidis, Panos Groumas, Christos
    Kouloumentas, and Hercules Avramopoulos. 2022. Novel Benes Network Routing Algorithm
    and Hardware Implementation. Technologies 10, 1 (2022). [https://www.mdpi.com/2227-](https://www.mdpi.com/2227-7080/10/1/16)
    [7080/10/1/16](https://www.mdpi.com/2227-7080/10/1/16)'
  - '- <span id="page-14-14"></span><span id="page-14-10"></span>[44] NVIDIA. 2020.
    Mellanox SHARP. [https://docs.mellanox.com/display/sharpv214.](https://docs.mellanox.com/display/sharpv214)
    [45] NVIDIA. 2022. NVIDIA H100 Tensor Core GPU. [https://www.nvidia.com/en-](https://www.nvidia.com/en-us/data-center/h100/)'
  - '- <span id="page-14-16"></span>[us/data-center/h100/.](https://www.nvidia.com/en-us/data-center/h100/)
    [46] Saptadeep Pal and Puneet Gupta. 2020. Pathfinding for 2.5 D interconnect
    technologies. In Proceedings of the Workshop on System-Level Interconnect: Problems
    and Pathfinding Workshop. 1–8.'
  - '- <span id="page-14-5"></span>[47] Saptadeep Pal, Jingyang Liu, Irina Alam, Nicholas
    Cebry, Haris Suhail, Shi Bu, Subramanian S. Iyer, Sudhakar Pamarti, Rakesh Kumar,
    and Puneet Gupta. 2021. Designing a 2048-Chiplet, 14336-Core Waferscale Processor.
    In 2021 58th ACM/IEEE Design Automation Conference (DAC). 1183–1188. [https://doi.org/10.](https://doi.org/10.1109/DAC18074.2021.9586194)
    [1109/DAC18074.2021.9586194](https://doi.org/10.1109/DAC18074.2021.9586194)'
  - '- <span id="page-14-6"></span>[48] Saptadeep Pal, Daniel Petrisko, Matthew Tomei,
    Puneet Gupta, Subramanian S. Iyer, and Rakesh Kumar. 2019. Architecting Waferscale
    Processors - A GPU Case Study. In 2019 IEEE International Symposium on High Performance
    Computer Architecture (HPCA). 250–263.<https://doi.org/10.1109/HPCA.2019.00042>'
  - '- <span id="page-14-8"></span>[49] Huwan Peng, Scott Davidson, Richard Shi, Shuaiwen
    Leon Song, and Michael Taylor. 2024. Chiplet Cloud: Building AI Supercomputers
    for Serving Large Generative Language Models. arXiv[:2307.02666](https://arxiv.org/abs/2307.02666)
    [cs.AR] [https://arxiv.org/abs/](https://arxiv.org/abs/2307.02666) [2307.02666](https://arxiv.org/abs/2307.02666)'
  - '- <span id="page-14-9"></span>[50] Kun Qian, Yongqing Xi, Jiamin Cao, Jiaqi Gao,
    Yichi Xu, Yu Guan, Binzhang Fu, Xuemei Shi, Fangbo Zhu, Rui Miao, Chao Wang, Peng
    Wang, Pengcheng Zhang, Xianlong Zeng, Eddie Ruan, Zhiping Yao, Ennan Zhai, and
    Dennis Cai. 2024. Alibaba HPN: A Data Center Network for Large Language Model
    Training. In Proceedings of the ACM SIGCOMM 2024 Conference (Sydney, NSW, Australia)
    (ACM SIGCOMM ''24). Association for Computing Machinery, New York, NY, USA, 691–706.<https://doi.org/10.1145/3651890.3672265>'
  - '- <span id="page-14-22"></span>[51] Eric Qin, Ananda Samajdar, Hyoukjun Kwon,
    Vineet Nadella, Sudarshan Srinivasan, Dipankar Das, Bharat Kaul, and Tushar Krishna.
    2020. Sigma: A sparse and irregular gemm accelerator with flexible interconnects
    for dnn training. In 2020 IEEE International Symposium on High Performance Computer
    Architecture (HPCA). IEEE, 58–70.'
  - '- <span id="page-14-18"></span>[52] Samyam Rajbhandari, Jeff Rasley, Olatunji
    Ruwase, and Yuxiong He. 2020. ZeRO: Memory Optimizations Toward Training Trillion
    Parameter Models. arXiv[:1910.02054](https://arxiv.org/abs/1910.02054) [cs.LG]'
  - '- <span id="page-14-19"></span>[53] Saeed Rashidi, Matthew Denton, Srinivas Sridharan,
    Sudarshan Srinivasan, Amoghavarsha Suresh, Jade Nie, and Tushar Krishna. 2021.
    Enabling Compute-Communication Overlap in Distributed Deep Learning Training Platforms.
    In 2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture
    (ISCA). 540–553.<https://doi.org/10.1109/ISCA52012.2021.00049>'
  - '- <span id="page-14-2"></span>[54] Saeed Rashidi, Pallavi Shurpali, Srinivas
    Sridharan, Naader Hassani, Dheevatsa Mudigere, Krishnakumar Nair, Misha Smelyanski,
    and Tushar Krishna. 2020. Scalable Distributed Training of Recommendation Models:
    An ASTRA-SIM + NS3 case-study with TCP/IP transport. In 2020 IEEE Symposium on
    High-Performance Interconnects (HOTI). 33–42.<https://doi.org/10.1109/HOTI51249.2020.00020>'
  - '- <span id="page-14-3"></span>[55] Saeed Rashidi, Srinivas Sridharan, Sudarshan
    Srinivasan, and Tushar Krishna. 2020. ASTRA-SIM: Enabling SW/HW Co-Design Exploration
    for Distributed DL Training Platforms. In 2020 IEEE International Symposium on
    Performance Analysis of Systems and Software (ISPASS). 81–92. [https://doi.org/10.1109/ISPASS48437.](https://doi.org/10.1109/ISPASS48437.2020.00018)
    [2020.00018](https://doi.org/10.1109/ISPASS48437.2020.00018)'
  - '- <span id="page-14-20"></span>[56] Saeed Rashidi, William Won, Sudarshan Srinivasan,
    Srinivas Sridharan, and Tushar Krishna. 2022. Themis: A Network Bandwidth-Aware
    Collective Scheduling Policy for Distributed Training of DL Models. In Proceedings
    of the 49th Annual International Symposium on Computer Architecture (New York,
    New York) (ISCA ''22). Association for Computing Machinery, New York, NY, USA,
    581–596. <https://doi.org/10.1145/3470496.3527382>'
  - '- <span id="page-14-12"></span>[57] Amedeo Sapio, Marco Canini, Chen-Yu Ho, Jacob
    Nelson, Panos Kalnis, Changhoon Kim, Arvind Krishnamurthy, Masoud Moshref, Dan
    Ports, and Peter Richtarik. 2021. Scaling Distributed Machine Learning with In-Network
    Aggregation. In 18th USENIX Symposium on Networked Systems Design and Implementation
    (NSDI 21). USENIX Association, 785–808. [https://www.usenix.org/conference/](https://www.usenix.org/conference/nsdi21/presentation/sapio)
    [nsdi21/presentation/sapio](https://www.usenix.org/conference/nsdi21/presentation/sapio)'
  - '- <span id="page-14-1"></span>[58] Sean Lie. 2021. Thinking Outside the Die:
    Architecting the ML Accelerator of the Future. [https://www.microarch.org/micro54/media/lie-keynote.pdf.](https://www.microarch.org/micro54/media/lie-keynote.pdf)'
  - '- <span id="page-14-4"></span>[59] Yakun Sophia Shao, Jason Clemons, Rangharajan
    Venkatesan, Brian Zimmer, Matthew Fojtik, Nan Jiang, Ben Keller, Alicia Klinefelter,
    Nathaniel Pinckney, Priyanka Raina, Stephen G. Tell, Yanqing Zhang, William J.
    Dally, Joel Emer, C. Thomas Gray, Brucek Khailany, and Stephen W. Keckler. 2019.
    Simba: Scaling Deep-Learning Inference with Multi-Chip-Module-Based Architecture.
    In 52nd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO).
    14–27. <https://doi.org/10.1145/3352460.3358302>'
  - '- <span id="page-14-17"></span>[60] Debendra Das Sharma and Thomas Coughlin.
    2024. Universal Chiplet Interconnect Express: An Open Industry Standard for Memory
    and Storage Applications .'
  - Computer 57, 01 (Jan. 2024), 75–81.<https://doi.org/10.1109/MC.2023.3318769>
  - '- <span id="page-14-7"></span>[61] Anton Shilov. 2024. TSMC''s System-on-Wafer
    Platform Goes 3D: CoW-SoW Stacks Up the Chips. [https://www.anandtech.com/show/21372/tsmcs-system](https://www.anandtech.com/show/21372/tsmcs-system-on-wafer-platform-goes-3d-cow-sow)[on-wafer-platform-goes-3d-cow-sow.](https://www.anandtech.com/show/21372/tsmcs-system-on-wafer-platform-goes-3d-cow-sow)'
  - '- <span id="page-14-13"></span>[62] Mohammad Shoeybi, Mostofa Patwary, Raul Puri,
    Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2020. Megatron-LM: Training
    Multi-Billion Parameter Language Models Using Model Parallelism. arXiv[:1909.08053](https://arxiv.org/abs/1909.08053)
    [cs.CL]'
  - '- <span id="page-14-21"></span>[63] Siddharth Singh, Olatunji Ruwase, Ammar Ahmad
    Awan, Samyam Rajbhandari, Yuxiong He, and Abhinav Bhatele. 2023. A Hybrid Tensor-Expert-Data
    Parallelism Approach to Optimize Mixture-of-Experts Training. In Proceedings of
    the 37th International Conference on Supercomputing (ICS ''23). ACM, 203–214.
    [https:](https://doi.org/10.1145/3577193.3593704) [//doi.org/10.1145/3577193.3593704](https://doi.org/10.1145/3577193.3593704)'
  - '- <span id="page-14-11"></span>[64] Rajeev Thakur, Rolf Rabenseifner, and William
    Gropp. 2005. Optimization of Collective Communication Operations in MPICH. International
    Journal of High Performance Computing Applications 19, 1 (Feb. 2005), 49–66. [https://doi.org/10.](https://doi.org/10.1177/1094342005051521)
    [1177/1094342005051521](https://doi.org/10.1177/1094342005051521)'
- id: avant_garde_empowering_gpus_with_scaled_numeric_formats_minseong_gil_https_orcid_org_0009_0005_8664_9840
  title: 'Avant-Garde: Empowering GPUs with Scaled Numeric Formats'
  abstract: The escalating computational and memory demands of deep neural networks
    have outpaced chip density improvements, making arithmetic density a key bottleneck
    for GPUs. Scaled numeric formats, such as FP8 and Microscaling (MX), improve arithmetic
    density by applying adaptive scaling factors across varying block sizes and multiple
    scaling hierarchies. Unfortunately, supporting diverse scaled numeric formats
    often requires GPUs to rely on softwarebased implementations, increasing instruction
    and register overhead and degrading performance. We propose Avant-Garde, a GPU
    microarchitecture that natively supports diverse scaled numeric formats by converting
    them into a consistent single-level internal representation. Avant-Garde integrates
    an Operand Transformer, a hardware module that dynamically flattens multi-level
    scaling formats into single-level internal representations, a novel Tensor Core,
    and an optimized data layout to eliminate instruction and register overhead. Our
    evaluations show that Avant-Garde achieves up to 74% higher throughput and 44%
    lower execution time, while maintaining accuracy within 0.2% compared to conventional
    GPUs.
  keywords: GPU, Deep Neural Network, Scaled Numeric Format
  document: "![](_page_0_Picture_0.jpeg)\n\n# Avant-Garde: Empowering GPUs with Scaled\
    \ Numeric Formats\n\n# [Minseong Gil](https://orcid.org/0009-0005-8664-9840)<sup>∗</sup>\n\
    \nKorea University Seoul, Republic of Korea ms7859@korea.ac.kr\n\n# [Myung Kuk\
    \ Yoon](https://orcid.org/0000-0002-9332-0251)\n\nEwha Womans University Seoul,\
    \ Republic of Korea myungkuk.yoon@ewha.ac.kr\n\n# [Dongho Ha](https://orcid.org/0009-0005-4090-4025)<sup>∗</sup>\n\
    \nMangoBoost Inc. Seoul, Republic of Korea dongho.ha@mangoboost.io\n\n# [Babak\
    \ Falsafi](https://orcid.org/0000-0001-5916-8068)\n\nEcoCloud, EPFL Lausanne,\
    \ Vaud, Switzerland babak.falsafi@epfl.ch\n\n# [Yunho Oh](https://orcid.org/0000-0001-6442-3705)\n\
    \nKorea University Seoul, Republic of Korea yunho\\_oh@korea.ac.kr\n\n# [Simla\
    \ Burcu Harma](https://orcid.org/0000-0003-1233-5142)\n\nEcoCloud, EPFL Lausanne,\
    \ Vaud, Switzerland simla.harma@epfl.ch\n\n# [Won Woo Ro](https://orcid.org/0000-0001-5390-6445)\n\
    \nYonsei University Seoul, Republic of Korea wro@yonsei.ac.kr\n\n#### Abstract\n\
    \nThe escalating computational and memory demands of deep neural networks have\
    \ outpaced chip density improvements, making arithmetic density a key bottleneck\
    \ for GPUs. Scaled numeric formats, such as FP8 and Microscaling (MX), improve\
    \ arithmetic density by applying adaptive scaling factors across varying block\
    \ sizes and multiple scaling hierarchies. Unfortunately, supporting diverse scaled\
    \ numeric formats often requires GPUs to rely on softwarebased implementations,\
    \ increasing instruction and register overhead and degrading performance. We propose\
    \ Avant-Garde, a GPU microarchitecture that natively supports diverse scaled numeric\
    \ formats by converting them into a consistent single-level internal representation.\
    \ Avant-Garde integrates an Operand Transformer, a hardware module that dynamically\
    \ flattens multi-level scaling formats into single-level internal representations,\
    \ a novel Tensor Core, and an optimized data layout to eliminate instruction and\
    \ register overhead. Our evaluations show that Avant-Garde achieves up to 74%\
    \ higher throughput and 44% lower execution time, while maintaining accuracy within\
    \ 0.2% compared to conventional GPUs.\n\n#### CCS Concepts\n\n#### • Computer\
    \ systems organization → Parallel architectures.\n\n## Keywords\n\nGPU, Deep Neural\
    \ Network, Scaled Numeric Format\n\n#### ACM Reference Format:\n\nMinseong Gil,\
    \ Dongho Ha, Simla Burcu Harma, Myung Kuk Yoon, Babak Falsafi, Won Woo Ro, and\
    \ Yunho Oh. 2025. Avant-Garde: Empowering GPUs with Scaled Numeric Formats. In\
    \ Proceedings of the 52nd Annual International Symposium on Computer Architecture\
    \ (ISCA '25), June 21–25, 2025, Tokyo, Japan. ACM, New York, NY, USA, [13](#page-12-0)\
    \ pages. [https://doi.org/10.1145/3695053.](https://doi.org/10.1145/3695053.3731100)\
    \ [3731100](https://doi.org/10.1145/3695053.3731100)\n\n<sup>∗</sup>Minseong Gil\
    \ and Dongho Ha contributed equally to this research.\n\n![](_page_0_Picture_25.jpeg)\n\
    \n[This work is licensed under a Creative Commons Attribution 4.0 International\
    \ License.](https://creativecommons.org/licenses/by/4.0) ISCA '25, Tokyo, Japan\
    \ © 2025 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-1261-6/25/06\
    \ <https://doi.org/10.1145/3695053.3731100>\n\n# 1 Introduction\n\nDeep Neural\
    \ Networks (DNNs) are widely used across diverse market segments, driven by an\
    \ unprecedented growth in model and dataset sizes. For instance, GPT-3 contains\
    \ 175 billion parameters and requires over 3 × 10<sup>23</sup> arithmetic operations\
    \ for training [\\[10\\]](#page-11-0). The sizes of the state-of-the-art models\
    \ keep increasing exponentially every year [\\[35\\]](#page-12-1). This rapid\
    \ growth has outpaced improvements in chip density [\\[40\\]](#page-12-2), leading\
    \ to significant computational and memory challenges for both training and inference.\
    \ The energy consumption and environmental impact of both training and using these\
    \ models have also become significant concerns.\n\nScaled numeric formats have\
    \ emerged as a promising solution to improve arithmetic density (operations per\
    \ second per mm<sup>2</sup> ) while maintaining accuracy in large-scale DNN workloads\
    \ [\\[8,](#page-11-1) [9,](#page-11-2) [12,](#page-11-3) [14,](#page-11-4) [25,](#page-12-3)\
    \ [41,](#page-12-4) [42\\]](#page-12-5). These formats represent numeric values\
    \ by grouping them into recursive sets called blocks, each associated with a shared\
    \ scaling factor. The scaling factor adjusts the dynamic range uniformly for all\
    \ values within the same block, reducing quantization error and enhancing computational\
    \ efficiency. Blocks can be organized hierarchically into a single or multiple\
    \ levels. For instance, a single-level scaled numeric format applies a single\
    \ scaling factor uniformly across all elements within a block. In contrast, multilevel\
    \ scaled numeric formats divide each block further into smaller subsets, each\
    \ with its own scaling factor. At the lowest level, each element is represented\
    \ in floating or fixed point. The hierarchy, as such, enables precision adjustments\
    \ across levels and provides fine-grained control over data representation. FP8\
    \ and Microscaling (MX) formats apply this approach to balance memory efficiency\
    \ and computational accuracy for large-scale DNN workloads.\n\nUnfortunately,\
    \ modern GPUs rely mostly on software to support scaled numeric formats. Recent\
    \ NVIDIA GPUs only support FP8 which relies on per-tensor scaling factors implemented\
    \ directly in software [\\[30\\]](#page-12-6). Emerging MX formats on current\
    \ GPUs similarly require each operand to be converted in software from the scaled\
    \ numeric format to a fixed-point representation, applying the scaling factors\
    \ to each element at each level to prepare them for an arithmetic operation (e.g.,\
    \ matrix multiplication and accumulation). This software overhead is simply prohibitive\
    \ precluding the use of arbitrary scaled numeric formats in GPUs today.\n\nIn\
    \ this paper, we make the observation that for GPUs to natively support scaled\
    \ numeric formats, they must enable an operand representation in microarchitecture\
    \ that lends itself well to efficient arithmetic and operand storage in hardware.\
    \ Hardware can implement operand conversion from scaled numeric formats to this\
    \ representation to minimize the conversion overhead. Moreover, operands can remain\
    \ in this representation for the duration of a workload's execution to minimize\
    \ the conversion cadence. Despite variations in block size and the number of levels\
    \ in scaled numeric formats, the values can be converted to a single-level representation\
    \ used for computation. Fortunately, arithmetic with a single-level representation\
    \ can be readily implemented in microarchitecture [\\[8,](#page-11-1) [26,](#page-12-7)\
    \ [50\\]](#page-12-8) supporting all arithmetic operations required to execute\
    \ DNN workloads. Therefore, a GPU that natively suppports scaled numeric formats\
    \ can virtually eliminate the software overhead incurred to use them.\n\nWe propose\
    \ Avant-Garde, a novel GPU microarchitecture, which is to the best of our knowledge,\
    \ the first of its kind to natively support scaled numeric formats. The core innovation\
    \ of Avant-Garde lies in \"flattening\" operands in hardware from scaled numeric\
    \ formats into a single-level format for computation. We call a block transformed\
    \ from a scaled numeric format to a single-level one a flattened block. Avant-Garde\
    \ microarchitecture incorporates key components such as Operand Transformer, a\
    \ specialized module that transforms operands from a multi-level format into flattened\
    \ blocks, and a redesigned Tensor Core architecture that performs seamless arithmetic\
    \ operations using a flattened format. Avant-Garde also employs an optimized data\
    \ layout and storage mechanism that manages scaling factors and numeric values\
    \ in memory efficiently without altering their original format.\n\nOur evaluation\
    \ of Avant-Garde on DNN workloads employing scaled numeric formats (i.e., MX and\
    \ Hybrid Block Floating Point (HBFP)) demonstrates significant improvements in\
    \ performance and energy efficiency over a baseline GPU architecture. Avant-Garde\
    \ achieves a 74% improvement in throughput compared to baseline GPUs that support\
    \ FP8 only. Also, Avant-Garde reduces inference execution time by 44%.\n\nThis\
    \ paper makes the following contributions:\n\n- We analyze the bottlenecks in\
    \ supporting scaled numeric formats in GPUs and reveal inefficiencies caused by\
    \ the lack of native support for dynamic scaling. Standard Tensor Cores do not\
    \ support multi-level scaling factors, leaving software-based alternatives as\
    \ the only option. This results in increased instruction overhead and register\
    \ usage, which in turn reduces arithmetic density.\n- We propose Avant-Garde,\
    \ a novel GPU microarchitecture that efficiently handles scaled numeric formats.\
    \ Avant-Garde integrates an Operand Transformer, a Avant-Garde Tensor Core architecture,\
    \ and an optimized data layout to improve arithmetic density.\n- We evaluate Avant-Garde\
    \ on MX-compatible formats across a range of DNN workloads. Avant-Garde achieves\
    \ a 74% increase in throughput and a 44% reduction in inference time compared\
    \ to the baseline.\n\n• We demonstrate using ViT-Base, BERT, and GPT-2 that flattened\
    \ MX9 deviates by less than 0.2% in accuracy or perplexity compared to both original\
    \ MX9 and FP32.\n\nThe remainder of this paper is organized as follows. Section\
    \ [2](#page-1-0) explains why GPUs need a native support for scaled numeric formats\
    \ and our insights. Section [3](#page-3-0) introduces the Avant-Garde architecture.\
    \ Section [4](#page-7-0) and Section [5](#page-8-0) present the methodology and\
    \ the experimental results, respectively. Section [6](#page-10-0) discusses related\
    \ work. Finally, Section [7](#page-11-5) concludes this paper.\n\n#### <span id=\"\
    page-1-0\"></span>2 Why Avant-Garde?\n\nIn this section, we first explain the\
    \ fundamentals of scaled numeric formats and the operations they involve. We then\
    \ describe the challenges of executing these operations efficiently on GPUs. Finally,\
    \ we present our key insights for overcoming these challenges.\n\n#### 2.1 The\
    \ Rise of Scaled Numeric Formats\n\nArithmetic density has emerged as a major\
    \ bottleneck in platforms designed for DNN workloads, posing limitations on computational\
    \ efficiency and scalability [\\[13,](#page-11-6) [19,](#page-11-7) [32\\]](#page-12-9).\
    \ The rapid growth in DNN model and dataset sizes has led to substantial electrical\
    \ consumption, raising both environmental and operational concerns across industries.\
    \ Modern large language models, such as GPT-3 with 175 billion parameters, require\
    \ over 3×10<sup>23</sup> arithmetic operations [\\[10\\]](#page-11-0), creating\
    \ unprecedented computational demands. As Moore's Law slows down and approaches\
    \ its limits, maximizing arithmetic density in GPUs has become an enduring challenge\
    \ for chip designers.\n\nQuantization and mixed-precision methods have been widely\
    \ explored to improve the efficiency of DNN workloads, especially under the constraints\
    \ of current GPUs. Prior work has proposed various quantization methods, which\
    \ are particularly beneficial for inference tasks by lowering precision to decrease\
    \ computational load and data traffic [\\[1,](#page-11-8) [48,](#page-12-10) [52\\\
    ]](#page-12-11). However, these methods are constrained by the numeric formats\
    \ supported in GPUs. Similarly, mixed-precision methods have been developed to\
    \ reduce computational and memory requirements [\\[17,](#page-11-9) [28,](#page-12-12)\
    \ [36,](#page-12-13) [51\\]](#page-12-14). While effective, these methods often\
    \ rely on floating-point formats to maintain accuracy, which limits the achievable\
    \ throughput compared to fixed-point formats. As a result, these approaches fall\
    \ short of fully leveraging GPU efficiency, highlighting the need for a solution\
    \ that can optimize both arithmetic density and memory overhead in training and\
    \ inference.\n\nScaled numeric formats have emerged as a promising solution for\
    \ improving arithmetic density and memory overhead while maintaining sufficient\
    \ accuracy in both training and inference. These formats are characterized by\
    \ two core concepts: blocks and levels. Blocks group multiple elements under a\
    \ shared scaling factor, reducing the need for per-element scaling and improving\
    \ memory efficiency. This block-based format enables coarse-grained precision\
    \ management, which is particularly effective for DNN workloads. Levels define\
    \ the hierarchical depth of scaling within blocks, representing how scaling factors\
    \ are organized. Single-level formats apply a uniform scaling factor across all\
    \ elements in a block, offering a simple and efficient mechanism for precision\
    \ adjustment. In contrast, multi-level formats introduce additional scaling factors\
    \ at finer granularities, such as subsets of elements, to enable more\n\nSubset\
    \ Block Size: 2\n\nE4M3 or E5M2\n\n2b or 4b or 7b\n\nBlock Size: ~10K\n\nBlock\
    \ Size: 16\n\n<span id=\"page-2-0\"></span>![](_page_2_Figure_2.jpeg)\n\nE4M3\
    \ or E5M2\n\nFP32\n\nBlock Size: ~10K\n\n1b 2b or 4b or 7b Block Size: 2 8b Figure\
    \ 1: Emerging scaled numeric formats. The 8-bit floating-point format consists\
    \ of two encodings: one with a 4-bit exponent and a 3-bit mantissa (E4M3), and\
    \ another with a 5-bit exponent and a 2-bit mantissa (E5M2) [\\[30\\]](#page-12-6).\
    \ The shared micro exponent format offers three configurations: MX4, MX6, and\
    \ MX9, with mantissa sizes of 2 bits, 4 bits, and 7 bits, respectively [\\[4\\\
    ]](#page-11-10).\n\nBlock Size: ~576 8b precise control over data representation.\
    \ For example, two-level formats apply a secondary scaling factor within each\
    \ subset, providing finer-grained control over precision within a block.\n\nSubset\n\
    \n8b Figure [1](#page-2-0) shows examples of emerging scaled numeric formats.\
    \ Recent NVIDIA GPUs support an 8-bit floating-point format (FP8), optimized for\
    \ DNN workloads (Figure [1a\\)](#page-2-0) [\\[30\\]](#page-12-6). FP8 incorporates\
    \ a per-tensor scaling factor to mitigate the limited dynamic range of 8-bit representations.\
    \ This per-tensor scaling enables the use of low-bitwidth operations, which increases\
    \ arithmetic density by reducing memory and compute requirements. However, FP8\
    \ relies on software to implement and manage these scaling factors, which might\
    \ limit GPU performance.\n\nMicrosoft has introduced an alternative numeric format\
    \ called shared microexponents (MX) [\\[4,](#page-11-10) [42\\]](#page-12-5),\
    \ which has been made available through the Open Compute Project (OCP) [\\[2\\\
    ]](#page-11-11). The MX format (Figure [1b\\)](#page-2-0) offers three configurations\
    \ (MX4, MX6, and MX9) depending on the bitwidth allocated per element. These formats\
    \ commonly use two-level scaling: a set of 16 elements shares a first-level scaling\
    \ factor as a block, while each subset of two elements within the block forms\
    \ a second-level block sharing a second-level scaling factor. Among the MX formats,\
    \ MX9, which includes a 7-bit mantissa, is the only configuration designed to\
    \ achieve training and inference accuracy comparable to traditional formats. Its\
    \ peak arithmetic throughput is equivalent to that of a 7-bit fixed-point format.\n\
    \nHybrid block floating point (HBFP), which uses a shared scaling factor across\
    \ a block of numbers, improves arithmetic density and reduces memory usage (Figure\
    \ [1c\\)](#page-2-0) [\\[8,](#page-11-1) [9\\]](#page-11-2). HBFP is functionally\
    \ equivalent to MXINT but employs a larger block size (576 instead of 32) [\\\
    [2\\]](#page-11-11). Recent advancements have introduced HBFP variants with 6-bit\
    \ and 4-bit mantissas, extending beyond the original 8-bit configuration. These\
    \ lower-precision HBFP formats enable energyefficient computation for large-scale\
    \ model training and inference, addressing the growing demand for scalable and\
    \ power-efficient deep learning solutions [\\[14\\]](#page-11-4).\n\n#### <span\
    \ id=\"page-2-2\"></span>2.2 Challenges with Scaled Numeric Formats\n\nNative\
    \ GPU support for scaled numeric formats beyond FP8 is essential to fully leverage\
    \ their computational efficiency benefits. Software stacks for quantization methods\
    \ tailored to MX formats have been developed to address the limitations of native\
    \ FP8 [\\[15,](#page-11-12) [16\\]](#page-11-13).\n\n<span id=\"page-2-1\"></span>![](_page_2_Figure_10.jpeg)\n\
    \nFP32\n\n8b\n\nFigure 2: MMA operations with scaled numeric formats with conventional\
    \ GPUs. Blocks highlighted in grey represent operations that Tensor Core cannot\
    \ process natively. These operations involve calculating and applying a scaling\
    \ factor to the product result before accumulation.\n\nThese methods mitigate\
    \ precision loss and improve numerical robustness, making them more effective\
    \ for low-bitwidth computation. Also, several post-training quantization methods\
    \ that employ MX formats have been introduced to enhance inference accuracy [\\\
    [18,](#page-11-14) [24,](#page-12-15) [39,](#page-12-16) [43\\]](#page-12-17).\
    \ For training, an energy-efficient approach based on HBFP has been introduced\
    \ to address challenges related to dynamic range limitations and precision degradation\
    \ [\\[14\\]](#page-11-4). These developments indicate that DNN models can achieve\
    \ improved efficiency and accuracy by adopting various scaled numeric formats\
    \ beyond FP8. While these formats can be quantized to FP8 for execution on GPUs,\
    \ such conversion may compromise their superior arithmetic density or accuracy.\n\
    \nFigure [2](#page-2-1) illustrates how computation with scaled numeric formats\
    \ adapts to different scaling hierarchies. The execution flow is determined by\
    \ the scaling hierarchy and precision of the target scaled numeric format. In\
    \ a single-level format, the GPU computes dot products directly on the elements\
    \ and multiplies the scaling factors from each input block separately. The GPU\
    \ then multiplies the dot product result by the combined scaling factor to produce\
    \ the final output. In a multi-level format, the GPU first applies all scaling\
    \ factors, except that of the first-level block, to the corresponding\n\n<span\
    \ id=\"page-3-1\"></span>ISCA '25, June 21–25, 2025, Tokyo, Japan Minseong Gil,\
    \ Dongho Ha, Simla Burcu Harma, Myung Kuk Yoon, Babak Falsafi, Won Woo Ro, and\
    \ Yunho Oh\n\n**Register File Usage**\n\n| wmma.load.a | {R0-1}, [RD0], R21  \
    \             |                  |                  |  |  |\n|-------------|----------------------------------|------------------|------------------|--|--|\n\
    | wmma.load.b | {R2-3}, [RD1], R22               |                  |        \
    \          |  |  |\n| wmma.mma    | {R0-7}, {R0-1}, {R2-3},<br>{R23} |       \
    \           |                  |  |  |\n| ld.global   | R16, [RD2];          \
    \            |                  |                  |  |  |\n| ld.global   | R17,\
    \ [RD2+0x4];                  |                  |                  |  |  |\n\
    | ld.global   | R18, [RD3];                      |                  |        \
    \          |  |  |\n| ld.global   | R19, [RD3+0x4];                  |       \
    \           |                  |  |  |\n| mul         |                      \
    \            | R20, R16, R18    |                  |  |  |\n| mad         |  \
    \                                | R8, R0, R20, R8  |                  |  |  |\n\
    | mad         |                                  | R9, R1, R20, R9  |        \
    \          |  |  |\n| mul         |                                  | R0, R17,\
    \ R18     |                  |  |  |\n| mad         |                        \
    \          | R10, R2, R0, R10 |                  |  |  |\n| mad         |    \
    \                              | R11, R3, R0, R11 |                  |  |  |\n\
    | mul         |                                  | R0, R16, R19     |        \
    \          |  |  |\n| mad         |                                  | R12, R4,\
    \ R0, R12 |                  |  |  |\n| mad         |                        \
    \          | R13, R5, R0, R13 |                  |  |  |\n| mul         |    \
    \                              | R1, R18, R19     |                  |  |  |\n\
    | mad         |                                  | R14, R6, R1, R14 |        \
    \          |  |  |\n| mad         |                                  |       \
    \           | R15, R7, R1, R15 |  |  |\n\nFigure 3: CUDA instruction stream for\
    \ Matrix Multiply-Accumulate (MMA) operations using scaled numeric formats on\
    \ conventional GPUs.\n\nelements. This adjustment transforms the data layout into\
    \ a form equivalent to a single-level format. After the adjustment, the GPU computes\
    \ dot products over the modified elements and multiplies the accumulated result\
    \ by the first-level scaling factor.\n\nImplementing scaled numeric formats beyond\
    \ FP8 in GPUs poses a significant challenge. These formats rely on shared scaling\
    \ factors across blocks or multi-level scaling factors within subsets. MMA operations\
    \ with a single- or multi-level scaled numeric format involve additional computational\
    \ steps beyond those required by traditional formats. Conventional Tensor Cores\
    \ primarily support operations with standard numeric formats but cannot efficiently\
    \ handle the per-block scaling adjustments required by scaled formats. As a result,\
    \ CUDA Cores should perform these scaling adjustments. This division of labor\
    \ between Tensor and CUDA Cores increases instruction count and register file\
    \ utilization. The added instruction count prolongs the execution time per thread,\
    \ while higher register utilization further degrades energy efficiency.\n\nTo\
    \ quantify the overhead of scaled numeric formats, we implement a GPU program\
    \ that performs MMA operations on two 16×16 matrices using MXINT8, a single-level\
    \ scaled numeric format. We compile the program with nvcc and analyze the executable\
    \ with NVIDIA Nsight Compute to measure register usage and instruction count.\
    \ The 16×16 matrix size matches the native MMA tile size of Tensor Cores. This\
    \ choice also aligns with the recommendation of OCP, which suggests a block size\
    \ of 32 for MX formats [\\[2\\]](#page-11-11).\n\nFigure [3](#page-3-1) shows\
    \ a stream of GPU instructions (PTX format) for an MMA operation with 16×16 tiles.\
    \ During each MMA operation, the GPU loads shared scaling factors and applies\
    \ them across multiple registers. For example, loading scaling factors into registers\
    \ (e.g., R16, R17, R18, and R19) involves multiple steps, where each scaling factor\
    \ adjusts the corresponding operands in matrices A and B before accumulation.\
    \ This process introduces additional instructions, such as 'mul' and 'mad' (multiply-add),\
    \ to scale each element individually. As a result, repeated multiplication and\
    \ accumulation\n\n<span id=\"page-3-2\"></span>![](_page_3_Figure_8.jpeg)\n\n\
    Figure 4: Comparison of register file usage and instruction count between traditional\
    \ and scaled numeric formats. All results are normalized to INT8.\n\nincrease\
    \ the instruction count, reflecting the inherent computational cost of handling\
    \ scaled numeric formats. With multi-level scaled numeric formats, the GPU executes\
    \ additional CUDA Core instructions to convert the data into a single-level format\
    \ before MMA operations.\n\nThrough this detailed profiling, we also observe that\
    \ the additional registers required to store shared scaling factors and intermediate\
    \ values substantially contribute to the increased register footprint necessary\
    \ for scaled numeric operations. We measure the register file usage and the number\
    \ of executed instructions for traditional and scaled numeric formats in real\
    \ DNN workloads. Specifically, we compare INT8, a traditional numeric format,\
    \ with MX9, a two-level scaled numeric format. We implement the matrix multiplication\
    \ operation in the first layer of ViT-Huge using each format and analyze the register\
    \ usage with the nvcc compiler.\n\nFigure [4](#page-3-2) shows the experimental\
    \ results. In our experiments, the register file usage for MX9 is 1.38× higher\
    \ than that of INT8. Excessive register usage per thread may reduce the number\
    \ of threads that can execute concurrently on each streaming multiprocessor (SM)\
    \ [\\[34\\]](#page-12-18). This reduction in executable threads can lead to performance\
    \ degradation. Also, as shown in Figure [4b,](#page-3-2) the number of instructions\
    \ executed for MX9 is 2.14× higher than that of INT8, further degrading performance\
    \ and efficiency. Thus, while scaled numeric formats improve arithmetic density,\
    \ their increased register and instruction requirements may result in significant\
    \ slowdown.\n\n#### <span id=\"page-3-0\"></span>3 Avant-Garde\n\nThe key idea\
    \ behind Avant-Garde is the \"flattening\" of scaled numeric formats into a predefined\
    \ single-level scaled numeric format in microarchitecture to perform arithmetic\
    \ operations on the flattened operands using custom Tensor Cores. Both single-level\
    \ and multi-level scaled numeric formats follow a common flattening process. This\
    \ process is straightforward for single-level formats, where one scaling factor\
    \ is uniformly shared among all elements in the original and the flattened representation.\
    \ In contrast, multi-level formats recursively apply scaling factors one level\
    \ at a time to arrive at a flattened representation. The resulting flattened representation\
    \ can be stored in memory and maintained in the register file, eliminating the\
    \ need to flatten operands for subsequent computation. Flattening not only enables\
    \ consistent computation patterns for\n\n<span id=\"page-4-0\"></span>![](_page_4_Figure_2.jpeg)\n\
    \nFigure 5: Operand flattening in Avant-Garde. Avant-Garde loads operands in scaled\
    \ numeric formats into the register file, then flattens them into a flattened\
    \ format before performing arithmetic operations in a Tensor Core. Avant-Garde\
    \ maintains the operands in the flattened format in the register file and memory\
    \ for subsequent operations.\n\nDNN workloads but also aligns with the GPU execution\
    \ model, ensuring efficient utilization of GPU resources.\n\nFigure [5](#page-4-0)\
    \ illustrates the flattening process in Avant-Garde for twolevel scaled numeric\
    \ formats. Flattening results in a single-level format, with a flattened block\
    \ of elements and a scaling factor. For two-level scaled numeric formats, flattening\
    \ applies all second-level scaling factors to the elements while retaining the\
    \ first-level scaling factor as is. As GPU execution is organized into warps (typically\
    \ 32 threads/warp), we set the flattened block size to the GPU warp size and coalesce\
    \ smaller blocks into a full warp register (typically 128 bytes) to maximize the\
    \ utilization of resources. The figure shows that for small block sizes (< 16\
    \ elements), Avant-Garde first flattens each block, then coalesces as many blocks\
    \ into a single flattened block while preserving their scaling factors until the\
    \ number of elements reaches a maximum of 32. For large block sizes (> 32 elements),\
    \ Avant-Garde slices the block into multiple flattened blocks, with each slice\
    \ retaining the original scaling factor.\n\nA key requirement for the flattening\
    \ process is to give the application developer the flexibility to freely use scaled\
    \ numeric formats allowing for denser storage and computation. Recent DNN research\
    \ focuses on maximizing compression along multiple computational dimensions with\
    \ mixed precision. Implementing the proposed flattening process for a wide spectrum\
    \ of scaled numeric formats requires careful consideration of the data layout\
    \ and storage. If not careful, flattening may lead to a low utilization of resources\
    \ for formats with two or more scaling levels because applying the scaling factors\
    \ results in scalar computation that may not align with the GPU's single-instruction\
    \ multiple-thread (SIMT) execution model.\n\nTo address this challenge, Avant-Garde\
    \ introduces a novel GPU microarchitecture and provides an API for efficient data\
    \ transformations. Avant-Garde integrates custom microarchitecture to handle scaling\
    \ factors, supporting both existing and future scaled numeric formats without\
    \ requiring hardware modifications. In addition, perelement formatting flexibility\
    \ within a block can be expanded by adopting prior software-based solutions. For\
    \ example, VitBit offers fine-grained control over bit allocation and scaling\
    \ mechanisms\n\nat the software level [\\[20\\]](#page-12-19). By combining Avant-Garde\
    \ with the element format-aware software processing of VitBit, future scaled numeric\
    \ formats can be accommodated with minimal hardware.\n\nThe Avant-Garde API provides\
    \ a structured interface for programmers to retrieve data. It assumes that programmers\
    \ understand the data layout and use the API to fetch elements and scaling factors\
    \ accordingly. With this design, elements and scaling factors are stored side\
    \ by side in the register file, supporting co-located access. The latter aligns\
    \ with the flattening strategy and maintains optimized storage and data flow for\
    \ high-performance DNN workloads.\n\n#### 3.1 Basic Pipeline\n\nFigure [6](#page-5-0)\
    \ presents the anatomy of the Avant-Garde pipeline. To flatten operands, Avant-Garde\
    \ adds a pipeline stage for Operand Transformer, a dedicated microarchitecture\
    \ component between the operand read and execute stages in the baseline GPU pipeline.\
    \ Avant-Garde adds instructions to use Operand Transformer to flatten operands\
    \ from scaled numeric formats. Avant-Garde also includes instructions for the\
    \ developer to specify the scaled numeric format parameters including the number\
    \ of scaling levels, the block size and bitwidth of each element in scalar registers.\
    \ The flattening instructions use these parameters with a pair of source and destination\
    \ 128-byte warp registers to implement the transformation.\n\nAvant-Garde's Tensor\
    \ Core also includes an MMA instruction that operates on flattened operands. To\
    \ support MMA operations, Avant-Garde incorporates a custom Tensor Core enhanced\
    \ to operate on the flattened format. Avant-Garde's Tensor Core directly performs\
    \ computation on operands represented with a flattened block with a scaling factor.\
    \ Arithmetic involving both a scaling factor and a block elements is similar to\
    \ exponent and mantissa management with a conventional floating point as scaling\
    \ factors should be adjusted for both source and destination operands depending\
    \ on the operation. For example, a multiplication of two operands requires addition\
    \ of scaling factors and a pairwise multiplication of elements in each block,\
    \ while addition requires adjust scaling factors to allow for pairwise addition\
    \ of elements.\n\nISCA '25, June 21–25, 2025, Tokyo, Japan Minseong Gil, Dongho\
    \ Ha, Simla Burcu Harma, Myung Kuk Yoon, Babak Falsafi, Won Woo Ro, and Yunho\
    \ Oh\n\n<span id=\"page-5-0\"></span>![](_page_5_Figure_2.jpeg)\n\nFigure 6: Basic\
    \ Avant-Garde pipeline. The pipeline is derived from a conventional GPU with modified\
    \ components highlighted in gray. The Operand Transform stage which flattens operands\
    \ is selectively skipped if the operands do not use a scaled numeric format. Avant-Garde's\
    \ Tensor Core performs dot product operations on flattened operands.\n\nAvant-Garde\
    \ uses the GPU's existing load/store instructions to transfer operands between\
    \ the memory and register file. Operands are laid out contiguously in memory for\
    \ any format including the conventional data types, scaled numeric formats and\
    \ the flattened format. Tensor Cores in conventional GPUs have hard-wired connections\
    \ that scatter elements from 128-byte warp registers to arithmetic units [\\[46\\\
    ]](#page-12-20). This microarchitecture supports operand fetching and supplying\
    \ even if elements are not 4-byte aligned within a warp register. Leveraging this\
    \ microarchitecture capability, Avant-Garde's Tensor Cores interpret flattened\
    \ blocks without 4 byte alignment constraints. This functionality is integrated\
    \ into the register file arbitrator. For example, with the MX6 format, Avant-Garde\
    \ requires only 192 bytes for a block, occupying two warp registers and leaving\
    \ 64 bytes unused.\n\nFinally, for all non-GEMM operations, Avant-Garde maintains\
    \ operands in registers in the same manner as the baseline GPU described in Section\
    \ [2.2.](#page-2-2) These operations require a source operand for a warp to be\
    \ stored in an aligned warp register consisting of 32 four-byte registers. For\
    \ instance, if a block in a scaled numeric format contains four-bit elements,\
    \ each element is stored in a four-byte register, leaving the remaining 28 bits\
    \ unused. Because operations other than GEMM computations represent only a small\
    \ portion of the total workload, GPU kernels using non-GEMM operations do not\
    \ significantly affect average register file utilization. Moreover, as discussed\
    \ in Section [2.2,](#page-2-2) register pressure in conventional GPUs during computations\
    \ with scaled numeric formats arises from additional registers used to temporarily\
    \ store intermediate results for arithmetic operations. Avant-Garde mitigates\
    \ this overhead through dedicated Operand Transformers and redesigned Tensor Cores.\
    \ This design achieves significant improvements over conventional GPUs without\
    \ increasing register file utilization.\n\n#### 3.2 Implementation & API\n\nFigure\
    \ [7](#page-5-1) describes the microarchitecture of Operand Transformer. Its input,\
    \ blocks in a scaled numeric format, comes from operand collectors connected to\
    \ the register file. The outputs generated by Operand Transformer include one\
    \ or more scaling factors (if there are multiple flattened blocks per warp) and\
    \ sequences of elements, with each element associated with its respective scaling\
    \ factor. Within the Operand Transformer, the warp operand formation\n\n<span\
    \ id=\"page-5-1\"></span>![](_page_5_Figure_8.jpeg)\n\nFigure 7: Operand Transformer.\
    \ This unit incorporates 16 FP8/INT8 multipliers and thirty-two temporal registers.\
    \ Each multiplier output is propagated to one of two temporal registers. To handle\
    \ 32 elements, the unit reuses 16 multipliers twice.\n\ncontroller assigns flattened\
    \ blocks to warps based on the strategy shown in Figure [5.](#page-4-0) Each flattened\
    \ block contains up to 32 elements and one or more associated scaling factors,\
    \ aligned to the warp size. Small blocks may be coalesced into a warp, while large\
    \ blocks may be split across multiple flattened blocks.\n\nAs operand flattening\
    \ involves transforming multi-level formats, its latency depends on both the scaling\
    \ level and the block size of the scaled numeric format. For a scaling level of\
    \ , Operand Transformer performs 2 × ( − 1) iterations to flatten the multi-level\
    \ format into a single-level one. The block size also influences the number of\
    \ required iterations. Larger block sizes, such as those used in HBFP [\\[14\\\
    ]](#page-11-4), reduce the frequency of transformation operations because multiple\
    \ operands can be processed simultaneously. During each iteration, intermediate\
    \ values generated by the flattening process are stored in temporary registers,\
    \ each occupying 32 bytes.\n\nAlthough Operand Transformer may perform multiple\
    \ iterations for complex operand flattening scenarios, this overhead has minimal\
    \ impact on overall performance. During DNN workload execution, operand flattening\
    \ occurs as a preprocessing step rather than on a per-operation basis, minimizing\
    \ its impact on execution time. For model weights, the transformation is applied\
    \ once before inference\n\n<span id=\"page-6-0\"></span>![](_page_6_Figure_1.jpeg)\n\
    \nFigure 8: Avant-Garde Tensor Core. This unit incorporates an 8-bit fixed-point\
    \ adder for combining the scaling factors of two operands and a scaling unit for\
    \ multiplying the dot product result with the combined scaling factor before passing\
    \ it to the accumulator.\n\nor training to convert pre-stored multi-level representations\
    \ into flattened formats, avoiding repeated transformations. For input data, operand\
    \ flattening is applied at the beginning of computation and remains unchanged\
    \ throughout execution. Activations are computed and retained in this flattened\
    \ format, eliminating the need for additional flattening. Moreover, the latency\
    \ introduced by operand flattening is often hidden by the interleaved warp execution\
    \ of GPUs. As a result, Avant-Garde processes multi-level formats efficiently\
    \ without degrading performance. We evaluate the impact of operand flattening\
    \ latency across various scaled numeric formats. Our results demonstrate that\
    \ Avant-Garde maintains performance improvements in these scenarios (details in\
    \ Section [5.6\\)](#page-10-1).\n\nFigure [8](#page-6-0) describes the detailed\
    \ microarchitecture of Avant-Garde's Tensor Core. It incorporates two specialized\
    \ microarchitecture units: an 8-bit fixed-point adder for combining scaling factors\
    \ and a scaling unit that applies combined scaling factors to the results of dot\
    \ products across elements. As scaling factors are encoded as exponents, the 8-bit\
    \ fixed-point adder computes the sum of scaling factors from each operand.\n\n\
    Avant-Garde's Tensor Cores send the combined scaling factor to a dedicated scaling\
    \ unit, which performs a low-latency multiplication that applies the combined\
    \ scaling factor to the dot product result before accumulation. The result of\
    \ scaling unit then proceeds to the accumulator and continues through the rest\
    \ of the pipeline. By embedding this operation within the core pipeline, Avant-Garde\
    \ eliminates the need for separate scaling instructions, reducing register pressure\
    \ and instruction count.\n\nThe Avant-Garde API extends the warp matrix multiplication\
    \ and accumulation (WMMA) API to support predefined scaled numeric formats. Figure\
    \ [9](#page-6-1) illustrates a use case of the Avant-Garde API. In this example,\
    \ the function declare tensors and allocate corresponding memory space. The \\\
    _\\_ function\n\n// Load matrix tiles to compute.\n\n// Store the computation\
    \ results. **STG.E.64** [R20.64], R8 **STG.E.64** [R2.64], R10 **STG.E.64** [R20.64+0x20],\
    \ R24 **STG.E.64** [R2.64+0x20], R26\n\n// Load elements and scaling factors\n\
    \n// Flatten the concatenated data\n\n// Store the flattened block **STG.E** [R18],\
    \ R11\n\n// Concatenate elements and scaling factors\n\n// MMA operations for\
    \ scaled numeric format. **SMMA.16816.mx9.mx9** R8, R4.ROW, R10.COL, R0 **SMMA.16816.mx9.mx9**\
    \ R24, R4.ROW, R18.COL, R0\n\n**SASS Instructions for MMA Operations**\n\n**SASS\
    \ Instructions for flattenScale**\n\n**LDG.E** R4, [R12.64] **LDG.E** R10, [R16.64]\
    \ **LDG.E** R5, [R14.64] **LDG.E** R18, [R18.64]\n\n**LDG.E.U8** R0, [R2] **LDG.E.U8**\
    \ R4, [R4] **LDG.E.U8** R6, [R6]\n\n**STS.U8** [R13], R0 **STS.U8** [R9], R4 **STS.U8**\
    \ [R8], R6 **LDS.U.32** R11, [R10]\n\n**FLAT** R16, R11\n\n```\n__global__ \n\
    void compute_gemm_scaled(half *A, half *B, float *C, \n int M, int N, int K, scaled\
    \ *format) {\n wmma::fragment<wmma::matrix_a, M, N, K, format, wmma::row_major>\
    \ a_frag;\n wmma::fragment<wmma::matrix_b, M, N, K, format, \n               \
    \       wmma::col_major> b_frag;\n wmma::fragment<wmma::accumulator, M, N, K,\
    \ format> acc_frag;\n ...\n wmma::load_matrix_sync(a_frag, A, M); \n wmma::load_matrix_sync(b_frag,\
    \ B, K);\n wmma::mma_sync(acc_frag, a_frag, b_frag, acc_frag);\n wmma::store_matrix_sync(C,\
    \ acc_frag, N, wmma::mem_row_major);\n}\nvoid inference(half *i, half *w, half\
    \ *o, scaled *format) {\n ...\n compute_gemm_scaled<<<...>>>(A, B, C, M, K, N,\
    \ format);\n}\nint main(int argc, char** argv) {\n scaled mx9;\n cudaMalloc((void**)&input,\
    \ mx9.getSize() * INPUT_SIZE);\n cudaMalloc((void**)&weight, mx9.getSize() * WEIGHT_SIZE);\n\
    \ cudaMemcpy(weight, weight_h, mx9.getSize() * WEIGHT_SIZE,\n     cudaMemcpyHostToDevice);\n\
    \ ...\n // Flatten multi level into single level.\n flatten(w_flattened, weight);\n\
    \ while (runningInference) { cudaMemcpy(input, input_h, mx9.getSize() * INPUT_SIZE,\n\
    \ cudaMemcpyHostToDevice);\n flatten(i_flattened, input);\n inference(i_flattened,\
    \ w_flattened, output, mx9);\n }\n ...\n}\n```\nFigure 9: Avant-Garde API and\
    \ use case. User can configure settings such as the scaling level and block size\
    \ of the scaled format through the API. Once configurations for the scaled numeric\
    \ format are specified, then GPU memory functions can exploit the specifications.\n\
    \nperforms matrix multiplication using scaled numeric formats. These tensors are\
    \ defined with MX9 format. The PTX or SASS interpreter in Avant-Garde identifies\
    \ the target scaled numeric format through a predetermined variable, , and assigns\
    \ it to the relevant intermediate representations. The Avant-Garde API provides\
    \ data formats for predefined scaled numeric formats, specifying scaling levels,\
    \ block sizes, and the bit compositions of scaling factors and elements. For example,\
    \ if a user selects MX9, the Avant-Garde API provides predefined information including\
    \ its scaling level (2), the number of subsets per block (8), and the block size\
    \ (16). Based on this information, Avant-Garde accesses data in a scaled numeric\
    \ format and flattens it accordingly without requiring additional manual configuration.\n\
    \nTo transform data from a scaled numeric format into flattened blocks, the Avant-Garde\
    \ API provides the function. In the example shown in Figure [9,](#page-6-1) this\
    \ function takes pointers to tensors in the MX9 format. Avant-Garde applies the\
    \ function to generate flattened blocks, invoking it twice: once for the weights\
    \ and once for the inputs.\n\nAvant-Garde API in GPU kernels is compiled into\
    \ a SASS instruction stream that includes newly defined instructions. To perform\
    \ operand flattening, we define FLAT instruction, which transforms multi-level\
    \ scaled numeric formats into a flattened format. Also, we define Flattened MMA\
    \ (FMMA) instruction, an extension of the WMMA instruction for scaled numeric\
    \ formats. For example, the .16816.9.9 instruction performs matrix-matrix multiplication\
    \ using flattened blocks generated from MX9 formats. The 16816 suffix denotes\
    \ the tile dimensions used in the operation, where 16 × 16 represents the output\
    \ tile size and 8 × 16 indicates the shape of the input operands. This suffix\
    \ is the same as that used in existing WMMA instructions.\n\nUsers can specify\
    \ a scaled numeric format while declaring fragments, enabling the extended wmma\
    \ functions to correctly interpret and process the data. The :: \\_ function supports\
    \ two modes of MMA operations. It can either (1) convert the Tensor Core's floating-point\
    \ result into a flattened format, or (2) retain it in floating point. Users can\
    \ configure the mode depending on whether a subsequent layer in a DNN model requires\
    \ flattened blocks or floating-point values. If :: \\_ (or :: \\_) is selected,\
    \ the result remains in floating point. In this case, the output of a single MMA\
    \ operation on Avant-Garde's Tensor Core is a floating-point value, which is directly\
    \ written to the register file. In contrast, if :: is selected, the floating-point\
    \ results are converted into a flattened block once Avant-Garde's Tensor Cores\
    \ generate a predetermined number of them. The :: \\_ function additionally issues\
    \ instructions, which take the floating-point results from the Tensor Core and\
    \ transform it into the flattened format. In this way, Avant-Garde produces the\
    \ outputs of MMA operations in the flattened format without incurring significant\
    \ overhead.\n\nDuring training, Avant-Garde performs unflattening operations that\
    \ reconstruct the predefined original scaled numeric formats from flattened blocks.\
    \ We design an \"unflattening\" API that converts the updated weights into the\
    \ scaled numeric format and stores them in memory. The unflattening API first\
    \ reorganizes the elements by splitting or coalescing flattened blocks to match\
    \ the specified block size. For formats with two or more scaling levels, the API\
    \ groups elements into subsets, selects appropriate scaling factors based on the\
    \ maximum magnitude, and scales the elements accordingly. It recursively applies\
    \ this scaling process to generate multiple levels of scaling factors until the\
    \ format specification is fully satisfied. Finally, the API packs the scaling\
    \ factors and elements and writes them back to memory.\n\nThe unflattening API\
    \ leverages CUDA cores to perform data transformation. CUDA cores operate on operands\
    \ in flattened format stored in the register file. Since each element in a flattened\
    \ block is represented in fixed-point format, extracting the exponent and applying\
    \ scaling operations can be implemented in a straightforward manner. In this manner,\
    \ Avant-Garde transforms and stores flattened data in a scaled numeric format,\
    \ thereby reducing memory usage while minimizing precision loss. As these operations\
    \ are performed on CUDA cores, they introduce a long latency. However, since unflattening\
    \ occurs infrequently, its overhead has minimal impact on overall GPU performance.\n\
    \n#### Table 1: Simulation configurations\n\n<span id=\"page-7-1\"></span>\n\n\
    | GPU           | NVIDIA Hopper (H100), 114 SMs    |  |  |\n|---------------|----------------------------------|--|--|\n\
    |               | max 64 warps/SM, max 32 block/SM |  |  |\n| Scheduler     |\
    \ 4 warp schedulers/SM, GTO        |  |  |\n| L1            | 192 KB         \
    \                  |  |  |\n| L2            | 40 MB, 32 way                  \
    \  |  |  |\n| Register File | 256 KB, 8 banks per SM           |  |  |\n\nTable\
    \ 2: Numeric formats used in evaluation\n\n<span id=\"page-7-3\"></span>\n\n|\
    \        | Scaling Level 1 |         |       | Scaling Level 2 |             \
    \      |\n|--------|-----------------|---------|-------|-----------------|-------------------|\n\
    | Format | Block           | Scaling | Block | Scaling         | Element     \
    \      |\n|        | Size            | Factor  | Size  | Factor          |   \
    \                |\n| HBFP   | 64              | 8 bits  | N/A   | N/A       \
    \      | 8-bit fixed point |\n| MX9    | 16              | 8 bits  | 2     | 1\
    \ bit           | 8-bit fixed point |\n| MXFP8  | 32              | 8 bits  |\
    \ N/A   | N/A             | FP8               |\n\n#### <span id=\"page-7-2\"\
    ></span>3.3 Silicon Overhead\n\nWe analyze the microarchitecture overhead introduced\
    \ by the added components in Avant-Garde. We synthesize Operand Transformer and\
    \ Avant-Garde's Tensor Core using FreePDK 45nm technology to evaluate area and\
    \ power overhead [\\[44\\]](#page-12-21). We set the area and power consumption\
    \ of an H100 GPU as the baseline. Operand Transformer integrates logic for scaling\
    \ factor alignment and flattened block creation. The Operand Transform stage incorporates\
    \ 16 FP8/INT8 multipliers and temporary registers for intermediate value storage.\
    \ This stage adds 1.2% area and 1.7% power overhead relative to a standard SM\
    \ and introduces a latency impact of two cycles per warp due to iterative flattening\
    \ for multi-level formats. Avant-Garde's Tensor Core applies combined scaling\
    \ factors to the output of the adder tree before accumulation. The redesigned\
    \ Tensor Core integrates logic for shared scaling factors and a scaling unit to\
    \ support scaled numeric formats, resulting in 3.9% area overhead and 3.1% power\
    \ overhead compared to the standard Tensor Core.\n\nThe overall microarchitecture\
    \ overhead integrating Operand Transformer and the redesigned Tensor Core amounts\
    \ to roughly 1.4% in area and 1.2% in power compared to a conventional GPU pipeline.\
    \ Of the 1.2% power overhead, leakage power contributes only 0.1% to the total\
    \ power while dynamic power accounts for roughly 1.1%. This overhead is offset\
    \ by significant gains in arithmetic density and computational efficiency enabled\
    \ by Avant-Garde.\n\n#### <span id=\"page-7-0\"></span>4 Methodology\n\nWe model\
    \ Avant-Garde with Accel-Sim [\\[21\\]](#page-12-22). The detailed specifications\
    \ of our baseline are described in Table [1.](#page-7-1) We use the NVIDIA H100\
    \ GPU as our baseline, which supports FP8 [\\[33\\]](#page-12-23). Aligning with\
    \ NVIDIA's approach, we assume that Avant-Garde supports FP8 in the same manner\
    \ as the baseline. As Accel-Sim does not support FP8, we modify the simulator\
    \ to compute a scaling factor so that FP8 operations execute with the same latency\
    \ as INT8. As both formats have the same 8-bit data width, they exhibit identical\
    \ memory access patterns and storage characteristics. We also model the\n\nAvant-Garde:\
    \ Empowering GPUs with Scaled Numeric Formats ISCA '25, June 21–25, 2025, Tokyo,\
    \ Japan **0.5 NormalizedThroughput**\n\n**Normalized**\n\n**μbench ViT-B ViT-L\
    \ BERT GPT2 HM**\n\n<span id=\"page-8-2\"></span>![](_page_8_Figure_2.jpeg)\n\n\
    **Baseline Avant-Garde**\n\n**μbench ViT-B ViT-L BERT GPT2 HM**\n\n**1 1.5 2 1\
    \ 1.5 2** Figure 10: Throughput comparison. All results are normalized to the\
    \ baseline. ViT-B and ViT-L represent ViT-Base and ViT-Large, respectively. HM\
    \ represents the harmonic mean of all workloads.\n\n**0 0.5**\n\n**0 0.5**\n\n\
    **0**\n\n**1 1.5 2 2.5**\n\n<span id=\"page-8-1\"></span>\n\n| Benchmark     \
    \              | Parameters | Dataset        |\n|-----------------------------|------------|----------------|\n\
    | Microbenchmark              | 1M         | N/A            |\n| Vision Transformer\
    \          |            | ImageNet       |\n| Base (ViT-Base)             | 86M\
    \        |                |\n| Vision Transformer          | 307M       | ImageNet\
    \       |\n| Large (ViT-Large)           |            |                |\n| Bert-base-uncased\
    \           |            | English        |\n| (BERT)                      | 110M\
    \       | Wikipedia Text |\n| Generative Pre-trained      |            | English\
    \        |\n| Transformer 2 Small (GPT-2) | 124M       | Wikipedia Text |\n\n\
    Table 3: Benchmarks\n\nlatency and throughput of Tensor Cores and the microarchitecture\
    \ to account for FP8 execution in our simulator. Note that the core behavior of\
    \ Avant-Garde is orthogonal to the specific features of any particular GPU.\n\n\
    For power modeling, we extend AccelWattch to include FP8 specific power characteristics\
    \ by scaling the power values of INT8 Tensor Core operations, given their similar\
    \ compute units and memory access patterns. Also, we account for FP8 instruction-level\
    \ power overhead based on our analysis in Section [3.3.](#page-7-2)\n\nWe evaluate\
    \ the performance of Avant-Garde using three scaled numeric formats: HBFP, MX9,\
    \ and MXFP8. For HBFP, we use an 8-bit mantissa and a block size of 64 [\\[14\\\
    ]](#page-11-4). We use MX-compliant formats provided by prior work and the OCP\
    \ Specification, including MX9, MXFP8 [\\[2,](#page-11-11) [42\\]](#page-12-5).\
    \ The detailed configuration of the scaling factor is provided in Table [2.](#page-7-3)\n\
    \nWe list the benchmarks in Table [3.](#page-8-1) To evaluate the performance\
    \ of Avant-Garde, we implement two types of workloads. We implement a matrix multiplication\
    \ microbenchmark with 1M parameters. Although the microbenchmark cannot perform\
    \ inference, it closely resembles a real DNN model, and the majority of its operations\
    \ are MMA operations. Along with the microbenchmark, we use four DNN models: Vision\
    \ Transformer (ViT-Base, ViT-Large) [\\[7\\]](#page-11-15), BERT[\\[6\\]](#page-11-16),\
    \ and GPT-2 [\\[38\\]](#page-12-24). We use the ImageNet dataset [\\[5\\]](#page-11-17)\
    \ for ViT models, and English Wikipedia [\\[11\\]](#page-11-18) for BERT and GPT2.\n\
    \nIn the baseline, we implement a DNN model that handles the scaling factor in\
    \ software to support the scaled numeric formats.\n\n**μbench ViT-B ViT-L BERT\
    \ GPT2 HM Benchmark μbench ViT-B ViT-L BERT GPT2 HM Benchmark** Using the WMMA\
    \ API, we perform matrix multiplication on Tensor Cores and incorporate CUDA instructions\
    \ to compute and apply the scaling factor. We then embed the functionalities of\
    \ the Avant-Garde API in Accel-Sim and evaluate the GPU behaviors.\n\n**0 0.5\
    \ 1 1.5 2 2.5**\n\n**3.0**\n\n### <span id=\"page-8-0\"></span>5 Evaluation\n\n\
    #### 5.1 Performance Comparison\n\nWe first evaluate Avant-Garde's throughput\
    \ across all configurations, defining throughput as the number of operations per\
    \ clock cycle. We run a microbenchmark and a single inference pass of each DNN\
    \ model.\n\nFigure [10](#page-8-2) shows the experimental results. Avant-Garde\
    \ achieves a throughput improvement ranging from 1.65× to 1.93× compared to the\
    \ baseline, with an overall improvement of 1.74× (harmonic mean). This gain is\
    \ primarily due to a reduced instruction count. Tensor Cores of the baseline do\
    \ not support applying a scaling factor to the dot product results. Therefore,\
    \ while performing matrix multiplication with single-level formats on the baseline,\
    \ additional instructions from CUDA Cores are required for every MMA operation\
    \ on Tensor Cores. Avant-Garde processes scaled formats efficiently by applying\
    \ the scaling factor directly to the dot product results and accumulating it within\
    \ Avant-Garde's Tensor Cores, without the need for additional instructions.\n\n\
    For both HBFP and MXFP8, Avant-Garde achieves a 1.66 × higher harmonic mean throughput\
    \ than the baseline in DNN workloads. HBFP and MXFP8 leverage the INT8 and FP8\
    \ engines of Tensor Cores, respectively, as their elements are represented in\
    \ INT8 and FP8 formats. On the H100 GPU, these two engines provide equivalent\
    \ throughput, resulting in identical performance improvements across the two formats.\n\
    \nFor MX9, Avant-Garde achieves a 1.93 × higher harmonic mean throughput than\
    \ the baseline in DNN workloads. In the baseline, computing two-level formats\
    \ requires additional software overhead. Avant-Garde eliminates this overhead\
    \ by introducing the Operand Transform stage. By removing the overhead of two-level\
    \ scaling format conversion and matrix multiplication on the flattened format,\
    \ Avant-Garde achieves its highest throughput with MX9.\n\nThe throughput improvement\
    \ of Avant-Garde slightly decreases as model size increases. For ViT-Base (ViT-B),\
    \ the average performance improvement is 6% higher than for ViT-Large (ViT-L)\
    \ across\n\n<span id=\"page-9-0\"></span>ISCA '25, June 21–25, 2025, Tokyo, Japan\
    \ Minseong Gil, Dongho Ha, Simla Burcu Harma, Myung Kuk Yoon, Babak Falsafi, Won\
    \ Woo Ro, and Yunho Oh **0**\n\n**0.2**\n\n**0.2 0.4 0.6 0.8 1**\n\n> **μbench\
    \ ViT-B ViT-L BERT GPT2 HM Benchmark**\n\n![](_page_9_Figure_2.jpeg)\n\n**μbench\
    \ ViT-B ViT-L BERT GPT2 HM**\n\n**Avant-Garde**\n\n**0.4 0.6 0.8 0.4 0.6 0.8**\
    \ Figure 11: Execution time comparison. In DNN workloads, execution time corresponds\
    \ to inference time. All results are normalized to the baseline. ViT-B and ViT-L\
    \ represent ViT-Base and ViT-Large, respectively. HM represents the harmonic mean\
    \ of all workloads. **0 HBFP MX9 MXFP8 Numeric Format**\n\n**0.2**\n\n**0.2**\n\
    \n**1**\n\n**1**\n\n**0.2 0.4 0.6 0.8 1**\n\n**Normalized**\n\n**Execution Time**\n\
    \n<span id=\"page-9-1\"></span>![](_page_9_Figure_4.jpeg)\n\nFigure 12: Instruction\
    \ count comparison. All results are normalized to the baseline.\n\nall three numeric\
    \ formats. Larger models require more frequent memory accesses than small models,\
    \ and the operand flattening mechanism may introduce additional accesses for scaling\
    \ factors. This behavior could slightly offset the benefits of operand transformation,\
    \ leading to diminishing returns in larger models.\n\n#### 5.2 Execution Time\n\
    \nWe compare the execution time of all the configurations. Figure [11](#page-9-0)\
    \ shows the experimental results. In case of microbenchmark, the reported values\
    \ indicate the inference time for a single request. The improvements in throughput\
    \ directly translate to reduced execution time. In DNN models, Avant-Garde reduces\
    \ the harmonic mean of DNN execution time by 44%. Also, as mentioned in Section\
    \ [5,](#page-8-0) performance gains slightly diminish with increasing model size.\
    \ In our experiments, Avant-Garde reduces the execution time of ViT-L by 48% while\
    \ it reduces that of ViT-B by 51%.\n\nAvant-Garde achieves a significant reduction\
    \ in execution time for the microbenchmark, ranging from 55% to 67%. This reduction\
    \ in execution time stems from Avant-Garde's ability to efficiently mitigate the\
    \ overhead introduced by MMA operations involving scaled numeric formats on Tensor\
    \ Cores. Avant-Garde reduces the overhead associated with applying scaling factors\
    \ in MMA operations and achieves its best performance in the microbenchmark.\n\
    \n# 5.3 Instruction Count Comparison\n\nWe measure the number of instructions\
    \ executed by the baseline and Avant-Garde using the traditional, single-level,\
    \ and two-level\n\n<span id=\"page-9-2\"></span>![](_page_9_Figure_12.jpeg)\n\n\
    Figure 13: Energy consumption comparison while running DNN models with scaled\
    \ numeric formats. All results are normalized to the baseline.\n\nformats. We\
    \ use INT8 as the baseline traditional numeric format. Figure [12](#page-9-1)\
    \ presents the number of instructions executed by Avant-Garde. We observe that\
    \ HBFP and MXFP8 exhibit similar trends in the instruction count. Therefore, we\
    \ report their average under the label \"One Level\". Likewise, the results for\
    \ MX9 are denoted as \"Two Levels\" to maintain consistent representation across\
    \ experiments. For the traditional format, the baseline and Avant-Garde execute\
    \ the same number of instructions. However, in DNN workloads using the single-level\
    \ and two-level formats, compared to the baseline, Avant-Garde reduces the number\
    \ of executed instructions by 52.2% and 65.7%, respectively. This trend is consistently\
    \ observed across all benchmarks.\n\nBy natively supporting scaled numeric formats\
    \ and integrating Operand Transformer, Avant-Garde eliminates the need for software-based\
    \ adjustments that rely on extra instructions executed by CUDA Cores. This reduction\
    \ in instruction count is particularly pronounced for two-level formats compared\
    \ to single-level formats, as Operand Transformer absorbs the second-level scaling\
    \ factors into the element values. This process eliminates the need for separate\
    \ instructions to handle the second-level scaling factors.\n\n# 5.4 Energy Consumption\n\
    \nWe measure the energy consumption of DNN models using scaled numeric formats.\
    \ Figure [13](#page-9-2) presents the harmonic mean of energy consumption across\
    \ various DNN models. For all formats except FP8, Avant-Garde achieves a reduction\
    \ in energy consumption. As analyzed in Section [3.3,](#page-7-2) Avant-Garde\
    \ consumes 1.2% more power\n\n<span id=\"page-10-2\"></span>Table 4: Accuracy\
    \ comparison of FP32 and MX9 (Flattened and Not Flattened). For ViT, higher accuracy\
    \ is better. For BERT and GPT-2, lower perplexity is better.\n\n| Model      \
    \            | FP32  | MX9<br>(Flattened) | MX9<br>(Non-Flattend) |\n|------------------------|-------|--------------------|-----------------------|\n\
    | ViT-Base (Accuracy, %) | 80.3  | 80.3               | 80.3                 \
    \ |\n| Bert (Perplexity)      | 10.09 | 10.11              | 10.10           \
    \      |\n| GPT-2 (Perplexity)     | 17.48 | 17.49              | 17.49      \
    \           |\n\nthan the baseline. Although the power consumption of Avant-Garde\
    \ slightly increases compared to the baseline, the execution time is reduced for\
    \ scaled formats, resulting in lower energy consumption. Avant-Garde achieves\
    \ energy consumption reductions of 40%, 49%, and 40% for the HBFP, MX9, and MXFP8\
    \ formats, respectively. Among all formats, MX9 achieves the greatest reduction\
    \ in energy consumption, as MX9 effectively removes instructions for handling\
    \ two levels of scaling. Such behavior results in the most significant reduction\
    \ in execution time.\n\nFor FP8-based workloads, the additional hardware in Avant-Garde\
    \ remains inactive, and thus does not contribute to dynamic power consumption.\
    \ Compared to the baseline, Avant-Garde consumes only 0.1% more energy, primarily\
    \ due to a small amount of leakage power from the additional hardware resources,\
    \ as discussed in Section [3.3.](#page-7-2) As such, Avant-Garde does not incur\
    \ unnecessary energy consumption while handling FP8.\n\n#### 5.5 Accuracy Impact\
    \ of Operand Flattening\n\nFor single-level formats, Avant-Garde can compute without\
    \ requiring operand flattening, allowing lossless computation. As a result, it\
    \ maintains the same accuracy as software implementations as discussed in Section\
    \ [2.2.](#page-2-2) However, flattening a multi-level format into a single-level\
    \ representation may introduce additional quantization error. To evaluate the\
    \ impact of this flattening on model accuracy, we conduct functional simulations\
    \ of inference using a two-level format within Avant-Garde and compare the accuracy\
    \ results to FP32. Our methodology involves emulating Avant-Garde by modifying\
    \ Microsoft's MX emulator to capture the numerical behavior of the flattened format\
    \ [\\[31\\]](#page-12-25). For comparison, we also measure the inference accuracy\
    \ of the original (non-flattened) MX9 format using the same emulator.\n\nTable\
    \ [4](#page-10-2) presents the accuracy results for FP32, flattened MX9, and original\
    \ MX9 across ViT-Base, BERT, and GPT-2 models. All representations achieve nearly\
    \ identical accuracies. The flattened MX9 format achieves the same accuracy as\
    \ the non-flattened MX9 and shows less than a 0.2% difference compared to FP32.\
    \ These results demonstrate that Avant-Garde maintains accuracy parity with FP32,\
    \ demonstrating no significant degradation due to the flattening of the multi-level\
    \ format. Operand transformation introduces no significant loss in precision,\
    \ confirming that Avant-Garde is well-suited for accurate, low-bitwidth inference.\n\
    \n#### <span id=\"page-10-1\"></span>5.6 Sensitivity Study\n\nWe measure the execution\
    \ time while running DNN workloads to evaluate the scalability of Avant-Garde\
    \ across various scaling levels and block sizes of scaled numeric formats. For\
    \ the experiments, we use ViT-Large as the target model and set the baseline as\
    \ inference using a single-level format with 8-bit elements and a block size of\
    \ 32. All configurations are based on hypothetical numeric formats designed for\
    \ evaluation purposes. To analyze the effect of scaling levels, we fix the block\
    \ size at 32 and experiment with formats up to four scaling levels, using a subset\
    \ size of 2. To evaluate sensitivity to block size, we maintain a single-level\
    \ format while increasing the block size from 32 to 512.\n\nIn all cases, operand\
    \ transformation remains a preprocessing step, completing flattening with a low\
    \ latency compared to GEMM operations and requiring only minor memory accesses.\
    \ The latency caused by these operations is often hidden by interleaved warp execution,\
    \ minimizing its performance impact. As a result, operand transformation accounts\
    \ for less than 1% of total execution time. As the overall performance across\
    \ configurations shows minimal variation, we omit a plot for this analysis. The\
    \ highest execution time increase occurs if the block size is 512, yet even in\
    \ this case, the execution time increases by only 1.1% relative to the baseline.\
    \ These results show that Avant-Garde continues to provide significant performance\
    \ improvements over conventional GPUs, regardless of the specific scaling levels\
    \ or block sizes used.\n\n#### <span id=\"page-10-0\"></span>6 Related Work\n\n\
    Low-Precision and Scaled Numeric Formats. Quantization has been employed for inference\
    \ to create lightweight DNN models that have parameters with narrow bitwidth formats.\
    \ Using quantization methods, DNN model parameters can be compressed to formats\
    \ as small as INT4 [\\[1,](#page-11-8) [17,](#page-11-9) [52\\]](#page-12-11).\
    \ Not only for inference but also for training, a value-aware quantization technique\
    \ has been proposed [\\[36\\]](#page-12-13). Prior work has explored various quantization\
    \ methods for training, including specialized methods for INT8 [\\[51\\]](#page-12-14)\
    \ and Piecewise Fixed Point to mitigate quantization bias in low-precision training\
    \ [\\[28\\]](#page-12-12). Prior work has introduced a mixed-precision training\
    \ method that utilizes FP4 (1-bit sign, 3-bit exponent, and 0-bit mantissa) [\\\
    [45\\]](#page-12-26), representing the narrowest bitwidth format in DNN training.\n\
    \nScaled numeric formats enhance efficiency by sharing scaling factors across\
    \ blocks of values. HBFP combines the high accuracy of floating-point representations\
    \ with the efficiency of block floatingpoint arithmetic [\\[9,](#page-11-2) [14\\\
    ]](#page-11-4). MSFP achieves 4× higher arithmetic density compared to INT8 by\
    \ sharing exponents within bounding boxes, preserving accuracy with minimal fine-tuning\
    \ [\\[4,](#page-11-10) [41\\]](#page-12-4) The subsequent microscaling builds\
    \ upon shared microexponent by incorporating ultra-fine scaling factors at sub-block\
    \ levels, enabling sub-8-bit precision for efficient training and inference [\\\
    [42\\]](#page-12-5). These formats balance computational efficiency and accuracy,\
    \ making them essential for DNNs.\n\nAccelerators for Scaled Numeric Formats.\
    \ Accelerators designed for scaled numeric formats are reshaping DNN performance\
    \ paradigms. In particular, prior work has focused on DNN accelerators leveraging\
    \ BFP formats, which fall under single-level scaled numeric formats.\n\nThe dynamic\
    \ block size and precision scaling (DBPS) core exemplifies the potential of dynamic\
    \ scaling by enabling adjustments to block size and precision during training\
    \ [\\[26\\]](#page-12-7). Similarly, MSFP-based\n\narchitectures leverage shared\
    \ exponents to optimize hardware efficiency, significantly improving arithmetic\
    \ density and energy usage [\\[41\\]](#page-12-4). FAST introduces a novel processing\
    \ element that performs dot product operations with variable precision at the\
    \ layer level [\\[50\\]](#page-12-8). Bucket Getter addresses power and area overheads\
    \ in BFP processing engines caused by floating-point adders [\\[29\\]](#page-12-27).\
    \ MXbased accelerators extend these benefits by supporting ultra-low precision\
    \ formats for diverse DNN tasks [\\[42\\]](#page-12-5). Avant-Garde builds on\
    \ these concepts by introducing a GPU microarchitecture that flattens multi-level\
    \ scaled numeric formats into single-level formats, maximizing arithmetic density\
    \ and energy efficiency for both training and inference workloads.\n\nGPU Software\
    \ and Architectures for DNNs. Prior work proposed various methods that use GPU-based\
    \ systems efficiently [\\[3,](#page-11-19) [20,](#page-12-19) [27,](#page-12-28)\
    \ [37,](#page-12-29) [47,](#page-12-30) [49\\]](#page-12-31). Those methods are\
    \ implemented in software and are supposed to utilize existing hardware resources\
    \ in GPUs while running DNN wokloads. Kim et al. proposed a GPU architecture for\
    \ DNN workloads called Duplo [\\[22\\]](#page-12-32). Duplo includes a microarchitecture\
    \ design inspired by hardware memoization [\\[23\\]](#page-12-33). With this scheme,\
    \ Duplo could reduce the total computation count by exploiting fixed computation\
    \ patterns and reusing the calculation results. While effective, prior work does\
    \ not focus on reducing energy consumption further by designing a new numeric\
    \ format. As such, they are orthogonal to Avant-Garde, which focuses on improving\
    \ arithmetic density. We believe that the methods can be combined with Avant-Garde.\n\
    \n#### <span id=\"page-11-5\"></span>7 Conclusion\n\nIn this paper, we address\
    \ the challenges of executing computations with scaled numeric formats in GPUs.\
    \ These challenges stem from the lack of native GPU support for handling scaling\
    \ factors, which requires software-based adjustments. This reliance increases\
    \ register usage, introduces additional instructions during DNN workload execution.\
    \ To address these issues, we propose Avant-Garde, an innovative GPU architecture\
    \ designed specifically for efficient processing of scaled numeric formats. Avant-Garde\
    \ introduces two primary innovations. The Avant-Garde microarchitecture incorporates\
    \ a dedicated pipeline stage and enhanced Tensor Cores capable of natively handling\
    \ scaling factors, streamlining MMA operations, and eliminating the software overhead\
    \ typically associated with scaling factor adjustments. The Avant-Garde API simplifies\
    \ developer interaction with the hardware, enabling easy integration while fully\
    \ leveraging its capability. Avant-Garde significantly improves throughput and\
    \ reduces execution time for DNN workloads compared to conventional GPUs.\n\n\
    #### Acknowledgments\n\nThe authors thank the anonymous reviewers, the members\
    \ of Com-Sys at Korea University, and the members of PARSA at EPFL for their precious\
    \ comments and feedback. This work was supported in part by the National Research\
    \ Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (NRF-2022R1C1C1011021,\
    \ RS-2024-00357037, RS-2025-00553645), a Microsoft Research PhD Fellowship, and\
    \ \"Unified Accelerators for Post-Moore Machine Learning\" (200021\\_212757) from\
    \ the Swiss National Science Foundation (SNSF). Babak Falsafi, Won Woo Ro, and\
    \ Yunho Oh are the co-corresponding authors.\n\n#### References\n\n- <span id=\"\
    page-11-8\"></span>[1] Ron Banner, Yury Nahshan, and Daniel Soudry. 2019. Post\
    \ Training 4-Bit Quantization of Convolutional Networks for Rapid-Deployment.\
    \ In Proceedings of the 33rd International Conference on Neural Information Processing\
    \ Systems (NIPS). Curran Associates Inc., Red Hook, NY, USA, Article 714, 9 pages.\n\
    - <span id=\"page-11-11\"></span>[2] Tom Savell Ankit More Kyung-Nam Han Ritchie\
    \ Zhao Mathew Hall Jasmine Klar Eric Chung Yuan Yu Michael Schulte Ralph Wittig\
    \ Ian Bratt Nigel Stephens Jelena Milanovic John Brothers Pradeep Dubey Marius\
    \ Cornea Alexander Heinecke Andres Rodriguez Martin Langhammer Summer Deng Maxim\
    \ Naumov Paulius Micikevicius Michael Siu Bita Darvish Rouhani, Nitin Garegrat\
    \ and Colin Verrilli. 2023. OCP Microscaling Formats (MX) Specification. [https://www.opencompute.](https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf)\
    \ [org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf.](https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf)\n\
    - <span id=\"page-11-19\"></span>[3] Shubham Chaudhary, Ramachandran Ramjee, Muthian\
    \ Sivathanu, Nipun Kwatra, and Srinidhi Viswanatha. 2020. Balancing Efficiency\
    \ and Fairness in Heterogeneous GPU Clusters for Deep Learning. In Proceedings\
    \ of the Fifteenth European Conference on Computer Systems (Heraklion, Greece)\
    \ (EuroSys '20). Association for Computing Machinery, New York, NY, USA, Article\
    \ 1, 16 pages.\n- <span id=\"page-11-10\"></span>[4] Bita Darvish Rouhani, Ritchie\
    \ Zhao, Venmugil Elango, Rasoul Shafipour, Mathew Hall, Maral Mesmakhosroshahi,\
    \ Ankit More, Levi Melnick, Maximilian Golub, Girish Varatkar, Lai Shao, Gaurav\
    \ Kolhe, Dimitry Melts, Jasmine Klar, Renee L'Heureux, Matt Perry, Doug Burger,\
    \ Eric Chung, Zhaoxia (Summer) Deng, Sam Naghshineh, Jongsoo Park, and Maxim Naumov.\
    \ 2023. With Shared Microexponents, A Little Shifting Goes a Long Way. In Proceedings\
    \ of the 50th Annual International Symposium on Computer Architecture (Orlando,\
    \ FL, USA) (ISCA '23). Association for Computing Machinery, New York, NY, USA,\
    \ Article 83, 13 pages.\n- <span id=\"page-11-17\"></span>[5] Jia Deng, Wei Dong,\
    \ Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. ImageNet: A large-scale\
    \ hierarchical image database. In 2009 IEEE Conference on Computer Vision and\
    \ Pattern Recognition. 248–255.\n- <span id=\"page-11-16\"></span>[6] Jacob Devlin,\
    \ Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: Pre-training\
    \ of Deep Bidirectional Transformers for Language Understanding. CoRR abs/1810.04805\
    \ (2018). arXiv[:1810.04805](https://arxiv.org/abs/1810.04805) <http://arxiv.org/abs/1810.04805>\n\
    - <span id=\"page-11-15\"></span>[7] Alexey Dosovitskiy, Lucas Beyer, Alexander\
    \ Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani,\
    \ Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.\
    \ 2021. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.\
    \ arXiv[:2010.11929](https://arxiv.org/abs/2010.11929) [cs.CV]\n- <span id=\"\
    page-11-1\"></span>[8] Mario Drumond, Louis Coulon, Arash Pourhabibi, Ahmet Caner\
    \ Yüzügüler, Babak Falsafi, and Martin Jaggi. 2021. Equinox: Training (for Free)\
    \ on a Custom Inference Accelerator. In MICRO-54: 54th Annual IEEE/ACM International\
    \ Symposium on Microarchitecture (Virtual Event, Greece) (MICRO '21). Association\
    \ for Computing Machinery, New York, NY, USA, 421–433.\n- <span id=\"page-11-2\"\
    ></span>[9] Mario Drumond, Tao LIN, Martin Jaggi, and Babak Falsafi. 2018. Training\
    \ DNNs with Hybrid Block Floating Point. In Advances in Neural Information Processing\
    \ Systems (NIPS), Vol. 31. Curran Associates, Inc.\n- <span id=\"page-11-0\"></span>[10]\
    \ Luciano Floridi and Massimo Chiriatti. 2020. GPT-3: Its nature, scope, limits,\
    \ and consequences. Minds and Machines 30, 4 (2020), 681–694.\n- <span id=\"page-11-18\"\
    ></span>[11] Wikimedia Foundation. [n. d.]. Toggle the table of contents English\
    \ Wikipedia. [https://en.wikipedia.org/wiki/English\\\\_Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia)\n\
    - <span id=\"page-11-3\"></span>[12] Marcelo Gennari do Nascimento, Victor Adrian\
    \ Prisacariu, Roger Fawcett, and Martin Langhammer. 2023. HyperBlock Floating\
    \ Point: Generalised Quantization Scheme for Gradient and Inference Computation.\
    \ In 2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV).\
    \ 6353–6362.\n- <span id=\"page-11-6\"></span>[13] Song Han, Xingyu Liu, Huizi\
    \ Mao, Jing Pu, Ardavan Pedram, Mark A. Horowitz, and William J. Dally. 2016.\
    \ EIE: Efficient Inference Engine on Compressed Deep Neural Network. In 43rd ACM/IEEE\
    \ Annual International Symposium on Computer Architecture, ISCA 2016, Seoul, South\
    \ Korea, June 18-22, 2016. IEEE Computer Society, 243–254.\n- <span id=\"page-11-4\"\
    ></span>[14] Simla Burcu Harma, Ayan Chakraborty, Nicholas Sperry, Babak Falsafi,\
    \ Martin Jaggi, and Yunho Oh. 2022. Accuracy Booster: Enabling 4-bit Fixed-point\
    \ Arithmetic for DNN Training. arXiv[:2211.10737](https://arxiv.org/abs/2211.10737)\
    \ [cs.LG]\n- <span id=\"page-11-12\"></span>[15] Intel. [n. d.]. Intel neural\
    \ compressor. [https://intel.github.io/neural-compressor.](https://intel.github.io/neural-compressor)\
    \ Accessed: 2025-02-16.\n- <span id=\"page-11-13\"></span>[16] Intel. [n. d.].\
    \ OpenVINO: Open-source software toolkit for optimizing and deploying deep learning\
    \ models. [https://github.com/openvinotoolkit/openvino.](https://github.com/openvinotoolkit/openvino)\
    \ Accessed: 2025-02-16.\n- <span id=\"page-11-9\"></span>[17] Benoit Jacob, Skirmantas\
    \ Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and\
    \ Dmitry Kalenichenko. 2018. Quantization and Training of Neural Networks for\
    \ Efficient Integer-Arithmetic-Only Inference. In Proceedings of the IEEE Conference\
    \ on Computer Vision and Pattern Recognition (CVPR).\n- <span id=\"page-11-14\"\
    ></span>[18] Wonsuk Jang and Thierry Tambe. 2025. BlockDialect: Block-wise Finegrained\
    \ Mixed Format Quantization for Energy-Efficient LLM Inference. arXiv[:2501.01144](https://arxiv.org/abs/2501.01144)\
    \ [cs.CL] <https://arxiv.org/abs/2501.01144>\n- <span id=\"page-11-7\"></span>[19]\
    \ Anand Jayarajan, Jinliang Wei, Garth Gibson, Alexandra Fedorova, and Gennady\
    \ Pekhimenko. 2019. Priority-based Parameter Propagation for Distributed DNN\n\
    \n<span id=\"page-12-0\"></span>Training. In Proceedings of Machine Learning and\
    \ Systems (MLSys), Vol. 1. 132– 145.\n\n- <span id=\"page-12-19\"></span>[20]\
    \ Jaebeom Jeon, Minseong Gil, Junsu Kim, Jaeyong Park, Gunjae Koo, Myung Kuk Yoon,\
    \ and Yunho Oh. 2024. VitBit: Enhancing Embedded GPU Performance for AI Workloads\
    \ through Register Operand Packing. In Proceedings of the 53rd International Conference\
    \ on Parallel Processing. 1012–1021.\n- <span id=\"page-12-22\"></span>[21] Mahmoud\
    \ Khairy, Zhesheng Shen, Tor M. Aamodt, and Timothy G. Rogers. 2020. Accel-Sim:\
    \ An Extensible Simulation Framework for Validated GPU Modeling. In 2020 ACM/IEEE\
    \ 47th Annual International Symposium on Computer Architecture (ISCA). 473–486.\n\
    - <span id=\"page-12-32\"></span>[22] Hyeonjin Kim, Sungwoo Ahn, Yunho Oh, Bogil\
    \ Kim, Won Woo Ro, and William J. Song. 2020. Duplo: Lifting Redundant Memory\
    \ Accesses of Deep Neural Networks for GPU Tensor Cores. In 2020 53rd Annual IEEE/ACM\
    \ International Symposium on Microarchitecture (MICRO). 725–737.\n- <span id=\"\
    page-12-33\"></span>[23] Keunsoo Kim and Won Woo Ro. 2018. WIR: Warp Instruction\
    \ Reuse to Minimize Repeated Computations in GPUs. In 2018 IEEE International\
    \ Symposium on High Performance Computer Architecture (HPCA). 389–402.\n- <span\
    \ id=\"page-12-15\"></span>[24] Jahyun Koo, Dahoon Park, Sangwoo Jung, and Jaeha\
    \ Kung. 2024. OPAL: Outlier-Preserved Microscaling Quantization Accelerator for\
    \ Generative Large Language Models. In Proceedings of the 61st ACM/IEEE Design\
    \ Automation Conference (San Francisco, CA, USA) (DAC '24). Association for Computing\
    \ Machinery, New York, NY, USA, Article 259, 6 pages.\n- <span id=\"page-12-3\"\
    ></span>[25] Urs Köster, Tristan Webb, Xin Wang, Marcel Nassar, Arjun K Bansal,\
    \ William Constable, Oguz Elibol, Scott Gray, Stewart Hall, Luke Hornof, Amir\
    \ Khosrowshahi, Carey Kloss, Ruby J Pai, and Naveen Rao. 2017. Flexpoint: An Adaptive\
    \ Numerical Format for Efficient Training of Deep Neural Networks. In Advances\
    \ in Neural Information Processing Systems (NIPS), Vol. 30. Curran Associates,\
    \ Inc.\n- <span id=\"page-12-7\"></span>[26] Seunghyun Lee, Jeik Choi, Seockhwan\
    \ Noh, Jahyun Koo, and Jaeha Kung. 2023. DBPS: Dynamic Block Size and Precision\
    \ Scaling for Efficient DNN Training Supported by RISC-V ISA Extensions. In 2023\
    \ 60th ACM/IEEE Design Automation Conference (DAC). 1–6.\n- <span id=\"page-12-28\"\
    ></span>[27] Gangmuk Lim, Jeongseob Ahn, Wencong Xiao, Youngjin Kwon, and Myeongjae\
    \ Jeon. 2021. Zico: Efficient GPU Memory Sharing for Concurrent DNN Training.\
    \ In 2021 USENIX Annual Technical Conference (USENIX ATC 21). USENIX Association,\
    \ 161–175. <https://www.usenix.org/conference/atc21/presentation/lim>\n- <span\
    \ id=\"page-12-12\"></span>[28] Chang Liu, Xishan Zhang, Rui Zhang, Ling Li, Shiyi\
    \ Zhou, Di Huang, Zhen Li, Zidong Du, Shaoli Liu, and Tianshi Chen. 2022. Rethinking\
    \ the Importance of Quantization Bias, Toward Full Low-Bit Training. IEEE Transactions\
    \ on Image Processing 31 (2022), 7006–7019.\n- <span id=\"page-12-27\"></span>[29]\
    \ Yun-Chen Lo and Ren-Shuo Liu. 2023. Bucket Getter: A Bucket-based Processing\
    \ Engine for Low-bit Block Floating Point (BFP) DNNs. In Proceedings of the 56th\
    \ Annual IEEE/ACM International Symposium on Microarchitecture. 1002–1015.\n-\
    \ <span id=\"page-12-6\"></span>[30] Paulius Micikevicius, Dusan Stosic, Neil\
    \ Burgess, Marius Cornea, Pradeep Dubey, Richard Grisenthwaite, Sangwon Ha, Alexander\
    \ Heinecke, Patrick Judd, John Kamalu, Naveen Mellempudi, Stuart Oberman, Mohammad\
    \ Shoeybi, Michael Siu, and Hao Wu. 2022. FP8 Formats for Deep Learning. arXiv[:2209.05433](https://arxiv.org/abs/2209.05433)\
    \ [cs.LG]\n- <span id=\"page-12-25\"></span>[31] Microsoft. 2024. MicroScaling\
    \ Emulator. [https://github.com/microsoft/](https://github.com/microsoft/microxcaling)\
    \ [microxcaling.](https://github.com/microsoft/microxcaling)\n- <span id=\"page-12-9\"\
    ></span>[32] Deepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, and Matei\
    \ Zaharia. 2021. Memory-Efficient Pipeline-Parallel DNN Training. In Proceedings\
    \ of the 38th International Conference on Machine Learning (Proceedings of Machine\
    \ Learning Research, Vol. 139). PMLR, 7937–7947.\n- <span id=\"page-12-23\"></span>[33]\
    \ NVIDIA. 2020. NVIDIA A100 Tensor Core GPU Architecture. v1.0 (2020), 1–82.\n\
    - <span id=\"page-12-18\"></span>[34] Yunho Oh, Myung Kuk Yoon, William J. Song,\
    \ and Won Woo Ro. 2018. FineReg: Fine-Grained Register File Management for Augmenting\
    \ GPU Throughput. In 2018 51st Annual IEEE/ACM International Symposium on Microarchitecture\
    \ (MI-CRO). 364–376.\n- <span id=\"page-12-1\"></span>[35] OpenAI. 2023. GPT-4\
    \ Technical Report. arXiv[:2303.08774](https://arxiv.org/abs/2303.08774) [cs.CL]\n\
    - <span id=\"page-12-13\"></span>[36] Eunhyeok Park, Sungjoo Yoo, and Peter Vajda.\
    \ 2018. Value-Aware Quantization for Training and Inference of Neural Networks.\
    \ In Computer Vision – ECCV 2018: 15th European Conference, Munich, Germany, September\
    \ 8-14, 2018, Proceedings, Part IV (Munich, Germany). Springer-Verlag, Berlin,\
    \ Heidelberg, 608–624.\n- <span id=\"page-12-29\"></span>[37] Jay H. Park, Gyeongchan\
    \ Yun, Chang M. Yi, Nguyen T. Nguyen, Seungmin Lee, Jaesik Choi, Sam H. Noh, and\
    \ Young ri Choi. 2020. HetPipe: Enabling Large DNN Training on (Whimpy) Heterogeneous\
    \ GPU Clusters through Integration of Pipelined Model Parallelism and Data Parallelism.\
    \ In 2020 USENIX Annual Technical Conference (USENIX ATC 20). USENIX Association,\
    \ 307–321. [https:](https://www.usenix.org/conference/atc20/presentation/park)\
    \ [//www.usenix.org/conference/atc20/presentation/park](https://www.usenix.org/conference/atc20/presentation/park)\n\
    - <span id=\"page-12-24\"></span>[38] Alec Radford, Jeffrey Wu, Rewon Child, David\
    \ Luan, Dario Amodei, and Ilya Sutskever. 2018. Language Models are Unsupervised\
    \ Multitask Learners. (2018).\n- <span id=\"page-12-16\"></span>[39] Akshat Ramachandran,\
    \ Souvik Kundu, and Tushar Krishna. 2024. MicroScopiQ: Accelerating Foundational\
    \ Models through Outlier-Aware Microscaling Quantization. arXiv[:2411.05282](https://arxiv.org/abs/2411.05282)\
    \ [cs.AR] <https://arxiv.org/abs/2411.05282>\n- <span id=\"page-12-2\"></span>[40]\
    \ MIT Technology Review. 2023. Multi-die systems define the future of semiconductors.\
    \ [https://www.technologyreview.com/2023/03/31/1070527/multi-die](https://www.technologyreview.com/2023/03/31/1070527/multi-die-systems-define-the-future-of-semiconductors/)[systems-define-the-future-of-semiconductors/.](https://www.technologyreview.com/2023/03/31/1070527/multi-die-systems-define-the-future-of-semiconductors/)\
    \ Accessed: 2023-03-31.\n- <span id=\"page-12-4\"></span>[41] Bita Rouhani, Daniel\
    \ Lo, Ritchie Zhao, Ming Liu, Jeremy Fowers, Kalin Ovtcharov, Anna Vinogradsky,\
    \ Sarah Massengill, Lita Yang, Ray Bittner, Alessandro Forin, Haishan Zhu, Taesik\
    \ Na, Prerak Patel, Shuai Che, Lok Chand Koppaka, Xia Song, Subhojit Som, Kaustav\
    \ Das, Saurabh Tiwary, Steve Reinhardt, Sitaram Lanka, Eric Chung, and Doug Burger.\
    \ 2020. Pushing the Limits of Narrow Precision Inferencing at Cloud Scale with\
    \ Microsoft Floating Point. In NeurIPS 2020. ACM.\n- <span id=\"page-12-5\"></span>[42]\
    \ Bita Darvish Rouhani, Ritchie Zhao, Ankit More, Mathew Hall, Alireza Khodamoradi,\
    \ Summer Deng, Dhruv Choudhary, Marius Cornea, Eric Dellinger, Kristof Denolf,\
    \ Stosic Dusan, Venmugil Elango, Maximilian Golub, Alexander Heinecke, Phil James-Roxby,\
    \ Dharmesh Jani, Gaurav Kolhe, Martin Langhammer, Ada Li, Levi Melnick, Maral\
    \ Mesmakhosroshahi, Andres Rodriguez, Michael Schulte, Rasoul Shafipour, Lei Shao,\
    \ Michael Siu, Pradeep Dubey, Paulius Micikevicius, Maxim Naumov, Colin Verrilli,\
    \ Ralph Wittig, Doug Burger, and Eric Chung. 2023. Microscaling Data Formats for\
    \ Deep Learning. arXiv[:2310.10537](https://arxiv.org/abs/2310.10537) [cs.LG]\n\
    - <span id=\"page-12-17\"></span>[43] Sayeh Sharify, Utkarsh Saxena, Zifei Xu,\
    \ Wanzin Yazar, Ilya Soloveychik, and Xin Wang. 2024. Post Training Quantization\
    \ of Large Language Models with Microscaling Formats. In Proceedings of The 4th\
    \ NeurIPS Efficient Natural Language and Speech Processing Workshop (Proceedings\
    \ of Machine Learning Research, Vol. 262), Mehdi Rezagholizadeh, Peyman Passban,\
    \ Soheila Samiee, Vahid Partovi Nia, Yu Cheng, Yue Deng, Qun Liu, and Boxing Chen\
    \ (Eds.). PMLR, 241–258. <https://proceedings.mlr.press/v262/sharify24a.html>\n\
    - <span id=\"page-12-21\"></span>[44] James E. Stine, Ivan Castellanos, Michael\
    \ Wood, Jeff Henson, Fred Love, W. Rhett Davis, Paul D. Franzon, Michael Bucher,\
    \ Sunil Basavarajaiah, Julie Oh, and Ravi Jenkal. 2007. FreePDK: An Open-Source\
    \ Variation-Aware Design Kit. In 2007 IEEE International Conference on Microelectronic\
    \ Systems Education (MSE'07). 173–174.\n- <span id=\"page-12-26\"></span>[45]\
    \ Xiao Sun, Naigang Wang, Chia-Yu Chen, Jiamin Ni, Ankur Agrawal, Xiaodong Cui,\
    \ Swagath Venkataramani, Kaoutar El Maghraoui, Vijayalakshmi (Viji) Srinivasan,\
    \ and Kailash Gopalakrishnan. 2020. Ultra-Low Precision 4-bit Training of Deep\
    \ Neural Networks. In Advances in Neural Information Processing Systems (NIPS),\
    \ Vol. 33. Curran Associates, Inc., 1796–1807.\n- <span id=\"page-12-20\"></span>[46]\
    \ Seunghwan Sung, Sujin Hur, Sungwoo Kim, Dongho Ha, Yunho Oh, and Won Woo Ro.\
    \ 2023. MAD MAcce: Supporting Multiply-Add Operations for Democratizing Matrix-Multiplication\
    \ Accelerators. In Proceedings of the 56th Annual IEEE/ACM International Symposium\
    \ on Microarchitecture (Toronto, ON, Canada) (MICRO '23). Association for Computing\
    \ Machinery, New York, NY, USA, 367–379.\n- <span id=\"page-12-30\"></span>[47]\
    \ Linnan Wang, Jinmian Ye, Yiyang Zhao, Wei Wu, Ang Li, Shuaiwen Leon Song, Zenglin\
    \ Xu, and Tim Kraska. 2018. Superneurons: Dynamic GPU memory management for training\
    \ deep neural networks. In Proceedings of the 23rd ACM SIGPLAN symposium on principles\
    \ and practice of parallel programming. 41–53.\n- <span id=\"page-12-10\"></span>[48]\
    \ Yuke Wang, Boyuan Feng, and Yufei Ding. 2022. QGTC: Accelerating Quantized Graph\
    \ Neural Networks via GPU Tensor Core. In Proceedings of the 27th ACM SIGPLAN\
    \ Symposium on Principles and Practice of Parallel Programming (Seoul, Republic\
    \ of Korea) (PPoPP '22). Association for Computing Machinery, New York, NY, USA,\
    \ 107–119.\n- <span id=\"page-12-31\"></span>[49] Jianbang Yang, Dahai Tang, Xiaoniu\
    \ Song, Lei Wang, Qiang Yin, Rong Chen, Wenyuan Yu, and Jingren Zhou. 2022. GNNLab:\
    \ A Factored System for Sample-Based GNN Training over GPUs. In Proceedings of\
    \ the Seventeenth European Conference on Computer Systems (Rennes, France) (EuroSys\
    \ '22). Association for Computing Machinery, New York, NY, USA, 417–434.\n- <span\
    \ id=\"page-12-8\"></span>[50] Sai Qian Zhang, Bradley McDanel, and HT Kung. 2022.\
    \ Fast: Dnn training under variable precision block floating point with stochastic\
    \ rounding. In 2022 IEEE International Symposium on High-Performance Computer\
    \ Architecture (HPCA). IEEE, 846–860.\n- <span id=\"page-12-14\"></span>[51] Kang\
    \ Zhao, Sida Huang, Pan Pan, Yinghan Li, Yingya Zhang, Zhenyu Gu, and Yinghui\
    \ Xu. 2021. Distribution Adaptive INT8 Quantization for Training CNNs. Proceedings\
    \ of the AAAI Conference on Artificial Intelligence 35, 4 (May 2021), 3483–3491.\n\
    - <span id=\"page-12-11\"></span>[52] Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu,\
    \ and Yurong Chen. 2017. Incremental Network Quantization: Towards Lossless CNNs\
    \ with Low-precision Weights. In 5th International Conference on Learning Representations,\
    \ ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings."
  references:
  - '- <span id="page-11-8"></span>[1] Ron Banner, Yury Nahshan, and Daniel Soudry.
    2019. Post Training 4-Bit Quantization of Convolutional Networks for Rapid-Deployment.
    In Proceedings of the 33rd International Conference on Neural Information Processing
    Systems (NIPS). Curran Associates Inc., Red Hook, NY, USA, Article 714, 9 pages.'
  - '- <span id="page-11-11"></span>[2] Tom Savell Ankit More Kyung-Nam Han Ritchie
    Zhao Mathew Hall Jasmine Klar Eric Chung Yuan Yu Michael Schulte Ralph Wittig
    Ian Bratt Nigel Stephens Jelena Milanovic John Brothers Pradeep Dubey Marius Cornea
    Alexander Heinecke Andres Rodriguez Martin Langhammer Summer Deng Maxim Naumov
    Paulius Micikevicius Michael Siu Bita Darvish Rouhani, Nitin Garegrat and Colin
    Verrilli. 2023. OCP Microscaling Formats (MX) Specification. [https://www.opencompute.](https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf)
    [org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf.](https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf)'
  - '- <span id="page-11-19"></span>[3] Shubham Chaudhary, Ramachandran Ramjee, Muthian
    Sivathanu, Nipun Kwatra, and Srinidhi Viswanatha. 2020. Balancing Efficiency and
    Fairness in Heterogeneous GPU Clusters for Deep Learning. In Proceedings of the
    Fifteenth European Conference on Computer Systems (Heraklion, Greece) (EuroSys
    ''20). Association for Computing Machinery, New York, NY, USA, Article 1, 16 pages.'
  - '- <span id="page-11-10"></span>[4] Bita Darvish Rouhani, Ritchie Zhao, Venmugil
    Elango, Rasoul Shafipour, Mathew Hall, Maral Mesmakhosroshahi, Ankit More, Levi
    Melnick, Maximilian Golub, Girish Varatkar, Lai Shao, Gaurav Kolhe, Dimitry Melts,
    Jasmine Klar, Renee L''Heureux, Matt Perry, Doug Burger, Eric Chung, Zhaoxia (Summer)
    Deng, Sam Naghshineh, Jongsoo Park, and Maxim Naumov. 2023. With Shared Microexponents,
    A Little Shifting Goes a Long Way. In Proceedings of the 50th Annual International
    Symposium on Computer Architecture (Orlando, FL, USA) (ISCA ''23). Association
    for Computing Machinery, New York, NY, USA, Article 83, 13 pages.'
  - '- <span id="page-11-17"></span>[5] Jia Deng, Wei Dong, Richard Socher, Li-Jia
    Li, Kai Li, and Li Fei-Fei. 2009. ImageNet: A large-scale hierarchical image database.
    In 2009 IEEE Conference on Computer Vision and Pattern Recognition. 248–255.'
  - '- <span id="page-11-16"></span>[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee,
    and Kristina Toutanova. 2018. BERT: Pre-training of Deep Bidirectional Transformers
    for Language Understanding. CoRR abs/1810.04805 (2018). arXiv[:1810.04805](https://arxiv.org/abs/1810.04805)
    <http://arxiv.org/abs/1810.04805>'
  - '- <span id="page-11-15"></span>[7] Alexey Dosovitskiy, Lucas Beyer, Alexander
    Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani,
    Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
    2021. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.
    arXiv[:2010.11929](https://arxiv.org/abs/2010.11929) [cs.CV]'
  - '- <span id="page-11-1"></span>[8] Mario Drumond, Louis Coulon, Arash Pourhabibi,
    Ahmet Caner Yüzügüler, Babak Falsafi, and Martin Jaggi. 2021. Equinox: Training
    (for Free) on a Custom Inference Accelerator. In MICRO-54: 54th Annual IEEE/ACM
    International Symposium on Microarchitecture (Virtual Event, Greece) (MICRO ''21).
    Association for Computing Machinery, New York, NY, USA, 421–433.'
  - '- <span id="page-11-2"></span>[9] Mario Drumond, Tao LIN, Martin Jaggi, and Babak
    Falsafi. 2018. Training DNNs with Hybrid Block Floating Point. In Advances in
    Neural Information Processing Systems (NIPS), Vol. 31. Curran Associates, Inc.'
  - '- <span id="page-11-0"></span>[10] Luciano Floridi and Massimo Chiriatti. 2020.
    GPT-3: Its nature, scope, limits, and consequences. Minds and Machines 30, 4 (2020),
    681–694.'
  - '- <span id="page-11-18"></span>[11] Wikimedia Foundation. [n. d.]. Toggle the
    table of contents English Wikipedia. [https://en.wikipedia.org/wiki/English\\_Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia)'
  - '- <span id="page-11-3"></span>[12] Marcelo Gennari do Nascimento, Victor Adrian
    Prisacariu, Roger Fawcett, and Martin Langhammer. 2023. HyperBlock Floating Point:
    Generalised Quantization Scheme for Gradient and Inference Computation. In 2023
    IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). 6353–6362.'
  - '- <span id="page-11-6"></span>[13] Song Han, Xingyu Liu, Huizi Mao, Jing Pu,
    Ardavan Pedram, Mark A. Horowitz, and William J. Dally. 2016. EIE: Efficient Inference
    Engine on Compressed Deep Neural Network. In 43rd ACM/IEEE Annual International
    Symposium on Computer Architecture, ISCA 2016, Seoul, South Korea, June 18-22,
    2016. IEEE Computer Society, 243–254.'
  - '- <span id="page-11-4"></span>[14] Simla Burcu Harma, Ayan Chakraborty, Nicholas
    Sperry, Babak Falsafi, Martin Jaggi, and Yunho Oh. 2022. Accuracy Booster: Enabling
    4-bit Fixed-point Arithmetic for DNN Training. arXiv[:2211.10737](https://arxiv.org/abs/2211.10737)
    [cs.LG]'
  - '- <span id="page-11-12"></span>[15] Intel. [n. d.]. Intel neural compressor.
    [https://intel.github.io/neural-compressor.](https://intel.github.io/neural-compressor)
    Accessed: 2025-02-16.'
  - '- <span id="page-11-13"></span>[16] Intel. [n. d.]. OpenVINO: Open-source software
    toolkit for optimizing and deploying deep learning models. [https://github.com/openvinotoolkit/openvino.](https://github.com/openvinotoolkit/openvino)
    Accessed: 2025-02-16.'
  - '- <span id="page-11-9"></span>[17] Benoit Jacob, Skirmantas Kligys, Bo Chen,
    Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko.
    2018. Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only
    Inference. In Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition (CVPR).'
  - '- <span id="page-11-14"></span>[18] Wonsuk Jang and Thierry Tambe. 2025. BlockDialect:
    Block-wise Finegrained Mixed Format Quantization for Energy-Efficient LLM Inference.
    arXiv[:2501.01144](https://arxiv.org/abs/2501.01144) [cs.CL] <https://arxiv.org/abs/2501.01144>'
  - '- <span id="page-11-7"></span>[19] Anand Jayarajan, Jinliang Wei, Garth Gibson,
    Alexandra Fedorova, and Gennady Pekhimenko. 2019. Priority-based Parameter Propagation
    for Distributed DNN'
  - <span id="page-12-0"></span>Training. In Proceedings of Machine Learning and Systems
    (MLSys), Vol. 1. 132– 145.
  - '- <span id="page-12-19"></span>[20] Jaebeom Jeon, Minseong Gil, Junsu Kim, Jaeyong
    Park, Gunjae Koo, Myung Kuk Yoon, and Yunho Oh. 2024. VitBit: Enhancing Embedded
    GPU Performance for AI Workloads through Register Operand Packing. In Proceedings
    of the 53rd International Conference on Parallel Processing. 1012–1021.'
  - '- <span id="page-12-22"></span>[21] Mahmoud Khairy, Zhesheng Shen, Tor M. Aamodt,
    and Timothy G. Rogers. 2020. Accel-Sim: An Extensible Simulation Framework for
    Validated GPU Modeling. In 2020 ACM/IEEE 47th Annual International Symposium on
    Computer Architecture (ISCA). 473–486.'
  - '- <span id="page-12-32"></span>[22] Hyeonjin Kim, Sungwoo Ahn, Yunho Oh, Bogil
    Kim, Won Woo Ro, and William J. Song. 2020. Duplo: Lifting Redundant Memory Accesses
    of Deep Neural Networks for GPU Tensor Cores. In 2020 53rd Annual IEEE/ACM International
    Symposium on Microarchitecture (MICRO). 725–737.'
  - '- <span id="page-12-33"></span>[23] Keunsoo Kim and Won Woo Ro. 2018. WIR: Warp
    Instruction Reuse to Minimize Repeated Computations in GPUs. In 2018 IEEE International
    Symposium on High Performance Computer Architecture (HPCA). 389–402.'
  - '- <span id="page-12-15"></span>[24] Jahyun Koo, Dahoon Park, Sangwoo Jung, and
    Jaeha Kung. 2024. OPAL: Outlier-Preserved Microscaling Quantization Accelerator
    for Generative Large Language Models. In Proceedings of the 61st ACM/IEEE Design
    Automation Conference (San Francisco, CA, USA) (DAC ''24). Association for Computing
    Machinery, New York, NY, USA, Article 259, 6 pages.'
  - '- <span id="page-12-3"></span>[25] Urs Köster, Tristan Webb, Xin Wang, Marcel
    Nassar, Arjun K Bansal, William Constable, Oguz Elibol, Scott Gray, Stewart Hall,
    Luke Hornof, Amir Khosrowshahi, Carey Kloss, Ruby J Pai, and Naveen Rao. 2017.
    Flexpoint: An Adaptive Numerical Format for Efficient Training of Deep Neural
    Networks. In Advances in Neural Information Processing Systems (NIPS), Vol. 30.
    Curran Associates, Inc.'
  - '- <span id="page-12-7"></span>[26] Seunghyun Lee, Jeik Choi, Seockhwan Noh, Jahyun
    Koo, and Jaeha Kung. 2023. DBPS: Dynamic Block Size and Precision Scaling for
    Efficient DNN Training Supported by RISC-V ISA Extensions. In 2023 60th ACM/IEEE
    Design Automation Conference (DAC). 1–6.'
  - '- <span id="page-12-28"></span>[27] Gangmuk Lim, Jeongseob Ahn, Wencong Xiao,
    Youngjin Kwon, and Myeongjae Jeon. 2021. Zico: Efficient GPU Memory Sharing for
    Concurrent DNN Training. In 2021 USENIX Annual Technical Conference (USENIX ATC
    21). USENIX Association, 161–175. <https://www.usenix.org/conference/atc21/presentation/lim>'
  - '- <span id="page-12-12"></span>[28] Chang Liu, Xishan Zhang, Rui Zhang, Ling
    Li, Shiyi Zhou, Di Huang, Zhen Li, Zidong Du, Shaoli Liu, and Tianshi Chen. 2022.
    Rethinking the Importance of Quantization Bias, Toward Full Low-Bit Training.
    IEEE Transactions on Image Processing 31 (2022), 7006–7019.'
  - '- <span id="page-12-27"></span>[29] Yun-Chen Lo and Ren-Shuo Liu. 2023. Bucket
    Getter: A Bucket-based Processing Engine for Low-bit Block Floating Point (BFP)
    DNNs. In Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture.
    1002–1015.'
  - '- <span id="page-12-6"></span>[30] Paulius Micikevicius, Dusan Stosic, Neil Burgess,
    Marius Cornea, Pradeep Dubey, Richard Grisenthwaite, Sangwon Ha, Alexander Heinecke,
    Patrick Judd, John Kamalu, Naveen Mellempudi, Stuart Oberman, Mohammad Shoeybi,
    Michael Siu, and Hao Wu. 2022. FP8 Formats for Deep Learning. arXiv[:2209.05433](https://arxiv.org/abs/2209.05433)
    [cs.LG]'
  - '- <span id="page-12-25"></span>[31] Microsoft. 2024. MicroScaling Emulator. [https://github.com/microsoft/](https://github.com/microsoft/microxcaling)
    [microxcaling.](https://github.com/microsoft/microxcaling)'
  - '- <span id="page-12-9"></span>[32] Deepak Narayanan, Amar Phanishayee, Kaiyu
    Shi, Xie Chen, and Matei Zaharia. 2021. Memory-Efficient Pipeline-Parallel DNN
    Training. In Proceedings of the 38th International Conference on Machine Learning
    (Proceedings of Machine Learning Research, Vol. 139). PMLR, 7937–7947.'
  - '- <span id="page-12-23"></span>[33] NVIDIA. 2020. NVIDIA A100 Tensor Core GPU
    Architecture. v1.0 (2020), 1–82.'
  - '- <span id="page-12-18"></span>[34] Yunho Oh, Myung Kuk Yoon, William J. Song,
    and Won Woo Ro. 2018. FineReg: Fine-Grained Register File Management for Augmenting
    GPU Throughput. In 2018 51st Annual IEEE/ACM International Symposium on Microarchitecture
    (MI-CRO). 364–376.'
  - '- <span id="page-12-1"></span>[35] OpenAI. 2023. GPT-4 Technical Report. arXiv[:2303.08774](https://arxiv.org/abs/2303.08774)
    [cs.CL]'
  - '- <span id="page-12-13"></span>[36] Eunhyeok Park, Sungjoo Yoo, and Peter Vajda.
    2018. Value-Aware Quantization for Training and Inference of Neural Networks.
    In Computer Vision – ECCV 2018: 15th European Conference, Munich, Germany, September
    8-14, 2018, Proceedings, Part IV (Munich, Germany). Springer-Verlag, Berlin, Heidelberg,
    608–624.'
  - '- <span id="page-12-29"></span>[37] Jay H. Park, Gyeongchan Yun, Chang M. Yi,
    Nguyen T. Nguyen, Seungmin Lee, Jaesik Choi, Sam H. Noh, and Young ri Choi. 2020.
    HetPipe: Enabling Large DNN Training on (Whimpy) Heterogeneous GPU Clusters through
    Integration of Pipelined Model Parallelism and Data Parallelism. In 2020 USENIX
    Annual Technical Conference (USENIX ATC 20). USENIX Association, 307–321. [https:](https://www.usenix.org/conference/atc20/presentation/park)
    [//www.usenix.org/conference/atc20/presentation/park](https://www.usenix.org/conference/atc20/presentation/park)'
  - '- <span id="page-12-24"></span>[38] Alec Radford, Jeffrey Wu, Rewon Child, David
    Luan, Dario Amodei, and Ilya Sutskever. 2018. Language Models are Unsupervised
    Multitask Learners. (2018).'
  - '- <span id="page-12-16"></span>[39] Akshat Ramachandran, Souvik Kundu, and Tushar
    Krishna. 2024. MicroScopiQ: Accelerating Foundational Models through Outlier-Aware
    Microscaling Quantization. arXiv[:2411.05282](https://arxiv.org/abs/2411.05282)
    [cs.AR] <https://arxiv.org/abs/2411.05282>'
  - '- <span id="page-12-2"></span>[40] MIT Technology Review. 2023. Multi-die systems
    define the future of semiconductors. [https://www.technologyreview.com/2023/03/31/1070527/multi-die](https://www.technologyreview.com/2023/03/31/1070527/multi-die-systems-define-the-future-of-semiconductors/)[systems-define-the-future-of-semiconductors/.](https://www.technologyreview.com/2023/03/31/1070527/multi-die-systems-define-the-future-of-semiconductors/)
    Accessed: 2023-03-31.'
  - '- <span id="page-12-4"></span>[41] Bita Rouhani, Daniel Lo, Ritchie Zhao, Ming
    Liu, Jeremy Fowers, Kalin Ovtcharov, Anna Vinogradsky, Sarah Massengill, Lita
    Yang, Ray Bittner, Alessandro Forin, Haishan Zhu, Taesik Na, Prerak Patel, Shuai
    Che, Lok Chand Koppaka, Xia Song, Subhojit Som, Kaustav Das, Saurabh Tiwary, Steve
    Reinhardt, Sitaram Lanka, Eric Chung, and Doug Burger. 2020. Pushing the Limits
    of Narrow Precision Inferencing at Cloud Scale with Microsoft Floating Point.
    In NeurIPS 2020. ACM.'
  - '- <span id="page-12-5"></span>[42] Bita Darvish Rouhani, Ritchie Zhao, Ankit
    More, Mathew Hall, Alireza Khodamoradi, Summer Deng, Dhruv Choudhary, Marius Cornea,
    Eric Dellinger, Kristof Denolf, Stosic Dusan, Venmugil Elango, Maximilian Golub,
    Alexander Heinecke, Phil James-Roxby, Dharmesh Jani, Gaurav Kolhe, Martin Langhammer,
    Ada Li, Levi Melnick, Maral Mesmakhosroshahi, Andres Rodriguez, Michael Schulte,
    Rasoul Shafipour, Lei Shao, Michael Siu, Pradeep Dubey, Paulius Micikevicius,
    Maxim Naumov, Colin Verrilli, Ralph Wittig, Doug Burger, and Eric Chung. 2023.
    Microscaling Data Formats for Deep Learning. arXiv[:2310.10537](https://arxiv.org/abs/2310.10537)
    [cs.LG]'
  - '- <span id="page-12-17"></span>[43] Sayeh Sharify, Utkarsh Saxena, Zifei Xu,
    Wanzin Yazar, Ilya Soloveychik, and Xin Wang. 2024. Post Training Quantization
    of Large Language Models with Microscaling Formats. In Proceedings of The 4th
    NeurIPS Efficient Natural Language and Speech Processing Workshop (Proceedings
    of Machine Learning Research, Vol. 262), Mehdi Rezagholizadeh, Peyman Passban,
    Soheila Samiee, Vahid Partovi Nia, Yu Cheng, Yue Deng, Qun Liu, and Boxing Chen
    (Eds.). PMLR, 241–258. <https://proceedings.mlr.press/v262/sharify24a.html>'
  - '- <span id="page-12-21"></span>[44] James E. Stine, Ivan Castellanos, Michael
    Wood, Jeff Henson, Fred Love, W. Rhett Davis, Paul D. Franzon, Michael Bucher,
    Sunil Basavarajaiah, Julie Oh, and Ravi Jenkal. 2007. FreePDK: An Open-Source
    Variation-Aware Design Kit. In 2007 IEEE International Conference on Microelectronic
    Systems Education (MSE''07). 173–174.'
  - '- <span id="page-12-26"></span>[45] Xiao Sun, Naigang Wang, Chia-Yu Chen, Jiamin
    Ni, Ankur Agrawal, Xiaodong Cui, Swagath Venkataramani, Kaoutar El Maghraoui,
    Vijayalakshmi (Viji) Srinivasan, and Kailash Gopalakrishnan. 2020. Ultra-Low Precision
    4-bit Training of Deep Neural Networks. In Advances in Neural Information Processing
    Systems (NIPS), Vol. 33. Curran Associates, Inc., 1796–1807.'
  - '- <span id="page-12-20"></span>[46] Seunghwan Sung, Sujin Hur, Sungwoo Kim, Dongho
    Ha, Yunho Oh, and Won Woo Ro. 2023. MAD MAcce: Supporting Multiply-Add Operations
    for Democratizing Matrix-Multiplication Accelerators. In Proceedings of the 56th
    Annual IEEE/ACM International Symposium on Microarchitecture (Toronto, ON, Canada)
    (MICRO ''23). Association for Computing Machinery, New York, NY, USA, 367–379.'
  - '- <span id="page-12-30"></span>[47] Linnan Wang, Jinmian Ye, Yiyang Zhao, Wei
    Wu, Ang Li, Shuaiwen Leon Song, Zenglin Xu, and Tim Kraska. 2018. Superneurons:
    Dynamic GPU memory management for training deep neural networks. In Proceedings
    of the 23rd ACM SIGPLAN symposium on principles and practice of parallel programming.
    41–53.'
  - '- <span id="page-12-10"></span>[48] Yuke Wang, Boyuan Feng, and Yufei Ding. 2022.
    QGTC: Accelerating Quantized Graph Neural Networks via GPU Tensor Core. In Proceedings
    of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming
    (Seoul, Republic of Korea) (PPoPP ''22). Association for Computing Machinery,
    New York, NY, USA, 107–119.'
  - '- <span id="page-12-31"></span>[49] Jianbang Yang, Dahai Tang, Xiaoniu Song,
    Lei Wang, Qiang Yin, Rong Chen, Wenyuan Yu, and Jingren Zhou. 2022. GNNLab: A
    Factored System for Sample-Based GNN Training over GPUs. In Proceedings of the
    Seventeenth European Conference on Computer Systems (Rennes, France) (EuroSys
    ''22). Association for Computing Machinery, New York, NY, USA, 417–434.'
  - '- <span id="page-12-8"></span>[50] Sai Qian Zhang, Bradley McDanel, and HT Kung.
    2022. Fast: Dnn training under variable precision block floating point with stochastic
    rounding. In 2022 IEEE International Symposium on High-Performance Computer Architecture
    (HPCA). IEEE, 846–860.'
  - '- <span id="page-12-14"></span>[51] Kang Zhao, Sida Huang, Pan Pan, Yinghan Li,
    Yingya Zhang, Zhenyu Gu, and Yinghui Xu. 2021. Distribution Adaptive INT8 Quantization
    for Training CNNs. Proceedings of the AAAI Conference on Artificial Intelligence
    35, 4 (May 2021), 3483–3491.'
  - '- <span id="page-12-11"></span>[52] Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu,
    and Yurong Chen. 2017. Incremental Network Quantization: Towards Lossless CNNs
    with Low-precision Weights. In 5th International Conference on Learning Representations,
    ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.'
- id: wsc_llm_efficient_llm_service_and_architecture_co_exploration_for_wafer_scale_chips_zheng_xu_https_orcid_org_0009_0009_5264_2818_tsinghua_university_school_of_integrated_circuits_bnrist_beijing_china_xuz22_mails_tsinghua_edu_cn
  title: 'WSC-LLM: Efficient LLM Service and Architecture Co-exploration for Wafer-scale
    Chips'
  abstract: 'The deployment of large language models (LLMs) imposes significant demands
    on computing, memory, and communication resources. Wafer-scale technology enables
    the high-density integration of multiple single-die chips with high-speed Die-to-Die
    (D2D) interconnections, presenting a promising solution to meet these demands
    arising from LLMs. However, given the limited wafer area, a trade-off needs to
    be made among computing, storage, and communication resources. Maximizing the
    benefits and minimizing the drawbacks of wafer-scale technology is crucial for
    enhancing the performance of LLM service systems, which poses challenges to both
    architecture and scheduling. Unfortunately, existing methods cannot effectively
    address these challenges.

    To bridge the gap, we propose WSC-LLM, an architecture and scheduling co-exploration
    framework. We first define a highly configurable general hardware template designed
    to explore optimal architectural parameters for wafer-scale chips. Based on it,
    we

    <sup>∗</sup>Corresponding author.

    [This work is licensed under a Creative Commons Attribution 4.0 International
    License.](https://creativecommons.org/licenses/by/4.0) ISCA ''25, Tokyo, Japan
    © 2025 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-1261-6/25/06
    <https://doi.org/10.1145/3695053.3731101>

    capitalize on the high D2D bandwidth and fine-grained operation advantages inherent
    to wafer-scale chips to investigate optimal disaggregated scheduling strategies,
    effectively addressing the highly dynamic demands of LLM workloads. Compared to
    the state-of-theart (SOTA) LLM service systems, WSC-LLM can achieve an average
    overall performance improvement of 3.12× across various LLM models and datasets.
    Moreover, we leverage WSC-LLM to reveal intriguing insights about wafer-scale
    architecture design and the execution of LLM workloads.'
  keywords: scheduling, architecture, wafer-scale chips, large language model
  document: "![](_page_0_Picture_0.jpeg)\n\n# WSC-LLM: Efficient LLM Service and Architecture\
    \ Co-exploration for Wafer-scale Chips\n\n[Zheng Xu](https://orcid.org/0009-0009-5264-2818)\
    \ Tsinghua University School of Integrated Circuits, BNRist Beijing, China xuz22@mails.tsinghua.edu.cn\n\
    \n[Jinxi Li](https://orcid.org/0009-0002-4841-3798) Tsinghua University School\
    \ of Integrated Circuits, BNRist Beijing, China ljx23@mails.tsinghua.edu.cn\n\n\
    [Chao Li](https://orcid.org/0000-0001-6218-4659) Shanghai Jiao Tong University\
    \ Shanghai, China lichao@cs.sjtu.edu.cn\n\n[Dehao Kong](https://orcid.org/0009-0005-1087-307X)\
    \ Tsinghua University School of Integrated Circuits, BNRist Beijing, China kdh24@mails.tsinghua.edu.cn\n\
    \n[Jingxiang Hou](https://orcid.org/0009-0008-9504-9611) Tsinghua University School\
    \ of Integrated Circuits, BNRist Beijing, China houjx22@mails.tsinghua.edu.cn\n\
    \n[Shaojun Wei](https://orcid.org/0000-0001-5117-7920) Tsinghua University School\
    \ of Integrated Circuits, BNRist Beijing, China wsj@tsinghua.edu.cn\n\n[Shouyi\
    \ Yin](https://orcid.org/0000-0002-8438-8588) Tsinghua University School of Integrated\
    \ Circuits, BNRist Beijing, China Shanghai AI Laboratory Shanghai, China yinsy@tsinghua.edu.cn\n\
    \n[Jiaxin Liu](https://orcid.org/0009-0009-9405-8548)\n\nTsinghua University School\
    \ of Integrated Circuits, BNRist Beijing, China jiaxin-l24@mails.tsinghua.edu.cn\n\
    \n> [Xu Dai](https://orcid.org/0009-0009-4702-0635) Shanghai AI Laboratory Shanghai,\
    \ China daixu@pjlab.org.cn\n\n[Yang Hu](https://orcid.org/0000-0001-6942-4395)<sup>∗</sup>\
    \ Tsinghua University School of Integrated Circuits, BNRist Beijing, China hu\\\
    _yang@tsinghua.edu.cn\n\n# Abstract\n\nThe deployment of large language models\
    \ (LLMs) imposes significant demands on computing, memory, and communication resources.\
    \ Wafer-scale technology enables the high-density integration of multiple single-die\
    \ chips with high-speed Die-to-Die (D2D) interconnections, presenting a promising\
    \ solution to meet these demands arising from LLMs. However, given the limited\
    \ wafer area, a trade-off needs to be made among computing, storage, and communication\
    \ resources. Maximizing the benefits and minimizing the drawbacks of wafer-scale\
    \ technology is crucial for enhancing the performance of LLM service systems,\
    \ which poses challenges to both architecture and scheduling. Unfortunately, existing\
    \ methods cannot effectively address these challenges.\n\nTo bridge the gap, we\
    \ propose WSC-LLM, an architecture and scheduling co-exploration framework. We\
    \ first define a highly configurable general hardware template designed to explore\
    \ optimal architectural parameters for wafer-scale chips. Based on it, we\n\n\
    <sup>∗</sup>Corresponding author.\n\n[This work is licensed under a Creative Commons\
    \ Attribution 4.0 International License.](https://creativecommons.org/licenses/by/4.0)\
    \ ISCA '25, Tokyo, Japan © 2025 Copyright held by the owner/author(s). ACM ISBN\
    \ 979-8-4007-1261-6/25/06 <https://doi.org/10.1145/3695053.3731101>\n\ncapitalize\
    \ on the high D2D bandwidth and fine-grained operation advantages inherent to\
    \ wafer-scale chips to investigate optimal disaggregated scheduling strategies,\
    \ effectively addressing the highly dynamic demands of LLM workloads. Compared\
    \ to the state-of-theart (SOTA) LLM service systems, WSC-LLM can achieve an average\
    \ overall performance improvement of 3.12× across various LLM models and datasets.\
    \ Moreover, we leverage WSC-LLM to reveal intriguing insights about wafer-scale\
    \ architecture design and the execution of LLM workloads.\n\n# CCS Concepts\n\n\
    • Computer systems organization → Architectures; • Software and its engineering\
    \ → Compilers; • Computing methodologies → Machine learning.\n\n# Keywords\n\n\
    scheduling, architecture, wafer-scale chips, large language model\n\n# ACM Reference\
    \ Format:\n\nZheng Xu, Dehao Kong, Jiaxin Liu, Jinxi Li, Jingxiang Hou, Xu Dai,\
    \ Chao Li, Shaojun Wei, Yang Hu, and Shouyi Yin. 2025. WSC-LLM: Efficient LLM\
    \ Service and Architecture Co-exploration for Wafer-scale Chips. In Proceedings\
    \ of the 52nd Annual International Symposium on Computer Architecture (ISCA '25),\
    \ June 21–25, 2025, Tokyo, Japan. ACM, New York, NY, USA, [17](#page-16-0) pages.\
    \ <https://doi.org/10.1145/3695053.3731101>\n\n#### 1 Introduction\n\nIn recent\
    \ years, large language models (LLMs) based on the transformer architecture have\
    \ emerged as a driving force behind advancements in artificial intelligence (AI).\
    \ It has demonstrated remarkable capabilities and exceptional performance across\
    \ various domains, including natural language understanding and content generation\
    \ [\\[8,](#page-12-0) [9,](#page-12-1) [13,](#page-13-0) [18,](#page-13-1) [76,](#page-16-1)\
    \ [77\\]](#page-16-2). To achieve superior performance, the number of parameters\
    \ in LLMs has increased dramatically, resulting in a substantial rise in computational\
    \ resource demands. In addition to the large number of model parameters, the LLM\
    \ inference process also generates a significant amount of key-value (KV) cache,\
    \ further exacerbating memory requirements.\n\nAffected by the end of Moore's\
    \ Law [\\[55\\]](#page-15-0) and the constraints imposed by limited photomask\
    \ size [\\[5\\]](#page-12-2), it is difficult for a single hardware device, such\
    \ as an individual GPU or NPU, to achieve sufficient transistor scale and memory\
    \ capacity to support LLMs tasks. As a result, distributed computing architectures\
    \ is essential for efficient LLM inference. However, deploying LLMs across multiple\
    \ devices introduces additional challenges in communication. Disaggregate LLM\
    \ inference necessitates large-scale transmission of model weights and KV cache\
    \ across devices, imposing demands on high-bandwidth interconnects. Currently,\
    \ interconnect communication bandwidth has become the primary bottleneck limiting\
    \ the performance of these LLM serving systems [\\[10,](#page-13-2) [45,](#page-14-0)\
    \ [85\\]](#page-16-3).\n\nWafer-scale chip design method [\\[33\\]](#page-14-1),\
    \ using advanced packaging technologies such as CoWoS [\\[29\\]](#page-14-2) to\
    \ expand the chip area, provides a promising solution to meet the computation,\
    \ memory and communication resource demands of LLM inference and enable high-density\
    \ 3D integration. Vertical integration of modules such as power supply, cooling,\
    \ and I/O interfaces achieves exceptional integration density, which in turn maximizes\
    \ compute density and interconnect bandwidth. Currently, wafer-scale GPU [\\[61\\\
    ]](#page-15-1), UCLA & UIUC's work [\\[60\\]](#page-15-2), Cerebras WSE2 [\\[50\\\
    ]](#page-15-3), and Tesla's Dojo [\\[73\\]](#page-16-4) have been introduced.\
    \ One widely used, cost-effective, and high-yield wafer-scale approach involves\
    \ integrating NPU and DRAM chiplets together [\\[60,](#page-15-2) [61,](#page-15-1)\
    \ [73\\]](#page-16-4). Compared with the most advanced GPUs [\\[7\\]](#page-12-3)\
    \ and high-speed rack-scale fabrics (i.e., NVLinks [\\[6\\]](#page-12-4)), these\
    \ waferscale chip works can provide around 50× the number of transistors and 6×\
    \ the inter-chip bandwidth.\n\nHowever, this technology introduces new challenges\
    \ for architectural design and LLM inference scheduling for wafer-scale chips,\
    \ which are outlined below:\n\nFor architecture design, the key challenge is determining\
    \ the optimal DRAM capacity per die. The weights of LLMs can reach hundreds of\
    \ gigabytes [\\[2](#page-12-5)[–4,](#page-12-6) [9,](#page-12-1) [13,](#page-13-0)\
    \ [18,](#page-13-1) [66,](#page-15-4) [75,](#page-16-5) [96\\]](#page-16-6), and\
    \ KV cache storage demands are similarly substantial. As illustrated in Fig. [1\\\
    (](#page-1-0)a), a higher DRAM capacity facilitates the simultaneous execution\
    \ of a greater number of requests, thereby enhancing the overall quality of LLM\
    \ services. However, larger DRAM capacity implies that DRAM chiplets will occupy\
    \ more wafer area and demand additional Die-to-Die (D2D) interconnect interfaces,\
    \ which will reduce the available compute and communication resources. All the\
    \ above adverse effects are collectively referred to as DRAM Costs. Consequently,\
    \ a trade-off arises between integrating larger DRAM capacities within a single\
    \ die to support extensive weights and KV cache storage demands and smaller DRAM\
    \ capacities to reduce DRAM Costs. Balancing this trade-off remains an unsolved\
    \ challenge.\n\n<span id=\"page-1-0\"></span>![](_page_1_Figure_8.jpeg)\n\nFigure\
    \ 1: The challenges when LLMs meet wafer-scale chip.\n\nFor LLM inference scheduling,\
    \ the main challenge lies in managing the highly dynamic nature of LLM workloads,\
    \ which stems from the division of LLM request processing into two distinct phases:\
    \ prefill and decoding. Resource demands of the same request in different phases\
    \ vary significantly [\\[28,](#page-14-3) [30,](#page-14-4) [62,](#page-15-5)\
    \ [71,](#page-16-7) [84,](#page-16-8) [85,](#page-16-3) [100\\]](#page-16-9).\
    \ Consequently, using identical devices and execution strategies for both phases\
    \ often leads to inefficiency. A potential solution is disaggregated scheduling,\
    \ which partitions devices into multiple groups, each deploying an LLM instance\
    \ for a specific phase and applying a phase-specific execution strategy tailored\
    \ to its characteristics [\\[30,](#page-14-4) [31,](#page-14-5) [62,](#page-15-5)\
    \ [84,](#page-16-8) [100\\]](#page-16-9). However, as illustrated in Fig. [1\\\
    (](#page-1-0)b), it faces several limitations. First, optimizing tensor parallelism\
    \ (TP) strategy and group configuration is crucial for maximizing computational\
    \ performance during both the prefill and decoding phases. Nonetheless, most existing\
    \ works select fixed configurations based on engineering experience, lacking precise\
    \ problem definition and exhaustive exploration. Second, these solutions [\\[30,](#page-14-4)\
    \ [31,](#page-14-5) [62,](#page-15-5) [100\\]](#page-16-9) require KV cache transfers\
    \ during the phase transition, and the lack of design for wafer-scale 2D-mesh\
    \ topology may incur substantial communication overhead that cannot be masked\
    \ effectively by pipeline execution. Third, due to isolation among groups, the\
    \ memory of different device groups cannot be collectively utilized to store the\
    \ KV cache for a single request. And most KV cache storage is concentrated in\
    \ the decoding device group, leading to inefficient memory utilization.\n\nThe\
    \ above analysis reveals that employing wafer-scale chips to efficiently serve\
    \ LLMs introduces intricate trade-offs and challenges. This not only poses challenges\
    \ for architectural design but\n\nalso necessitates efficient parallel execution\
    \ strategies and scheduling solutions to fully leverage the computation, memory,\
    \ and communication resources of wafer-scale chips. To address these challenges,\
    \ we have made the following contributions:\n\n- Based on a highly configurable\
    \ and universal hardware template, we propose WSC-LLM, an efficient scheduling\
    \ and architecture co-exploration framework for wafer-scale chips. To the best\
    \ of our knowledge, this isthe first work to achieve it.\n- We specifically develop\
    \ strategies tailored to the characteristics of the wafer-scale architecture to\
    \ optimize tensor parallelism, resource allocation and placement, and memory management.\
    \ These algorithms aim to explore the most efficient disaggregated scheduling\
    \ strategies while efficiently managing KV cache storage, thereby maximizing the\
    \ utilization of hardware resources on wafer-scale chips.\n- We utilize WSC-LLM\
    \ to reveal several intriguing insights about wafer-scale architecture design\
    \ and the execution of LLM workloads as follows. (1) With the support of WSC-LLM,\
    \ wafer-scale chips equipped with moderate DRAM capacity per die can effectively\
    \ balance computation, storage, and communication resources, thereby delivering\
    \ the best LLM service quality. (2) Supported by our advanced memory management\
    \ strategies, achieving efficient LLM service requires not only high DRAM bandwidth\
    \ but also sufficient communication bandwidth. A balance between memory and communication\
    \ resources is essential for optimal performance. (3) Our investigation into the\
    \ impact of various optimization strategies on LLM inference performance reveals\
    \ that the memory optimization strategies in WSC-LLM contribute more significantly\
    \ to enhancing LLM service quality compared to computation optimization strategies.\n\
    - Compared to the SOTA LLM service systems, WSC-LLM can achieve an average overall\
    \ performance improvement of 3.12× across various LLM models and datasets.\n\n\
    # 2 Background\n\n# 2.1 LLM Inference\n\nMost popular large language models (LLMs)\
    \ [\\[2,](#page-12-5) [8,](#page-12-0) [9,](#page-12-1) [76,](#page-16-1) [77\\\
    ]](#page-16-2) employ a decoder-only Transformer architecture [\\[78\\]](#page-16-10).\
    \ The model consists of a stack of transformer layers, each containing an attention\
    \ layer and a feed-forward network (FFN) layer. Attention layers make tokens in\
    \ a request interact with other tokens, while FFN layers process requests in a\
    \ token-wise manner.\n\nAs shown in Fig. [2,](#page-2-0) LLM inference process\
    \ comprises two distinct phases: prefill and decoding. During the prefill phase,\
    \ the LLM model processes all input tokens in parallel upon receiving a user request,\
    \ generating the first token while caching the computed key-value (KV) pairs across\
    \ all transformer layers. This cached data, formally termed KV cache [\\[63\\\
    ]](#page-15-6), eliminates redundant computation of historical token matrices\
    \ during subsequent decoding phase. The prefill process exhibits characteristics\
    \ similar to the training forward pass and is computation-intensive. In the decoding\
    \ phase, it uses the last generated token and the KV cache as inputs to generate\
    \ tokens iteratively until reaching the termination token. The KV cache exhibits\
    \ linear growth proportional to both the model\n\n<span id=\"page-2-0\"></span>![](_page_2_Figure_11.jpeg)\n\
    \nFigure 2: Prefill and decoding phase in the LLM inference.\n\nparameters and\
    \ the input sequence length. For GPT-175B [\\[13\\]](#page-13-0), a single request\
    \ with an input sequence length of 1,024 tokens generates approximately 4.5 GB\
    \ of KV cache. Consequently, the decoding phase involves massive KV cache accesses,\
    \ imposing significant demands on both memory capacity and bandwidth. Furthermore,\
    \ transferring KV cache data between different hardware devices imposes substantial\
    \ communication bandwidth demands.\n\n# 2.2 LLM Serving System\n\nWith the increasing\
    \ size of models, model parallelism has become essential for both training and\
    \ inference. Model parallelism allows a model to be distributed across multiple\
    \ devices. The two primary types of model parallelism used in inference are tensor\
    \ parallelism (TP) [\\[48,](#page-14-6) [54,](#page-15-7) [64,](#page-15-8) [70,](#page-16-11)\
    \ [87,](#page-16-12) [88,](#page-16-13) [92,](#page-16-14) [93,](#page-16-15)\
    \ [97\\]](#page-16-16) and pipeline parallelism (PP) [\\[12,](#page-13-3) [21,](#page-14-7)\
    \ [31,](#page-14-5) [36,](#page-14-8) [49,](#page-15-9) [56\\]](#page-15-10).\
    \ TP divides tensors along specific dimensions, distributing the partitioned computations\
    \ across multiple devices to allow concurrent execution of different parts of\
    \ an operator. In contrast, PP distributes different operators in the computational\
    \ graph across devices, representing an orthogonal approach. Typically, TP is\
    \ more effective than PP for devices within the same large high-bandwidth domain\
    \ [\\[62\\]](#page-15-5). Given the wafer-scale chip's capability to deliver a\
    \ large high-bandwidth domain across the entire wafer, our optimization efforts\
    \ focus primarily on TP.\n\nIn real-time online LLM service systems [\\[10,](#page-13-2)\
    \ [45,](#page-14-0) [85,](#page-16-3) [91\\]](#page-16-17), continuous batching\
    \ is a commonly used technique that processes the prefill and decoding phases\
    \ of multiple requests in each iteration. However, since the prefill and decoding\
    \ phases are constrained by computation and memory bandwidth, respectively, continuous\
    \ batching results in prefill-decoding interference. To address this issue, disaggregated\
    \ inference [\\[31,](#page-14-5) [62,](#page-15-5) [72,](#page-16-18) [100\\]](#page-16-9)\
    \ separates prefill and decoding phases onto different devices. While this approach\
    \ can enhance service quality and achieve simultaneous optimization of the prefill\
    \ and decoding phases, challenges such as imbalanced resource allocation, additional\
    \ communication overhead, and suboptimal memory utilization persist.\n\n# <span\
    \ id=\"page-2-1\"></span>2.3 Wafer-scale Chip\n\nBenefiting from advances in packaging\
    \ technologies like Through-Silicon Vias (TSV), Chip on Wafer on Substrate (CoWoS),\
    \ Redistribution Layer (RDL) [\\[26,](#page-14-9) [29,](#page-14-2) [32,](#page-14-10)\
    \ [34,](#page-14-11) [46,](#page-14-12) [47,](#page-14-13) [95\\]](#page-16-19),\
    \ wafer-scale chip is a feasible and promising architectural solution to further\
    \ scaling up chip sizes. There are two mainstream technical routes. The first\
    \ is monolithic integration, where all compute dies and interconnects are directly\
    \ integrated on wafer-scale chip [\\[50,](#page-15-3) [60,](#page-15-2) [68\\\
    ]](#page-15-11). The other method is chiplet-based integration, in which compute\
    \ dies are fabricated separately and then integrated into the wafer-scale chip\
    \ along with interconnects [\\[60,](#page-15-2) [61,](#page-15-1) [69,](#page-15-12)\
    \ [81\\]](#page-16-20). This approach enables flexibility, configurability, and\
    \ high scalability through integration\n\n<span id=\"page-3-0\"></span>![](_page_3_Figure_1.jpeg)\n\
    \nFigure 3: Architecture of wafer-scale chips.\n\nwith diverse devices on the\
    \ wafer. Moreover, compared to the first approach, chiplet-based wafer-scale chips\
    \ made through mature technology and production enhance yield and reduce redundancy.\
    \ Therefore, we adopt the chiplet-based integration.\n\n(1) Overall Architecture:\
    \ As depicted in Fig. [3\\(](#page-3-0)a), the hardware template architecture\
    \ consists of three hierarchical levels: wafer, die, and computing core. The architecture\
    \ employs a mesh topology for the 215 mm×215 mm wafer, utilizing Die-to-Die (D2D)\
    \ interconnect interfaces to facilitate communication between adjacent dies. This\
    \ configuration supports various communication types, such as Dieto-Die, Die-to-DRAM,\
    \ and DRAM-to-Die. Although alternative topologies, such as torus and fully-connected\
    \ designs, are feasible, the constraints on interconnect distances and the need\
    \ to maintain signal integrity often reduce bandwidth and efficiency. Therefore,\
    \ the mesh topology remains the optimal choice.\n\n(2) Die Architecture: Each\
    \ die, measuring 24.99 mm×33.25 mm, integrates both computational and storage\
    \ functionalities. This integration marks a significant shift from conventional\
    \ chiplet designs [\\[15,](#page-13-4) [69,](#page-15-12) [74\\]](#page-16-21),\
    \ which typically separate these functions. As shown in Fig. [3\\(](#page-3-0)a),\
    \ each die comprises multiple computing cores, on-die memory (DRAM chiplets),\
    \ NoC interconnections, and D2D interconnect interfaces. The NoC interconnections\
    \ also follow a mesh topology, facilitating high-speed communication between cores.\
    \ The placement of D2D interfaces around the die periphery contributes to a scalable\
    \ architecture.\n\n(3) Computing Core Architecture: As illustrated in Fig. [3\\\
    (](#page-3-0)a), each computing core is responsible for executing computational\
    \ tasks. The controller manages core operations using pre-defined instructions\
    \ and coordinates data transfers across external cores and DRAM. DMA and NoC systems\
    \ handle communication between cores, ensuring efficient data flow within the\
    \ framework. Each core has globally shared SRAM, while the PE array and vector\
    \ unit performs General Matrix Multiplication (GEMM)/General Matrix-Vector Multiplication\
    \ (GEMV) and vector/scalar operations, respectively.\n\n(4) Configurable Parameters:\
    \ Our hardware template offers extensive configurability and scalability suited\
    \ to wafer-scale chips, including the number of dies in both horizontal and vertical\
    \ directions, DRAM and SRAM bandwidth and capacity, as well as NoC and D2D bandwidth.\
    \ Additionally, the parameters of the\n\ncompute die are also configurable, encompassing\
    \ the number of cores in the horizontal direction, the number of cores in the\
    \ vertical direction, and the number of MACs in the PE array. The design of the\
    \ PE array and its dataflows are derived from existing works [\\[14,](#page-13-5)\
    \ [17,](#page-13-6) [19,](#page-13-7) [27,](#page-14-14) [41,](#page-14-15) [43,](#page-14-16)\
    \ [74,](#page-16-21) [80,](#page-16-22) [90\\]](#page-16-23).\n\n(5) Package Sectional\
    \ View: As illustrated in Fig. [3\\(](#page-3-0)b), the wafer-scale chip employs\
    \ an interposer layer to enable high-speed communication both between compute\
    \ dies and between compute dies and DRAM. This design avoids low-speed communication\
    \ by bypassing the multiple dielectric transitions typically encountered from\
    \ the chip, through the interposer and substrate, to the PCB. Consequently, this\
    \ configuration improves communication speed (higher D2D bandwith), reduces transmission\
    \ distances, and enhances signal integrity.\n\n# 3 Motivation\n\n# 3.1 Trade-off\
    \ Introduced by DRAM capacity\n\nLLM workloads impose substantial demands on computation,\
    \ storage, and communication resources. Although wafer-scale technology can effectively\
    \ scale monolithic devices, it remains constrained by the limits of the wafer\
    \ size. For example, the maximum usable area of a 12-inch wafer is approximately\
    \ 46225 <sup>2</sup> (215 × 215 ) [\\[33\\]](#page-14-1). Consequently, wafer-scale\
    \ chips necessitate trade-offs among various hardware resources. However, existing\
    \ research has not examined this. In this section, we discuss the benefits and\
    \ drawbacks of increasing DRAM capacity on wafer-scale chips.\n\nFig. [4\\(](#page-4-0)a)\
    \ and (b) illustrate large and small DRAM capacities configurations within a single\
    \ die, respectively. In Fig. [4\\(](#page-4-0)a), DRAM chiplets are positioned\
    \ on both sides of the compute die, while in Fig. [4\\(](#page-4-0)b), they are\
    \ arranged on only one side. Increasing the number of DRAM chiplets allows for\
    \ higher DRAM bandwidth by utilizing more compute-die interconnect interfaces.\n\
    \nAs shown in Fig. [4,](#page-4-0) expanding DRAM capacity on wafer-scale chips\
    \ offers two primary advantages. First, increased DRAM capacity allows for storing\
    \ more weights and KV caches, theoretically enabling the simultaneous processing\
    \ of a higher number of LLM user requests. Second, it can enhance DRAM bandwidth,\
    \ which is particularly crucial for decoding, as this phase is memory-bound. Given\
    \ that the decoding phase often constitutes the majority of the runtime across\
    \ most workloads, higher DRAM bandwidth can significantly improve the quality\
    \ of service for LLMs by reducing memory-related bottlenecks.\n\nHowever, Fig.\
    \ [4](#page-4-0) also highlights its associated disadvantages. First, the number\
    \ of interconnect interfaces is fixed for the same computing die. An increase\
    \ in the number of DRAM chiplets results in a higher proportion of computing-die\
    \ interfaces being occupied by memory access bandwidth, reducing the number of\
    \ interfaces available for Die-to-Die (D2D) communication. Consequently, the D2D\
    \ communication bandwidth decreases, thereby limiting inter-die data transfer\
    \ efficiency. Second, additional DRAM chiplets occupy more wafer area, leading\
    \ to a reduction in the total number of dies per wafer and consequently lowering\
    \ the overall computational capacity of the system.\n\nThese trade-offs underscore\
    \ the critical balance between DRAM bandwidth and capacity, D2D bandwidth, and\
    \ computational resources for achieving optimal LLM performance.\n\n<span id=\"\
    page-4-0\"></span>![](_page_4_Figure_1.jpeg)\n\nFigure 4: Trade-offs Introduced\
    \ by DRAM capacity on waferscale chips.\n\n#### 3.2 Problems with existing LLM\
    \ serving systems\n\nAlthough existing LLM serving systems [30, 31, 62, 100] recognize\
    \ the differences between the prefill and decoding phases and map them to separate\
    \ machines, they still face the following issues.\n\nFirst, these systems overlook\
    \ the exploration of optimal TP granularity and resource allocation and placement\
    \ for both prefill and decoding instances. Current approaches typically set the\
    \ TP size to a fixed value such as 8 GPUs (one node) and rely on engineering experience\
    \ for instance allocation. Fig. 5(a) shows the average iteration time for the\
    \ prefill and decoding phase in LLaMA3-70B under the optimal TP strategy across\
    \ different TP sizes and batch sizes. Detailed TP strategies and compute die configurations\
    \ are provided in Section 4.5.1 and Section 5.1. The result reveals that increasing\
    \ TP size accelerates the computation-intensive prefill phase while saving DRAM\
    \ space. However, in the decoding phase with lighter computational demands, larger\
    \ TP size may reduce performance due to additional communication overhead. Thus,\
    \ the optimal TP size may differ between two phases, necessitating distinct execution\
    \ strategies. Furthermore, the optimal allocation and placement of hardware resources\
    \ between two phases remains an open problem requiring systematic exploration.\
    \ Prefill phase Decoding phase *(a) Iteration time for LLaMA3-70B with different\
    \ size of TP partition*\n\nSecond, these systems require KV cache transfers from\
    \ prefill to decoding instances, resulting in substantial inter-node communication\
    \ overhead. Due to the limited communication bandwidth across GPU clusters, this\
    \ overhead cannot be effectively overlapped with computation. While wafer-scale\
    \ chips provide higher bandwidth that holds promise for solving this probelm,\
    \ their 2D-mesh topology differs fundamentally from the all-to-all topology of\
    \ GPU clusters. Without topology-aware scheduling, communication traffic becomes\
    \ imbalanced, leading to severe congestion on certain links and making high communication\
    \ overhead perisist.\n\nThird, the memory utilization of these systems is suboptimal.\
    \ Fig. [5\\(](#page-4-1)b) illustrates peak DRAM utilization for prefill and decoding\
    \ instances when running the LLaMA3-70B model and the conversation dataset on\
    \ wafer-scale chips, where each die is equipped with 96GB of DRAM. Details of\
    \ the die configurations and workloads are provided in Section [5.1.](#page-8-0)\
    \ DRAM utilization in prefill instances is significantly low due to the KV cache\
    \ transfer. Furthermore, decoding instances also cannot utilize all DRAM resources\
    \ because they cannot store the KV cache of the same request across instances.\n\
    \n<span id=\"page-4-1\"></span>![](_page_4_Figure_8.jpeg)\n\n*(b) Peak memory\
    \ of each DRAM for LLaMA3-70B on the wafer-scale chip.*\n\nFigure 5: Request performance\
    \ under different TP sizes in different phases and memory utilization in prefill\
    \ and decoding instances.\n\n#### 4 WSC-LLM Framework\n\n#### 4.1 WSC-LLM Overview\n\
    \nWSC-LLM is a scheduling and architecture co-exploration framework for wafer-scale\
    \ chips, marking the first work of its kind. As depicted in the left of Fig. 6,\
    \ the inputs for WSC-LLM consist of architecture parameter candidates, LLM models,\
    \ and workload. Within the constraints of wafer area limitations, architecture\
    \ parameter candidates are derived from arbitrary combinations of configurable\
    \ parameters (introduced in Section 2.3).\n\n**Prefill Decoding** All architectural\
    \ candidates are exhaustively explored by the Scheduling Engine to achieve optimal\
    \ LLM service quality on waferscale chip with varying architecture parameters.\
    \ At the core of the Scheduling Engine is the Central Scheduler, which leverages\
    \ the TP Engine to explore the optimal TP configurations, resource allocation\
    \ and placement strategies while managing the allocation of user requests within\
    \ the Execution Engine. The exploration of the optimal TP configuration and resource\
    \ allocation (Section [4.2.1\\)](#page-4-2), as well as the resource placement\
    \ strategy (Section [4.2.2\\)](#page-5-1), are both executed offline. Therefore,\
    \ they do not introduce any additional system overhead during the online execution\
    \ of the Central Scheduler. Furthermore, the Memory Scheduler determines the optimal\
    \ KV cache storage location, ensuring efficient utilization of storage resources\
    \ and minimizing additional communication overhead.\n\n#### 4.2 Central Scheduler\n\
    \n<span id=\"page-4-2\"></span>4.2.1 Optimal Resource Partition Strategy. Motivated\
    \ by the need to support the dynamic nature of LLM workloads efficiently, we map\
    \ the prefill and decoding phases to separate hardware resources, creating distinct\
    \ instances of prefill and decoding phases. However, this approach introduces\
    \ two new unresolved issues: determining the optimal instance size and TP setup\
    \ for each phase and selecting\n\n<span id=\"page-5-0\"></span>![](_page_5_Figure_2.jpeg)\n\
    \nFigure 6: WSC-LLM framework.\n\nthe appropriate number of instances for the\
    \ prefill and decoding phases.\n\nTo tackle these challenges, we propose Algorithm\
    \ [1,](#page-6-0) which systematically optimizes the resource partitioning for\
    \ prefill and decoding phases. The algorithm first optimizes the prefill and decoding\
    \ instance size along with their TP setup independently to achieve phase-level\
    \ optimal per-die throughput. Subsequently, to achieve globally optimal performance,\
    \ we employ a replication strategy, which means the instance size for each phase\
    \ is uniform, and the same TP strategy is applied across the same phase. At last,\
    \ the number of prefill and decoding instances is determined based on the traffic\
    \ balance between the prefill phase and the decoding phase.\n\nGiven the model\
    \ , constraints on the number of dies per instance, memory capacity of a die ,\
    \ workload , TP partition strategy (Detailed in Section [4.5.1\\)](#page-7-0),\
    \ and the total number of dies , the algorithm can determine the best partitioning\
    \ setup \\_ and \\_) and allocates the appropriate number of instances for each\
    \ phase ( and ). The workload is obtained by resampling the distribution after\
    \ fitting the test dataset (introduced in Section [5.1\\)](#page-8-0).\n\nThe\
    \ algorithm iterates through all feasible instance sizes, which are constrained\
    \ by the die limit per instance (line 3) and the total memory capacity (line 4).\
    \ Additionally, it is important to note that, in order to satisfy the communication\
    \ requirements imposed by the 2D-mesh topology of wafer-scale chips, the arrangement\
    \ of dies within each instance must be a rectangular shape, which means that linear\
    \ or other irregular shapes are not permitted. The algorithm then enumerates all\
    \ possible TP partition strategies for a fixed instance size (line 5). Subsequently,\
    \ we determine the goodput for the prefill phase by the prefill workload \\_ (line\
    \ 7), and similarly, determine it for the decoding phase by the decoding workload\
    \ \\_ (line 8). Notably, the function operates as a simulator that executes workload\
    \ on a single instance, without incorporating cross-instance scheduling. While\
    \ throughput is influenced by both placement and scheduling strategies, the goodput\
    \ measured here solely reflects the performance within a single\n\ninstance. It\
    \ represents the upper performance bound that can be achieved after optimized\
    \ global scheduling, helping identify setup with greater potential for scheduling\
    \ improvements. Once the optimal setup for both phases are identified (lines 9-13),\
    \ we replicate them to achieve the optimal overall performance by balancing the\
    \ throughput of each phase. We determine the number of prefill and decoding instances\
    \ according to the total number of dies and the proportion of their throughput\
    \ (lines 18-19).\n\nThe complexity of Algorithm [1](#page-6-0) is (), where is\
    \ the die limit per instance, and is the number of pre-defined TP partition strategies.\
    \ The search space remains manageable, with a solving time of only a few minutes,\
    \ even in the largest instance and LLM model scenario.\n\n<span id=\"page-5-1\"\
    ></span>4.2.2 Optimal Resource Placement Strategy. In Section [4.2.1,](#page-4-2)\
    \ we explored the optimal instance sizes and numbers for the prefill and decoding\
    \ phases. However, due to the characteristics of the 2Dmesh topology in wafer-scale\
    \ chips, it is also necessary to determine the optimal placement of each instance.\n\
    \nSince the results of prefill instances (i.e., KV cache) are used by decoding\
    \ instances, data transfer only occurs from prefill instances to decoding instances.\
    \ Given this scenario, the objective is to minimize the total distance between\
    \ each prefill instance and its nearest decoding instance while avoiding overlapping\
    \ transmission paths. To achieve this, we adopt a strategy that prioritizes central\
    \ placement for decoding instances: the decoding instances are placed in the central\
    \ region first, followed by the placement of prefill instances around the perimeter.\
    \ If placement schemes are not unique, we select the one that minimizes the total\
    \ communication cost. This selection is formalized as:\n\n$$\\text{TransferCost}\
    \ = \\sum\\_{i=0}^{NP-1} \\min\\_{j=0}^{ND-1} \\text{Distance}(P\\_i, D\\_f) \\\
    tag{1}$$\n\nwhere and denote the number of prefill and decoding instances, respectively.\
    \ and represent the center positions of prefill and decoding instances, where\
    \ ∈ [0, − 1] and ∈\n\nAlgorithm 1: Optimal Resource Partition Algorithm\n\n<span\
    \ id=\"page-6-0\"></span>\n\n| Input: Model \U0001D440, dies limit<br>\U0001D437\
    , memory capacity per die<br>\U0001D450,                                     |\
    \  |  |  |  |\n|----------------------------------------------------------------------------------------------------|--|--|--|--|\n\
    | workload \U0001D44A<br>, TP split strategy \U0001D446, total dies<br>\U0001D441\
    .                                              |  |  |  |  |\n| Output: \U0001D45D\
    \U0001D45F\U0001D452 \U0001D453 \U0001D456\U0001D459\U0001D459_\U0001D460\U0001D452\
    \U0001D461\U0001D462\U0001D45D,<br>\U0001D451\U0001D452\U0001D450\U0001D45C\U0001D451\
    \U0001D456\U0001D45B\U0001D454_\U0001D460\U0001D452\U0001D461\U0001D462\U0001D45D\
    , number of prefill                                      |  |  |  |  |\n| instances\
    \ \U0001D441 \U0001D443, number of decoding instances<br>\U0001D441\U0001D437\
    .                                                 |  |  |  |  |\n| 1 \U0001D45D\
    \U0001D45F\U0001D452 \U0001D453 \U0001D456\U0001D459\U0001D459_\U0001D460\U0001D452\
    \U0001D461\U0001D462\U0001D45D<br>← ∅                                        \
    \                                   |  |  |  |  |\n| 2 \U0001D451\U0001D452\U0001D450\
    \U0001D45C\U0001D451\U0001D456\U0001D45B\U0001D454_\U0001D460\U0001D452\U0001D461\
    \U0001D462\U0001D45D<br>← ∅                                                  \
    \                          |  |  |  |  |\n| 3 for \U0001D456\U0001D45B\U0001D460\
    <br>∈ {1,<br>2, , \U0001D437}<br>do                                          \
    \                      |  |  |  |  |\n| if \U0001D440.\U0001D464\U0001D452\U0001D456\
    \U0001D454ℎ\U0001D461<br><<br>\U0001D441<br>·\U0001D450<br>then<br>4<br>\U0001D456\
    \U0001D45B\U0001D460                                                    |  | \
    \ |  |  |\n| for \U0001D460<br>∈ \U0001D446<br>do<br>5                       \
    \                                                     |  |  |  |  |\n| \U0001D447\
    \ \U0001D443<br>← (\U0001D440,\U0001D456\U0001D45B\U0001D460, \U0001D460)<br>6\
    \                                                                           |\
    \  |  |  |  |\n| \U0001D45D\U0001D45F\U0001D452 \U0001D453 \U0001D456\U0001D459\
    \U0001D459_\U0001D454\U0001D45C\U0001D45C\U0001D451\U0001D45D\U0001D462\U0001D461\
    <br>← \U0001D461\U0001D452\U0001D460\U0001D461(\U0001D447 \U0001D443,\U0001D44A\
    <br>_\U0001D45D\U0001D452\U0001D45F \U0001D453 \U0001D456\U0001D459\U0001D459\
    )<br>7                                              |  |  |  |  |\n| \U0001D451\
    \U0001D452\U0001D450\U0001D45C\U0001D451\U0001D456\U0001D45B\U0001D454_\U0001D454\
    \U0001D45C\U0001D45C\U0001D451\U0001D45D\U0001D462\U0001D461<br>← \U0001D461\U0001D452\
    \U0001D460\U0001D461(\U0001D447 \U0001D443,\U0001D44A<br>_\U0001D451\U0001D452\
    \U0001D450\U0001D45C\U0001D451\U0001D456\U0001D45B\U0001D454)<br>8           \
    \                                     |  |  |  |  |\n| if \U0001D45D\U0001D45F\
    \U0001D452 \U0001D453 \U0001D456\U0001D459\U0001D459_\U0001D460\U0001D452\U0001D461\
    \U0001D462\U0001D45D.\U0001D454\U0001D45C\U0001D45C\U0001D451\U0001D45D\U0001D462\
    \U0001D461<br>\U0001D45D\U0001D45F\U0001D452 \U0001D453 \U0001D456\U0001D459\U0001D459\
    _\U0001D454\U0001D45C\U0001D45C\U0001D451\U0001D45D\U0001D462\U0001D461<br><<br>then<br>9<br>\U0001D456\
    \U0001D45B\U0001D460                           |  |  |  |  |\n| \U0001D45D\U0001D45F\
    \U0001D452 \U0001D453 \U0001D456\U0001D459\U0001D459_\U0001D460\U0001D452\U0001D461\
    \U0001D462\U0001D45D.\U0001D447 \U0001D443.\U0001D456\U0001D45B\U0001D460<br>\U0001D45D\
    \U0001D45F\U0001D452 \U0001D453 \U0001D456\U0001D459\U0001D459_\U0001D460\U0001D452\
    \U0001D461\U0001D462\U0001D45D<br>← (\U0001D447 \U0001D443, \U0001D45D\U0001D45F\
    \U0001D452 \U0001D453 \U0001D456\U0001D459\U0001D459_\U0001D454\U0001D45C\U0001D45C\
    \U0001D451\U0001D45D\U0001D462\U0001D461)<br>10                     |  |  |  |\
    \  |\n| end<br>11                                                            \
    \                              |  |  |  |  |\n| if \U0001D451\U0001D452\U0001D450\
    \U0001D45C\U0001D451\U0001D456\U0001D45B\U0001D454_\U0001D460\U0001D452\U0001D461\
    \U0001D462\U0001D45D.\U0001D454\U0001D45C\U0001D45C\U0001D451\U0001D45D\U0001D462\
    \U0001D461<br>\U0001D451\U0001D452\U0001D450\U0001D45C\U0001D451\U0001D456\U0001D45B\
    \U0001D454_\U0001D454\U0001D45C\U0001D45C\U0001D451\U0001D45D\U0001D462\U0001D461\
    <br><                                                 |  |  |  |  |\n| 12<br>\U0001D456\
    \U0001D45B\U0001D460<br>\U0001D451\U0001D452\U0001D450\U0001D45C\U0001D451\U0001D456\
    \U0001D45B\U0001D454_\U0001D460\U0001D452\U0001D461\U0001D462\U0001D45D.\U0001D447\
    \ \U0001D443.\U0001D456\U0001D45B\U0001D460                                  \
    \                              |  |  |  |  |\n| then                         \
    \                                                                      |  |  |\
    \  |  |\n| \U0001D451\U0001D452\U0001D450\U0001D45C\U0001D451\U0001D456\U0001D45B\
    \U0001D454_\U0001D460\U0001D452\U0001D461\U0001D462\U0001D45D<br>← (\U0001D447\
    \ \U0001D443, \U0001D451\U0001D452\U0001D450\U0001D45C\U0001D451\U0001D456\U0001D45B\
    \U0001D454_\U0001D454\U0001D45C\U0001D45C\U0001D451\U0001D45D\U0001D462\U0001D461\
    )<br>13                                                  |  |  |  |  |\n| end<br>14\
    \                                                                            \
    \              |  |  |  |  |\n| end<br>15                                    \
    \                                                      |  |  |  |  |\n| end<br>16\
    \                                                                            \
    \              |  |  |  |  |\n| 17 end                                       \
    \                                                      |  |  |  |  |\n| \U0001D441\
    \ ×\U0001D45D\U0001D45F\U0001D452 \U0001D453 \U0001D456\U0001D459\U0001D459_\U0001D460\
    \U0001D452\U0001D461\U0001D462\U0001D45D.\U0001D454\U0001D45C\U0001D45C\U0001D451\
    \U0001D45D\U0001D462\U0001D461<br>l<br>\U0001D45D\U0001D45F\U0001D452 \U0001D453\
    \ \U0001D456\U0001D459\U0001D459_\U0001D460\U0001D452\U0001D461\U0001D462\U0001D45D\
    .\U0001D454\U0001D45C\U0001D45C\U0001D451\U0001D45D\U0001D462\U0001D461+\U0001D451\
    \U0001D452\U0001D450\U0001D45C\U0001D451\U0001D456\U0001D45B\U0001D454_\U0001D460\
    \U0001D452\U0001D461\U0001D462\U0001D45D.\U0001D454\U0001D45C\U0001D45C\U0001D451\
    \U0001D45D\U0001D462\U0001D461 m<br>18 \U0001D441 \U0001D443<br>← |  |  |  | \
    \ |\n| 19 \U0001D441\U0001D437<br>← \U0001D441<br>− \U0001D441 \U0001D443    \
    \                                                                          | \
    \ |  |  |  |\n| 20 return \U0001D45D\U0001D45F\U0001D452 \U0001D453 \U0001D456\
    \U0001D459\U0001D459_\U0001D460\U0001D452\U0001D461\U0001D462\U0001D45D,<br>\U0001D451\
    \U0001D452\U0001D450\U0001D45C\U0001D451\U0001D456\U0001D45B\U0001D454_\U0001D460\
    \U0001D452\U0001D461\U0001D462\U0001D45D,<br>\U0001D441 \U0001D443,<br>\U0001D441\
    \U0001D437                                        |  |  |  |  |\n|           \
    \                                                                            \
    \             |  |  |  |  |\n\n[0, −1]. refers to the shortest path between prefill\
    \ and decoding instances. In cases where multiple shortest paths exist, all such\
    \ paths are enumerated. Among these, a non-overlapping path configuration is selected\
    \ to achieve optimal matching.\n\nIt is noteworthy to add a hyperparameter ≥ 1\
    \ when there is the reuse of communication links. For example, in Fig. [7\\(](#page-6-1)a),\
    \ the Distance(1, 1) = 2 hops, whereas Distance(5, 1) = (2 + 2) hops, because\
    \ the paths from <sup>5</sup> to <sup>1</sup> and <sup>1</sup> to <sup>1</sup>\
    \ share 2 hops. The value of depends on the number of shared links and the ratio\
    \ between D2D and DRAM bandwidths. For instance, in the scenario depicted in Fig.\
    \ [7\\(](#page-6-1)a), the number of shared links is 2 (for <sup>1</sup> and <sup>5</sup>\
    \ to 1). Therefore, if the D2D bandwidth exceeds twice the DRAM bandwidth, then\
    \ = 1.\n\nFig. [7\\(](#page-6-1)a) and (b) illustrate the placement results of\
    \ applying the random strategy and our decoding-centered strategy, respectively,\
    \ on a wafer-scale chip with 48 dies, where NP is 8 and ND is 4. The corresponding\
    \ TransferCost are (8 + 16) hops and 16 hops, respectively, demonstrating the\
    \ superiority of our approach.\n\n# 4.3 Execution Scheduler\n\n4.3.1 Prefill Pool.\
    \ In the prefill pool, each prefill instance maintains a queue to manage user\
    \ requests assigned to it. Each queue employs a first-come-first-serve (FCFS)\
    \ strategy for scheduling batched requests. To maximize hardware utilization when\
    \ handling irregular prefill prompt lengths, we adopt the chunked\n\n<span id=\"\
    page-6-1\"></span>![](_page_6_Figure_9.jpeg)\n\nprefill technique [\\[10\\]](#page-13-2).\
    \ This approach divides long prompts into uniformly sized chunks, which are processed\
    \ iteratively during the prefill phase. By combining batching with chunked prefill\
    \ technology, we can efficiently handle input prompts of varying lengths across\
    \ diverse hardware resources.\n\nTaking mapping scheduling in Fig. [8](#page-7-1)\
    \ as an example, the prefill pool contains three queues with 8 dies. <sup>1</sup>\
    \ handles requests 1 and 2, <sup>2</sup> manages request 3 and newly arrived request\
    \ 6, and <sup>3</sup> processes requests 4 and 5. The central scheduler has allocated\
    \ request 6 to the least occupied queue, which is 2. Similarly, requests 7 and\
    \ 8 are allocated to <sup>3</sup> and 1, respectively. Due to the long input tokens\
    \ of requests 3 and 4, the execution of each request individually can reach the\
    \ hardware's computation intensive region. Consequently, the batching strategy\
    \ is not employed for these requests, thereby avoiding the additional time required\
    \ to batch with subsequent requests. Specifically, <sup>2</sup> will only process\
    \ request 3 and not request 6, while <sup>3</sup> will exclusively execute request\
    \ 4 without handling request 5.\n\n4.3.2 Decoding Pool. In the decoding pool,\
    \ each decoding instance maintains a queue to manage user requests. Each queue\
    \ also utilizes the FCFS method to process batched requests. Given the lower computation\
    \ requirements of the decoding phase, we aim to maximize the efficiency by batching\
    \ tokens whenever possible. To achieve this, we utilize the continuous batching\
    \ technique [\\[91\\]](#page-16-17), which allows parallel computation of batched\
    \ tokens for non-attention operations while sequentially computing attention operations.\n\
    \nAs shown in Fig. [8,](#page-7-1) we assume the decoding pool has three queues\
    \ and each queue is equipped with 4 dies. <sup>1</sup> holds requests 1 and 2,<sup>2</sup>\
    \ includes requests 3, and<sup>3</sup> contains requests 4 and 5. When generating\
    \ the next token, requests 1 and 2 are inserted after request 8. These three requests\
    \ are batched together and executed on dies. <sup>2</sup> executes requests 6\
    \ and 3. Since request 5 only decodes once and terminates, <sup>3</sup> runs requests\
    \ 7 and 4.\n\n# 4.4 Memory Scheduler\n\nWafer-scale chips offer high D2D bandwidth,\
    \ typically exceeding DRAM access bandwidth. Thus, in the absence of D2D link\
    \ congestion, cross-die DRAM read and write operations are constrained only by\
    \ DRAM bandwidth rather than D2D bandwidth. Leveraging this advantage of the wafer-scale\
    \ architecture, the extensive interdie transfer of KV cache can be overlapped\
    \ by its DRAM access,\n\n<span id=\"page-7-1\"></span>![](_page_7_Figure_2.jpeg)\n\
    \nFigure 8: The architecture of the Execution Scheduler.\n\nallowing us to fully\
    \ utilize all DRAMs on the wafer for KV cache storage to achieve higher performance\
    \ of LLM service.\n\nAs illustrated in Algorithm [2,](#page-7-2) we implement\
    \ an optimal KV cache placement algorithm to minimize the additional D2D transmission\
    \ overhead by allocating the optimal storage location for each request's KV cache.\
    \ Given a group requests , these are executed by prefill instance and decoding\
    \ instance . The DRAM set is represented by . The algorithm aims to find the optimal\
    \ storage location of KV cache for each request.\n\nThe algorithm first identifies\
    \ the associated set of DRAMs ′ that require no additional communication, which\
    \ is determined by the shortest transmission paths of prefill and decoding instances\
    \ and their respective locations (line 1). For instance, in Fig. [7\\(](#page-6-1)b),\
    \ the associated DRAM set for <sup>1</sup> and <sup>1</sup> includes all DRAMs\
    \ within 1, <sup>1</sup> and 2. The algorithm then sorts these DRAMs based on\
    \ their locations and available capacities, forming a priority queue where location\
    \ priority supersedes residual capacity (line 2).\n\nSubsequently, for each request\
    \ (line 3), an empty storage set is initialized (line 4), and the total remaining\
    \ DRAM capacity of is verified to ensure it meets the storage requirement of KV\
    \ cache (line 5). If the total capacity of all remaining DRAMs in is insufficient\
    \ to satisfy the KV cache demands of , the memory scheduler notifies the central\
    \ scheduler to stop the execution of and subsequent requests on the prefill instance\
    \ (line 6). Otherwise, the algorithm iteratively allocates DRAM resources to the\
    \ request (line 8-18). The capacity of each DRAM unit is reduced by the portion\
    \ utilized during the allocation process. After calculating the remaining capacity,\
    \ the DRAM unit is reinserted into the queue , where it is reordered to maintain\
    \ the queue's dynamic prioritization for subsequent allocations. The allocation\
    \ loop continues until the entire KV cache requirement of is met.\n\nThe complexity\
    \ of Algorithm [2](#page-7-2) can be approximately regarded as (), where represents\
    \ the number of batched requests executed in a instance, typically not exceeding\
    \ 16. Therefore, the decision-making process of the memory scheduler is highly\
    \ efficient, ensuring that it does not hinder the execution of WSC-LLM.\n\n# 4.5\
    \ Operator Execution Engine\n\nTo efficiently execute LLM operators, computational\
    \ tasks must be partitioned and mapped across different dies and cores. As illustrated\
    \ in Fig. [9,](#page-8-1) taking the GEMM-type operator as an example, it can\
    \ be partitioned along four dimensions: , , , and . represents the batch size,\
    \ represents the sequence length, represents the hidden size, and represents the\
    \ other dimension of the weight matrix apart from . Other operator types, such\
    \ as Vector (e.g., nonlinear activation functions), feed-forward network (FFN),\
    \ and Attention mechanism in Transformer architectures [\\[78\\]](#page-16-10),\
    \ can also\n\n|  |  | Algorithm 2: Optimal KV Cache Placement Algorithm |\n|--|--|---------------------------------------------------|\n\
    |--|--|---------------------------------------------------|\n\n```\nInput: Batched\
    \ requests set  = {1, 2, . . . , }, DRAM set\n        = {1, 2, . . . , }, prefill/decoding\
    \ instance /\n                                               .\n  Output: KV cache\
    \ storage set {1, 2, . . . , }.\n1 \n   ′ ← Relevant(\n                , )\n2\
    \  ← Sort(\n           ′\n            .location, ′\n                      .capacity)\n\
    3 for  ∈ {1, 2, . . . , } do\n4  ← ∅\n5 if Í\n\n         ′∈ \n             ′\n\
    \              .capacity < \n                        .kv then\n6 break\n7 end\n\
    8 else\n9 while \n              .kv > 0 do\n10 \n           ′ ← .pop()\n11 \n\
    \            .add(\n                 ′\n                  )\n12 if \n        \
    \     ′\n              .capacity > \n                        .kv then\n13 \n \
    \             ′\n               .capacity −= \n                           .kv\n\
    14 .insert(\n                     ′\n                      )\n15 break\n16 end\n\
    17 else\n18 \n               .kv −= \n                     ′\n               \
    \       .capacity\n19 end\n20 end\n21 end\n22 end\n23 return {1, 2, . . . , }\n\
    ```\nbe partitioned across multiple dimensions in a manner similar to the GEMM\
    \ operator.\n\nThe computational task allocation strategy is structured into two\
    \ levels: (1) Die-level (Section [4.5.1\\)](#page-7-0): The TP Engine partitions\
    \ the overall computational task into sub-computation tasks and maps them across\
    \ different dies. (2) Core-level (Section [4.5.2\\)](#page-8-2): Each subtask\
    \ is further partitioned into atom-computation tasks, with each core executing\
    \ a single atom-computation per time unit.\n\n<span id=\"page-7-0\"></span>4.5.1\
    \ TP Engine. We classify TP partition strategies according to the number of partition\
    \ dimensions in sub-computation tasks, categorizing them into two main types:\
    \ basic TP partition strategy and hybrid TP partition strategy.\n\nBasic TP partition\
    \ strategy refers to partitioning an operator along a single dimension. Taking\
    \ the GEMM operator as an example, it involves four dimensions: , , , and , corresponding\
    \ to four basic TP partition strategies, respectively. Partitioning along , and\
    \ requires All-gather communication among different dies within the same instance,\
    \ and partitioning along involves Allreduce communication due to the summation\
    \ operations.\n\nHybrid TP partition strategies involve operator partition across\
    \ multiple dimensions. While each additional partition dimension incurs extra\
    \ All-gather or All-reduce communication overhead, it enables larger TP and instance\
    \ sizes. Since each instance retains a full copy of the model weights, increasing\
    \ the instance size reduces the memory footprint of model weights in DRAMs, and\
    \ provides\n\n<span id=\"page-8-1\"></span>![](_page_8_Figure_2.jpeg)\n\n*(b)\
    \ GEMM partition and mapping*\n\nFigure 9: All-reduce communication process and\
    \ the twolevel operator partition and mapping.\n\nthe Memory Scheduler with greater\
    \ optimization space, ultimately leading to potential improved overall system\
    \ performance.\n\nTP partition strategies for other operator types follow a similar\
    \ definition as for GEMM. Notably, for the Attention operator, we prioritize partition\
    \ it along the Q, K, and V heads, which form the basic TP partition strategy.\
    \ LLaMA3-70B employs the Grouped Query Attention (GQA) mechanism, featuring only\
    \ eight KV heads. As shown in Fig. [5,](#page-4-1) when TP size is 16, a hybrid\
    \ TP partition strategy is required. In this case, each K and V head is further\
    \ partitioned along an additional dimension, such as , ensuring that each die\
    \ retains only half of the K and V head.\n\nWe implement the All-gather and All-reduce\
    \ communications in the TP process using the bidirectional Ring algorithm [\\\
    [82\\]](#page-16-24), which is well-suited for the 2D-mesh communication network\
    \ of wafer-scale chips. Fig. [9\\(](#page-8-1)a) illustrates the All-reduce communication\
    \ process when TP size is 4, requiring a total of four time steps. All-reduce\
    \ can be decomposed into two phases: Reduce-Scatter and All-gather, with each\
    \ phase comprising two time steps.\n\n<span id=\"page-8-2\"></span>4.5.2 Intra-Die\
    \ Engine. Upon receiving the sub-computation task assigned by the TP Engine, each\
    \ die further partitions it along multiple dimensions, such as , , , and , to\
    \ generate atomcomputation tasks. Each atom-computation task must comply with\
    \ the SRAM constraints of the computing core and is mapped to a single computing\
    \ core for execution within one time step.\n\nFig. [9\\(](#page-8-1)b) illustrates\
    \ the complete two-level task allocation process. First, the TP Engine partitions\
    \ the operator along the dimension and maps the sub-computation task to Die1.\
    \ Then, further partitioning along the and dimensions is performed, mapping each\
    \ atom-computation task based on the Weight Stationary (WS) approach, iteratively\
    \ completing the computation.\n\nIt is important to note that the WS mapping strategy\
    \ shown in Fig. [9\\(](#page-8-1)b) may not always be the most efficient, as it\
    \ might overlook opportunities to reuse SRAM within computing cores to reduce\
    \ system overhead. Therefore, we further explore optimal intra-die mapping strategies\
    \ to achieve the best execution performance. To exploring the mapping space of\
    \ atom-computation tasks efficiently, we employ heuristic algorithms [\\[24,](#page-14-17)\
    \ [79,](#page-16-25) [99\\]](#page-16-26) to make full use of the intra-layer\
    \ pipelining [\\[23,](#page-14-18) [27,](#page-14-14) [35,](#page-14-19) [42–](#page-14-20)[44,](#page-14-21)\
    \ [52,](#page-15-13) [80,](#page-16-22) [86\\]](#page-16-27).\n\n# 4.6 Evaluator\n\
    \nThe evaluator in the WSC-LLM framework is built upon ASTRAsim [\\[83\\]](#page-16-28),\
    \ an open-source distributed training simulator developed using real hardware\
    \ measurements. Our intra-die engine (Section [4.5.2\\)](#page-8-2) serves as\
    \ the interface for global performance evaluation, analyzing the execution time\
    \ of computation tasks within a single die. Based on the prior work, we introduce\
    \ two significant efforts.\n\nFirst, we extend ASTRA-sim to support the inference\
    \ process of LLMs, which is inherently dynamic across iterations, unlike the relatively\
    \ static nature of the training process. This extension enables independent simulation\
    \ of each iteration in the LLM inference process and aggregates the statistical\
    \ results across all iterations at the end, thereby achieving iteration-level\
    \ hardware simulation.\n\nSecondly, to meet the real-time requirements of LLM\
    \ inference, we pre-build and store the mapping lookup table offline. First, we\
    \ explore the optimal intra-die mapping strategy for representative configurations\
    \ in test datasets, including different models, phases, token counts, and batch\
    \ sizes. For each configuration, key results such as latency, DRAM access volume,\
    \ and communication overhead are recorded. Subsequently, these results are used\
    \ to train a DNN to model the relationship between input metrics and outcomes.\
    \ Finally, the DNN is employed to estimate results across all possible configurations\
    \ within the test dataset to complete the construction of the mapping lookup table.\
    \ During the online execution of WSC-LLM, the lookup table remains static, requiring\
    \ only read operations with negligible overhead. Existing simulator works [\\\
    [37,](#page-14-22) [89\\]](#page-16-29) validate the feasibility of this approach,\
    \ demonstrating that the error of the fitted results is within a controllable\
    \ range.\n\n# 5 Evaluation\n\n# <span id=\"page-8-0\"></span>5.1 Experiment Setup\n\
    \n<span id=\"page-8-3\"></span>5.1.1 Basic Hardware Configurations. To facilitate\
    \ the exploration of wafer-scale chip architecture, we configured the parameters\
    \ of the compute die according to the setup described in Fig. [3.](#page-3-0)\
    \ The compute die measures 21.92 mm × 22.81 mm and consists of a 16 × 16 array\
    \ of Dojo-style[\\[73\\]](#page-16-4) compute cores. It is manufactured using\
    \ TSMC's 7 nm process technology and operates at a frequency of 1 GHz. The interconnect\
    \ interface of the compute die provides a total interconnect bandwidth of 6 TB/s\
    \ across four directions. Such high interconnect bandwidth enables overlapping\
    \ communication with memory access during the decoding phase or computation during\
    \ the prefill phase. Each core delivers a computational power of 1.02 TFLOPS (Tensor\
    \ FP16) and features 1.25 MB of SRAM.\n\n5.1.2 Architectural Design Space Exploration.\
    \ In this study, we conduct an architectural design space exploration of wafer-scale\
    \ chips with the compute die described in Section [5.1.1.](#page-8-3) A single\
    \ die can\n\nTable 1: Configuration Parameters\n\n<span id=\"page-9-0\"></span>\n\
    \n| Config. Type   | Case 1  | Case 2   | Case 3 | Case 4  |\n|----------------|---------|----------|--------|---------|\n\
    | #Die in X-dim  | 7       | 7        | 6      | 6       |\n| #Die in Y-dim  |\
    \ 9       | 8        | 9      | 8       |\n| DRAM Bandwidth | 1TB/s   | 1.5TB/s\
    \  | 2TB/s  | 3TB/s   |\n| DRAM Capacity  | 32GB    | 48GB     | 64GB   | 96GB\
    \    |\n| D2D Bandwidth  | 2.5TB/s | 2.25TB/s | 2TB/s  | 1.5TB/s |\n\nintegrate\
    \ varying numbers of DRAM chiplets, offering different memory capacities and bandwidth.\
    \ This exploration aims to demonstrate the advantages of WSC-LLM and gain a deeper\
    \ understanding of the trade-offs among computation, memory, and communication\
    \ resources within wafer-scale chips. Table [1](#page-9-0) outlines four feasible\
    \ wafer-scale chip configurations within a 12-inch wafer area, which serve as\
    \ the basis for our design space exploration (DSE).\n\n5.1.3 Workloads. We choose\
    \ a series of models from 7B to 175B for our experiments, including LLaMA2-7B\
    \ [\\[77\\]](#page-16-2), LLaMA-30B [\\[76\\]](#page-16-1), LLaMA3-70B [\\[20\\\
    ]](#page-13-8), and GPT-175B [\\[13\\]](#page-13-0). LLaMA, LLaMA2, LLaMA3, and\
    \ GPT are the most representative and widely used decoderonly LLM model, with\
    \ a similar structure to other models like Mistral [\\[38\\]](#page-14-23), Falcon\
    \ [\\[11\\]](#page-13-9), OPT [\\[96\\]](#page-16-6), BLOOM [\\[67\\]](#page-15-14)\
    \ and so on. LLaMA3- 70B utilize group query attention (GQA), which partitions\
    \ queries into groups and each group shares attention scores and a key-value (KV)\
    \ matrix. In contrast, other models employ multi-head attention (MHA), in which\
    \ each query maintains an independent KV matrix. We use FP16 precision for all\
    \ workloads.\n\n5.1.4 Datasets. We employ the actual production traces [\\[1\\\
    ]](#page-12-7) from a cloud provider, Azure. The traces include the arrival time,\
    \ input prompt size, and output token number for requests in one-hour serving.\
    \ We use traces of code and conversation representing the most common scenarios\
    \ in LLM inference. For the conversation (conv) dataset, the median number of\
    \ prompt tokens is 1020 tokens and the average number is 1155, while the median\
    \ number of decoding tokens is 129 tokens and the average number is 212. For the\
    \ code dataset, the median number of prompt tokens is 1500, and the average is\
    \ 2048, while the median number of decoding tokens is 13 and the average number\
    \ is 28. Moreover, we increase and decrease the request load per second by adjusting\
    \ the Poisson arrival rate for various model and cluster sizes.\n\n5.1.5 Metrics.\
    \ To comprehensively measure the overall performance of our system, we choose\
    \ End-to-end (E2E) latency and Transaction Per Second (TPS). E2E latency represents\
    \ the total response time for a user request and it includes prefill execution,\
    \ decoding execution, and request waiting time. As decoding dominates LLM workloads,\
    \ computation and unoverlapped communication should constitute a very low portion\
    \ (<5%) of the total execution time in an efficient LLM service system. TPS represents\
    \ the number of user requests processed per second.\n\n# 5.2 Architectural Design\
    \ Space Exploration\n\nFig. [10](#page-9-1) illustrates the overall performance\
    \ results of four feasible wafer-scale architectures across four LLM models and\
    \ two datasets. These experiments cover variations in model architecture, input\
    \ sequence length, and batch size, providing a comprehensive\n\n<span id=\"page-9-1\"\
    ></span>![](_page_9_Figure_10.jpeg)\n\nFigure 10: Overall comparisons of E2E latency\
    \ and TPS among case 1, 2, 3, and 4.\n\nand general evaluation. The wafer-scale\
    \ chip with 54 dies (case 3) consistently achieves optimal performance in nearly\
    \ all scenarios, demonstrating its ability to effectively adapt to variations\
    \ in model architectures and workloads. This establishes case 3 as the best wafer-scale\
    \ chip architecture. The above result demonstrates that wafer-scale chips with\
    \ moderate DRAM capacity per die can effectively balance computation, storage,\
    \ and communication resources, delivering superior quality of service for LLM\
    \ workloads. Conversely, architectures with either excessively large or small\
    \ DRAM capacity per die tend to suffer from a negative impact due to the deficiency\
    \ of one or two hardware resources, leading to degraded performance.\n\nA comparison\
    \ of cases 1, 2, and 3 reveals that the overall performance of cases 1 and 2 is\
    \ significantly inferior to that of case 3, particularly when evaluated on the\
    \ conv dataset. This disparity can be attributed to the higher proportion of decoding\
    \ phases in the conv dataset, where performance is heavily constrained by memory\
    \ bandwidth and capacity. Specifically, during the decoding phase, neither enhanced\
    \ computation power nor increased D2D bandwidth can compensate for insufficient\
    \ DRAM bandwidth and capacity. This finding underscores DRAM bandwidth and capacity\
    \ as key factors in determining the quality of LLM service.\n\nWhen comparing\
    \ case 3 and case 4, we find that although case 4 has higher DRAM bandwidth than\
    \ case 3, its overall performance is inferior. This performance discrepancy arises\
    \ due to its lower D2D bandwidth. During inter-die data access, the limited D2D\
    \ bandwidth becomes the new bottleneck, replacing DRAM bandwidth as the primary\
    \ constraint. This observation highlights a critical insight: achieving efficient\
    \ LLM service requires not only high DRAM bandwidth but also sufficient communication\
    \ bandwidth. A balance between memory and communication resources is essential\
    \ for optimal performance. This principle applies not only to wafer-scale chips\
    \ but also to other hardware platforms, emphasizing the importance of balanced\
    \ resource allocation in achieving high-quality LLM services.\n\n# <span id=\"\
    page-9-2\"></span>5.3 Overall Performance\n\nWe select the optimal wafer-scale\
    \ chip configuration (case 3) for comparison with Splitwise [\\[62\\]](#page-15-5),\
    \ the SOTA LLM service system\n\n<span id=\"page-10-0\"></span>![](_page_10_Figure_1.jpeg)\n\
    \n**LLaMA2-7B LLaMA-30B LLaMA3-70B GPT-175B** Figure 11: Overall comparisons of\
    \ E2E latency and TPS between Splitwise-GPU, Splitwise-Wafer and WSC-LLM.\n\n\
    based on GPU clusters, denoted as SW-GPU. To ensure a fair comparison, we use\
    \ a configuration for Splitwise comprising six 8-A100- 80GB GPU nodes, with inter-node\
    \ bandwidth of 400 GB/s. In terms of computation resources, the total compute\
    \ capability of Splitwise is 14,976 TFLOPS (Tensor FP16), which exceeds the 14,100\
    \ TFLOPS provided by the wafer-scale chip. For memory resources, Splitwise has\
    \ a total DRAM capacity of 3,840 GB, also surpassing the 3,456 GB of DRAM available\
    \ on the wafer-scale chip. Both systems feature a DRAM bandwidth of 2 TB/s. Additionally,\
    \ we directly apply Splitwise's scheduling strategy to our wafer-scale chip (case\
    \ 3) as a control experiment, denoted as SW-Wafer.\n\nFig. [11](#page-10-0) illustrates\
    \ the normalized E2E latency and TPS results on code and conversation datasets.\
    \ WSC-LLM achieves an average 3.12× improvement in E2E latency and a 36.47% increase\
    \ in TPS compared to SW-GPU, and an average 4.81× improvement in E2E latency and\
    \ a 57.17% increase in TPS compared to SW-Wafer, across four LLM models and two\
    \ datasets. Despite having fewer computation and memory resources, WSC-LLM demonstrates\
    \ superior performance due to its highly efficient operator execution and scheduling\
    \ strategies, which are specifically tailored to leverage the unique advantages\
    \ of wafer-scale architecture. The performance gap between SW-GPU and SW-Wafer\
    \ can be attributed both to less hardware resources (with SW-GPU having approximately\
    \ 6% less compute and 11% less memory resource) and to the fact that the Splitwise\
    \ strategy is designed primarily for GPU architectures, lacking the optimizations\
    \ necessary for wafer-scale chips. These results indicate that using WSC-LLM on\
    \ wafer-scale chips shows exceptional promise for LLM services.\n\n## 5.4 Ablation\
    \ Studies\n\nTo evaluate the impact of different WSC-LLM's optimization strategies,\
    \ we conduct experiments on a 54-die wafer-scale chip (case 3). As shown in Fig.\
    \ [12,](#page-10-1) WSC-LLM is compared with two baselines, each disabling one\
    \ optimization strategy. no-Central removes TP size search and resource allocation\
    \ and placement strategy in Central Scheduler, fixing TP size at six dies for\
    \ prefill and decoding instances (18 dies for GPT3-175B) and using a random placement\
    \ strategy. no-Memory disables Memory Scheduler, requiring the\n\n<span id=\"\
    page-10-1\"></span>![](_page_10_Figure_7.jpeg)\n\n**LLaMA2-7B LLaMA-30B LLaMA3-70B\
    \ GPT-175B** Figure 12: Overall comparisons of E2E latency and TPS among no-Central,\
    \ no-Memory, and WSC-LLM.\n\ngenerated KV cache from prefill instances to be fully\
    \ transferred to decoding instances for storage.\n\nWe can observe the following\
    \ phenomena: (1) Phenomenon 1: As model size increases, performance gains from\
    \ the Central Scheduler gradually diminish; (2) Phenomenon 2: Conversely, benefits\
    \ of the Memory Scheduler grow with the model size.\n\nThe explanation for Phenomenon\
    \ 1 lies in the fact that smaller LLMs typically have smaller optimal instance\
    \ size and a larger number of instances, making them deviate more from a fixed\
    \ instance size and more affected by resource allocation and placement strategies.\
    \ For Phenomenon 2, larger LLMs generally require more memory resource due to\
    \ their increased weight parameters and KV cache. Although LLaMA3-70B reduces\
    \ KV cache through GQA mechanism, its weight size is still 2.33× that of LLaMA-30B.\
    \ Due to the absence of the Memory Scheduler, no-Memory fails to fully utilize\
    \ DRAM resources, resulting in increased memory pressure when handling larger\
    \ LLMs with higher memory demands, leading to degraded performance. These results\
    \ confirm that by leveraging the Central Scheduler and Memory Scheduler, WSC-LLM\
    \ can support high-performance LLM services across various models and workloads.\n\
    \nIn addition, we observe that for LLaMA-30B, LLaMA3-70B, and GPT-175B, the Memory\
    \ Scheduler plays a more significant role in performance improvement whereas the\
    \ Central Scheduler has a greater impact only on LLaMA2-7B. Combined with the\
    \ result in Fig. [11,](#page-10-0) which show WSC-LLM achieves greater performance\
    \ gains over Splitwise as model size grows, we conclude that: The Memory Scheduler\
    \ in WSC-LLM plays a more critical role than the Central Scheduler in enhancing\
    \ LLM service quality.\n\n# 5.5 Learn from a Complete LLM Service Process\n\n\
    In this section, we demonstrate the advantages of WSC-LLM according to a complete\
    \ real-world LLM inference process (Fig. [13\\)](#page-11-0) and analyze the principles\
    \ of its performance improvements. Splitwise-Wafer serves as the baseline, representing\
    \ the direct application of Splitwise's scheduling strategy to wafer-scale chips.\n\
    \nFig. [13\\(](#page-11-0)a) presents the die utilization heatmap for LLaMA3-70B\
    \ on the conversation dataset. It is evident that WSC-LLM achieves\n\n<span id=\"\
    page-11-0\"></span>![](_page_11_Figure_1.jpeg)\n\nFigure 13: Overall comparisons\
    \ of die utilization and DRAM\n\nutilization for LLaMA3-70B on the conversation\
    \ dataset.\n\nsignificantly higher overall die utilization compared to Splitwise-Wafer,\
    \ as indicated by the absence of blue or dark green dies. The performance improvement\
    \ primarily stems from the designs of our Central Scheduler and Memory Scheduler:\n\
    \n*(b) DRAM utilization for LLaMA3-70B on the conversation dataset* (1) Splitwise-Wafer\
    \ suffers from inefficient resource partition and placement strategies, leading\
    \ to a higher total number of communication hops for edge dies. This results in\
    \ frequent communication congestion, forcing computations to stall until data\
    \ transmission is completed. This also explains why prefill and decoding dies\
    \ in Splitwise-Wafer exhibit higher utilization in central dies while remaining\
    \ underutilized at the edges. In contrast, WSC-LLM, benefiting from our Central\
    \ Scheduler, achieves a more balanced utilization pattern and improved overall\
    \ performance.\n\n(2) Our Memory Scheduler can effectively leverage the idle memory\
    \ resources in Splitwise-Wafer. As shown in Fig. 13(b), WSC-LLM achieves significantly\
    \ higher DRAM utilization compared to Splitwise-Wafer. More DRAM space allows\
    \ for storing additional KV cache of user requests, enhancing the ability to execute\
    \ decoding requests in parallel and reducing the frequency of prefill dies being\
    \ stalled due to insufficient DRAM space.\n\nFurthermore, both Splitwise-Wafer\
    \ and WSC-LLM exhibit higher utilization in decoding dies, highlighting the dominant\
    \ role of the decoding phase. Notably, simply allocating more dies for decoding\
    \ phase while reducing prefill dies does not improve LLM service quality. This\
    \ is because the decoding phase is constrained by DRAM access bandwidth, and increasing\
    \ dies cannot resolve it. On the contrary, it may degrade the performance of the\
    \ prefill phase.\n\n#### 6 Discussion\n\n#### 6.1 Architectural Generality\n\n\
    As mentioned in Section 2.3, WSC-LLM is equipped with various configurable architectural\
    \ parameters. For wafer-scale chips with\n\n**utilization** varying parameter\
    \ configurations, WSC-LLM can automatically perform end-to-end scheduling to achieve\
    \ optimal LLM inference results. Notably, WSC-LLM is capable of optimizing not\
    \ only for scenarios with high D2D bandwidth but also for cases with lower D2D\
    \ bandwidth, as demonstrated in Section 5.1 with case 4. Moreover, the results\
    \ in Section 5.3 demonstrate that WSC-LLM achieves significant performance improvements\
    \ on various models including LLaMA and GPT and different model sizes from 7B\
    \ to 175B. In summary, WSC-LLM demonstrates excellent generality, adapting well\
    \ to different LLM models and sizes and different architectural parameters of\
    \ wafer-scale chips.\n\n#### 6.2 Scalability\n\nWhile wafer-scale chips offer\
    \ higher integration density, their limited area constrains the computation and\
    \ storage resources compared to GPU clusters with dozens or even hundreds of nodes.\
    \ For LLM inference scenarios with large parameter sizes and high user request\
    \ rates, utilizing multi-wafer-scale chip systems becomes imperative.\n\nWe select\
    \ a wafer-scale chip array composed of 2×2 case 3 and compare its performance\
    \ to Splitwise[62], which consists of twentyfour 8-A100-80GB GPU nodes, to demonstrate\
    \ the scalability of WSC-LLM. For wafer-to-wafer (W2W) interconnect bandwidth,\
    \ we considered two configurations: the state-of-the-art 1.8 TB/s W2W interconnect[73],\
    \ denoted as WSC-LLM-18, and a configuration with 400 GB/s W2W bandwidth, matching\
    \ the interconnect bandwidth of Splitwise inter-node, denoted as WSC-LLM-4. In\
    \ addition, we test the LLaMA3-405B to demonstrate WSC-LLM's effective support\
    \ for ultra-large models.\n\nAs shown in Fig. 14, we can observe two key phenomena.\
    \ (1) Phenomenon 1: WSC-LLM consistently outperforms Splitwise in both high and\
    \ low W2W bandwidth scenarios. (2) Phenomenon 2: WSC-LLM can achieve greater performance\
    \ improvement on the ultra-large model LLaMA3-405B.\n\nFor Phenomenon 1, the superior\
    \ performance of WSC-LLM stems from its ability to fully utilize the computational\
    \ and memory resources of a single wafer to efficiently execute LLM inference.\
    \ Additionally, multiple wafers can independently handle different requests in\
    \ parallel, effectively minimizing the need for extensive W2W communication. In\
    \ scenarios with higher W2W bandwidth, WSC-LLM can further optimize performance\
    \ by employing more aggressive cross-wafer memory scheduling strategies.\n\nFor\
    \ Phenomenon 2, this can be attributed to the substantial memory demands of LLaMA3-405B,\
    \ which has 910GB model weights, requiring at least two GPU nodes to store a single\
    \ weight replica. As a result, Splitwise incurs significant inter-node communication\
    \ overhead when executing intra-instance computations, leading to performance\
    \ degradation. In contrast, the memory capacity of a single wafer-scale chip (case\
    \ 3) is sufficient to accommodate a complete model replica. Consequently, even\
    \ WSC-LLM-4 can achieve substantial performance improvements.\n\n# 7 Related Work\n\
    \nLLM Serving and Disaggregated Inference. In real-time online LLM service systems\
    \ [10, 45, 84, 91, 100], continuous batching is a\n\n<span id=\"page-12-8\"></span>![](_page_12_Figure_2.jpeg)\n\
    \nFigure 14: Overall comparisons of E2E latency and TPS among Splitwise, WSC-LLM-4,\
    \ and WSC-LLM-18.\n\nwidely adopted technique that processes both the prefill\
    \ and decoding phases of multiple requests within each iteration. This method\
    \ aims to enhance the overall system throughput by minimizing idle hardware resources.\
    \ However, since the prefill and decoding phases are constrained by computing\
    \ power and memory bandwidth, respectively, continuous batching causes prefill-decoding\
    \ interference. This interference leads to a trade-off within the LLM service\
    \ system between optimizing the prefill and decoding phase, making it challenging\
    \ to achieve simultaneous performance optimization for both phases.\n\nSome existing\
    \ LLM service systems [\\[31,](#page-14-5) [62,](#page-15-5) [72,](#page-16-18)\
    \ [100\\]](#page-16-9) have recognized the differences between the prefill and\
    \ decoding phases and have executed them on separate devices, known as disaggregated\
    \ inference. However, these efforts lack a fine-grained exploration of optimal\
    \ resource allocation and do not focus on wafer-scale chips. As a result, they\
    \ fail to fully leverage the flexible scheduling and high-bandwidth characteristics\
    \ of wafer-scale chips, missing the opportunity to improve compute and memory\
    \ utilization and to reduce additional KV cache transfer overhead.\n\nWafer-scale\
    \ chips and Chiplet accelerators. For wafer-scale chips, existing research primarily\
    \ focuses on architectural exploration [\\[16,](#page-13-10) [22,](#page-14-24)\
    \ [60,](#page-15-2) [65,](#page-15-15) [101\\]](#page-16-30) and layout design\
    \ [\\[39,](#page-14-25) [51,](#page-15-16) [53,](#page-15-17) [59\\]](#page-15-18).\
    \ However, these studies lack an understanding of the imbalances between prefill\
    \ and decoding phases, and the memory management.\n\nRegarding chiplet accelerators,\
    \ research has addressed design space exploration (DSE) [\\[15,](#page-13-4) [40,](#page-14-26)\
    \ [58,](#page-15-19) [94,](#page-16-31) [98\\]](#page-16-32), data flow scheduling\
    \ [\\[24,](#page-14-17) [57\\]](#page-15-20), and SRAM optimization [\\[25,](#page-14-27)\
    \ [69\\]](#page-15-12). Nevertheless, these efforts have mainly concentrated on\
    \ optimizing traditional DNNs, such as CNNs, and SRAM, whereas our focus is on\
    \ optimizing LLMs and DRAM utilization. Furthermore, all research overlooks the\
    \ critical issue of balancing memory, computation, and communication resources\
    \ within wafer-scale chip architectures.\n\n#### 8 Conclusion\n\nIn this work,\
    \ we introduce WSC-LLM, an efficient LLM service and architecture co-exploration\
    \ framework for wafer-scale chips. To the\n\n best of our knowledge, this is the\
    \ first work to achieve it. Leveraging the high bandwidth and topology characteristics\
    \ of wafer-scale chips, WSC-LLM introduces optimal hardware resource allocation\
    \ and placement strategies, as well as KV cache memory management policies, unearthing\
    \ hidden optimization opportunities. Regarding architecture, we provide a highly\
    \ configurable hardware template and an accurate performance evaluator built upon\
    \ existing wellestablished frameworks. Our experiments demonstrate that the architectures\
    \ and scheduling strategies explored by WSC-LLM on wafer-scale chips significantly\
    \ outperform both the SOTA LLM service system based on GPU clusters and the direct\
    \ application of its scheduling strategy to wafer-scale chips. Furthermore, we\
    \ provide profound insights into the design of wafer-scale architectures and the\
    \ execution of LLM workloads.\n\n# Acknowledgments\n\nThis work was supported\
    \ in part by the National Science and Technology Major Project under Grant 2022ZD0115200;\
    \ in part by the NSFC under Grant 62125403, Grant 92464302,Grant U24B20164 and\
    \ Grant 92164301; in part by Shanghai Municipal Science and Technology Major Project;\
    \ in part by the Natural Science Foundation of Jiangsu Province Basic Research\
    \ Program under Grant BK20243042; in part by the Beijing National Research Center\
    \ for Information Science and Technology; in part by the Northern IC Technology\
    \ Innovation Center (Beijing) Co., Ltd under Grant QYJS20232801B; and in part\
    \ by the Beijing Advanced Innovation Center for Integrated Circuits.\n\n# References\n\
    \n- <span id=\"page-12-7\"></span>[1] [n. d.]. Azure Public Dataset. [Online].\
    \ Available: [https://github.com/Azure/](https://github.com/Azure/AzurePublicDataset/tree/master)\
    \ [AzurePublicDataset/tree/master.](https://github.com/Azure/AzurePublicDataset/tree/master)\n\
    - <span id=\"page-12-5\"></span>[2] [n. d.]. Claude. [Online]. Available: [https://claude.ai/.](https://claude.ai/)\n\
    - [3] [n. d.]. Gemini. [Online]. Available: [https://gemini.google.com/.](https://gemini.google.com/)\n\
    - <span id=\"page-12-6\"></span><span id=\"page-12-2\"></span>[4] [n. d.]. Grok.\
    \ [Online]. Available: [https://x.ai/blog/grok-1.5.](https://x.ai/blog/grok-1.5)\n\
    - <span id=\"page-12-4\"></span>[5] [n. d.]. Mask / reticle. [Online]. Available:\
    \ [https://en.wikichip.org/wiki/mask.](https://en.wikichip.org/wiki/mask)\n- [6]\
    \ [n. d.]. NVIDIA GB200 NVL72: Powering the new era of computing. [Online]. Available:\
    \ [https://www.nvidia.com/en-us/data-center/gb200-nvl72/.](https://www.nvidia.com/en-us/data-center/gb200-nvl72/)\n\
    - <span id=\"page-12-3\"></span>[7] [n. d.]. Nvidia H100. [Online]. Available:\
    \ [https://www.nvidia.com/en-us/data](https://www.nvidia.com/en-us/data-center/h100)[center/h100.](https://www.nvidia.com/en-us/data-center/h100)\n\
    - <span id=\"page-12-0\"></span>[8] 2023. Bard, an experiment by google. [Online].\
    \ Available: [https://bard.google.](https://bard.google.com/) [com/.](https://bard.google.com/)\n\
    - <span id=\"page-12-1\"></span>[9] Josh Achiam, Steven Adler, Sandhini Agarwal,\
    \ Lama Ahmad, Ilge Akkaya, Florencia Leoni, Diogo Almeida, Janko Altenschmidt,\
    \ Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie\
    \ Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello,\
    \ Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff,\
    \ Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles\
    \ Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey,\
    \ Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek\
    \ Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho,\
    \ Casey Chu, Hyung Won, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux,\
    \ Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling,\
    \ Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam\
    \ Fedus, Niko Felix, Simón Posada, Juston Forte, Isabella Fulford, Leo Gao, Elie\
    \ Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes,\
    \ Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang\
    \ Shane, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton,\
    \ Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon\
    \ Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn\
    \ Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino\
    \ Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar\
    \ Kanitscheider, Nitish Shirish, Tabarak Khan, Logan Kilpatrick, Jong Wook, Christina\
    \ Kim, Yongjik Kim, Jan Hendrik, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz\
    \ Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger,\
    \ Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel\
    \ Levy, Chak Ming, Rachel Lim, Molly Lin, Stephanie\n\nLin, Mateusz Litwin, Theresa\
    \ Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor\
    \ Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew,\
    \ Scott Mayer, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok\
    \ Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco,\
    \ Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély,\
    \ Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo,\
    \ Hyeonwoo Noh, Long Ouyang, Cullen O'Keefe, Jakub Pachocki, Alex Paino, Joe Palermo,\
    \ Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex\
    \ Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de, Michael Petrov,\
    \ Henrique Ponde, Michael (Rai)Pokorny, Michelle Pokrass, Vitchyr H., Tolly Powell,\
    \ Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack\
    \ Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross,\
    \ Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani\
    \ Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel\
    \ Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam,\
    \ Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian\
    \ Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski, Natalie\
    \ Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B., Phil Tillet,\
    \ Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek,\
    \ Juan Felipe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright,\
    \ Justin Jay, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila\
    \ Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner,\
    \ Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff\
    \ Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech\
    \ Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng,\
    \ Juntang Zhuang, William Zhuk, and Barret Zoph. 2023. Gpt-4 technical report.\
    \ arXiv preprint arXiv:2303.08774 (2023).\n\n- <span id=\"page-13-2\"></span>[10]\
    \ Amey Agrawal, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav S Gulavani,\
    \ and Ramachandran Ramjee. 2023. Sarathi: Efficient llm inference by piggybacking\
    \ decodes with chunked prefills. arXiv preprint arXiv:2308.16369 (2023).\n- <span\
    \ id=\"page-13-9\"></span>[11] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz\
    \ Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet,\
    \ Daniel Hesslow, Julien Launay, Quentin Malartic, Daniele Mazzotta, Badreddine\
    \ Noune, Baptiste Pannier, and Guilherme Penedo. 2023. The Falcon Series of Open\
    \ Language Models. arXiv preprint arXiv:2311.16867 (2023).\n- <span id=\"page-13-3\"\
    ></span>[12] Paul Barham, Aakanksha Chowdhery, Jeff Dean, Sanjay Ghemawat, Steven\
    \ Hand, Dan Hurt, Michael Isard, Hyeontaek Lim, Ruoming Pang, Sudip Roy, Brennan\
    \ Saeta, Parker Schuh, Ryan Sepassi, Laurent El Shafey, Chandramohan A. Thekkath,\
    \ and Yonghui Wu. 2022. Pathways: Asynchronous distributed dataflow for ml. Proceedings\
    \ of Machine Learning and Systems 4 (2022), 430–449.\n- <span id=\"page-13-0\"\
    ></span>[13] Tom B.Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,\
    \ Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,\
    \ Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon\
    \ Child, Aditya Ramesh, Daniel M.Ziegler, Jeffrey Wu, Clemens Winter, Christopher\
    \ Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack\
    \ Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and\
    \ Dario Amodei. 2020. Language models are few-shot learners. Advances in neural\
    \ information processing systems 33 (2020), 1877–1901.\n- <span id=\"page-13-5\"\
    ></span>[14] Jingwei Cai, Yuchen Wei, Zuotong Wu, Sen Peng, and Kaisheng Ma. 2023.\
    \ Inter-layer scheduling space definition and exploration for tiled accelerators.\
    \ In Proceedings of the 50th Annual International Symposium on Computer Architecture.\
    \ 1–17.\n- <span id=\"page-13-4\"></span>[15] Jingwei Cai, Zuotong Wu, Sen Peng,\
    \ Yuchen Wei, Zhanhong Tan, Guiming Shi, Mingyu Gao, and Kaisheng Ma. 2024. Gemini:\
    \ Mapping and Architecture Coexploration for Large-scale DNN Chiplet Accelerators.\
    \ In 2024 IEEE International Symposium on High-Performance Computer Architecture,\
    \ HPCA 2024. IEEE, 156– 171.\n- <span id=\"page-13-10\"></span>[16] Shuangliang\
    \ Chen, Saptadeep Pal, and Rakesh Kumar. 2024. Waferscale network switches. In\
    \ 2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture,\
    \ ISCA 2024. IEEE, 215–229.\n- <span id=\"page-13-6\"></span>[17] Yu-Hsin Chen,\
    \ Tushar Krishna, Joel S Emer, and Vivienne Sze. 2016. Eyeriss: An energy-efficient\
    \ reconfigurable accelerator for deep convolutional neural networks. IEEE journal\
    \ of solid-state circuits 52, 1 (2016), 127–138.\n- <span id=\"page-13-1\"></span>[18]\
    \ Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,\
    \ Adam Roberts, Paul Barham, Hyung Chung, Charles Sutton, Sebastian Gehrmann,\
    \ Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao,\
    \ Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan\
    \ Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard,\
    \ Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa\
    \ Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus,\
    \ Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander\
    \ Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew\
    \ M. Dai, Thanumalayan Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon\
    \ Child, Oleksandr Polozov,\n\nKatherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan\
    \ Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern,\
    \ Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2023. Palm: Scaling language\
    \ modeling with pathways. Journal of Machine Learning Research 24, 240 (2023),\
    \ 1–113.\n\n- <span id=\"page-13-7\"></span>[19] Jinyi Deng, Xinru Tang, Jiahao\
    \ Zhang, Yuxuan Li, Linyun Zhang, Boxiao Han, Hongjun He, Fengbin Tu, Leibo Liu,\
    \ Shaojun Wei, Yang Hu, and Shouyi Yin. 2023. Towards efficient control flow handling\
    \ in spatial architecture via architecting the control flow plane. In Proceedings\
    \ of the 56th Annual IEEE/ACM International Symposium on Microarchitecture. 1395–1408.\n\
    - <span id=\"page-13-8\"></span>[20] Abhimanyu Dubey, Abhinav Jauhri, Abhinav\
    \ Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten,\
    \ Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra,\
    \ Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien\
    \ Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh\
    \ Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra,\
    \ Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong,\
    \ Cristian Canton, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz,\
    \ Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano,\
    \ Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova,\
    \ Emily Dinan, Eric Michael, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle\
    \ Lee, Georgia Lewis, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell,\
    \ Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta,\
    \ Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert,\
    \ Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van, Jennifer Billock,\
    \ Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie\
    \ Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua\
    \ Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden, Kartikeya Upasani, Kate\
    \ Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer,\
    \ Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van,\
    \ Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo,\
    \ Lukas Blecher, Lukas Landzaat, Luke de, Madeline Muzzi, Mahesh Pasupuleti, Mannat\
    \ Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova,\
    \ Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar, Mona Hassan, Naman Goyal,\
    \ Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Olivier\
    \ Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic,\
    \ Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh,\
    \ Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer,\
    \ Ricardo Silveira, Robert Stojnic, Roberta Raileanu, Rohit Girdhar, Rohit Patel,\
    \ Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva,\
    \ Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean\
    \ Bell, Seohyun Sonia, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy,\
    \ Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya\
    \ Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney\
    \ Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas\
    \ Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj\
    \ Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie\
    \ Do, Vish Vogeti, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney\
    \ Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen, Xinfeng Xie, Xuchao Jia,\
    \ Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen\
    \ Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre, Zheng Yan, Zhengxing\
    \ Chen, Zoe Papakipos, Aaditya Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey,\
    \ Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon,\
    \ Ajay Sharma, Alex Boesenberg, Alex Vaughan, Alexei Baevski, Allie Feinstein,\
    \ Amanda Kallet, Amit Sangani, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew\
    \ Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani,\
    \ Annie Franco, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin\
    \ Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi,\
    \ Bernie Huang, Beth Loyd, Beto De, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu\
    \ Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido,\
    \ Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Changhan Wang, Changkyu\
    \ Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph\
    \ Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, Danny Wyatt,\
    \ David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich,\
    \ Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil,\
    \ Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman,\
    \ Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian,\
    \ Firat Ozgenel, Francesco Caggioni, Francisco Guzmán, Frank Kanayet, Frank Seide,\
    \ Gabriela Medina, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern,\
    \ Govind Thattai, Grant Herman, Grigory Sizov, Guangyi (Jack)Zhang, Guna Lakshminarayanan,\
    \ Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison\
    \ Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Ibrahim Damlaj, Igor Molybog,\
    \ Igor Tufanov, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James\
    \ Kohli, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan,\
    \ Jenny Zhen, Jeremy Reizenstein, Jeremy\n\nTeboul, Jessica Zhong, Jian Jin, Jingyi\
    \ Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres,\
    \ Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou, Karan Saxena, Karthik Prasad, Kartikay\
    \ Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena,\
    \ Keqian Li, Kun Huang, Kunal Chawla, Kushal Lakhotia, Kyle Huang, Lailin Chen,\
    \ Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo,\
    \ Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani,\
    \ Manish Bhatt, Maria Tsimpoukelli, Martynas Mankus, Matan Hasson, Matthew Lennie,\
    \ Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Michael\
    \ L., Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan,\
    \ Mike Clark, Mike Macey, Mike Wang, Miquel Jubert, Mo Metanat, Mohammad Rastegari,\
    \ Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa,\
    \ Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikolay Pavlovich, Ning Dong, Ning\
    \ Zhang, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli,\
    \ Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager,\
    \ Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish\
    \ Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy,\
    \ Raghu Nayani, Rahul Mitra, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang,\
    \ Rohan Maheswari, Russ Howes, Ruty Rinott, Sai Jayesh, Samyak Datta, Sara Chugh,\
    \ Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Verma, Seiji\
    \ Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao\
    \ Lin, Shengxin Cindy, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang,\
    \ Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen,\
    \ Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Sungmin\
    \ Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez,\
    \ Tamar Glaser, Tamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun\
    \ Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria\
    \ Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish, Vishal Mangla, Vítor Albiero,\
    \ Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu, Vladimir Ivanov, Wei Li, Wenchen Wang,\
    \ Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaofang Wang, Xiaojian\
    \ Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye Hu, Ye Jia,\
    \ Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu (Sid)Wang,\
    \ Yuchen Hao, Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo\
    \ Wen, Zhenyu Yang, and Zhiwei Zhao. 2024. The llama 3 herd of models. arXiv preprint\
    \ arXiv:2407.21783 (2024).\n\n- <span id=\"page-14-7\"></span>[21] Shiqing Fan,\
    \ Yi Rong, Chen Meng, Zongyan Cao, Siyu Wang, Zhen Zheng, Chuan Wu, Guoping Long,\
    \ Jun Yang, Lixue Xia, Lansong Diao, Xiaoyong Liu, and Wei Lin. 2021. DAPPLE:\
    \ A pipelined data parallel approach for training large models. In Proceedings\
    \ of the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming.\
    \ 431–445.\n- <span id=\"page-14-24\"></span>[22] Yinxiao Feng and Kaisheng Ma.\
    \ 2024. Switch-Less Dragonfly on Wafers: A Scalable Interconnection Architecture\
    \ based on Wafer-Scale Integration. arXiv preprint arXiv:2407.10290 (2024).\n\
    - <span id=\"page-14-18\"></span>[23] Mingyu Gao, Jing Pu, Xuan Yang, Mark Horowitz,\
    \ and Christos Kozyrakis. 2017. Tetris: Scalable and efficient neural network\
    \ acceleration with 3d memory. In Proceedings of the Twenty-Second International\
    \ Conference on Architectural Support for Programming Languages and Operating\
    \ Systems. 751–764.\n- <span id=\"page-14-17\"></span>[24] Mingyu Gao, Xuan Yang,\
    \ Jing Pu, Mark Horowitz, and Christos Kozyrakis. 2019. Tangram: Optimized coarse-grained\
    \ dataflow for scalable nn accelerators. In Proceedings of the Twenty-Fourth International\
    \ Conference on Architectural Support for Programming Languages and Operating\
    \ Systems. 807–820.\n- <span id=\"page-14-27\"></span>[25] Raveesh Garg, Hyoukjun\
    \ Kwon, Eric Qin, Yu-Hsin Chen, Tushar Krishna, and Liangzhen Lai. 2024. PipeOrgan:\
    \ Efficient Inter-operation Pipelining with Flexible Spatial Organization and\
    \ Interconnects. arXiv preprint arXiv:2405.01736 (2024).\n- <span id=\"page-14-9\"\
    ></span>[26] Haoming Guo, Shengbin Cao, Lei Li, and Xiaofeng Zhang. 2022. A review\
    \ on the mainstream through-silicon via etching methods. Materials Science in\
    \ Semiconductor Processing 137 (2022), 106182.\n- <span id=\"page-14-14\"></span>[27]\
    \ Kartik Hegde, Po-An Tsai, Sitao Huang, Vikas Chandra, Angshuman Parashar, and\
    \ Christopher W Fletcher. 2021. Mind mappings: enabling efficient algorithmaccelerator\
    \ mapping space search. In Proceedings of the 26th ACM International Conference\
    \ on Architectural Support for Programming Languages and Operating Systems. 943–958.\n\
    - <span id=\"page-14-3\"></span>[28] Connor Holmes, Masahiro Tanaka, Michael Wyatt,\
    \ Ammar Ahmad, Jeff Rasley, Samyam Rajbhandari, Reza Yazdani, Heyang Qin, Arash\
    \ Bakhtiari, Lev Kurilenko, and Yuxiong He. 2024. Deepspeed-fastgen: High-throughput\
    \ text generation for llms via mii and deepspeed-inference. arXiv preprint arXiv:2401.08671\
    \ (2024).\n- <span id=\"page-14-2\"></span>[29] SY Hou, W Chris Chen, Clark Hu,\
    \ Christine Chiu, KC Ting, TS Lin, WH Wei, WC Chiou, Vic JC Lin, Victor CY Chang,\
    \ C Wang, C Wu, and D Yu. 2017. Wafer-level integration of an advanced logic-memory\
    \ system through the second-generation CoWoS technology. IEEE Transactions on\
    \ Electron Devices 64, 10 (2017), 4071– 4077.\n- <span id=\"page-14-4\"></span>[30]\
    \ Cunchen Hu, Heyang Huang, Junhao Hu, Jiang Xu, Xusheng Chen, Tao Xie, Chenxi\
    \ Wang, Sa Wang, Yungang Bao, Ninghui Sun, and Yizhou Shan. 2024. Memserve: Context\
    \ caching for disaggregated llm serving with elastic memory\n\npool. arXiv preprint\
    \ arXiv:2406.17565 (2024).\n\n- <span id=\"page-14-5\"></span>[31] Cunchen Hu,\
    \ Heyang Huang, Liangliang Xu, Xusheng Chen, Jiang Xu, Shuang Chen, Hao Feng,\
    \ Chenxi Wang, Sa Wang, Yungang Bao, Ninghui Sun, and Yizhou Shan. 2024. Inference\
    \ without interference: Disaggregate llm inference for mixed downstream workloads.\
    \ arXiv preprint arXiv:2401.11181 (2024).\n- <span id=\"page-14-10\"></span>[32]\
    \ YuChen Hu, YuMin Liang, HsiehPin Hu, ChiaYen Tan, ChihTa Shen, ChienHsun Lee,\
    \ and SY Hou. 2023. CoWoS architecture evolution for next generation HPC on 2.5\
    \ D system in package. In 2023 IEEE 73rd Electronic Components and Technology\
    \ Conference, ECTC 2023. IEEE, 1022–1026.\n- <span id=\"page-14-1\"></span>[33]\
    \ Yang Hu, Xinhan Lin, Huizheng Wang, Zhen He, Xingmao Yu, Jiahao Zhang, Qize\
    \ Yang, Zheng Xu, Sihan Guan, Jiahao Fang, Haoran Shang, Xinru Tang, Xu Dai, Shaojun\
    \ Wei, and Shouyi Yin. 2024. Wafer-Scale Computing: Advancements, Challenges,\
    \ and Future Perspectives. IEEE Circuits and Systems Magazine 24, 1 (2024), 52–81.\n\
    - <span id=\"page-14-11\"></span>[34] PK Huang, CY Lu, WH Wei, Christine Chiu,\
    \ KC Ting, Clark Hu, CH Tsai, SY Hou, WC Chiou, CT Wang, and Douglas Yu. 2021.\
    \ Wafer level system integration of the fifth generation CoWoS®-S with high performance\
    \ Si interposer at 2500 mm2. In 2021 IEEE 71st Electronic Components and Technology\
    \ Conference, ECTC 2021. IEEE, 101–104.\n- <span id=\"page-14-19\"></span>[35]\
    \ Qijing Huang, Minwoo Kang, Grace Dinh, Thomas Norell, Aravind Kalaiah, James\
    \ Demmel, John Wawrzynek, and Yakun Sophia Shao. 2021. Cosa: Scheduling by constrained\
    \ optimization for spatial accelerators. In 2021 ACM/IEEE 48th Annual International\
    \ Symposium on Computer Architecture, ISCA 2021. IEEE, 554–566.\n- <span id=\"\
    page-14-8\"></span>[36] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat,\
    \ Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, and\
    \ Zhifeng Chen. 2019. Gpipe: Efficient training of giant neural networks using\
    \ pipeline parallelism. Advances in neural information processing systems 32 (2019).\n\
    - <span id=\"page-14-22\"></span>[37] Zhihao Jia, James Thomas, Todd Warszawski,\
    \ Mingyu Gao, Matei Zaharia, and Alex Aiken. 2019. Optimizing DNN computation\
    \ with relaxed graph substitutions. Proceedings of Machine Learning and Systems\
    \ 1 (2019), 27–39.\n- <span id=\"page-14-23\"></span>[38] Albert Q. Jiang, Alexandre\
    \ Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de\
    \ las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier,\
    \ Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut\
    \ Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7B.\
    \ arXiv preprint arXiv:2310.06825 (2023).\n- <span id=\"page-14-25\"></span>[39]\
    \ Bentian Jiang, Jingsong Chen, Jinwei Liu, Lixin Liu, Fangzhou Wang, Xiaopeng\
    \ Zhang, and Evangeline FY Young. 2021. CU. POKer: Placing DNNs on WSE With Optimal\
    \ Kernel Sizing and Efficient Protocol Optimization. IEEE Transactions on Computer-Aided\
    \ Design of Integrated Circuits and Systems 41, 6 (2021), 1888– 1901.\n- <span\
    \ id=\"page-14-26\"></span>[40] Divya Kadiyala, Saeed Rashidi, Taekyung Heo, Abhimanyu\
    \ Bambhaniya, Tushar Krishna, and Alexandros Daglis. 2024. Leveraging Memory Expansion\
    \ to Accelerate Large-Scale DL Training. In 2024 IEEE International Symposium\
    \ on Performance Analysis of Systems and Software, ISPASS 2024. IEEE, 292–294.\n\
    - <span id=\"page-14-15\"></span>[41] Sheng-Chun Kao, Geonhwa Jeong, and Tushar\
    \ Krishna. 2020. Confuciux: Autonomous hardware resource assignment for dnn accelerators\
    \ using reinforcement learning. In 2020 53rd Annual IEEE/ACM International Symposium\
    \ on Microarchitecture, MICRO 2020. IEEE, 622–636.\n- <span id=\"page-14-20\"\
    ></span>[42] Sheng-Chun Kao and Tushar Krishna. 2020. Gamma: Automating the hw\
    \ mapping of dnn models on accelerators via genetic algorithm. In Proceedings\
    \ of the 39th International Conference on Computer-Aided Design. 1–9.\n- <span\
    \ id=\"page-14-16\"></span>[43] Hyoukjun Kwon, Prasanth Chatarasi, Michael Pellauer,\
    \ Angshuman Parashar, Vivek Sarkar, and Tushar Krishna. 2019. Understanding reuse,\
    \ performance, and hardware cost of dnn dataflow: A data-centric approach. In\
    \ Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture.\
    \ 754–768.\n- <span id=\"page-14-21\"></span>[44] Hyoukjun Kwon, Ananda Samajdar,\
    \ and Tushar Krishna. 2018. Maeri: Enabling flexible dataflow mapping over dnn\
    \ accelerators via reconfigurable interconnects. ACM SIGPLAN Notices 53, 2 (2018),\
    \ 461–475.\n- <span id=\"page-14-0\"></span>[45] Woosuk Kwon, Zhuohan Li, Siyuan\
    \ Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang,\
    \ and Ion Stoica. 2023. Efficient memory management for large language model serving\
    \ with pagedattention. In Proceedings of the 29th Symposium on Operating Systems\
    \ Principles. 611–626.\n- <span id=\"page-14-12\"></span>[46] John H Lau, Gary\
    \ ChangFu Chen, Jones YuCheng Huang, Ricky TsunSheng Chou, Channing ChengLin Yang,\
    \ HsingNing Liu, and TzyyJang Tseng. 2021. Hybrid substrate by fan-out RDL-first\
    \ panel-level packaging. IEEE Transactions on Components, Packaging and Manufacturing\
    \ Technology 11, 8 (2021), 1301– 1309.\n- <span id=\"page-14-13\"></span>[47]\
    \ John H Lau, ChengTa Ko, KaiMing Yang, ChiaYu Peng, Tim Xia, Puru Bruce Lin,\
    \ JJ Chen, Patrick PoChun Huang, HsingNing Liu, TzyyJang Tseng, Eagle Lin, and\
    \ Leo Chang. 2020. Panel-level fan-out RDL-first packaging for heterogeneous integration.\
    \ IEEE Transactions on Components, Packaging and Manufacturing Technology 10,\
    \ 7 (2020), 1125–1137.\n- <span id=\"page-14-6\"></span>[48] Dmitry Lepikhin,\
    \ HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim\
    \ Krikun, Noam Shazeer, and Zhifeng Chen. 2020. Gshard: Scaling giant models with\
    \ conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668\
    \ (2020).\n- <span id=\"page-15-9\"></span>[49] Zhuohan Li, Siyuan Zhuang, Shiyuan\
    \ Guo, Danyang Zhuo, Hao Zhang, Dawn Song, and Ion Stoica. 2021. Terapipe: Token-level\
    \ pipeline parallelism for training large-scale language models. In International\
    \ Conference on Machine Learning. PMLR, 6543–6552.\n- <span id=\"page-15-3\"></span>[50]\
    \ Sean Lie. 2022. Cerebras architecture deep dive: First look inside the hw/sw\
    \ codesign for deep learning: Cerebras systems. In 2022 IEEE Hot Chips 34 Symposium,\
    \ HCS 2022. IEEE Computer Society, 1–34.\n- <span id=\"page-15-16\"></span>[51]\
    \ Jinwei Liu, Xiaopeng Zhang, Shiju Lin, Xinshi Zang, Jingsong Chen, Bentian Jiang,\
    \ Martin DF Wong, and Evangeline FY Young. 2022. Partition and place finite element\
    \ model on wafer-scale engine. In Proceedings of the 59th ACM/IEEE Design Automation\
    \ Conference. 631–636.\n- <span id=\"page-15-13\"></span>[52] Liqiang Lu, Naiqing\
    \ Guan, Yuyue Wang, Liancheng Jia, Zizhang Luo, Jieming Yin, Jason Cong, and Yun\
    \ Liang. 2021. Tenet: A framework for modeling tensor dataflow based on relation-centric\
    \ notation. In 2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture,\
    \ ISCA 2021. IEEE, 720–733.\n- <span id=\"page-15-17\"></span>[53] Canhui Luo,\
    \ Zhouxing Su, and Zhipeng Lü. 2023. MS-CLS: An Effective Partitioning and Placement\
    \ Metaheuristic for Wafer-Scale Physics Modeling. IEEE Transactions on Emerging\
    \ Topics in Computational Intelligence (2023).\n- <span id=\"page-15-7\"></span>[54]\
    \ Azalia Mirhoseini, Hieu Pham, Quoc V Le, Benoit Steiner, Rasmus Larsen, Yuefeng\
    \ Zhou, Naveen Kumar, Mohammad Norouzi, Samy Bengio, and Jeff Dean. 2017. Device\
    \ placement optimization with reinforcement learning. In International conference\
    \ on machine learning. PMLR, 2430–2439.\n- <span id=\"page-15-0\"></span>[55]\
    \ Gordon E Moore. 1998. Cramming more components onto integrated circuits. Proc.\
    \ IEEE 86, 1 (1998), 82–85.\n- <span id=\"page-15-10\"></span>[56] Deepak Narayanan,\
    \ Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R Devanur, Gregory R\
    \ Ganger, Phillip B Gibbons, and Matei Zaharia. 2019. PipeDream: Generalized pipeline\
    \ parallelism for DNN training. In Proceedings of the 27th ACM symposium on operating\
    \ systems principles. 1–15.\n- <span id=\"page-15-20\"></span>[57] Marcelo Orenes-Vera,\
    \ Esin Tureci, Margaret Martonosi, and David Wentzlaff. 2023. DCRA: A distributed\
    \ chiplet-based reconfigurable architecture for irregular applications. arXiv\
    \ preprint arXiv:2311.15443 (2023).\n- <span id=\"page-15-19\"></span>[58] Marcelo\
    \ OrenesVera, Esin Tureci, Margaret Martonosi, and David Wentzlaff. 2024. Muchisim:\
    \ A simulation framework for design exploration of multichip manycore systems.\
    \ In 2024 IEEE International Symposium on Performance Analysis of Systems and\
    \ Software, ISPASS 2024. IEEE, 48–60.\n- <span id=\"page-15-18\"></span>[59] Sarp\
    \ Özdemir, Mohammad Khasawneh, Smriti Rao, and Patrick H Madden. 2022. Kernel\
    \ mapping techniques for deep learning neural network accelerators. In Proceedings\
    \ of the 2022 International Symposium on Physical Design. 21–28.\n- <span id=\"\
    page-15-2\"></span>[60] Saptadeep Pal, Jingyang Liu, Irina Alam, Nicholas Cebry,\
    \ Haris Suhail, Shi Bu, Subramanian S Iyer, Sudhakar Pamarti, Rakesh Kumar, and\
    \ Puneet Gupta. 2021. Designing a 2048-chiplet, 14336-core waferscale processor.\
    \ In 2021 58th ACM/IEEE Design Automation Conference, DAC 2021. IEEE, 1183–1188.\n\
    - <span id=\"page-15-1\"></span>[61] Saptadeep Pal, Daniel Petrisko, Matthew Tomei,\
    \ Puneet Gupta, Subramanian S Iyer, and Rakesh Kumar. 2019. Architecting waferscale\
    \ processors-a GPU case study. In 2019 IEEE International Symposium on High Performance\
    \ Computer Architecture, HPCA 2019. IEEE, 250–263.\n- <span id=\"page-15-5\"></span>[62]\
    \ Pratyush Patel, Esha Choukse, Chaojie Zhang, Aashaka Shah, Íñigo Goiri, Saeed\
    \ Maleki, and Ricardo Bianchini. 2023. Splitwise: Efficient generative llm inference\
    \ using phase splitting. Power 400, 700W (2023), 1–75.\n- <span id=\"page-15-6\"\
    ></span>[63] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James\
    \ Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. 2023. Efficiently\
    \ scaling transformer inference. Proceedings of Machine Learning and Systems 5\
    \ (2023), 606–624.\n- <span id=\"page-15-8\"></span>[64] Samyam Rajbhandari, Jeff\
    \ Rasley, Olatunji Ruwase, and Yuxiong He. 2020. Zero: Memory optimizations toward\
    \ training trillion parameter models. In SC20: International Conference for High\
    \ Performance Computing, Networking, Storage and Analysis. IEEE, 1–16.\n- <span\
    \ id=\"page-15-15\"></span>[65] Saeed Rashidi, William Won, Sudarshan Srinivasan,\
    \ Puneet Gupta, and Tushar Krishna. 2024. FRED: Flexible REduction-Distribution\
    \ Interconnect and Communication Implementation for Wafer-Scale Distributed Training\
    \ of DNN Models. arXiv preprint arXiv:2406.19580 (2024).\n- <span id=\"page-15-4\"\
    ></span>[66] Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai\
    \ Gat, Xiaoqing Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, Jérémy\
    \ Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian\
    \ Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal\
    \ Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel\
    \ Synnaeve. 2023. Code llama: Open foundation models for code. arXiv preprint\
    \ arXiv:2308.12950 (2023).\n- <span id=\"page-15-14\"></span>[67] Teven Scao,\
    \ Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman\
    \ Castagné, Alexandra Luccioni, François Yvon, Matthias Gallé, Jonathan Tow, Alexander\
    \ Rush, Stella Biderman, Albert Webson, Pawan Ammanamanchi, Thomas Wang, Benoît\
    \ Sagot, Niklas Muennighoff, Albert Moral, Olatunji Ruwase, Rachel Bawden, Stas\
    \ Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson\
    \ Tan, Pedro Suarez, Victor Sanh, Hugo Laurençon, Yacine Jernite, Julien Launay,\
    \ Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham\
    \ Aji, Amit Alfassy, Anna Rogers, Ariel Nitzav, Canwen Xu, Chenghao Mou, Chris\
    \ Emezue, Christopher Klamm, Colin Leong, Daniel Strien, David Adelani, Dragomir\
    \ Radev, Eduardo Ponferrada,\n\nEfrat Levkovizh, Ethan Kim, Eyal Natan, Francesco\
    \ Toni, Gérard Dupont, Germán Kruszewski, Giada Pistilli, Hady Elsahar, Hamza\
    \ Benyamina, Hieu Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios,\
    \ Javier Rosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan Chang, Jörg Frohberg,\
    \ Joseph Tobing, Joydeep Bhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo,\
    \ Leandro Werra, Leon Weber, Long Phan, Loubna allal, Ludovic Tanguy, Manan Dey,\
    \ Manuel Muñoz, Maraim Masoud, María Grandury, Mario Šaško, Max Huang, Maximin\
    \ Coavoux, Mayank Singh, Mike Jiang, Minh Vu, Mohammad Jauhar, Mustafa Ghaleb,\
    \ Nishant Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen, Omar Espejel,\
    \ Ona Gibert, Paulo, Peter Henderson, Pierre Colombo, Priscilla Amuok, Quentin\
    \ Lhoest, Rheza Harliman, Rishi Bommasani, Roberto López, Rui Ribeiro, Salomey\
    \ Osei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose, Shamsuddeen Muhammad, Shanya\
    \ Sharma, Shayne Longpre, Somaieh Nikpoor, Stanislav Silberberg, Suhas Pai, Sydney\
    \ Zink, Tiago Torrent, Timo Schick, Tristan Thrush, Valentin Danchev, Vassilina\
    \ Nikoulina, Veronika Laippala, Violette Lepercq, Vrinda Prabhu, Zaid Alyafeai,\
    \ Zeerak Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si, Davut Taşar, Elizabeth\
    \ Salesky, Sabrina Mielke, Wilson Lee, Abheesht Sharma, Andrea Santilli, Antoine\
    \ Chaffin, Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla, Gunjan Chhablani,\
    \ Han Wang, Harshit Pandey, Hendrik Strobelt, Jason Fries, Jos Rozen, Leo Gao,\
    \ Lintang Sutawika, M Bari, Maged Al-shaibani, Matteo Manica, Nihal Nayak, Ryan\
    \ Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen Bach, Taewoon\
    \ Kim, Tali Bers, Thibault Fevry, Trishala Neeraj, Urmish Thakker, Vikas Raunak,\
    \ Xiangru Tang, Zheng-Xin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri, Hadar Tojarieh,\
    \ Adam Roberts, Hyung Chung, Jaesung Tae, Jason Phang, Ofir Press, Conglong Li,\
    \ Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin,\
    \ Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry,\
    \ Nouamane Tazi, Omar Sanseviero, Patrick Platen, Pierre Cornette, Pierre Lavallée,\
    \ Rémi Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith, Stéphane Requena,\
    \ Suraj Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva,\
    \ Anne-Laure Ligozat, Arjun Subramonian, Aurélie Névéol, Charles Lovering, Dan\
    \ Garrette, Deepak Tunuguntla, Ehud Reiter, Ekaterina Taktasheva, Ekaterina Voloshina,\
    \ Eli Bogdanov, Genta Winata, Hailey Schoelkopf, Jan-Christoph Kalo, Jekaterina\
    \ Novikova, Jessica Forde, Jordan Clive, Jungo Kasai, Ken Kawamura, Liam Hazan,\
    \ Marine Carpuat, Miruna Clinciu, Najoung Kim, Newton Cheng, Oleg Serikov, Omer\
    \ Antverg, Oskar Wal, Rui Zhang, Ruochen Zhang, Sebastian Gehrmann, Shachar Mirkin,\
    \ Shani Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun, Tomasz Limisiewicz,\
    \ Verena Rieser, Vitaly Protasov, Vladislav Mikhailov, Yada Pruksachatkun, Yonatan\
    \ Belinkov, Zachary Bamberger, Zdeněk Kasner, Alice Rueda, Amanda Pestana, Amir\
    \ Feizpour, Ammar Khan, Amy Faranak, Ana Santos, Anthony Hevia, Antigona Unldreaj,\
    \ Arash Aghagol, Arezoo Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh\
    \ Behroozi, Benjamin Ajibade, Bharat Saxena, Carlos Ferrandis, Daniel McDuff,\
    \ Danish Contractor, David Lansky, Davis David, Douwe Kiela, Duong Nguyen, Edward\
    \ Tan, Emi Baylor, Ezinwanne Ozoani, Fatima Mirza, Frankline Ononiwu, Habib Rezanejad,\
    \ Hessie Jones, Indrani Bhattacharya, Irene Solaiman, Irina Sedenko, Isar Nejadgholi,\
    \ Jesse Passmore, Josh Seltzer, Julio Sanz, Livia Dutra, Mairon Samagaio, Maraim\
    \ Elbadri, Margot Mieskes, Marissa Gerchick, Martha Akinlolu, Michael McKenna,\
    \ Mike Qiu, Muhammed Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour\
    \ Elkott, Nour Fahmy, Olanrewaju Samuel, Ran An, Rasmus Kromann, Ryan Hao, Samira\
    \ Alizadeh, Sarmad Shubber, Silas Wang, Sourav Roy, Sylvain Viguier, Thanh Le,\
    \ Tobi Oyebade, Trieu Le, Yoyo Yang, Zach Nguyen, Abhinav Kashyap, Alfredo Palasciano,\
    \ Alison Callahan, Anima Shukla, Antonio Miranda-Escalada, Ayush Singh, Benjamin\
    \ Beilharz, Bo Wang, Caio Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu, Clémentine\
    \ Fourrier, Daniel Periñán, Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio\
    \ Barth, Florian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak, Gully Burns, Helena\
    \ Vrabec, Imane Bello, Ishani Dash, Jihyun Kang, John Giorgi, Jonas Golde, Jose\
    \ Posada, Karthik Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa Shinzato, Madeleine\
    \ Bykhovetz, Maiko Takeuchi, Marc Pàmies, Maria Castillo, Marianna Nezhurina,\
    \ Mario Sänger, Matthias Samwald, Michael Cullan, Michael Weinberg, Michiel Wolf,\
    \ Mina Mihaljcic, Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam, Nathan\
    \ Dahlberg, Nicholas Broad, Nikolaus Muellner, Pascale Fung, Patrick Haller, Ramya\
    \ Chandrasekhar, Renata Eisenberg, Robert Martin, Rodrigo Canalli, Rosaline Su,\
    \ Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok Deshmukh, Shubhanshu Mishra,\
    \ Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Stefan Schweter,\
    \ Sushil Bharati, Tanmay Laud, Théo Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis\
    \ Labrak, Yash Bajaj, Yash Venkatraman, Yifan Xu, Yingxin Xu, Yu Xu, Zhe Tan,\
    \ Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, and Thomas Wolf. 2023.\
    \ Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint\
    \ arXiv:2211.05100v4 (2023).\n\n- <span id=\"page-15-11\"></span>[68] Johannes\
    \ Schemmel, Daniel Brüderle, Andreas Grübl, Matthias Hock, Karlheinz Meier, and\
    \ Sebastian Millner. 2010. A wafer-scale neuromorphic hardware system for large-scale\
    \ neural modeling. In 2010 IEEE International Symposium on Circuits and Systems\
    \ , ISCAS 2010. IEEE, 1947–1950.\n- <span id=\"page-15-12\"></span>[69] Yakun\
    \ Sophia Shao, Jason Clemons, Rangharajan Venkatesan, Brian Zimmer, Matthew Fojtik,\
    \ Nan Jiang, Ben Keller, Alicia Klinefelter, Nathaniel Pinckney,\n\n<span id=\"\
    page-16-0\"></span>Priyanka Raina, S G.Tell, Y Zhang, J.Dally, Joel Emer, C Gray,\
    \ B Khailany, and S W.Keckler. 2019. Simba: Scaling deep-learning inference with\
    \ multi-chipmodule-based architecture. In Proceedings of the 52nd Annual IEEE/ACM\
    \ International Symposium on Microarchitecture. 14–27.\n\n- <span id=\"page-16-11\"\
    ></span>[70] Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani,\
    \ Penporn Koanantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young,\
    \ R Sepassi, and B Hechtman. 2018. Mesh-tensorflow: Deep learning for supercomputers.\
    \ Advances in neural information processing systems 31 (2018).\n- <span id=\"\
    page-16-7\"></span>[71] Mingcong Song, Xinru Tang, Fengfan Hou, Jing Li, Wei Wei,\
    \ Yipeng Ma, Runqiu Xiao, Hongjie Si, Dingcheng Jiang, Shouyi Yin, Yang Hu, and\
    \ Guoping Long. 2024. Tackling the dynamicity in a production llm serving system\
    \ with sota optimizations via hybrid prefill/decode/verify scheduling on efficient\
    \ metakernels. arXiv preprint arXiv:2412.18106 (2024).\n- <span id=\"page-16-18\"\
    ></span>[72] Foteini Strati, Sara Mcallister, Amar Phanishayee, Jakub Tarnawski,\
    \ and Ana Klimovic. 2024. DéjàVu: KV-cache Streaming for Fast, Fault-tolerant\
    \ Generative LLM Serving. arXiv preprint arXiv:2403.01876 (2024).\n- <span id=\"\
    page-16-4\"></span>[73] Emil Talpes, Douglas Williams, and Debjit Das Sarma. 2022.\
    \ Dojo: The microarchitecture of tesla's exa-scale computer. In 2022 IEEE Hot\
    \ Chips 34 Symposium, HCS 2022. IEEE Computer Society, 1–28.\n- <span id=\"page-16-21\"\
    ></span>[74] Zhanhong Tan, Hongyu Cai, Runpei Dong, and Kaisheng Ma. 2021. Nn-baton:\
    \ Dnn workload orchestration and chiplet granularity exploration for multichip\
    \ accelerators. In 2021 ACM/IEEE 48th Annual International Symposium on Computer\
    \ Architecture, ISCA 2021. IEEE, 1013–1026.\n- <span id=\"page-16-5\"></span>[75]\
    \ Romal Thoppilan, Daniel Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha,\
    \ HengTze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae\
    \ Lee, Huaixiu Steven, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun,\
    \ Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts,\
    \ Maarten Bosma, Vincent Zhao, Yanqi Zhou, ChungChing Chang, Igor Krivokon, Will\
    \ Rusch, Marc Pickett, Pranesh Srinivasan, Laichee Man, Kathleen MeierHellstern,\
    \ Meredith Ringel, Tulsee Doshi, Renelito Delos, Toju Duke, Johnny Soraker, Ben\
    \ Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson,\
    \ Alejandra Molina, Erin HoffmanJohn, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena\
    \ Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein,\
    \ Ray Kurzweil, Blaise AgueraArcas, Claire Cui, Marian Croak, Ed Chi, and Quoc\
    \ Le. 2022. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239\
    \ (2022).\n- <span id=\"page-16-1\"></span>[76] Hugo Touvron, Thibaut Lavril,\
    \ Gautier Izacard, Xavier Martinet, MarieAnne Lachaux, Timothée Lacroix, Baptiste\
    \ Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, A Rodriguez, A Joulin, E Grave,\
    \ and G Lample. 2023. Llama: Open and efficient foundation language models. arXiv\
    \ preprint arXiv:2302.13971 (2023).\n- <span id=\"page-16-2\"></span>[77] Hugo\
    \ Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\
    \ Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel,\
    \ Lukas Blecher, Cristian Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude\
    \ Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami,\
    \ Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin\
    \ Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit\
    \ Koura, MarieAnne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai\
    \ Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog,\
    \ Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,\
    \ Alan Schelten, Ruan Silva, Eric Smith, Ranjan Subramanian, Xiaoqing Ellen, Binh\
    \ Tang, Ross Taylor, Adina Williams, Jian Xiang, Puxin Xu, Zheng Yan, Iliyan Zarov,\
    \ Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez,\
    \ Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation\
    \ and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023).\n- <span\
    \ id=\"page-16-10\"></span>[78] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\
    \ Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin.\
    \ 2017. Attention is all you need. Advances in neural information processing systems\
    \ 30 (2017).\n- <span id=\"page-16-25\"></span>[79] Swagath Venkataramani, Ashish\
    \ Ranjan, Subarno Banerjee, Dipankar Das, Sasikanth Avancha, Ashok Jagannathan,\
    \ Ajaya Durg, Dheemanth Nagaraj, Bharat Kaul, Pradeep Dubey, and A Raghunathan.\
    \ 2017. Scaledeep: A scalable compute architecture for learning and evaluating\
    \ deep networks. In Proceedings of the 44th Annual International Symposium on\
    \ Computer Architecture. 13–26.\n- <span id=\"page-16-22\"></span>[80] Rangharajan\
    \ Venkatesan, Yakun Sophia Shao, Miaorong Wang, Jason Clemons, Steve Dai, Matthew\
    \ Fojtik, Ben Keller, Alicia Klinefelter, Nathaniel Pinckney, Priyanka Raina,\
    \ Yanqing Zhang, B Zimmer, W J. Dally, J Emer, Stephen W. Keckler, and Brucek\
    \ Khailany. 2019. Magnet: A modular accelerator generator for neural networks.\
    \ In 2019 IEEE/ACM International Conference on Computer-Aided Design, ICCAD 2019.\
    \ IEEE, 1–8.\n- <span id=\"page-16-20\"></span>[81] Huizheng Wang, Qize Yang,\
    \ Taiquan Wei, Xingmao Yu, Chengran Li, Jiahao Fang, Guangyang Lu, Xu Dai, Liang\
    \ Liu, Shenfei Jiang, Yang Hu, Shouyi Yin, and Shaojun Wei. 2024. TMAC: Training-targeted\
    \ Mapping and Architecture Co-exploration for Wafer-scale Chips. Integrated Circuits\
    \ and Systems (2024).\n- <span id=\"page-16-24\"></span>[82] William Won, Midhilesh\
    \ Elavazhagan, Sudarshan Srinivasan, Swati Gupta, and Tushar Krishna. 2024. TACOS:\
    \ Topology-Aware Collective Algorithm Synthesizer for Distributed Machine Learning.\
    \ In 2024 57th IEEE/ACM International Symposium on Microarchitecture (MICRO).\
    \ 856–870.\n- <span id=\"page-16-28\"></span>[83] William Won, Taekyung Heo, Saeed\
    \ Rashidi, Srinivas Sridharan, Sudarshan Srinivasan, and Tushar Krishna. 2023.\
    \ Astra-sim2. 0: Modeling hierarchical networks and disaggregated systems for\
    \ large-model training at scale. In 2023 IEEE International Symposium on Performance\
    \ Analysis of Systems and Software, ISPASS 2023. IEEE, 283–294.\n- <span id=\"\
    page-16-8\"></span>[84] Bingyang Wu, Shengyu Liu, Yinmin Zhong, Peng Sun, Xuanzhe\
    \ Liu, and Xin Jin. 2024. LoongServe: Efficiently Serving Long-context Large Language\
    \ Models with Elastic Sequence Parallelism. arXiv preprint arXiv:2404.09526 (2024).\n\
    - <span id=\"page-16-3\"></span>[85] Bingyang Wu, Yinmin Zhong, Zili Zhang, Shengyu\
    \ Liu, Fangyue Liu, Yuanhang Sun, Gang Huang, Xuanzhe Liu, and Xin Jin. 2023.\
    \ Fast distributed inference serving for large language models. arXiv preprint\
    \ arXiv:2305.05920 (2023).\n- <span id=\"page-16-27\"></span>[86] Qingcheng Xiao,\
    \ Size Zheng, Bingzhe Wu, Pengcheng Xu, Xuehai Qian, and Yun Liang. 2021. Hasco:\
    \ Towards agile hardware and software co-design for tensor computation. In 2021\
    \ ACM/IEEE 48th Annual International Symposium on Computer Architecture, ISCA\
    \ 2021. IEEE, 1055–1068.\n- <span id=\"page-16-12\"></span>[87] Yuanzhong Xu,\
    \ HyoukJoong Lee, Dehao Chen, Hongjun Choi, Blake Hechtman, and Shibo Wang. 2020.\
    \ Automatic cross-replica sharding of weight update in data-parallel training.\
    \ arXiv preprint arXiv:2004.13336 (2020).\n- <span id=\"page-16-13\"></span>[88]\
    \ Yuanzhong Xu, HyoukJoong Lee, Dehao Chen, Blake Hechtman, Yanping Huang, Rahul\
    \ Joshi, Maxim Krikun, Dmitry Lepikhin, Andy Ly, Marcello Maggioni, Ruoming Pang,\
    \ Noam Shazeer, Shibo Wang, Tao Wang, Yonghui Wu, and Zhifeng Chen. 2021. Gspmd:\
    \ general and scalable parallelization for ml computation graphs. arXiv preprint\
    \ arXiv:2105.04663 (2021).\n- <span id=\"page-16-29\"></span>[89] Zheng Xu, Xu\
    \ Dai, Shaojun Wei, Shouyi Yin, and Yang Hu. 2024. GSPO: A Graph Substitution\
    \ and Parallelization Joint Optimization Framework for DNN Inference. In Proceedings\
    \ of the 61st ACM/IEEE Design Automation Conference. 1–6.\n- <span id=\"page-16-23\"\
    ></span>[90] Xuan Yang, Mingyu Gao, Qiaoyi Liu, Jeff Setter, Jing Pu, Ankita Nayak,\
    \ Steven Bell, Kaidi Cao, Heonjae Ha, Priyanka Raina, C Kozyrakis, and M Horowitz.\
    \ 2020. Interstellar: Using halide's scheduling language to analyze dnn accelerators.\
    \ In Proceedings of the Twenty-Fifth International Conference on Architectural\
    \ Support for Programming Languages and Operating Systems. 369–383.\n- <span id=\"\
    page-16-17\"></span>[91] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong\
    \ Kim, and Byung-Gon Chun. 2022. Orca: A distributed serving system for Transformer-Based\
    \ generative models. In 16th USENIX Symposium on Operating Systems Design and\
    \ Implementation, OSDI 22. 521–538.\n- <span id=\"page-16-14\"></span>[92] Jinhui\
    \ Yuan, Xinqi Li, Cheng Cheng, Juncheng Liu, Ran Guo, Shenghang Cai, Chi Yao,\
    \ Fei Yang, Xiaodong Yi, Chuan Wu, H Zhang, and Zhao J. 2021. Oneflow: Redesign\
    \ the distributed deep learning framework from scratch. arXiv preprint arXiv:2110.15032\
    \ (2021).\n- <span id=\"page-16-15\"></span>[93] Hao Zhang, Yuan Li, Zhijie Deng,\
    \ Xiaodan Liang, Lawrence Carin, and Eric Xing. 2020. Autosync: Learning to synchronize\
    \ for data-parallel distributed deep learning. Advances in Neural Information\
    \ Processing Systems 33 (2020), 906–917.\n- <span id=\"page-16-31\"></span>[94]\
    \ Hengrui Zhang, August Ning, Rohan Baskar Prabhakar, and David Wentzlaff. 2024.\
    \ Llmcompass: Enabling efficient hardware design for large language model inference.\
    \ In 2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture,\
    \ ISCA 2024. IEEE, 1080–1096.\n- <span id=\"page-16-19\"></span>[95] Min Zhang,\
    \ Fei Qin, Si Chen, Yanwei Dai, Pei Chen, and Tong An. 2022. Protrusion of through-silicon-via\
    \ (TSV) copper with double annealing processes. Journal of Electronic Materials\
    \ 51, 5 (2022), 2433–2449.\n- <span id=\"page-16-6\"></span>[96] Susan Zhang,\
    \ Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher\
    \ Dewan, Mona Diab, Xian Li, Xi Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt\
    \ Shuster, Daniel Simig, Punit Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer.\
    \ 2022. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068\
    \ (2022).\n- <span id=\"page-16-16\"></span>[97] Zhen Zhang, Shuai Zheng, Yida\
    \ Wang, Justin Chiu, George Karypis, Trishul Chilimbi, Mu Li, and Xin Jin. 2022.\
    \ MiCS: near-linear scaling for training gigantic model on public cloud. arXiv\
    \ preprint arXiv:2205.00119 (2022).\n- <span id=\"page-16-32\"></span>[98] Xiaoyan\
    \ Zhao, Jiale Zhang, Junna Zhang, Peiyan Yuan, Hu Jin, and Xiangyang Li. 2023.\
    \ CooCo: A Collaborative Offloading and Resource Configuration Algorithm in Edge\
    \ Networks. IEEE Internet of Things Journal (2023).\n- <span id=\"page-16-26\"\
    ></span>[99] Shixuan Zheng, Xianjue Zhang, Leibo Liu, Shaojun Wei, and Shouyi\
    \ Yin. 2022. Atomic dataflow based graph-level workload orchestration for scalable\
    \ DNN accelerators. In 2022 IEEE International Symposium on High-Performance Computer\
    \ Architecture, HPCA 2022. IEEE, 475–489.\n- <span id=\"page-16-9\"></span>[100]\
    \ Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu, Xin\
    \ Jin, and Hao Zhang. 2024. Distserve: Disaggregating prefill and decoding for\
    \ goodput-optimized large language model serving. arXiv preprint arXiv:2401.09670\
    \ (2024).\n- <span id=\"page-16-30\"></span>[101] Jingchen Zhu, Chenhao Xue, Yiqi\
    \ Chen, Zhao Wang, and Guangyu Sun. 2024. Theseus: Towards High-Efficiency Wafer-Scale\
    \ Chip Design Space Exploration for Large Language Models. arXiv preprint arXiv:2407.02079\
    \ (2024)."
  references:
  - '- <span id="page-12-7"></span>[1] [n. d.]. Azure Public Dataset. [Online]. Available:
    [https://github.com/Azure/](https://github.com/Azure/AzurePublicDataset/tree/master)
    [AzurePublicDataset/tree/master.](https://github.com/Azure/AzurePublicDataset/tree/master)'
  - '- <span id="page-12-5"></span>[2] [n. d.]. Claude. [Online]. Available: [https://claude.ai/.](https://claude.ai/)'
  - '- [3] [n. d.]. Gemini. [Online]. Available: [https://gemini.google.com/.](https://gemini.google.com/)'
  - '- <span id="page-12-6"></span><span id="page-12-2"></span>[4] [n. d.]. Grok.
    [Online]. Available: [https://x.ai/blog/grok-1.5.](https://x.ai/blog/grok-1.5)'
  - '- <span id="page-12-4"></span>[5] [n. d.]. Mask / reticle. [Online]. Available:
    [https://en.wikichip.org/wiki/mask.](https://en.wikichip.org/wiki/mask)'
  - '- [6] [n. d.]. NVIDIA GB200 NVL72: Powering the new era of computing. [Online].
    Available: [https://www.nvidia.com/en-us/data-center/gb200-nvl72/.](https://www.nvidia.com/en-us/data-center/gb200-nvl72/)'
  - '- <span id="page-12-3"></span>[7] [n. d.]. Nvidia H100. [Online]. Available:
    [https://www.nvidia.com/en-us/data](https://www.nvidia.com/en-us/data-center/h100)[center/h100.](https://www.nvidia.com/en-us/data-center/h100)'
  - '- <span id="page-12-0"></span>[8] 2023. Bard, an experiment by google. [Online].
    Available: [https://bard.google.](https://bard.google.com/) [com/.](https://bard.google.com/)'
  - '- <span id="page-12-1"></span>[9] Josh Achiam, Steven Adler, Sandhini Agarwal,
    Lama Ahmad, Ilge Akkaya, Florencia Leoni, Diogo Almeida, Janko Altenschmidt, Sam
    Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom,
    Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake
    Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg
    Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage,
    Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea
    Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen,
    Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu,
    Hyung Won, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas
    Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila
    Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko
    Felix, Simón Posada, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian
    Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon,
    Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane, Yufei
    Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke,
    Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny
    Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang,
    Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn,
    Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish
    Shirish, Tabarak Khan, Logan Kilpatrick, Jong Wook, Christina Kim, Yongjik Kim,
    Jan Hendrik, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew
    Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael
    Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming, Rachel
    Lim, Molly Lin, Stephanie'
  - Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim
    Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer,
    Andrew Mayne, Bob McGrew, Scott Mayer, Christine McLeavey, Paul McMillan, Jake
    McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko,
    Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati,
    Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan,
    Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O'Keefe, Jakub Pachocki, Alex Paino,
    Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita,
    Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de, Michael Petrov,
    Henrique Ponde, Michael (Rai)Pokorny, Michelle Pokrass, Vitchyr H., Tolly Powell,
    Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae,
    Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted,
    Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish
    Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard,
    Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric
    Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky,
    Yang Song, Natalie Staudacher, Felipe Petroski, Natalie Summers, Ilya Sutskever,
    Jie Tang, Nikolas Tezak, Madeleine B., Phil Tillet, Amin Tootoonchian, Elizabeth
    Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe, Andrea Vallone,
    Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay, Alvin Wang, Ben
    Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder,
    Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich,
    Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu,
    Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang,
    Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and
    Barret Zoph. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023).
  - '- <span id="page-13-2"></span>[10] Amey Agrawal, Ashish Panwar, Jayashree Mohan,
    Nipun Kwatra, Bhargav S Gulavani, and Ramachandran Ramjee. 2023. Sarathi: Efficient
    llm inference by piggybacking decodes with chunked prefills. arXiv preprint arXiv:2308.16369
    (2023).'
  - '- <span id="page-13-9"></span>[11] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz
    Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet,
    Daniel Hesslow, Julien Launay, Quentin Malartic, Daniele Mazzotta, Badreddine
    Noune, Baptiste Pannier, and Guilherme Penedo. 2023. The Falcon Series of Open
    Language Models. arXiv preprint arXiv:2311.16867 (2023).'
  - '- <span id="page-13-3"></span>[12] Paul Barham, Aakanksha Chowdhery, Jeff Dean,
    Sanjay Ghemawat, Steven Hand, Dan Hurt, Michael Isard, Hyeontaek Lim, Ruoming
    Pang, Sudip Roy, Brennan Saeta, Parker Schuh, Ryan Sepassi, Laurent El Shafey,
    Chandramohan A. Thekkath, and Yonghui Wu. 2022. Pathways: Asynchronous distributed
    dataflow for ml. Proceedings of Machine Learning and Systems 4 (2022), 430–449.'
  - '- <span id="page-13-0"></span>[13] Tom B.Brown, Benjamin Mann, Nick Ryder, Melanie
    Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish
    Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger,
    Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.Ziegler, Jeffrey Wu, Clemens
    Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,
    Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,
    Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners.
    Advances in neural information processing systems 33 (2020), 1877–1901.'
  - '- <span id="page-13-5"></span>[14] Jingwei Cai, Yuchen Wei, Zuotong Wu, Sen Peng,
    and Kaisheng Ma. 2023. Inter-layer scheduling space definition and exploration
    for tiled accelerators. In Proceedings of the 50th Annual International Symposium
    on Computer Architecture. 1–17.'
  - '- <span id="page-13-4"></span>[15] Jingwei Cai, Zuotong Wu, Sen Peng, Yuchen
    Wei, Zhanhong Tan, Guiming Shi, Mingyu Gao, and Kaisheng Ma. 2024. Gemini: Mapping
    and Architecture Coexploration for Large-scale DNN Chiplet Accelerators. In 2024
    IEEE International Symposium on High-Performance Computer Architecture, HPCA 2024.
    IEEE, 156– 171.'
  - '- <span id="page-13-10"></span>[16] Shuangliang Chen, Saptadeep Pal, and Rakesh
    Kumar. 2024. Waferscale network switches. In 2024 ACM/IEEE 51st Annual International
    Symposium on Computer Architecture, ISCA 2024. IEEE, 215–229.'
  - '- <span id="page-13-6"></span>[17] Yu-Hsin Chen, Tushar Krishna, Joel S Emer,
    and Vivienne Sze. 2016. Eyeriss: An energy-efficient reconfigurable accelerator
    for deep convolutional neural networks. IEEE journal of solid-state circuits 52,
    1 (2016), 127–138.'
  - '- <span id="page-13-1"></span>[18] Aakanksha Chowdhery, Sharan Narang, Jacob
    Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Chung,
    Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko,
    Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran,
    Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin,
    Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay
    Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson,
    Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph,
    Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick,
    Andrew M. Dai, Thanumalayan Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,
    Rewon Child, Oleksandr Polozov,'
  - 'Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat,
    Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav
    Petrov, and Noah Fiedel. 2023. Palm: Scaling language modeling with pathways.
    Journal of Machine Learning Research 24, 240 (2023), 1–113.'
  - '- <span id="page-13-7"></span>[19] Jinyi Deng, Xinru Tang, Jiahao Zhang, Yuxuan
    Li, Linyun Zhang, Boxiao Han, Hongjun He, Fengbin Tu, Leibo Liu, Shaojun Wei,
    Yang Hu, and Shouyi Yin. 2023. Towards efficient control flow handling in spatial
    architecture via architecting the control flow plane. In Proceedings of the 56th
    Annual IEEE/ACM International Symposium on Microarchitecture. 1395–1408.'
  - '- <span id="page-13-8"></span>[20] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,
    Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy
    Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie
    Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez,
    Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie
    Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell,
    Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton,
    Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits,
    David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino,
    Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric
    Michael, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia
    Lewis, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen,
    Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta, Isabel Kloumann,
    Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason
    Park, Jay Mahadeokar, Jeet Shah, Jelmer van, Jennifer Billock, Jenny Hong, Jenya
    Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna
    Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe,
    Junteng Jia, Kalyan Vasuden, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield,
    Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal
    Bhalla, Lauren Rantala-Yeary, Laurens van, Lawrence Chen, Liang Tan, Liz Jenkins,
    Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de,
    Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas,
    Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si,
    Mitesh Kumar, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay
    Bogoychev, Niladri Chatterji, Olivier Duchenne, Onur Çelebi, Patrick Alrassy,
    Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik
    Dubal, Praveen Krishnan, Punit Singh, Puxin Xu, Qing He, Qingxiao Dong, Ragavan
    Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira, Robert Stojnic, Roberta
    Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan
    Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa,
    Sanjay Singh, Sean Bell, Seohyun Sonia, Sergey Edunov, Shaoliang Nie, Sharan Narang,
    Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende,
    Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan,
    Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou,
    Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj
    Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie
    Do, Vish Vogeti, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney
    Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen, Xinfeng Xie, Xuchao Jia,
    Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song,
    Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre, Zheng Yan, Zhengxing Chen,
    Zoe Papakipos, Aaditya Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey, Adam
    Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay
    Sharma, Alex Boesenberg, Alex Vaughan, Alexei Baevski, Allie Feinstein, Amanda
    Kallet, Amit Sangani, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples,
    Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Franco,
    Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf
    Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang,
    Beth Loyd, Beto De, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock,
    Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl
    Parker, Carly Burton, Catalina Mejia, Changhan Wang, Changkyu Kim, Chao Zhou,
    Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer,
    Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, Danny Wyatt, David Adkins,
    David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem
    Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine
    Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, Esteban Arcaute,
    Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat Ozgenel, Francesco
    Caggioni, Francisco Guzmán, Frank Kanayet, Frank Seide, Gabriela Medina, Gabriella
    Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman,
    Grigory Sizov, Guangyi (Jack)Zhang, Guna Lakshminarayanan, Hamid Shojanazeri,
    Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk,
    Henry Aspegren, Hunter Goldman, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Irina-Elena
    Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Japhet Asher, Jean-Baptiste
    Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy'
  - Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard,
    Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou,
    Karan Saxena, Karthik Prasad, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich,
    Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kun Huang, Kunal Chawla, Kushal
    Lakhotia, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee
    Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt,
    Madian Khabsa, Manav Avalani, Manish Bhatt, Maria Tsimpoukelli, Martynas Mankus,
    Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya
    Lathi, Meghan Keneally, Michael L., Michal Valko, Michelle Restrepo, Mihir Patel,
    Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert,
    Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks,
    Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikolay
    Pavlovich, Ning Dong, Ning Zhang, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar
    Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro
    Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant
    Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi
    Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li, Rebekkah Hogan,
    Robin Battey, Rocky Wang, Rohan Maheswari, Russ Howes, Ruty Rinott, Sai Jayesh,
    Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan,
    Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay,
    Sheng Feng, Shenghao Lin, Shengxin Cindy, Shiva Shankar, Shuqiang Zhang, Shuqiang
    Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie
    Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit
    Gupta, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman,
    Tal Remez, Tamar Glaser, Tamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li,
    Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria
    Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish, Vishal Mangla, Vítor Albiero,
    Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu, Vladimir Ivanov, Wei Li, Wenchen Wang,
    Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaofang Wang, Xiaojian
    Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye Hu, Ye Jia, Ye
    Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu (Sid)Wang,
    Yuchen Hao, Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo
    Wen, Zhenyu Yang, and Zhiwei Zhao. 2024. The llama 3 herd of models. arXiv preprint
    arXiv:2407.21783 (2024).
  - '- <span id="page-14-7"></span>[21] Shiqing Fan, Yi Rong, Chen Meng, Zongyan Cao,
    Siyu Wang, Zhen Zheng, Chuan Wu, Guoping Long, Jun Yang, Lixue Xia, Lansong Diao,
    Xiaoyong Liu, and Wei Lin. 2021. DAPPLE: A pipelined data parallel approach for
    training large models. In Proceedings of the 26th ACM SIGPLAN Symposium on Principles
    and Practice of Parallel Programming. 431–445.'
  - '- <span id="page-14-24"></span>[22] Yinxiao Feng and Kaisheng Ma. 2024. Switch-Less
    Dragonfly on Wafers: A Scalable Interconnection Architecture based on Wafer-Scale
    Integration. arXiv preprint arXiv:2407.10290 (2024).'
  - '- <span id="page-14-18"></span>[23] Mingyu Gao, Jing Pu, Xuan Yang, Mark Horowitz,
    and Christos Kozyrakis. 2017. Tetris: Scalable and efficient neural network acceleration
    with 3d memory. In Proceedings of the Twenty-Second International Conference on
    Architectural Support for Programming Languages and Operating Systems. 751–764.'
  - '- <span id="page-14-17"></span>[24] Mingyu Gao, Xuan Yang, Jing Pu, Mark Horowitz,
    and Christos Kozyrakis. 2019. Tangram: Optimized coarse-grained dataflow for scalable
    nn accelerators. In Proceedings of the Twenty-Fourth International Conference
    on Architectural Support for Programming Languages and Operating Systems. 807–820.'
  - '- <span id="page-14-27"></span>[25] Raveesh Garg, Hyoukjun Kwon, Eric Qin, Yu-Hsin
    Chen, Tushar Krishna, and Liangzhen Lai. 2024. PipeOrgan: Efficient Inter-operation
    Pipelining with Flexible Spatial Organization and Interconnects. arXiv preprint
    arXiv:2405.01736 (2024).'
  - '- <span id="page-14-9"></span>[26] Haoming Guo, Shengbin Cao, Lei Li, and Xiaofeng
    Zhang. 2022. A review on the mainstream through-silicon via etching methods. Materials
    Science in Semiconductor Processing 137 (2022), 106182.'
  - '- <span id="page-14-14"></span>[27] Kartik Hegde, Po-An Tsai, Sitao Huang, Vikas
    Chandra, Angshuman Parashar, and Christopher W Fletcher. 2021. Mind mappings:
    enabling efficient algorithmaccelerator mapping space search. In Proceedings of
    the 26th ACM International Conference on Architectural Support for Programming
    Languages and Operating Systems. 943–958.'
  - '- <span id="page-14-3"></span>[28] Connor Holmes, Masahiro Tanaka, Michael Wyatt,
    Ammar Ahmad, Jeff Rasley, Samyam Rajbhandari, Reza Yazdani, Heyang Qin, Arash
    Bakhtiari, Lev Kurilenko, and Yuxiong He. 2024. Deepspeed-fastgen: High-throughput
    text generation for llms via mii and deepspeed-inference. arXiv preprint arXiv:2401.08671
    (2024).'
  - '- <span id="page-14-2"></span>[29] SY Hou, W Chris Chen, Clark Hu, Christine
    Chiu, KC Ting, TS Lin, WH Wei, WC Chiou, Vic JC Lin, Victor CY Chang, C Wang,
    C Wu, and D Yu. 2017. Wafer-level integration of an advanced logic-memory system
    through the second-generation CoWoS technology. IEEE Transactions on Electron
    Devices 64, 10 (2017), 4071– 4077.'
  - '- <span id="page-14-4"></span>[30] Cunchen Hu, Heyang Huang, Junhao Hu, Jiang
    Xu, Xusheng Chen, Tao Xie, Chenxi Wang, Sa Wang, Yungang Bao, Ninghui Sun, and
    Yizhou Shan. 2024. Memserve: Context caching for disaggregated llm serving with
    elastic memory'
  - pool. arXiv preprint arXiv:2406.17565 (2024).
  - '- <span id="page-14-5"></span>[31] Cunchen Hu, Heyang Huang, Liangliang Xu, Xusheng
    Chen, Jiang Xu, Shuang Chen, Hao Feng, Chenxi Wang, Sa Wang, Yungang Bao, Ninghui
    Sun, and Yizhou Shan. 2024. Inference without interference: Disaggregate llm inference
    for mixed downstream workloads. arXiv preprint arXiv:2401.11181 (2024).'
  - '- <span id="page-14-10"></span>[32] YuChen Hu, YuMin Liang, HsiehPin Hu, ChiaYen
    Tan, ChihTa Shen, ChienHsun Lee, and SY Hou. 2023. CoWoS architecture evolution
    for next generation HPC on 2.5 D system in package. In 2023 IEEE 73rd Electronic
    Components and Technology Conference, ECTC 2023. IEEE, 1022–1026.'
  - '- <span id="page-14-1"></span>[33] Yang Hu, Xinhan Lin, Huizheng Wang, Zhen He,
    Xingmao Yu, Jiahao Zhang, Qize Yang, Zheng Xu, Sihan Guan, Jiahao Fang, Haoran
    Shang, Xinru Tang, Xu Dai, Shaojun Wei, and Shouyi Yin. 2024. Wafer-Scale Computing:
    Advancements, Challenges, and Future Perspectives. IEEE Circuits and Systems Magazine
    24, 1 (2024), 52–81.'
  - '- <span id="page-14-11"></span>[34] PK Huang, CY Lu, WH Wei, Christine Chiu,
    KC Ting, Clark Hu, CH Tsai, SY Hou, WC Chiou, CT Wang, and Douglas Yu. 2021. Wafer
    level system integration of the fifth generation CoWoS®-S with high performance
    Si interposer at 2500 mm2. In 2021 IEEE 71st Electronic Components and Technology
    Conference, ECTC 2021. IEEE, 101–104.'
  - '- <span id="page-14-19"></span>[35] Qijing Huang, Minwoo Kang, Grace Dinh, Thomas
    Norell, Aravind Kalaiah, James Demmel, John Wawrzynek, and Yakun Sophia Shao.
    2021. Cosa: Scheduling by constrained optimization for spatial accelerators. In
    2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture, ISCA
    2021. IEEE, 554–566.'
  - '- <span id="page-14-8"></span>[36] Yanping Huang, Youlong Cheng, Ankur Bapna,
    Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui
    Wu, and Zhifeng Chen. 2019. Gpipe: Efficient training of giant neural networks
    using pipeline parallelism. Advances in neural information processing systems
    32 (2019).'
  - '- <span id="page-14-22"></span>[37] Zhihao Jia, James Thomas, Todd Warszawski,
    Mingyu Gao, Matei Zaharia, and Alex Aiken. 2019. Optimizing DNN computation with
    relaxed graph substitutions. Proceedings of Machine Learning and Systems 1 (2019),
    27–39.'
  - '- <span id="page-14-23"></span>[38] Albert Q. Jiang, Alexandre Sablayrolles,
    Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian
    Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud,
    Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang,
    Timothée Lacroix, and William El Sayed. 2023. Mistral 7B. arXiv preprint arXiv:2310.06825
    (2023).'
  - '- <span id="page-14-25"></span>[39] Bentian Jiang, Jingsong Chen, Jinwei Liu,
    Lixin Liu, Fangzhou Wang, Xiaopeng Zhang, and Evangeline FY Young. 2021. CU. POKer:
    Placing DNNs on WSE With Optimal Kernel Sizing and Efficient Protocol Optimization.
    IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems
    41, 6 (2021), 1888– 1901.'
  - '- <span id="page-14-26"></span>[40] Divya Kadiyala, Saeed Rashidi, Taekyung Heo,
    Abhimanyu Bambhaniya, Tushar Krishna, and Alexandros Daglis. 2024. Leveraging
    Memory Expansion to Accelerate Large-Scale DL Training. In 2024 IEEE International
    Symposium on Performance Analysis of Systems and Software, ISPASS 2024. IEEE,
    292–294.'
  - '- <span id="page-14-15"></span>[41] Sheng-Chun Kao, Geonhwa Jeong, and Tushar
    Krishna. 2020. Confuciux: Autonomous hardware resource assignment for dnn accelerators
    using reinforcement learning. In 2020 53rd Annual IEEE/ACM International Symposium
    on Microarchitecture, MICRO 2020. IEEE, 622–636.'
  - '- <span id="page-14-20"></span>[42] Sheng-Chun Kao and Tushar Krishna. 2020.
    Gamma: Automating the hw mapping of dnn models on accelerators via genetic algorithm.
    In Proceedings of the 39th International Conference on Computer-Aided Design.
    1–9.'
  - '- <span id="page-14-16"></span>[43] Hyoukjun Kwon, Prasanth Chatarasi, Michael
    Pellauer, Angshuman Parashar, Vivek Sarkar, and Tushar Krishna. 2019. Understanding
    reuse, performance, and hardware cost of dnn dataflow: A data-centric approach.
    In Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture.
    754–768.'
  - '- <span id="page-14-21"></span>[44] Hyoukjun Kwon, Ananda Samajdar, and Tushar
    Krishna. 2018. Maeri: Enabling flexible dataflow mapping over dnn accelerators
    via reconfigurable interconnects. ACM SIGPLAN Notices 53, 2 (2018), 461–475.'
  - '- <span id="page-14-0"></span>[45] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying
    Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica.
    2023. Efficient memory management for large language model serving with pagedattention.
    In Proceedings of the 29th Symposium on Operating Systems Principles. 611–626.'
  - '- <span id="page-14-12"></span>[46] John H Lau, Gary ChangFu Chen, Jones YuCheng
    Huang, Ricky TsunSheng Chou, Channing ChengLin Yang, HsingNing Liu, and TzyyJang
    Tseng. 2021. Hybrid substrate by fan-out RDL-first panel-level packaging. IEEE
    Transactions on Components, Packaging and Manufacturing Technology 11, 8 (2021),
    1301– 1309.'
  - '- <span id="page-14-13"></span>[47] John H Lau, ChengTa Ko, KaiMing Yang, ChiaYu
    Peng, Tim Xia, Puru Bruce Lin, JJ Chen, Patrick PoChun Huang, HsingNing Liu, TzyyJang
    Tseng, Eagle Lin, and Leo Chang. 2020. Panel-level fan-out RDL-first packaging
    for heterogeneous integration. IEEE Transactions on Components, Packaging and
    Manufacturing Technology 10, 7 (2020), 1125–1137.'
  - '- <span id="page-14-6"></span>[48] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong
    Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng
    Chen. 2020. Gshard: Scaling giant models with conditional computation and automatic
    sharding. arXiv preprint arXiv:2006.16668 (2020).'
  - '- <span id="page-15-9"></span>[49] Zhuohan Li, Siyuan Zhuang, Shiyuan Guo, Danyang
    Zhuo, Hao Zhang, Dawn Song, and Ion Stoica. 2021. Terapipe: Token-level pipeline
    parallelism for training large-scale language models. In International Conference
    on Machine Learning. PMLR, 6543–6552.'
  - '- <span id="page-15-3"></span>[50] Sean Lie. 2022. Cerebras architecture deep
    dive: First look inside the hw/sw codesign for deep learning: Cerebras systems.
    In 2022 IEEE Hot Chips 34 Symposium, HCS 2022. IEEE Computer Society, 1–34.'
  - '- <span id="page-15-16"></span>[51] Jinwei Liu, Xiaopeng Zhang, Shiju Lin, Xinshi
    Zang, Jingsong Chen, Bentian Jiang, Martin DF Wong, and Evangeline FY Young. 2022.
    Partition and place finite element model on wafer-scale engine. In Proceedings
    of the 59th ACM/IEEE Design Automation Conference. 631–636.'
  - '- <span id="page-15-13"></span>[52] Liqiang Lu, Naiqing Guan, Yuyue Wang, Liancheng
    Jia, Zizhang Luo, Jieming Yin, Jason Cong, and Yun Liang. 2021. Tenet: A framework
    for modeling tensor dataflow based on relation-centric notation. In 2021 ACM/IEEE
    48th Annual International Symposium on Computer Architecture, ISCA 2021. IEEE,
    720–733.'
  - '- <span id="page-15-17"></span>[53] Canhui Luo, Zhouxing Su, and Zhipeng Lü.
    2023. MS-CLS: An Effective Partitioning and Placement Metaheuristic for Wafer-Scale
    Physics Modeling. IEEE Transactions on Emerging Topics in Computational Intelligence
    (2023).'
  - '- <span id="page-15-7"></span>[54] Azalia Mirhoseini, Hieu Pham, Quoc V Le, Benoit
    Steiner, Rasmus Larsen, Yuefeng Zhou, Naveen Kumar, Mohammad Norouzi, Samy Bengio,
    and Jeff Dean. 2017. Device placement optimization with reinforcement learning.
    In International conference on machine learning. PMLR, 2430–2439.'
  - '- <span id="page-15-0"></span>[55] Gordon E Moore. 1998. Cramming more components
    onto integrated circuits. Proc. IEEE 86, 1 (1998), 82–85.'
  - '- <span id="page-15-10"></span>[56] Deepak Narayanan, Aaron Harlap, Amar Phanishayee,
    Vivek Seshadri, Nikhil R Devanur, Gregory R Ganger, Phillip B Gibbons, and Matei
    Zaharia. 2019. PipeDream: Generalized pipeline parallelism for DNN training. In
    Proceedings of the 27th ACM symposium on operating systems principles. 1–15.'
  - '- <span id="page-15-20"></span>[57] Marcelo Orenes-Vera, Esin Tureci, Margaret
    Martonosi, and David Wentzlaff. 2023. DCRA: A distributed chiplet-based reconfigurable
    architecture for irregular applications. arXiv preprint arXiv:2311.15443 (2023).'
  - '- <span id="page-15-19"></span>[58] Marcelo OrenesVera, Esin Tureci, Margaret
    Martonosi, and David Wentzlaff. 2024. Muchisim: A simulation framework for design
    exploration of multichip manycore systems. In 2024 IEEE International Symposium
    on Performance Analysis of Systems and Software, ISPASS 2024. IEEE, 48–60.'
  - '- <span id="page-15-18"></span>[59] Sarp Özdemir, Mohammad Khasawneh, Smriti
    Rao, and Patrick H Madden. 2022. Kernel mapping techniques for deep learning neural
    network accelerators. In Proceedings of the 2022 International Symposium on Physical
    Design. 21–28.'
  - '- <span id="page-15-2"></span>[60] Saptadeep Pal, Jingyang Liu, Irina Alam, Nicholas
    Cebry, Haris Suhail, Shi Bu, Subramanian S Iyer, Sudhakar Pamarti, Rakesh Kumar,
    and Puneet Gupta. 2021. Designing a 2048-chiplet, 14336-core waferscale processor.
    In 2021 58th ACM/IEEE Design Automation Conference, DAC 2021. IEEE, 1183–1188.'
  - '- <span id="page-15-1"></span>[61] Saptadeep Pal, Daniel Petrisko, Matthew Tomei,
    Puneet Gupta, Subramanian S Iyer, and Rakesh Kumar. 2019. Architecting waferscale
    processors-a GPU case study. In 2019 IEEE International Symposium on High Performance
    Computer Architecture, HPCA 2019. IEEE, 250–263.'
  - '- <span id="page-15-5"></span>[62] Pratyush Patel, Esha Choukse, Chaojie Zhang,
    Aashaka Shah, Íñigo Goiri, Saeed Maleki, and Ricardo Bianchini. 2023. Splitwise:
    Efficient generative llm inference using phase splitting. Power 400, 700W (2023),
    1–75.'
  - '- <span id="page-15-6"></span>[63] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery,
    Jacob Devlin, James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and
    Jeff Dean. 2023. Efficiently scaling transformer inference. Proceedings of Machine
    Learning and Systems 5 (2023), 606–624.'
  - '- <span id="page-15-8"></span>[64] Samyam Rajbhandari, Jeff Rasley, Olatunji
    Ruwase, and Yuxiong He. 2020. Zero: Memory optimizations toward training trillion
    parameter models. In SC20: International Conference for High Performance Computing,
    Networking, Storage and Analysis. IEEE, 1–16.'
  - '- <span id="page-15-15"></span>[65] Saeed Rashidi, William Won, Sudarshan Srinivasan,
    Puneet Gupta, and Tushar Krishna. 2024. FRED: Flexible REduction-Distribution
    Interconnect and Communication Implementation for Wafer-Scale Distributed Training
    of DNN Models. arXiv preprint arXiv:2406.19580 (2024).'
  - '- <span id="page-15-4"></span>[66] Baptiste Rozière, Jonas Gehring, Fabian Gloeckle,
    Sten Sootla, Itai Gat, Xiaoqing Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre,
    Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish
    Bhatt, Cristian Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade
    Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom,
    and Gabriel Synnaeve. 2023. Code llama: Open foundation models for code. arXiv
    preprint arXiv:2308.12950 (2023).'
  - '- <span id="page-15-14"></span>[67] Teven Scao, Angela Fan, Christopher Akiki,
    Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Luccioni,
    François Yvon, Matthias Gallé, Jonathan Tow, Alexander Rush, Stella Biderman,
    Albert Webson, Pawan Ammanamanchi, Thomas Wang, Benoît Sagot, Niklas Muennighoff,
    Albert Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major,
    Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Suarez, Victor Sanh,
    Hugo Laurençon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel,
    Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Aji, Amit Alfassy, Anna Rogers,
    Ariel Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin
    Leong, Daniel Strien, David Adelani, Dragomir Radev, Eduardo Ponferrada,'
  - 'Efrat Levkovizh, Ethan Kim, Eyal Natan, Francesco Toni, Gérard Dupont, Germán
    Kruszewski, Giada Pistilli, Hady Elsahar, Hamza Benyamina, Hieu Tran, Ian Yu,
    Idris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios, Javier Rosa, Jenny Chim,
    Jesse Dodge, Jian Zhu, Jonathan Chang, Jörg Frohberg, Joseph Tobing, Joydeep Bhattacharjee,
    Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro Werra, Leon Weber, Long Phan, Loubna
    allal, Ludovic Tanguy, Manan Dey, Manuel Muñoz, Maraim Masoud, María Grandury,
    Mario Šaško, Max Huang, Maximin Coavoux, Mayank Singh, Mike Jiang, Minh Vu, Mohammad
    Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier
    Nguyen, Omar Espejel, Ona Gibert, Paulo, Peter Henderson, Pierre Colombo, Priscilla
    Amuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani, Roberto López, Rui Ribeiro,
    Salomey Osei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose, Shamsuddeen Muhammad,
    Shanya Sharma, Shayne Longpre, Somaieh Nikpoor, Stanislav Silberberg, Suhas Pai,
    Sydney Zink, Tiago Torrent, Timo Schick, Tristan Thrush, Valentin Danchev, Vassilina
    Nikoulina, Veronika Laippala, Violette Lepercq, Vrinda Prabhu, Zaid Alyafeai,
    Zeerak Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si, Davut Taşar, Elizabeth
    Salesky, Sabrina Mielke, Wilson Lee, Abheesht Sharma, Andrea Santilli, Antoine
    Chaffin, Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla, Gunjan Chhablani,
    Han Wang, Harshit Pandey, Hendrik Strobelt, Jason Fries, Jos Rozen, Leo Gao, Lintang
    Sutawika, M Bari, Maged Al-shaibani, Matteo Manica, Nihal Nayak, Ryan Teehan,
    Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen Bach, Taewoon Kim, Tali
    Bers, Thibault Fevry, Trishala Neeraj, Urmish Thakker, Vikas Raunak, Xiangru Tang,
    Zheng-Xin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri, Hadar Tojarieh, Adam Roberts,
    Hyung Chung, Jaesung Tae, Jason Phang, Ofir Press, Conglong Li, Deepak Narayanan,
    Hatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia
    Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi, Omar
    Sanseviero, Patrick Platen, Pierre Cornette, Pierre Lavallée, Rémi Lacroix, Samyam
    Rajbhandari, Sanchit Gandhi, Shaden Smith, Stéphane Requena, Suraj Patil, Tim
    Dettmers, Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure Ligozat,
    Arjun Subramonian, Aurélie Névéol, Charles Lovering, Dan Garrette, Deepak Tunuguntla,
    Ehud Reiter, Ekaterina Taktasheva, Ekaterina Voloshina, Eli Bogdanov, Genta Winata,
    Hailey Schoelkopf, Jan-Christoph Kalo, Jekaterina Novikova, Jessica Forde, Jordan
    Clive, Jungo Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat, Miruna Clinciu,
    Najoung Kim, Newton Cheng, Oleg Serikov, Omer Antverg, Oskar Wal, Rui Zhang, Ruochen
    Zhang, Sebastian Gehrmann, Shachar Mirkin, Shani Pais, Tatiana Shavrina, Thomas
    Scialom, Tian Yun, Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov, Vladislav
    Mikhailov, Yada Pruksachatkun, Yonatan Belinkov, Zachary Bamberger, Zdeněk Kasner,
    Alice Rueda, Amanda Pestana, Amir Feizpour, Ammar Khan, Amy Faranak, Ana Santos,
    Anthony Hevia, Antigona Unldreaj, Arash Aghagol, Arezoo Abdollahi, Aycha Tammour,
    Azadeh HajiHosseini, Bahareh Behroozi, Benjamin Ajibade, Bharat Saxena, Carlos
    Ferrandis, Daniel McDuff, Danish Contractor, David Lansky, Davis David, Douwe
    Kiela, Duong Nguyen, Edward Tan, Emi Baylor, Ezinwanne Ozoani, Fatima Mirza, Frankline
    Ononiwu, Habib Rezanejad, Hessie Jones, Indrani Bhattacharya, Irene Solaiman,
    Irina Sedenko, Isar Nejadgholi, Jesse Passmore, Josh Seltzer, Julio Sanz, Livia
    Dutra, Mairon Samagaio, Maraim Elbadri, Margot Mieskes, Marissa Gerchick, Martha
    Akinlolu, Michael McKenna, Mike Qiu, Muhammed Ghauri, Mykola Burynok, Nafis Abrar,
    Nazneen Rajani, Nour Elkott, Nour Fahmy, Olanrewaju Samuel, Ran An, Rasmus Kromann,
    Ryan Hao, Samira Alizadeh, Sarmad Shubber, Silas Wang, Sourav Roy, Sylvain Viguier,
    Thanh Le, Tobi Oyebade, Trieu Le, Yoyo Yang, Zach Nguyen, Abhinav Kashyap, Alfredo
    Palasciano, Alison Callahan, Anima Shukla, Antonio Miranda-Escalada, Ayush Singh,
    Benjamin Beilharz, Bo Wang, Caio Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu, Clémentine
    Fourrier, Daniel Periñán, Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio Barth,
    Florian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak, Gully Burns, Helena Vrabec,
    Imane Bello, Ishani Dash, Jihyun Kang, John Giorgi, Jonas Golde, Jose Posada,
    Karthik Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa Shinzato, Madeleine Bykhovetz,
    Maiko Takeuchi, Marc Pàmies, Maria Castillo, Marianna Nezhurina, Mario Sänger,
    Matthias Samwald, Michael Cullan, Michael Weinberg, Michiel Wolf, Mina Mihaljcic,
    Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam, Nathan Dahlberg, Nicholas
    Broad, Nikolaus Muellner, Pascale Fung, Patrick Haller, Ramya Chandrasekhar, Renata
    Eisenberg, Robert Martin, Rodrigo Canalli, Rosaline Su, Ruisi Su, Samuel Cahyawijaya,
    Samuele Garda, Shlok Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee
    Sang-aroonsiri, Srishti Kumar, Stefan Schweter, Sushil Bharati, Tanmay Laud, Théo
    Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis Labrak, Yash Bajaj, Yash Venkatraman,
    Yifan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes
    Belkada, and Thomas Wolf. 2023. Bloom: A 176b-parameter open-access multilingual
    language model. arXiv preprint arXiv:2211.05100v4 (2023).'
  - '- <span id="page-15-11"></span>[68] Johannes Schemmel, Daniel Brüderle, Andreas
    Grübl, Matthias Hock, Karlheinz Meier, and Sebastian Millner. 2010. A wafer-scale
    neuromorphic hardware system for large-scale neural modeling. In 2010 IEEE International
    Symposium on Circuits and Systems , ISCAS 2010. IEEE, 1947–1950.'
  - '- <span id="page-15-12"></span>[69] Yakun Sophia Shao, Jason Clemons, Rangharajan
    Venkatesan, Brian Zimmer, Matthew Fojtik, Nan Jiang, Ben Keller, Alicia Klinefelter,
    Nathaniel Pinckney,'
  - '<span id="page-16-0"></span>Priyanka Raina, S G.Tell, Y Zhang, J.Dally, Joel
    Emer, C Gray, B Khailany, and S W.Keckler. 2019. Simba: Scaling deep-learning
    inference with multi-chipmodule-based architecture. In Proceedings of the 52nd
    Annual IEEE/ACM International Symposium on Microarchitecture. 14–27.'
  - '- <span id="page-16-11"></span>[70] Noam Shazeer, Youlong Cheng, Niki Parmar,
    Dustin Tran, Ashish Vaswani, Penporn Koanantakool, Peter Hawkins, HyoukJoong Lee,
    Mingsheng Hong, Cliff Young, R Sepassi, and B Hechtman. 2018. Mesh-tensorflow:
    Deep learning for supercomputers. Advances in neural information processing systems
    31 (2018).'
  - '- <span id="page-16-7"></span>[71] Mingcong Song, Xinru Tang, Fengfan Hou, Jing
    Li, Wei Wei, Yipeng Ma, Runqiu Xiao, Hongjie Si, Dingcheng Jiang, Shouyi Yin,
    Yang Hu, and Guoping Long. 2024. Tackling the dynamicity in a production llm serving
    system with sota optimizations via hybrid prefill/decode/verify scheduling on
    efficient metakernels. arXiv preprint arXiv:2412.18106 (2024).'
  - '- <span id="page-16-18"></span>[72] Foteini Strati, Sara Mcallister, Amar Phanishayee,
    Jakub Tarnawski, and Ana Klimovic. 2024. DéjàVu: KV-cache Streaming for Fast,
    Fault-tolerant Generative LLM Serving. arXiv preprint arXiv:2403.01876 (2024).'
  - '- <span id="page-16-4"></span>[73] Emil Talpes, Douglas Williams, and Debjit
    Das Sarma. 2022. Dojo: The microarchitecture of tesla''s exa-scale computer. In
    2022 IEEE Hot Chips 34 Symposium, HCS 2022. IEEE Computer Society, 1–28.'
  - '- <span id="page-16-21"></span>[74] Zhanhong Tan, Hongyu Cai, Runpei Dong, and
    Kaisheng Ma. 2021. Nn-baton: Dnn workload orchestration and chiplet granularity
    exploration for multichip accelerators. In 2021 ACM/IEEE 48th Annual International
    Symposium on Computer Architecture, ISCA 2021. IEEE, 1013–1026.'
  - '- <span id="page-16-5"></span>[75] Romal Thoppilan, Daniel Freitas, Jamie Hall,
    Noam Shazeer, Apoorv Kulshreshtha, HengTze Cheng, Alicia Jin, Taylor Bos, Leslie
    Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven, Amin Ghafouri, Marcelo
    Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen,
    Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent Zhao, Yanqi Zhou,
    ChungChing Chang, Igor Krivokon, Will Rusch, Marc Pickett, Pranesh Srinivasan,
    Laichee Man, Kathleen MeierHellstern, Meredith Ringel, Tulsee Doshi, Renelito
    Delos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark
    Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin HoffmanJohn, Josh
    Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina,
    Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise AgueraArcas, Claire
    Cui, Marian Croak, Ed Chi, and Quoc Le. 2022. Lamda: Language models for dialog
    applications. arXiv preprint arXiv:2201.08239 (2022).'
  - '- <span id="page-16-1"></span>[76] Hugo Touvron, Thibaut Lavril, Gautier Izacard,
    Xavier Martinet, MarieAnne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman
    Goyal, Eric Hambro, Faisal Azhar, A Rodriguez, A Joulin, E Grave, and G Lample.
    2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971
    (2023).'
  - '- <span id="page-16-2"></span>[77] Hugo Touvron, Louis Martin, Kevin Stone, Peter
    Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal
    Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Ferrer, Moya Chen,
    Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
    Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini,
    Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann,
    Artem Korenev, Punit Koura, MarieAnne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Smith, Ranjan Subramanian, Xiaoqing Ellen,
    Binh Tang, Ross Taylor, Adina Williams, Jian Xiang, Puxin Xu, Zheng Yan, Iliyan
    Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez,
    Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation
    and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023).'
  - '- <span id="page-16-10"></span>[78] Ashish Vaswani, Noam Shazeer, Niki Parmar,
    Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin.
    2017. Attention is all you need. Advances in neural information processing systems
    30 (2017).'
  - '- <span id="page-16-25"></span>[79] Swagath Venkataramani, Ashish Ranjan, Subarno
    Banerjee, Dipankar Das, Sasikanth Avancha, Ashok Jagannathan, Ajaya Durg, Dheemanth
    Nagaraj, Bharat Kaul, Pradeep Dubey, and A Raghunathan. 2017. Scaledeep: A scalable
    compute architecture for learning and evaluating deep networks. In Proceedings
    of the 44th Annual International Symposium on Computer Architecture. 13–26.'
  - '- <span id="page-16-22"></span>[80] Rangharajan Venkatesan, Yakun Sophia Shao,
    Miaorong Wang, Jason Clemons, Steve Dai, Matthew Fojtik, Ben Keller, Alicia Klinefelter,
    Nathaniel Pinckney, Priyanka Raina, Yanqing Zhang, B Zimmer, W J. Dally, J Emer,
    Stephen W. Keckler, and Brucek Khailany. 2019. Magnet: A modular accelerator generator
    for neural networks. In 2019 IEEE/ACM International Conference on Computer-Aided
    Design, ICCAD 2019. IEEE, 1–8.'
  - '- <span id="page-16-20"></span>[81] Huizheng Wang, Qize Yang, Taiquan Wei, Xingmao
    Yu, Chengran Li, Jiahao Fang, Guangyang Lu, Xu Dai, Liang Liu, Shenfei Jiang,
    Yang Hu, Shouyi Yin, and Shaojun Wei. 2024. TMAC: Training-targeted Mapping and
    Architecture Co-exploration for Wafer-scale Chips. Integrated Circuits and Systems
    (2024).'
  - '- <span id="page-16-24"></span>[82] William Won, Midhilesh Elavazhagan, Sudarshan
    Srinivasan, Swati Gupta, and Tushar Krishna. 2024. TACOS: Topology-Aware Collective
    Algorithm Synthesizer for Distributed Machine Learning. In 2024 57th IEEE/ACM
    International Symposium on Microarchitecture (MICRO). 856–870.'
  - '- <span id="page-16-28"></span>[83] William Won, Taekyung Heo, Saeed Rashidi,
    Srinivas Sridharan, Sudarshan Srinivasan, and Tushar Krishna. 2023. Astra-sim2.
    0: Modeling hierarchical networks and disaggregated systems for large-model training
    at scale. In 2023 IEEE International Symposium on Performance Analysis of Systems
    and Software, ISPASS 2023. IEEE, 283–294.'
  - '- <span id="page-16-8"></span>[84] Bingyang Wu, Shengyu Liu, Yinmin Zhong, Peng
    Sun, Xuanzhe Liu, and Xin Jin. 2024. LoongServe: Efficiently Serving Long-context
    Large Language Models with Elastic Sequence Parallelism. arXiv preprint arXiv:2404.09526
    (2024).'
  - '- <span id="page-16-3"></span>[85] Bingyang Wu, Yinmin Zhong, Zili Zhang, Shengyu
    Liu, Fangyue Liu, Yuanhang Sun, Gang Huang, Xuanzhe Liu, and Xin Jin. 2023. Fast
    distributed inference serving for large language models. arXiv preprint arXiv:2305.05920
    (2023).'
  - '- <span id="page-16-27"></span>[86] Qingcheng Xiao, Size Zheng, Bingzhe Wu, Pengcheng
    Xu, Xuehai Qian, and Yun Liang. 2021. Hasco: Towards agile hardware and software
    co-design for tensor computation. In 2021 ACM/IEEE 48th Annual International Symposium
    on Computer Architecture, ISCA 2021. IEEE, 1055–1068.'
  - '- <span id="page-16-12"></span>[87] Yuanzhong Xu, HyoukJoong Lee, Dehao Chen,
    Hongjun Choi, Blake Hechtman, and Shibo Wang. 2020. Automatic cross-replica sharding
    of weight update in data-parallel training. arXiv preprint arXiv:2004.13336 (2020).'
  - '- <span id="page-16-13"></span>[88] Yuanzhong Xu, HyoukJoong Lee, Dehao Chen,
    Blake Hechtman, Yanping Huang, Rahul Joshi, Maxim Krikun, Dmitry Lepikhin, Andy
    Ly, Marcello Maggioni, Ruoming Pang, Noam Shazeer, Shibo Wang, Tao Wang, Yonghui
    Wu, and Zhifeng Chen. 2021. Gspmd: general and scalable parallelization for ml
    computation graphs. arXiv preprint arXiv:2105.04663 (2021).'
  - '- <span id="page-16-29"></span>[89] Zheng Xu, Xu Dai, Shaojun Wei, Shouyi Yin,
    and Yang Hu. 2024. GSPO: A Graph Substitution and Parallelization Joint Optimization
    Framework for DNN Inference. In Proceedings of the 61st ACM/IEEE Design Automation
    Conference. 1–6.'
  - '- <span id="page-16-23"></span>[90] Xuan Yang, Mingyu Gao, Qiaoyi Liu, Jeff Setter,
    Jing Pu, Ankita Nayak, Steven Bell, Kaidi Cao, Heonjae Ha, Priyanka Raina, C Kozyrakis,
    and M Horowitz. 2020. Interstellar: Using halide''s scheduling language to analyze
    dnn accelerators. In Proceedings of the Twenty-Fifth International Conference
    on Architectural Support for Programming Languages and Operating Systems. 369–383.'
  - '- <span id="page-16-17"></span>[91] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim,
    Soojeong Kim, and Byung-Gon Chun. 2022. Orca: A distributed serving system for
    Transformer-Based generative models. In 16th USENIX Symposium on Operating Systems
    Design and Implementation, OSDI 22. 521–538.'
  - '- <span id="page-16-14"></span>[92] Jinhui Yuan, Xinqi Li, Cheng Cheng, Juncheng
    Liu, Ran Guo, Shenghang Cai, Chi Yao, Fei Yang, Xiaodong Yi, Chuan Wu, H Zhang,
    and Zhao J. 2021. Oneflow: Redesign the distributed deep learning framework from
    scratch. arXiv preprint arXiv:2110.15032 (2021).'
  - '- <span id="page-16-15"></span>[93] Hao Zhang, Yuan Li, Zhijie Deng, Xiaodan
    Liang, Lawrence Carin, and Eric Xing. 2020. Autosync: Learning to synchronize
    for data-parallel distributed deep learning. Advances in Neural Information Processing
    Systems 33 (2020), 906–917.'
  - '- <span id="page-16-31"></span>[94] Hengrui Zhang, August Ning, Rohan Baskar
    Prabhakar, and David Wentzlaff. 2024. Llmcompass: Enabling efficient hardware
    design for large language model inference. In 2024 ACM/IEEE 51st Annual International
    Symposium on Computer Architecture, ISCA 2024. IEEE, 1080–1096.'
  - '- <span id="page-16-19"></span>[95] Min Zhang, Fei Qin, Si Chen, Yanwei Dai,
    Pei Chen, and Tong An. 2022. Protrusion of through-silicon-via (TSV) copper with
    double annealing processes. Journal of Electronic Materials 51, 5 (2022), 2433–2449.'
  - '- <span id="page-16-6"></span>[96] Susan Zhang, Stephen Roller, Naman Goyal,
    Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li,
    Xi Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit
    Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. Opt: Open pre-trained
    transformer language models. arXiv preprint arXiv:2205.01068 (2022).'
  - '- <span id="page-16-16"></span>[97] Zhen Zhang, Shuai Zheng, Yida Wang, Justin
    Chiu, George Karypis, Trishul Chilimbi, Mu Li, and Xin Jin. 2022. MiCS: near-linear
    scaling for training gigantic model on public cloud. arXiv preprint arXiv:2205.00119
    (2022).'
  - '- <span id="page-16-32"></span>[98] Xiaoyan Zhao, Jiale Zhang, Junna Zhang, Peiyan
    Yuan, Hu Jin, and Xiangyang Li. 2023. CooCo: A Collaborative Offloading and Resource
    Configuration Algorithm in Edge Networks. IEEE Internet of Things Journal (2023).'
  - '- <span id="page-16-26"></span>[99] Shixuan Zheng, Xianjue Zhang, Leibo Liu,
    Shaojun Wei, and Shouyi Yin. 2022. Atomic dataflow based graph-level workload
    orchestration for scalable DNN accelerators. In 2022 IEEE International Symposium
    on High-Performance Computer Architecture, HPCA 2022. IEEE, 475–489.'
  - '- <span id="page-16-9"></span>[100] Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo
    Hu, Yibo Zhu, Xuanzhe Liu, Xin Jin, and Hao Zhang. 2024. Distserve: Disaggregating
    prefill and decoding for goodput-optimized large language model serving. arXiv
    preprint arXiv:2401.09670 (2024).'
  - '- <span id="page-16-30"></span>[101] Jingchen Zhu, Chenhao Xue, Yiqi Chen, Zhao
    Wang, and Guangyu Sun. 2024. Theseus: Towards High-Efficiency Wafer-Scale Chip
    Design Space Exploration for Large Language Models. arXiv preprint arXiv:2407.02079
    (2024).'
- id: precise_exceptions_in_relaxed_architectures_ben_simner_https_orcid_org_0009_0000_8431_9577_university_of_cambridge_cambridge_united_kingdom_ben_simner_cl_cam_ac_uk
  title: Precise exceptions in relaxed architectures
  abstract: 'To manage exceptions, software relies on a key architectural guarantee,
    precision: that exceptions appear to execute between instructions. However, this
    definition, dating back over 60 years, fundamentally assumes a sequential programmers
    model. Modern architectures such as Arm-A with programmer-observable relaxed behaviour
    make such a naive definition inadequate, and it is unclear exactly what guarantees
    programmers have on exception entry and exit.

    In this paper, we clarify the concepts needed to discuss exceptions in the relaxed-memory
    setting – a key aspect of precisely specifying the architectural interface between
    hardware and software. We explore the basic relaxed behaviour across exception
    boundaries, and the semantics of external aborts, using Arm-A as a representative
    modern architecture. We identify an important problem, present yet unexplored
    for decades: pinning down what it means for exceptions to be precise in a relaxed
    setting. We describe key phenomena that any definition should account for. We
    develop an axiomatic model for Arm-A precise exceptions, tooling for axiomatic
    model execution, and a library of tests. Finally we explore the relaxed semantics
    of software-generated interrupts, as used in sophisticated programming patterns,
    and sketch how they too could be modelled.'
  keywords: Computer architecture, relaxed memory, exceptions and interrupts, exception
    handling, semantics [This work is licensed under a Creative Commons Attribution
    4.0 International License.](https://creativecommons.org/licenses/by/4.0) ISCA
    '25, Tokyo, Japan © 2025 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-1261-6/25/06
    <https://doi.org/10.1145/3695053.3731102>
  document: "![](_page_0_Picture_0.jpeg)\n\n# Precise exceptions in relaxed architectures\n\
    \n[Ben Simner](https://orcid.org/0009-0000-8431-9577) University of Cambridge\
    \ Cambridge, United Kingdom Ben.Simner@cl.cam.ac.uk\n\n[Brian Campbell](https://orcid.org/0000-0001-6941-5034)\
    \ University of Edinburgh Edinburgh, United Kingdom Brian.Campbell@ed.ac.uk\n\n\
    [Alasdair Armstrong](https://orcid.org/0000-0002-2910-0764) University of Cambridge\
    \ Cambridge, United Kingdom Alasdair.Armstrong@cl.cam.ac.uk\n\n[Ohad Kammar](https://orcid.org/0000-0002-2071-0929)\
    \ University of Edinburgh Edinburgh, United Kingdom ohad.kammar@ed.ac.uk\n\n[Peter\
    \ Sewell](https://orcid.org/0000-0001-9352-1013) University of Cambridge Cambridge,\
    \ United Kingdom Peter.Sewell@cl.cam.ac.uk\n\n[Thomas Bauereiss](https://orcid.org/0000-0001-9607-8942)\
    \ University of Cambridge Cambridge, United Kingdom Thomas.Bauereiss@cl.cam.ac.uk\n\
    \n[Jean Pichon-Pharabod](https://orcid.org/0000-0002-4442-6543) Aarhus University\
    \ Aarhus, Denmark Jean.Pichon@cs.au.dk\n\n# Abstract\n\nTo manage exceptions,\
    \ software relies on a key architectural guarantee, precision: that exceptions\
    \ appear to execute between instructions. However, this definition, dating back\
    \ over 60 years, fundamentally assumes a sequential programmers model. Modern\
    \ architectures such as Arm-A with programmer-observable relaxed behaviour make\
    \ such a naive definition inadequate, and it is unclear exactly what guarantees\
    \ programmers have on exception entry and exit.\n\nIn this paper, we clarify the\
    \ concepts needed to discuss exceptions in the relaxed-memory setting – a key\
    \ aspect of precisely specifying the architectural interface between hardware\
    \ and software. We explore the basic relaxed behaviour across exception boundaries,\
    \ and the semantics of external aborts, using Arm-A as a representative modern\
    \ architecture. We identify an important problem, present yet unexplored for decades:\
    \ pinning down what it means for exceptions to be precise in a relaxed setting.\
    \ We describe key phenomena that any definition should account for. We develop\
    \ an axiomatic model for Arm-A precise exceptions, tooling for axiomatic model\
    \ execution, and a library of tests. Finally we explore the relaxed semantics\
    \ of software-generated interrupts, as used in sophisticated programming patterns,\
    \ and sketch how they too could be modelled.\n\n# CCS Concepts\n\n• Computer systems\
    \ organization → Architectures; Multicore architectures; • Theory of computation\
    \ → Axiomatic semantics.\n\n# Keywords\n\nComputer architecture, relaxed memory,\
    \ exceptions and interrupts, exception handling, semantics\n\n[This work is licensed\
    \ under a Creative Commons Attribution 4.0 International License.](https://creativecommons.org/licenses/by/4.0)\
    \ ISCA '25, Tokyo, Japan © 2025 Copyright held by the owner/author(s). ACM ISBN\
    \ 979-8-4007-1261-6/25/06 <https://doi.org/10.1145/3695053.3731102>\n\n#### ACM\
    \ Reference Format:\n\nBen Simner, Alasdair Armstrong, Thomas Bauereiss, Brian\
    \ Campbell, Ohad Kammar, Jean Pichon-Pharabod, and Peter Sewell. 2025. Precise\
    \ exceptions in relaxed architectures. In Proceedings of the 52nd Annual International\
    \ Symposium on Computer Architecture (ISCA '25), June 21–25, 2025, Tokyo, Japan.\
    \ ACM, New York, NY, USA, [14](#page-13-0) pages. [https://doi.org/10.1145/3695053.](https://doi.org/10.1145/3695053.3731102)\
    \ [3731102](https://doi.org/10.1145/3695053.3731102)\n\n# 1 Introduction\n\nHardware\
    \ exceptions (and their many variants: interrupts, traps, faults, aborts, etc.)\
    \ provide support for many exceptional situations that systems software has to\
    \ manage. This includes explicit privilege transitions via system calls, implicit\
    \ privilege transitions from trappable instructions, inter-processor software-generated\
    \ interrupts, external interrupts from timers or devices, recoverable faults like\
    \ address translation faults, and non-recoverable faults like memory error correction\
    \ faults.\n\nTo confidently write concurrent systems code that handles exceptions,\
    \ e.g. mapping on demand at page faults, programmers need a well-defined and well-understood\
    \ semantics. The definition given in modern architectures (e.g. in the current\
    \ Arm-A documentation) is basically unchanged since the IBM System/360, roughly\
    \ as Hennessy and Patterson [\\[34\\]](#page-12-0) state: \"An exception is imprecise\
    \ if the processor state when an exception is raised does not look exactly as\
    \ if the instructions were executed sequentially in strict program order\". However,\
    \ on pipelined, out-of-order processors with observable speculative execution,\
    \ exceptions have subtle interactions with relaxed memory behaviour which have\
    \ not previously been investigated.\n\n# 1.1 Contributions\n\nIn this paper, we\
    \ investigate the relaxed concurrency semantics of exceptions on modern high-performance\
    \ architectures. We focus on the Arm-A application-profile architecture as a representative\
    \ example, although we expect that the challenges we describe also appear in other,\
    \ similarly relaxed, architectures. This work involved detailed discussions with\
    \ Arm senior staff, including the Arm Chief Architect and an Arm Generic Interrupt\
    \ Controller (GIC) expert. Our contributions are:\n\n- We clarify the concepts\
    \ and terminology needed to discuss exceptions in relaxed-memory executions ([§2\\\
    )](#page-1-0).\n- We explore the relaxed behaviour of exceptions: out-of-order\
    \ and speculative execution, and forwarding across exception entry/exit boundaries\
    \ ([§3\\)](#page-2-0). This is based on discussions with Arm and testing of several\
    \ processor implementations, using a test harness for hardware testing of exceptions,\
    \ and a library of hand-written litmus tests.\n- We explore the semantics of memory\
    \ errors ([§4\\)](#page-5-0). In Arm-A, these can generate external aborts. Some\
    \ implementations, including server designs, may exhibit synchronous external\
    \ aborts. Such implementations rule out load-buffering (LB) relaxed behaviour,\
    \ which substantially curtails how relaxed observable behaviour is.\n- We develop\
    \ an axiomatic model for Arm-A precise exceptions ([§5\\)](#page-6-0). We extend\
    \ Isla [\\[13\\]](#page-12-1) to support both ISA and relaxedmemory concurrency\
    \ aspects of exceptions, and we use it to evaluate the axiomatic model on tests.\n\
    - We identify and discuss the substantial open problem of what it means for exceptions\
    \ to be precise in relaxed setting ([§6\\)](#page-7-0). We characterise key properties\
    \ that a definition should respect, and highlight the challenge of giving a proper\
    \ definition of precision when relaxed behaviour is allowed across exception boundaries.\n\
    - Finally, we explore a significant use-case of exceptions that benefits from\
    \ the clarification of their interaction with relaxed memory: the relaxed semantics\
    \ of software-generated interrupts as used for sophisticated low-cost synchronisation,\
    \ e.g. in Linux's RCU [\\[52\\]](#page-13-1) and Verona [\\[14\\]](#page-12-2)\
    \ ([§7\\)](#page-8-0). We sketch this in an axiomatic model.\n\nThis is an essential\
    \ part of the necessary foundation for confidently programming systems code, building\
    \ on previous work that has clarified 'user' relaxed concurrency [\\[1–](#page-12-3)[3,](#page-12-4)\
    \ [7](#page-12-5)[–9,](#page-12-6) [13,](#page-12-1) [21,](#page-12-7) [28–](#page-12-8)\
    \ [32,](#page-12-9) [37,](#page-12-10) [57,](#page-13-2) [58,](#page-13-3) [60–](#page-13-4)[62,](#page-13-5)\
    \ [64,](#page-13-6) [68\\]](#page-13-7) and complementing recent work on the systems\
    \ aspects of instruction fetch [\\[67\\]](#page-13-8) and virtual memory [\\[4,](#page-12-11)\
    \ [66\\]](#page-13-9). It helps put processor architecture specifications such\
    \ as Arm-A on an unambiguous footing, where the allowed behaviour of systemscode\
    \ idioms can be computed from a precise and executable-as-testoracle definition\
    \ of the architecture.\n\n#### 1.2 Scope and limitations\n\nOur models cover important\
    \ use cases of exceptions, but there remain several questions to be addressed\
    \ by future work. Our testing suite is relatively small, and a much larger corpus\
    \ would give higher confidence, and ideally could be auto-generated [\\[5,](#page-12-12)\
    \ [9,](#page-12-6) [35\\]](#page-12-13). We do not give semantics to imprecise\
    \ exceptions, and it is unclear how to do so at an architectural level. For our\
    \ specific modelling of Arm: we do not define the behaviour of 'constrained unpredictable',\
    \ and merely flag when it is triggered. Clarifying it will require substantial\
    \ extensive discussions with Arm architects, likely affecting the wording in the\
    \ architectural specifications, beyond the scope of this paper. We do not try\
    \ to precisely model the relaxed behaviour of system registers, but merely sufficient\
    \ conditions for conservative use cases in the context of exceptions ([§3.1\\\
    )](#page-3-0). We do not model switching between Arm FEAT\\_ExS modes ([§3.5\\\
    )](#page-5-1): they are supported architecturally, but are not commonly implemented.\
    \ We rely on a specific\n\nconfiguration to illustrate the use of interrupts for\
    \ synchronisation ([§7\\)](#page-8-0), without detailed modelling of the Arm Generic\
    \ Interrupt Controller (GIC), or other system-on-chip (SoC) aspects. The GIC is\
    \ a complex hardware component, with a 950-page specification [\\[11,](#page-12-14)\
    \ H.b], and modelling it in full would be a major project in itself. This work\
    \ is validated by substantial discussion and hardware testing, but more extensive\
    \ testing on more devices is always desirable; we hope that our work will spur\
    \ such additional testing on devices not available to us. Finally, while we believe\
    \ our models correctly capture the Arm architectural intent, and that it gives\
    \ a solid basis for programmers, this paper is not an authoritative definition\
    \ of the architecture, which is in any case subject to change.\n\n# <span id=\"\
    page-1-0\"></span>2 Arm-A architectural concepts for exceptions\n\nWe start by\
    \ recalling, and then refining the architectural concepts for exceptions in Arm.\n\
    \n# 2.1 Exception taxonomy\n\nArm-A defines multiple kinds of exception [\\[10,](#page-12-15)\
    \ D1.3.1, p6060]: Synchronous exceptions (supervisor/hypervisor calls, traps,\
    \ data/instruction, page faults, etc.) and interrupts (IRQ/FIQ from processors/peripherals/timers\
    \ and system errors).\n\nThe preferred return address of synchronous exceptions\
    \ has an architecturally defined relationship with the instruction that caused\
    \ them. Such exceptions are precise. This means, roughly, that they are observed\
    \ at particular points in the instruction stream, and so can use the preferred\
    \ return address to resume executing it after handling the exception. All interrupts\
    \ are precise apart from external system aborts (SError), for which it is implementation-defined\
    \ (per-kind) whether they are precise. Such errors may or may not be recoverable\
    \ in practice. For example, an unrecoverable imprecise error may be generated\
    \ by late detection of an uncorrectable memory error correction error. In [§3,](#page-2-0)\
    \ we discuss how the choice of mechanism used to report external aborts affects\
    \ the relaxed behaviour.\n\n# 2.2 Architectural exception machinery\n\nIn Arm-A,\
    \ when an exception is taken, execution jumps to the exception vector, an offset\
    \ from the appropriate vector base address register (VBAR) value depending on\
    \ the kind of exception. The appropriate exception syndrome register (ESR), fault\
    \ address register (FAR), and exception link register (ELR) are written with information\
    \ about the cause and the preferred return address. In some cases, the exception\
    \ level (EL) register value, ranging in increasing privilege from 0 to 3, is also\
    \ changed. Exception handlers typically use ERET to return from an exception,\
    \ which restores some processor state and branches to the address in the appropriate\
    \ ELR. Most of these system registers (VBAR, ESR, etc.) are banked.\n\n# 2.3 Instructions\
    \ and instruction streams\n\nOne often thinks of processors as executing instructions\
    \ in some instruction sequence, and common terminology is based on those two concepts.\
    \ For example, the Arm manual has around 60 instances of instruction stream or\
    \ execution stream. However, to account for relaxed behaviours and exceptions,\
    \ we must refine these concepts.\n\n1\n\n1\n\n<span id=\"page-2-2\"></span>2.3.1\
    \ From instructions to fetch-decode-execute instances. Exceptions can arise at\
    \ multiple points within the fetch-decode-execute cycle, including during the\
    \ fetch and decode, when there is no 'instruction'. For Armv9.4-A, much of this\
    \ is captured in an Arm top-level function written in the Arm Architecture Specification\
    \ Language (ASL).\n\nWe have then integrated this into Sail-based tooling to obtain\
    \ an executable-as-test-oracle semantics of the sequential ISA aspects of Armv9.4-A\
    \ with exceptions ([§5.1\\)](#page-7-1). A highly simplified outline of a single-instruction\
    \ slice of the (400k line) instruction semantics is:\n\n```\nfunction __TopLevel()\
    \ =\n```\n\n```\n// in TakePendingInterrupts:\n```\n\n```\nif IRQ then AArch64_TakePhysicalIRQException()\n\
    if SE then AArch64_TakePhysicalSErrorException(...)\n// in AArch64_CheckPCAlignment:\n\
    if pc[1..0] != 0b00 then AArch64_PCAlignmentFault()\n// in __FetchInstr:\nopcode\
    \ = AArch64_MemSingle_read(pc, 4) // read memory\n// in __DecodeA64:\nmatch opcode\n\
    \  [1,_,1,1,1,0,0,1,0,1,_,_,_,_,_,_,_,_,_,_,_,_,_,_,_,_,_,_,\n   _,_,_,_] =\n\
    \    // the semantics for one family of instructions,\n    // including loads\
    \ LDR Xt,[Xn]\n    // execute_aarch64_instrs_memory_single_general_\n    // immediate_signed_post_idx(n,t,...)\n\
    \    let address = X_read(n, 64) // read register n\n    let data : bits('datasize)\
    \ = // read memory\n      Mem_read(address, DIV(datasize,8))\n    // write register\
    \ t\n    X_set(t, regsize) = ZeroExtend(data, regsize)\n```\nExecuting this semantics\
    \ may lead to one or more kinds of exception, calling the ASL/Sail function AArch64\\\
    _TakeException(). This function writes the appropriate values to registers, e.g.\
    \ computing the next PC, exception level, etc. and terminates this \\_\\_TopLevel()\
    \ execution. So instead of 'instruction' instances, we refer to fetch-decode-execute\
    \ (FDX) instances, each of which is a single execution of \\_\\_TopLevel().\n\n\
    2.3.2 Fetch-decode-execute trees and streams. One must relate the out-of-order\
    \ speculative execution of hardware implementations and the architectural definition\
    \ of the allowed behaviours. We will use the following concepts, well-understood\
    \ when modelling relaxed memory without exceptions. At any instant, each hardware\
    \ thread may be processing, out-of-order and speculatively, many instructions\
    \ (each corresponding to an architectural FDX instance). Partially executed instances\
    \ are restarted or discarded if they would violate the intended semantics (e.g.\
    \ on mispredicted branches).\n\nOne can visualise the state of a single core abstractly\
    \ as a tree of partially and completely executed instances, as in Fig. [1](#page-2-1)\
    \ (top). Abstract-microarchitectural operational models use this abstraction [\\\
    [28,](#page-12-8) [29,](#page-12-16) [32,](#page-12-9) [58,](#page-13-3) [60,](#page-13-4)\
    \ [61\\]](#page-13-10). We depict the retired (committed) FDX instances as solid\
    \ dark green, and partially/tentatively executed in-flight instances as light\
    \ green. The arrows depict program order. Committed instances can be program-order\
    \ after in-flight instances, and non-committed instances may need to be restarted.\
    \ Eventually\n\n<span id=\"page-2-1\"></span>![](_page_2_Figure_10.jpeg)\n\nFigure\
    \ 1: Top. The tree of (partially) executed FDX instances at one time, in hardware\
    \ or operational model execution. Bottom. The sequence of architecturally executed\
    \ FDX instances in a completed execution.\n\nall FDX instances for this hardware\
    \ thread will be either committed or discarded, e.g. as in Fig. [1](#page-2-1)\
    \ (bottom). These are the architecturally executed FDX instances. The architecture\
    \ definition, and any formal semantics thereof, have to define which such sequences\
    \ are allowed for each thread. This definition includes the register content;\
    \ memory read values; and their relationships with other threads, as determined\
    \ by the relaxed concurrency model. Architectural axiomatic concurrency models,\
    \ e.g. [\\[1](#page-12-3)[–3,](#page-12-4) [7](#page-12-5)[–9,](#page-12-6) [13,](#page-12-1)\
    \ [21,](#page-12-7) [30,](#page-12-17) [31,](#page-12-18) [35,](#page-12-13) [37,](#page-12-10)\
    \ [62,](#page-13-5) [64,](#page-13-6) [68\\]](#page-13-7), use candidate executions\
    \ containing the events just from these architecturally executed instances. Note\
    \ that the events comprising architectural FDX instances are abstract: they do\
    \ not represent any individual sequence of microarchitectural operations as one\
    \ would need for side-channel analysis [\\[23,](#page-12-19) [24,](#page-12-20)\
    \ [53\\]](#page-13-11) or to reason about individual microarchitectures [\\[48\\\
    ]](#page-13-12); we give no bound on the extent of non-architecturally-executed\
    \ instances except in that they cover what one needs to capture the architectural\
    \ bound.\n\nThe Arm prose specification in Fig. [2](#page-3-1) (top) previously\
    \ attempted to capture the relationship between implementation execution (out\
    \ of order and speculative) and the architectural definition of allowed behaviour\
    \ in terms of a notion of \"simple sequential execution\". As the prose says,\
    \ simple sequential execution does not hold for the intended relaxed-memory architecture.\
    \ We propose a more correct rephrasing that allows for exceptions and other systems\
    \ phenomena in Fig. [2](#page-3-1) (bottom).\n\nFig. [3](#page-3-2) depicts a\
    \ tree of instances involving exception entry (svc) and return (eret). Arm-A allows\
    \ implementations to observe the exception handling instances as executing before\
    \ program-order previous instances have been retired, and similarly exception\
    \ return. Exception entry and return may never be observed as starting to execute\
    \ speculatively, however, and so the three speculative branches may not observe\
    \ exception entry or return instances. Precision must account for these allowed\
    \ and prohibited relaxed behaviours.\n\n# <span id=\"page-2-0\"></span>3 Relaxed\
    \ behaviour of precise exceptions\n\nExceptions change the control flow and processor\
    \ context, that is, the collection of system and special registers which control\
    \ the execution of the machine, such as the current exception level (PSTATE.EL),\
    \ masking of interrupts (PSTATE.{D,A,I,F}), processor flags, etc. However, changes\
    \ to the context may not take effect immediately, and so, to ensure that program-order-later\
    \ instructions see\n\n<span id=\"page-3-1\"></span>Architecturally executed An\
    \ instruction is architecturally executed only if it would be executed in a simple\
    \ sequential execution of the program. [...]\n\nSimple sequential execution The\
    \ behavior of an implementation that fetches, decodes and completely executes\
    \ each instruction before proceeding to the next instruction. Such an implementation\
    \ performs no speculative accesses to memory, including to instruction memory.\
    \ The implementation does not pipeline any phase of execution. In practice, this\
    \ is the theoretical execution model that the architecture is based on, and Arm\
    \ does not expect this model to correspond to a realistic implementation of the\
    \ architecture.\n\nArchitecturally executed A candidate execution can be architecturally\
    \ executed if it is composed of a sequence of FDX instances for each thread that\
    \ together satisfy the Arm concurrency model [extended to cover exceptions, as\
    \ described here, and other systems phenomena], starting from the machine initial\
    \ state.\n\n1\n\n1\n\n<span id=\"page-3-2\"></span>Figure 2: Arm prose specification\
    \ [\\[10,](#page-12-15) Glossary, p14749] (top) and our suggested rephrasing (bottom).\n\
    \n![](_page_3_Figure_6.jpeg)\n\nFigure 3: The tree of partially and completely\
    \ executed FDX instances with exceptions, in hardware or operational model execution.\
    \ Instructions may execute out-of-order across exception boundaries, requiring\
    \ a modern definition for precision.\n\nsuch changes, exceptions usually come\
    \ with context synchronisation. It is this context synchronisation which imposes\
    \ ordering, and we show how, without such context synchronisation, we observe\
    \ reordering across exception boundaries. For this reason, exceptions are usually\
    \ context-synchronising on Arm.\n\nThere are many things that can trigger exceptions.\
    \ While exceptions like interrupts and page faults are likely the most common,\
    \ they may come with extra synchronisation and/or non-determinism. The simplest\
    \ way to explore the relaxed behaviours is therefore to use 'exception-generating\
    \ instructions', such as system calls (using the Arm SVC instruction), which unconditionally\
    \ generate an exception at a particular program point. These provide a baseline\
    \ for precision, and therefore we use them in our exploration of the behaviour\
    \ of exceptions in the remainder of this section; we return to discuss other exceptions\
    \ later on.\n\nIn this section, we explain relaxed behaviour of precise exceptions\
    \ through litmus tests, the usual standard for succinctly cataloguing the relaxed\
    \ behaviours allowed by an architecture [\\[8,](#page-12-21) [9,](#page-12-6)\
    \ [13\\]](#page-12-1). Litmus tests are small programs capturing specific software\
    \ patterns or hardware mechanisms, whose outcome depends on some kind of out-of-order\
    \ execution. Precise exceptions do not change the memory model between exception\
    \ boundaries, and so the interesting questions concern out-of-order execution\
    \ across exception boundaries.\n\nWe will talk about context synchronisation in\
    \ detail ([§3.1\\)](#page-3-0), explore the baseline out-of-order execution across\
    \ exception boundaries ([§3.2\\)](#page-3-3), then the stronger behaviour of specific\
    \ types of exceptions ([§3.3\\)](#page-5-2), touch on how the instruction semantics\
    \ needs to be adapted ([§3.4\\)](#page-5-3), and finally discuss a corner case\
    \ disabling context synchronisation ([§3.5\\)](#page-5-1).\n\n#### <span id=\"\
    page-3-0\"></span>3.1 Context-synchronisation\n\nUpdates to the context, such\
    \ as writes to system registers, need synchronisation to be guaranteed to have\
    \ an effect. We do not model the behaviour of such context-changing operations\
    \ when such synchronisation is not performed. Instead, we merely identify when\
    \ and how exceptions are context-synchronising, and note that this has a knock-on\
    \ effect on memory accesses.\n\nArchitecturally, a context synchronisation event\
    \ guarantees that no instruction program-order-after the event is observably fetched,\
    \ decoded, or executed until the context-synchronising event has happened. A simple\
    \ microarchitectural implementation for context synchronisation is to flush the\
    \ pipeline: restarting all programorder-later instances once the context-synchronising\
    \ effect occurs. More complex implementations may be more clever, as long as they\
    \ preserve the semantics. Software can explicitly generate context synchronisation\
    \ events by issuing an Instruction Synchronisation Barrier (ISB). Context synchronisation\
    \ can also happen implicitly, for example on exception entry and exit. This is\
    \ the case in Arm, except in a rare use case we return to in [§3.5.](#page-5-1)\n\
    \nThe effect of context synchronisation events in exception boundaries is that\
    \ any instance after the boundary has an ISB-equivalent dependency on the instances\
    \ before the boundary. This mechanism implies the following fundamental invariant:\
    \ context synchronising exceptions are never taken speculatively, and it limits\
    \ speculation to the same well-understood extent as ISB limits speculation. This\
    \ invariant has interesting interactions with external aborts, which we discuss\
    \ in [§4.](#page-5-0)\n\n#### <span id=\"page-3-3\"></span>3.2 Relaxed behaviours\n\
    \nIn this section, we explore the relaxed behaviour of exceptions, with a selection\
    \ of litmus tests from our larger suite of 61 hand-written tests. For each test,\
    \ we include whether the behaviour is allowed in our understanding of the architectural\
    \ intent; and a candidate execution graph. We mark behaviours as allowed/disallowed\
    \ based on discussions with Arm architects.\n\n3.2.1 Out-of-order execution across\
    \ exception boundaries. Exception boundaries do not act as memory barriers, so\
    \ loads and stores may be executed out-of-order over an exception entry or an\
    \ exception exit or the composition of both.\n\nFigure [4](#page-4-0) contains\
    \ an illustrative sample of three such shapes. Each test contains the code listing\
    \ with the pertinent (relaxed) final state and architectural intent, the graph\
    \ of architecturally-executed FDX instances comprising the candidate execution.\n\
    \n<span id=\"page-4-0\"></span>Precise exceptions in relaxed architectures ISCA\
    \ '25, June 21–25, 2025, Tokyo, Japan\n\n1\n\n1\n\n1\n\n1\n\n![](_page_4_Figure_2.jpeg)\n\
    \n1 Figure 4: Reads and writes may be executed out-of-order across exception entry,\
    \ exit, or even both.\n\n<span id=\"page-4-1\"></span>![](_page_4_Figure_4.jpeg)\n\
    \n1 Figure 5: Context synchronising exceptions are not executed speculatively.\n\
    \n3.2.2 Speculative exception entry or return. The invariant 'context synchronising\
    \ exceptions cannot be taken speculatively' imposes the same kind of barrier as\
    \ a ctrlisb dependency would impose between program-order-previous instances and\
    \ the instances in the handler. The control dependency is due to the branching\
    \ to the handling code, and the ISB dependency is due to context synchronisation.\
    \ As a consequence, the two behaviours in Figure [5](#page-4-1) are forbidden.\
    \ On architectures that allow the FEAT\\_ExS extension, they would be allowed\
    \ when the exception entry/exit is not context synchronising, i.e., when the corresponding\
    \ EIS/EOS bit is cleared. This mechanism also explains why we do not observe load-load\
    \ reordering on the Raspberry Pi devices, but we do observe them on the ODROID-N2+\
    \ (exhibited by the test MP+dmb+svc which can be found in the extended version\
    \ [\\[65\\]](#page-13-13)). These machines exhibit the same behaviour as they\
    \ would for the corresponding MP+dmb+isb behaviour from previous work.\n\n3.2.3\
    \ Privilege level. The privilege level (PSTATE.EL) has little to no additional\
    \ effect on the behaviours we present: their allowed/forbidden status remains\
    \ the same whether the privilege goes up/down in entry/exit or remains the same.\
    \ The one exception to this principle is the effect a privilege change has on\
    \ non-faulting translation table walks. A non-faulting translation walk for an\
    \ instance programorder-before a privilege-changing exception entry from ELn may\
    \ be reordered with the entry, but would then also be reordered with every subsequent\
    \ exception boundary until the privilege level returns to ELn. Explaining this\
    \ case in full detail would require substantial details of Arm's virtual memory\
    \ architecture [\\[66\\]](#page-13-9), and we leave it to future work.\n\n3.2.4\
    \ Forwarding writes. It is permitted for writes to be forwarded from a store to\
    \ a read across exception entry and return (SB+dmb+rfisvc-addr in Figure [6\\\
    )](#page-4-2).\n\n<span id=\"page-4-2\"></span>![](_page_4_Figure_9.jpeg)\n\n\
    1\n\n1 1 Figure 6: Forwarding into a non-speculative handler.\n\n<span id=\"page-4-3\"\
    ></span>![](_page_4_Figure_11.jpeg)\n\nFigure 7: System registers and context\
    \ synchronisation\n\n1\n\n3.2.5 Dependency through system registers. Where exceptions\
    \ are taken to and returned to are part of the context, and must be read by exception\
    \ taking and returning, and so they can be involved in register dependency chains.\
    \ Here, we do not characterise the general effect of such dependencies, but focus\
    \ on the effect exceptions have on them. Dependencies on system register accesses\
    \ compose with ordering from context synchronisation events to program-order-later\
    \ instructions. Test MP.EL1+dmb+dataesrsvc in Fig. [7](#page-4-3) demonstrates\
    \ that a write to the system register ESR that depends on a read forbids reordering\
    \ this read across the boundary, even though resolving the dependency does not\
    \ affect the exception.\n\nThe ELR register is a special-purpose register, and\
    \ is therefore 'self-synchronising'. Therefore, writes into the ELR do not need\
    \ context synchronisation to guarantee that they are seen by programorder-later\
    \ instructions, and this means that dependencies into the ELR are preserved (see\
    \ Fig. [7\\)](#page-4-3).\n\nThis has two related subtleties, and is currently\
    \ under investigation by Arm. The Software Thread ID Register (TPIDR) is a system\
    \ register in which the operating system can store thread identifying information,\
    \ but has no relevant indirect effects. Further testing\n\n<span id=\"page-5-4\"\
    ></span>**MOV** X0,#1 **STR** X0,[X1] **DMB** SY **MOV** X2,#1 **STR** X2,[X3]\
    \ Thread 0 **LDR** X0,[X1] **MOV** X5,#0 / segfault **LDR** X4,[X5] Thread 1 **LDR**\
    \ X2,[X3] T1 Handler Initial state: \\*x=0, \\*y=0; 0:X1=x, 0:X3=y; 1:X1=y, 1:X3=x\
    \ MP+dmb.sy+fault AArch64 Forbidden: 1:X0=1, 1:X2=0 a:W x=1 b:W y=1 Thread 0 c:R\
    \ y=1 d:Pagefault e:R x=0 Thread 1 dmb po po rf fr 1 **MOV** X0,#1 **STR** X0,[X1]\
    \ **DMB** SY **MOV** X2,#1 **STR** X2,[X3] Thread 0 **LDR** X0,[X1] L: NOP Thread\
    \ 1 **LDR** X2,[X3] T1 Handler Initial state: \\*x=0, \\*y=0; 0:X1=x, 0:X3=y;\
    \ 1:X1=y, 1:X3=x interrupt at=L MP+dmb.sy+int AArch64 Allowed: 1:X0=1, 1:X2=0\
    \ a:W x=1 b:W y=1 Thread 0 c:R y=1 d:TakeInterrupt e:R x=0 Thread 1 dmb po po\
    \ rf fr\n\nFigure 8: Different exception kinds can have different behaviour.\n\
    \nand discussions may clarify whether it forbids reordering. While dependencies\
    \ through special-purpose registers are preserved, context synchronisation does\
    \ not necessarily need to wait for those writes, and so these dependencies do\
    \ not necessarily pass to instructions after context synchronisation (in contrast\
    \ to system register writes).\n\n3.2.6 Ordering from asynchronous exceptions.\
    \ Asynchronous exceptions cannot be taken speculatively. Therefore, all instructions\
    \ program-order-after an asynchronous exception happen after that exception.\n\
    \n# <span id=\"page-5-2\"></span>3.3 Exception-specific mechanisms\n\nNot all\
    \ exception kinds are equal. For example, when an implementation supports the\
    \ Enhanced Translation Synchronisation feature (FEAT\\_ETS2), the translation-table-walks\
    \ which generate translation faults (pagefaults) gain additional ordering from\
    \ program-orderprevious instances. Figure [8](#page-5-4) compares the MP test\
    \ involving a page-fault (MP+dmb.sy+fault, forbidden under ETS) and the same shape\
    \ involving an asynchronous interrupt (MP+dmb.sy+int, allowed). As such, combining\
    \ the exceptions model here with any of the existing models (for virtual memory,\
    \ cache maintenance, memory tagging, transactional memory, etc.) would require\
    \ clarification around the domain-specific exceptions associated with those features.\
    \ We are aware that the specification of additional mechanisms per exception-kind\
    \ is an active area of interest for Arm.\n\n#### <span id=\"page-5-3\"></span>3.4\
    \ Intra-instruction exceptions\n\nWherever possible, we want to interpret the\
    \ intra-instruction ASL ordering as preserved, both for conceptual simplicity,\
    \ memorymodel tool execution, and reasoning. This has previously been possible\
    \ except in a few specific cases that are inherently concurrent, e.g. instructions\
    \ that do multiple accesses. Exceptions introduce a new interesting case for instructions\
    \ that do a register writeback concurrently with a memory access. For example,\
    \ STR (immediate) has a \"Post-index\" and a \"Pre-index\" versions [\\[10,](#page-12-15)\
    \ C6.2.365, p2442]. The post-index STR Xt, [Xn], #8, for example, stores the value\
    \ in Xt to the address initially in register Xn and adds 8 to Xn. The Arm ARM\
    \ ASL for STR puts that register write at the end, after the memory access has\
    \ completed. The architectural intent is that program-order-later instances that\
    \ depend on Xn can go ahead early, e.g. before the data in register Xt is available\
    \ to be written to memory, and this has been observed in practice [\\[36\\]](#page-12-22).\n\
    \nISCA '25, June 21–25, 2025, Tokyo, Japan Ben Simner, Alasdair Armstrong, Thomas\
    \ Bauereiss, Brian Campbell, Ohad Kammar, Jean Pichon-Pharabod, and Peter Sewell\n\
    \n<span id=\"page-5-5\"></span>\n\n| Name                        |        |  \
    \                      |        | m6g m7g m8g odroid                     | m2\
    \      | pi3    | pi4                                  | pi5               |\n\
    |-----------------------------|--------|------------------------|--------|----------------------------------------|---------|--------|--------------------------------------|-------------------|\n\
    | MP+dmb+ctrl-svc             | 0⁄16M  | 0⁄24M                  | 0⁄12M  | 0⁄329M\
    \                                 | 0⁄360M  | 0⁄10M  | 0⁄230M                \
    \               | 0⁄136M            |\n| MP+dmb+ctrlelr              | 0⁄16M \
    \ | 0⁄24M                  | 0⁄12M  | 0⁄329M                                 |\
    \ 0⁄360M  | 0⁄30M  | 0⁄318M                               | 0⁄130M           \
    \ |\n| MP+svc-eret+addr            | U0⁄16M | U0⁄24M                 |       \
    \ | U0⁄12M 149K⁄328M                       | U0⁄360M | 376⁄9M | U0⁄228M      \
    \                        | 12⁄136M           |\n| MP.EL1+dmb+dataesrsvc 0⁄16M\
    \ |        | 0⁄24M                  | 0⁄12M  | 0⁄16M                         \
    \         | 0⁄0     | 0⁄4M   | 0⁄14M                                | 0⁄27M  \
    \           |\n| S+dmb+svc                   | U0⁄16M | U0⁄24M               \
    \  | U0⁄12M | U0⁄328M                                | U0⁄360M | U0⁄41M | U0⁄222M\
    \                              | U0⁄101M           |\n| SB+dmb+eret          \
    \       |        | 60⁄16M 120⁄24M 213⁄12M |        |                         \
    \               |         |        | 262⁄328M 12K⁄360M 203K⁄41M 946K⁄222M | 4K⁄100M\
    \           |\n| SB+dmb+rfisvc-addr          |        |                      \
    \  |        | 4⁄16M 235⁄24M 1K⁄12M 305K⁄328M 12⁄360M |         | 1M⁄30M |    \
    \                                  | 7K⁄316M 197K⁄128M |\n| MP+dmb+fault     \
    \           | 0⁄16M  | 0⁄24M                  | 0⁄12M  | 0⁄74M               \
    \                   | 0⁄0     | 0⁄2M   | 0⁄46M                               \
    \ | 0⁄80M             |\n\n#### Figure 9: Experimental results.\n\n1 Previous\
    \ work captured this allowed by having the register writeback before the memory\
    \ access in the instruction semantics. However, exceptions require more care:\
    \ when the memory access generates an exception, the writeback register should\
    \ appear unchanged to instances after the exception boundary.\n\n# <span id=\"\
    page-5-1\"></span>3.5 Disabling context synchronisation\n\nOn Arm, the optional\
    \ FEAT\\_ExS feature provides two new fields in the system control register to\
    \ disable context synchronisation on exception entry or return, respectively:\
    \ EIS and EOS. While the semantics is clear for these systems, the programming\
    \ model is unpredictable and hard to program correctly, and so this configuration\
    \ is rarely encountered in practice.\n\n# 3.6 Hardware results\n\nWe extend the\
    \ testing harness of Simner et al. [\\[66\\]](#page-13-9) to collect preliminary\
    \ results from hardware, on the following implementations: AWS M6/7/8G instances\
    \ (with Neoverse N1/V1/V2), an ODROID N2+ (on the big Arm Cortex-A73 cores), an\
    \ Apple M2, and Raspberry Pi 3B+/4B/5 (with Arm Cortex-A53/A72/A76). Results can\
    \ be found in Fig. [9,](#page-5-5) given as observations over the total number\
    \ of runs. Results marked with U are allowed, but not observed on that device.\
    \ For the complete hardware results, see the extended version [\\[65\\]](#page-13-13).\n\
    \n# <span id=\"page-5-0\"></span>4 Synchronous external aborts\n\nThe memory system\
    \ may detect errors such as data corruption independently of the MMU or Debug\
    \ hardware, e.g., using parity bits or error correcting code. In those cases,\
    \ it will signal the error by a class of exceptions called external aborts. The\
    \ architecture does not define at which granularity implementations may report\
    \ such aborts synchronously, which we refer to as synchronous external aborts\
    \ (SEAs). Instances program-order-after a potential cause for synchronous external\
    \ aborts are considered speculative until this external abort can be ruled out,\
    \ resulting in stronger behaviour ([§4.1\\)](#page-6-1). In an implementation\
    \ that always reports external aborts asynchronously, the later instances become\
    \ non-speculative earlier, allowing them to exhibit weaker behaviours. When external\
    \ aborts are reported asynchronously, the simplest recovery is to wind down the\
    \ aborting process. To allow programmers more reliable recovery, implementations\
    \ can support the Reliability, Availability, and Serviceability (RAS) extension.\
    \ This extension is a substantial component of the architecture, far beyond the\
    \ scope of this work. Here, we are merely taking the first steps, describing a\
    \ baseline of behaviours in a very constrained setting, that further work may\
    \ be able to extend to account for the RAS.\n\nWhether any external abort could\
    \ be reported synchronously is implementation-defined, with no architected way\
    \ of identifying the choice. Nevertheless, the choice impacts the permissible\
    \ relaxed behaviours.\n\n# <span id=\"page-6-1\"></span>4.1 Behaviour resulting\
    \ from synchronous external aborts\n\nThere is an asymmetry between reads and\
    \ writes with respect to speculation: writes cannot be propagated speculatively,\
    \ whereas reads can be satisfied speculatively. We will therefore consider the\
    \ store and load cases separately.\n\nIf a store may generate an SEA, then program-order-later\
    \ instances are speculative until the store has (at least) propagated to memory.\
    \ In that case, write-write re-ordering (MP+po+addr) is forbidden. Reads program-order-after\
    \ writes are permitted to execute speculatively anyway, and so the presence of\
    \ these SEAs does not restrict their ability to execute early.\n\nMore interestingly,\
    \ if a load may generate an SEA, then programorder-later instances are speculative\
    \ until the load has completed all its reads, and is non-restartable. This forbids\
    \ interesting tests which would otherwise be allowed, namely context-synchronisation\
    \ after reads (e.g. MP+dmb.sy+isb), which must wait for that control flow to be\
    \ resolved [\\[63\\]](#page-13-14); and writes program-order-after reads (e.g.\
    \ LB+pos), since writes must not be propagated speculatively [\\[63\\]](#page-13-14).\n\
    \n# 4.2 Load buffering and the out-of-thin-air problem\n\nThis has an important\
    \ and hitherto not well-understood impact on programming-language concurrency\
    \ models. Ruling out LB enables substantially simpler design of programming language\
    \ concurrency models: they can execute instructions in-order and merely keep a\
    \ history of the writes seen so far, e.g. [\\[46\\]](#page-13-15), and thereby\
    \ avoid the notorious out-of-thin-air problem [\\[15\\]](#page-12-23). These simpler\
    \ semantics support a line of model checkers for C/C++ and LLVM [\\[42–](#page-12-24)[44\\\
    ]](#page-13-16). In contrast, the presence of LB seems to require significant\
    \ sophistication [\\[3,](#page-12-4) [15,](#page-12-23) [16,](#page-12-25) [19,](#page-12-26)\
    \ [38,](#page-12-27) [39,](#page-12-28) [55,](#page-13-17) [56\\]](#page-13-18).\n\
    \n# <span id=\"page-6-0\"></span>5 An axiomatic model of exceptions\n\nWe now\
    \ give a formal semantics that describes the concurrent behaviour of precise exceptions\
    \ on Arm-A. We give it as an extension of the previous model of Pulte et al. [\\\
    [58\\]](#page-13-3), a predecessor of the current Arm model [\\[25\\]](#page-12-29),\
    \ in the standard 'cat' format [\\[9,](#page-12-6) [13\\]](#page-12-1), in Figure\
    \ [10.](#page-6-2)\n\nWhile the model captures the architectural intent as we\
    \ understand it, the architecture remains the sole responsibility of Arm; the\
    \ intent may change over time and the model presented here is not officially endorsed\
    \ by Arm.\n\nThe model is parameterised along two axes:\n\n- FEAT\\_ExS corresponds\
    \ to the feature of the same name being implemented; we do not support runtime\
    \ changes of the related SCTLR\\_ELx.{EIS,EOS} fields, and so fix them as variants.\n\
    - SEA\\_R and SEA\\_W correspond to the ImplementationDefined choice of whether\
    \ loads or stores may generate synchronous external aborts.\n\n```\nPrecise exceptions\
    \ in relaxed architectures ISCA '25, June 21–25, 2025, Tokyo, Japan\n```\n(\\\
    * barrier-ordered-before\n\n\\*)\n\n```\n\"Arm-A exceptions\"\n```\ninclude \"\
    cos.cat\" include \"arm-common.cat\"\n\n```\n(* might-be speculatively\n     executed\
    \ *)\n```\n**let** speculative =\n\n```\nctrl\n| addr; po\n```\n\n```\n| if \"\
    SEA_R\" then [R]; po\n    else 0\n| if \"SEA_W\" then [W]; po\n```\nelse 0\n\n\
    ```\n(* context-sync-events *)\n```\n\n```\nlet CSE =\n```\n\n```\nISB\n| if \"\
    FEAT_ExS\" & ∼\"EIS\"\n   then 0 else TE\n| if \"FEAT_ExS\" & ∼\"EOS\"\n```\n\
    then 0 else ERET\n\n```\nlet ASYNC =\n  TakeInterrupt\n```\n\n```\n(* observed\
    \ by *)\n```\n**let** obs = rfe | fr | co\n\n#### (\\* dependency-orderedbefore\
    \ \\*)\n\n- **let** dob = addr | data\n\t- | speculative ; [W]\n\t- | speculative\
    \ ; [ISB]\n\t- | (addr | data); rfi\n\n### (\\* atomic-ordered-before \\*)\n\n\
    ```\nlet aob =\n   rmw\n | [range(rmw)]; rfi; [A|Q]\n```\n# **let** bob = [R]\
    \ ; po ; [dmbld] | [W] ; po ; [dmbst] | [dmbst]; po; [W] | [dmbld]; po; [R|W]\
    \ | [L]; po; [A] | [A | Q]; po; [R | W] | [R | W]; po; [L] | [dsb]; po (\\* contextually-orderedbefore\
    \ \\*) **let** ctxob = speculative; [MSR|CSE] | [MSR]; po; [CSE] | [CSE]; po (\\\
    * async-ordered-before \\*) **let** asyncob = speculative; [ASYNC] | [ASYNC];\
    \ po (\\* Ordered-before \\*) **let** ob = (obs | dob | aob | bob | ctxob | asyncob)+\
    \ (\\* Internal visibility requirement \\*) **acyclic** po-loc | fr | co | rf\
    \ **as** internal (\\* External visibility requirement \\*) **irreflexive** ob\
    \ **as** external\n\n(\\* Atomic: Basic LDXR/STXR constraint to forbid intervening\
    \ writes. \\*)\n\n**empty** rmw & (fre; coe) **as** atomic\n\n#### Figure 10:\
    \ Arm-A exceptional model (greyed out parts are unchanged from the original model).\n\
    \nMost current hardware does not support FEAT\\_ExS, and moreover, we expect that\
    \ most software would not use it. However, its semantics is relatively straight-forward\
    \ as we understand it, and so we include it in our model, although without the\
    \ hardware validation we have for the non-ExS fragment.\n\nWe add new events to\
    \ the candidate execution: TE (take exception) and ERET, which correspond to the\
    \ synchronisation points\n\n(whether they are synchronising) of taking or returning\
    \ from an exception; and MRS and MSR events, for reading and writing system registers,\
    \ corresponding to the Arm MRS and MSR instructions which change the context.\n\
    \nExceptions and program order. We include all the new events in program-order.\
    \ This includes the events from instructions directly before and after taking\
    \ or returning from an exception.\n\nInterrupts. While this cat model does not\
    \ support inter-processor interrupts and the generic interrupt controller (see\
    \ [§7](#page-8-0) for a draft extension to support them), it does support other\
    \ precise asynchronous exceptions (e.g. timers).\n\nOrdered-before. We expand\
    \ ordered-before:\n\n- Wherever ctrl|(addr;po) was used before, we also include\
    \ instructions program-order-after reads or writes when in the relevant SEA variant.\
    \ With those variants, the instructions program-order-after those events are speculative\
    \ up until the memory access has completed.\n- The previous model's use of ISB\
    \ was purely for its context synchronisation effect. Accordingly, wherever [ISB]\
    \ was used before, we include exception entry (TE) and exit (ERET), unless we\
    \ are in the variant where context synchronisation on those events is disabled.\n\
    - We extend barrier-ordered-before with the DSB barriers. The barrier event classes\
    \ are upwards-closed, so that DSB.SY is included in all the dmb events.\n- We\
    \ add a context-ordered-before (ctxob) sub-clause to the ordered-before relation,\
    \ which captures the ordering of contextchanging operations and context-synchronisation:\
    \ namely, that context-changes and context-synchronisation cannot happen speculatively;\
    \ that all context-changes are ordered before any context-synchronisation; and\
    \ that no instruction program-order-after context-synchronisation can be executed\
    \ until the synchronisation is complete.\n- We add an async-ordered-before (asyncob)\
    \ clause to orderedbefore, capturing that asynchronous events (such as interrupts)\
    \ cannot be done speculatively, and instructions programorder-after them may not\
    \ happen before the asynchronous event which precipitated them.\n\n#### <span\
    \ id=\"page-7-1\"></span>5.1 Executable-as-a-test-oracle implementation\n\nWe\
    \ implement the model in Isla [\\[13\\]](#page-12-1), an SMT-based executable\
    \ oracle for axiomatic concurrency models (and ISA semantics). Isla takes as input\
    \ a memory model in herdtools-like cat format, and a litmus tests. To support\
    \ tests with asynchronous exceptions, we added a construct to specify a label\
    \ where the exception will occur, so that Isla then pends an interrupt at that\
    \ program point.\n\nThe instruction semantics we use is a translation into the\
    \ Sail language of the Armv9.4-A ASL specification, including the toplevel function\
    \ provided by Arm [\\[17\\]](#page-12-30). The translation process [\\[12\\]](#page-12-31)\
    \ is mostly automatic, requiring select manual interventions mostly due to differences\
    \ in the type systems of ASL and Sail. We also added patches to support the integration\
    \ with Isla, in particular adding hooks to expose information about exceptions\
    \ being taken in a form that can be readily consumed by Isla. In doing so, we\
    \ encountered and fixed some bugs in the ASL model related to uses\n\nof uninitialised\
    \ fields in data structures, as well as missing checks for implemented processor\
    \ features that led to spurious system register accesses.\n\nFor all the (non-IPI)\
    \ tests, Isla, the architectural intent as we understand it, and the results of\
    \ hardware testing from [§3.2](#page-3-3) are consistent.\n\n# <span id=\"page-7-0\"\
    ></span>6 Challenges in defining precision\n\nThe phenomena we describe in [§3](#page-2-0)\
    \ highlight that the historical, naive definition of precision does not account\
    \ for relaxed memory. The open problem is then how to adequately define precision\
    \ in a relaxed-memory setting. This challenge is hinted at in the way the Arm\
    \ reference manual [\\[10,](#page-12-15) D1.3.1.4, p6060] defines precision as:\n\
    \nAn exception is precise if on taking the exception, the hardware thread (aka\
    \ processing element, PE) state and the memory system state is consistent with\
    \ the PE having executed all of the instructions up to but not including the point\
    \ in the instruction stream where the exception was taken from, and none afterwards.\
    \ [except that in certain specific cases some registers and memory values may\
    \ be UNKNOWN]\n\n1 This definition explicitly allows various side effects of an\
    \ instruction executing when an exception is taken to be visible. The details\
    \ are intricate, but in outline: registers that would be written by the instruction\
    \ but which are not used by it (to compute memory access addresses) can become\
    \ UNKNOWN, and for instructions that involve multiple single-copy-atomic memory\
    \ writes (e.g. misaligned writes and store-pair instructions), where each write\
    \ might generate an exception (e.g. a translation fault), the memory locations\
    \ of the writes that do not generate exceptions become UNKNOWN. These side effects\
    \ could be observed by the exception handler, and the memory write side effects\
    \ could be observed by other threads doing racy reads. Hardware updates to page-table\
    \ access flags and dirty bits, and to performance counters, could also be observable.\
    \ This means that the abstraction of a stream of instructions executed up to a\
    \ given point does not account for the relaxed-memory behaviour.\n\nArm classify\
    \ particular kinds of exceptions as precise or not, but all the above makes it\
    \ hard to define in general what it means for an exception to be precise in a\
    \ relaxed setting.\n\nThe ultimate architectural intent of precision is that it\
    \ is sufficient to meaningfully resume execution after the exception. For example,\
    \ for software that does mapping on demand, when an instruction causes a fault\
    \ by accessing an address which is not currently mapped, the exception handler\
    \ will map that address and return. This means that re-executing the original\
    \ instruction will overwrite these UNKNOWNs, and will have ordering properties\
    \ much like the original instruction would have had if the mapping had already\
    \ been in place.\n\nOur models are complete enough to reason about such cases\
    \ in concrete examples. However, a general definition of precision, and the accompanying\
    \ reasoning principle, would have to capture assumptions about the exception handler\
    \ and its concurrent context to ensure that they do not observe the above side\
    \ effects. More straightforwardly, the above definition of what becomes UN-KNOWN\
    \ would have to be codified, as that is not currently in the ASL architectural\
    \ pseudocode. Without a clear definition of precision architectures must independently\
    \ enumerate the possible relaxations across exception boundaries (as we do in\
    \ [§3](#page-2-0) for Arm).\n\nExceptions may also be imprecise, in which case\
    \ the behaviour is very loosely constrained. The current architectural intent\
    \ does not give well-defined guarantees in the presence of imprecise exceptions,\
    \ and models that account for imprecision likely need to expose more of the microarchitectural\
    \ state than we capture here [\\[33\\]](#page-12-32). All exceptions in Arm are\
    \ precise except for those external memory errors which are not reported synchronously\
    \ ([§4\\)](#page-5-0), which we do not cover.\n\n# <span id=\"page-8-0\"></span>7\
    \ Software-generated interrupts\n\nInter-processor interrupts (IPIs), known as\
    \ software-generated interrupts (SGIs) on Arm, are an important synchronisation\
    \ mechanism available to software. They are used throughout systems software to\
    \ signal other threads, including within the Linux kernel (in its RCU synchronisation\
    \ mechanism), in software (via Linux's sys\\_membarrier), e.g. in JITs [\\[67\\\
    ]](#page-13-8), and in programming language runtimes (e.g. in Microsoft'sVerona\
    \ [\\[14,](#page-12-2) [20\\]](#page-12-33)). Such use of SGIs critically depends\
    \ on a detailed understanding of the interaction of exceptions with relaxed-memory\
    \ behaviour.\n\nTo manage the sending, routing, prioritisation, and delivery of\
    \ interrupts, Arm define an optional generic interrupt controller (GIC). The GIC\
    \ provides a uniform API for sending and routing interrupts from peripherals to\
    \ threads, and comes in several versions. We focus on GICv3 and its CPU interface,\
    \ but expect the behaviour we describe should apply to GICv4.\n\nThere are many\
    \ interesting questions about SGIs. We cover just a simple baseline: enough to\
    \ reason about the synchronisation used by software, but ignoring much of the\
    \ complexity of the GIC. We fix a relatively simple configuration, and focus on\
    \ the relaxed-memory aspects of the interaction between SGIs and the rest of the\
    \ memory and processor state.\n\n# 7.1 The Generic Interrupt Controller – basic\
    \ machinery\n\nWe begin by introducing the context of the basic Arm GIC machinery,\
    \ before addressing its relaxed ordering in later subsections. An interrupt is\
    \ generated on its source (a hardware thread or some peripheral) for a particular\
    \ event (e.g. an SGI). This interrupt is then sent to the interrupt controller,\
    \ which is split into a distributor, the global machinery in charge of routing\
    \ interrupts to cores, and the per-thread redistributors, each of which maintains\
    \ a thread-local state for each interrupt (which we describe in more detail later).\
    \ Interrupts are identified in the GIC by its 'interrupt ID number' (IN-TID).\
    \ Each instance of an interrupt sent to the interrupt controller is associated\
    \ with an INTID, either by software or a peripheral, and is provided to the receiving\
    \ core in a register it can read (via acknowledgement, described later).\n\nEach\
    \ hardware thread (PE) has an interrupt status register (the ISR), which has a\
    \ single pending status bit for each interrupt class (IRQ, FIQ, SError, etc).\
    \ For each fetch-decode-execute cycle of the top-level loop (see [§2.3.1\\)](#page-2-2),\
    \ the processor checks these status bits to determine whether an interrupt is\
    \ pending; if an interrupt is pending and is not masked on that PE, the PE takes\
    \ that interrupt. It is the interrupt controller's responsibility to set and clear\
    \ the pending bit in that register, notifying the thread of a pending interrupt.\
    \ To determine when to deliver (set the bit in the interrupt status register)\
    \ interrupts to the core, the redistributor maintains three key pieces of state\
    \ (this is for an 'edge-triggered' interrupt, such as for SGIs; we do not discuss\
    \ 'level-sensitive' interrupts):\n\n- A priority to assign to each interrupt source,\
    \ and the current 'working' priority of the interrupt(s) being handled.\n- A priority\
    \ mask, which prevents interrupts with too low a priority from being delivered\
    \ to the core.\n- A per-INTID state, which is one of:\n\t- Inactive: there is\
    \ no current interrupt;\n\t- Pending: the GIC has received an interrupt, and maybe\
    \ delivered it, but the core has not begun handling it; or\n\t- Active: the core\
    \ has signalled it is handling the interrupt, but not yet signalled it is done.\n\
    \nLifecycle of an interrupt. Interrupts start out Inactive. When an interrupt\
    \ is asserted by the source, the GIC sets the state for this interrupt's INTID\
    \ to Pending. Within some unspecified, finite amount of time, the GIC will set\
    \ the pending bit in the interrupt status register for the core, enabling the\
    \ core to take an exception on the next fetch-decode-execute loop.\n\nThe core\
    \ should then acknowledge the interrupt, by reading the appropriate interrupt-acknowledge-register\
    \ (IAR); this returns the INTID for use by the core, and sends a request to the\
    \ redistributor to mark the INTID as Active. Transitioning to the active state\
    \ sets the working priority to the priority of that INTID's source, preventing\
    \ lower-priority interrupts from pre-empting the core, and clears the pending\
    \ bit in the interrupt-status-register on the core. If another interrupt with\
    \ the same INTID is asserted while the interrupt is active, that instance will\
    \ be buffered (only a single extra instance may be buffered) and taken later,\
    \ and the INTID is said to be 'Active and Pending'. While the interrupt is active,\
    \ it will not be re-delivered to the core, so even if the interrupt service routine\
    \ performs an ERET, it will not re-take the exception.\n\nAt some later time,\
    \ the core may finish handling the interrupt and be ready to receive further instances\
    \ of that INTID. There are two ways to do this, depending on whether one wants\
    \ to separate priority drop from deactivation, which is controlled by the EOImode.\
    \ With EOImode=0, by writing the INTID to the end-of-interrupt register (EOIR),\
    \ the interrupt is deactivated simultaneously with the the priority drop. With\
    \ EOImode=1, writes to the EOIR only perform priority drop, requiring separate\
    \ deactivation through a write to the deactivate-interrupt-register (DIR). Additionally,\
    \ the GIC interface provides registers which can manually set the current priority,\
    \ or mask, or explicitly set the state of an interrupt. Figure [11](#page-9-0)\
    \ shows the typical transitions between states.\n\nIntended software usage. Typically,\
    \ software use of interrupts falls into one of two categories:\n\n• Nested interrupt\
    \ servicing, where software readily uses priorities and handles the interrupt\
    \ directly in the interrupt service routine, as it typical in real-time OSs.\n\
    \nISCA '25, June 21–25, 2025, Tokyo, Japan Ben Simner, Alasdair Armstrong, Thomas\
    \ Bauereiss, Brian Campbell, Ohad Kammar, Jean Pichon-Pharabod, and Peter Sewell\n\
    \n<span id=\"page-9-0\"></span>![](_page_9_Figure_2.jpeg)\n\nFigure 11: GIC automaton,\
    \ for each PE and each INTID, based on Figure 4-3 \"Interrupt handling state machine\"\
    \ from Arm [\\[11,](#page-12-14) §4.1.2], specialised to edge-triggered behaviour.\n\
    \n• Deferred interrupt handling, where software acknowledges the interrupt directly,\
    \ but handles it later.\n\n1\n\nLinux falls into the second category, utilising\
    \ only a single interrupt priority. This 'split' approach to handling interrupts,\
    \ where the interrupt service routine merely acknowledges, and the actual handling\
    \ of the interrupt comes later, leads Linux to adopt EOImode=1. When the interrupt\
    \ is taken by the core, it is acknowledged, the INTID is checked against special\
    \ cases, priority is quickly dropped, and interrupts are unmasked. The actual\
    \ interrupt may then be handled, concurrently with new interrupts being signalled\
    \ to the core, although duplicates of the incident INTID will still be masked\
    \ as it is not yet deactivated. Eventually, the core completes the work for that\
    \ interrupt and then deactivates it, advancing the state machine.\n\n#### 7.2\
    \ Ordering of the propagation of SGIs\n\nAn SGI is generated by a write to the\
    \ appropriate register (e.g. ICC\\_SGI1R\\_EL1), and is received on one or several\
    \ thread(s). This gives rise to questions of three kinds:\n\n- What is required\
    \ to order the generation of the SGI after earlier accesses?\n- Does routing of\
    \ the SGI imply ordering? e.g. is the interrupt controller an observer wrt. multi-copy-atomicity?\n\
    - What is required to ensure that the sequence of acknowledgement and deactivation\
    \ happens correctly?\n\nThere are few guarantees about the order of propagation\
    \ of SGIs, or interrupts generally. Interrupts may be delivered to the core at\
    \ any time, and multiple pending interrupts may be delivered in any order (priorities\
    \ allowing). There are no guarantees analogous to the coherence or atomicity of\
    \ memory, and generated interrupts may be re-ordered, or delivered to different\
    \ cores in different orders. However, as discussed earlier, interrupts may not\
    \ be speculated, and so the interrupt cannot be delivered to the target PE before\
    \ it is generated.\n\nSGI litmus testing. We extract the fundamental Message-Pass-via-SGI\
    \ shape underlying Linux's implementation of RCU on Armv8 as a litmus test, MPviaSGIEIOmode1sequence,\
    \ in Figure [12.](#page-9-1) Passing a message through an SGI requires some synchronisation\
    \ between the write of the data and the generation of the SGI (here a DSB ST on\
    \ Thread 0), and requires observation of the data in the exception\n\n<span id=\"\
    page-9-1\"></span>\n\n| MPviaSGIEIOmode1sequence                             \
    \                                                                     | AArch64\
    \  |                                 |                                       \
    \                                                                            \
    \                                            |  |\n|---------------------------------------------------------------------------------------------------------------------------|----------|---------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------|--|\n\
    | Initial state: *x=0;<br>0:PSTATE.EL=1;                                     \
    \                                               |          |                 \
    \                | 1:EOIMode=1                                               \
    \                                                                            \
    \                        |  |\n| 0:X1=x;                                     \
    \                                                                            \
    \  |          |                                 | 1:X0=0, 1:X1=0, 1:X2=x     \
    \                                                                            \
    \                                                       |  |\n| Thread 0     \
    \                                                                            \
    \                                 | Thread 1 |                               \
    \  | T1 Handler                                                              \
    \                                                                            \
    \          |  |\n| MOV X0,#1<br>STR X0,[X1] // write data<br>DSB ST<br>MOV X2,\
    \ #1, LSL #40<br>//generate SGI<br>MSR ICC_SGI1R_EL1, X2<br>ISB | NOP      | DSB\
    \ SY<br>ISB<br>DSB SY<br>ERET | MRS X3, IAR // ack interrupt<br>AND X3, X3, #0xFFFFFF<br>MSR\
    \ EOIR, X3 // drop priority<br>MOV X0, #1<br>LDR X1,[X2] // read data<br>MSR DIR,\
    \ X3 // deactivate |  |\n| Forbidden: 1:X0=1, 1:X1=0                         \
    \                                                                        |   \
    \       |                                 | 1                                \
    \                                                                            \
    \                                                 |  |\n\n<span id=\"page-9-2\"\
    ></span>Figure 12: MPviaSGIEIOmode1sequence: Synchronisationvia-SGI with the full\
    \ acknowledge-drop-deactivate sequence appropriate for **EOImode=1**.\n\n![](_page_9_Figure_15.jpeg)\n\
    \n1\n\n1\n\nFigure 13: **MPviaSGI**: message passing via SGI, illustrating two\
    \ potential phenomena: (1) On the writer side: a po-earlier write gets reordered\
    \ with a po-later GenerateInterrupt. (2) On the reader side: a po-earlier TakeInterrupt\
    \ gets reordered with a po-later read (from the interrupt handler).\n\nhandler;\
    \ the SGI also needs to be is properly acknowledged and deactivated, with the\
    \ appropriate barriers.\n\nThis test is composed of two interacting parts: the\
    \ part that imposes the ordering between the write and the read of the data, and\
    \ the part that interacts with the GIC to manage the interrupt. Figure [13](#page-9-2)\
    \ asks the most basic question of this shape: if we try pass a message via an\
    \ SGI, without any further synchronisation, can we still read an old value? The\
    \ answer is yes, because the generation and subsequent delivery of the SGI could\
    \ happen before the propagation of the store. On the other hand, the extensive\
    \ synchronisation on the receiving thread imposed by GIC management is accidental\
    \ for the read, which is already strongly ordered after the taking of the exception.\n\
    \n#### 7.3 Software usage of SGIs\n\nSynchronisation mechanisms like those discussed\
    \ above rely on this link between memory accesses and interrupts to achieve lowoverhead\
    \ synchronisation. More specifically, they push the cost\n\naway from normal memory\
    \ accesses and onto a \"system-wide memory barrier\" implemented using interrupts.\
    \ This is a fork-join barrier, not a fence. Interestingly, RCU and the Verona\
    \ asymmetric lock rely on two different aspects of this system-wide memory barrier:\
    \ RCU relies on masking of interrupts to implement cheap read critical sections,\
    \ whereas the Verona asymmetric lock relies on precision of interrupts ([§6\\\
    )](#page-7-0).\n\nSystem-wide memory barrier. This system-wide memory barrier\
    \ is a two-way barrier: the issuing PE notifies all other PEs, and waits for a\
    \ reply from all of them. The notification is implemented using interrupts, relying\
    \ on the ordering described above, which is guaranteed by Arm-A. In Kernel RCU\
    \ (where this barrier forms the core of synchronize\\_rcu, exposed to userland\
    \ as the sys\\_membarrier syscall), the wait for a reply is implemented using\
    \ memory operations, namely a lock-protected counter that threads increment to\
    \ acknowledge receipt of the interrupt. We simplify this (to a write to a flag)\
    \ in our litmus tests to reduce complexity.\n\nRCU. The key concept of RCU is\
    \ that of a grace period [\\[51\\]](#page-13-19)[\\[50,](#page-13-20) §9], as\
    \ captured by Alglave et al. [\\[6\\]](#page-12-34) in the RCU-MP litmus test\
    \ (Figure [14\\)](#page-10-0).\n\nWe focus on the use of interrupts in Kernel\
    \ RCU. For performance, RCU also relies on address dependencies to implement cheap\
    \ ordering in read sections, but that is already explained in the 'user' model\
    \ of Arm-A [\\[28,](#page-12-8) [58\\]](#page-13-3) by MP+dmbst+addr.\n\nAt the\
    \ level of Arm assembly, the synchronize\\_rcu systemwide memory barrier is decomposed\
    \ into a DSB ST followed by an MSR to SGI1R, and a wait for the acknowledgement\
    \ (in our cutdown tests, a read acquire of the ack flag); entering the read critical\
    \ section via rcu\\_read\\_lock and leaving it via rcu\\_read\\_unlock decompose\
    \ to writes to the DAIF (pseudo)register that mask and unmask interrupts.\n\n\
    The crux of this litmus test is that interrupts are masked between the two reads,\
    \ and that the handler is therefore either before both reads, or after both reads,\
    \ but not in between (as in, no event of the handler is in between the two reads\
    \ in program order). At the Linux C level, this masking ensures that the interrupt\
    \ generated by the synchronize\\_rcu system-wide memory barrier is taken either\
    \ before or after the read section, but not during, providing the basis for mutual\
    \ exclusion. In the litmus tests, this is captured by the fact that if the read\
    \ of the flag y sees the flag, the read of the data x sees the new data.\n\nVerona\
    \ asymmetric lock. We capture the key scenario of the asymmetric lock of Verona\
    \ [\\[54\\]](#page-13-21) (and of 'biased locking' and 'asymmetric Dekker synchronisation'\
    \ [\\[18,](#page-12-35) [22,](#page-12-36) [26,](#page-12-37) [27,](#page-12-38)\
    \ [40,](#page-12-39) [41,](#page-12-40) [49,](#page-13-22) [59\\]](#page-13-23)\
    \ as used in the JVM). It occurs when an 'internal acquire' from the (unique)\
    \ owner thread contends with an 'external acquire' from another thread. The internal\
    \ acquire is meant to be cheap, and only involves writing to an 'external' flag\
    \ to express interest, and then, in program order, reading from an 'internal'\
    \ flag to ensure that other threads have not expressed interest (falling onto\
    \ the slow path if they have). Crucially, in C++, there is a Barrier::compiler()\
    \ that prevents reordering of two instructions by the compiler, but does not appear\
    \ in the generated assembly. The external acquire does the symmetric thing, writing\
    \ on the 'internal' flag to express interest, and then\n\n<span id=\"page-10-0\"\
    ></span>![](_page_10_Figure_9.jpeg)\n\n1 Figure 14: **RCU-MP**: the key test of\
    \ RCU: are two writes separated by the generation of an SGI ordered with respect\
    \ to a read critical section implemented via interrupts masking? With a **DSB\
    \ ST** between **a** and **b**, this is forbidden.\n\nreading from the 'external'\
    \ flag to ensure that the owner has not expressed interest. To order this, it\
    \ uses a Barrier::memory(), which involves a FlushProcessWriteBuffers(), which\
    \ on Linux is implemented using a sys\\_membarrier, which essentially boils down\
    \ to a synchronize\\_rcu.\n\nThe key guarantee that is relied in the 'cheap' thread\
    \ is that the interrupt must be taken precisely, and that it is therefore taken,\
    \ in program order, either entirely before the read of the internal flag, entirely\
    \ between the read of the internal flag and the write to the external flag, or\
    \ entirely after the write to the external flag. In all three cases, the system-wide\
    \ memory barrier ensures that at least one of the two threads must see that the\
    \ other thread has expressed interest (must read the recent write), and therefore\
    \ backs off, ensuring mutual exclusion.\n\n#### 7.4 Ordering of GIC register writes\n\
    \nThe Arm GIC Architecture Specification text (IHI 0069H.b) is reasonably clear\
    \ about the relaxed ordering of GIC events induced by accesses to GIC registers\
    \ with program-order later events (12.1.6 \"Observability of the effects of accesses\
    \ to the GIC registers\"), though there are still subtle requirements for barriers.\
    \ A DSB.SY enforces ordering of GIC events (generate, acknowledge, drop priority,\
    \ and deactivate) induced by accesses to GIC registers (SGI1R, IAR, EOIR, DIR)\
    \ with program-order-later events, as they are such effects. DSBs are not needed\
    \ to merely order the register accesses themselves.\n\nAn ISB ensures that any\
    \ pending interrupts are taken before executing the program-order later instructions.\n\
    \nIf there was an interrupt in the Active and Pending state at deactivation, then\
    \ it is immediately re-pended on the PE (and so delivery can immediately happen\
    \ again). But, if there is no DSB between the write of the deactivation and the\
    \ context synchronisation, it might be that the assertion and delivery did not\
    \ yet occur, causing the interrupt to be taken later.\n\n#### 7.5 A draft axiomatic\
    \ extension\n\nWe give a draft extension to the previous axiomatic model to support\
    \ inter-processor interrupts, noting the challenges.\n\nGIC candidates. Unlike\
    \ with most of the instruction semantics, there is very little public ASL from\
    \ Arm which describes the priority and INTID state machine system. While much\
    \ of the GIC's machinery, routing, virtualisation and so on, is not required to\
    \ discuss the usage of interrupts here, a large quantity of the base GIC architecture\
    \ would need to be turned into ASL and incorporated into the machinery. The rest\
    \ of this extension assumes one has such machinery in place.\n\nFirst, we must\
    \ extend the thread semantics: reads and writes of the registers of the CPU interface\
    \ to the GIC, and interrupt status register, must be treated differently than\
    \ other registers, lifting them to the memory model with a relation constraining\
    \ the values they could read, analogous to 'reads-from'. This allows us to tie\
    \ the thread's events interacting with the GIC, with those events coming from\
    \ the GIC ASL.\n\nWe add the following new events, grouped as GICEvents:\n\n-\
    \ GenerateInterrupt, for the GIC action from writing the SGI1R register, which\
    \ sends an IPI to other cores. It is associated with a target set of CPUs.\n-\
    \ Acknowledge, for the relevant effect in the GIC, i.e. the state machine change\
    \ and related updates to registers. Here, we assume the GIC update is atomic,\
    \ which ought to be true for simple physical SGIs.\n- DropPriority and Deactivate,\
    \ for the relevant effects on the GIC state machine and priority masking.\n\n\
    These new events are placed iio-after (intra-instruction-ordered) the respective\
    \ register events. Such events could instead be inserted into po, with suitable\
    \ modification of the previous relations, although for simplicity here we do not.\n\
    \nInterrupt witness. We add a new existentially-quantified relation to the witness:\
    \ interrupt. This associates the TakeInterrupt with the GenerateInterrupt which\
    \ caused it, constraining any program-order-later Acknowledge and corresponding\
    \ MRS event INTID values. This effectively assigns the INTID at the point the\
    \ interrupt is taken, and makes interrupt behave like rf for INTIDs; if the INTID\
    \ is never read, one must consider all possible interrupt sources.\n\nUpdate to\
    \ relations and axioms. The update to the relations is then fairly straightforward:\
    \ insert interrupt into ob, and make DSB instructions order GIC events in program-order.\
    \ We do not put GICEvents in program order to express that they may execute outof-order\
    \ with respect to other events in the same thread, including context-synchronisation,\
    \ unless explicitly ordered (e.g. by DSBs).\n\n#### 8 Conclusion\n\nWe identify\
    \ an open problem in giving a definition of precision on relaxed architectures,\
    \ and describe the challenge in doing so. We characterise some basic guarantees\
    \ of precision, which should make it possible to apply some of the abstraction\
    \ techniques used to reason about nesting of interrupts [\\[45,](#page-13-24)\
    \ [47\\]](#page-13-25).\n\nWe extend the Arm-A memory model to cover exceptions,\
    \ an important aspect of defining the architectural interface, clarifying the\
    \ behaviour at that interface, and giving an executable-as-a-testoracle implementation\
    \ of an axiomatic model usable as an exploration tool to investigate the effect\
    \ of synchronisation on hardware exceptions and interrupts. We describe the interaction\
    \ of hardware exceptions with memory errors, and the consequences on the user\
    \ model.\n\nWe begin building a model for software-generated interrupts and the\
    \ required parts of the interrupt machinery relied upon by the common computing\
    \ base, giving the key shapes and litmus tests, some baseline behaviours of the\
    \ Arm GIC, and a draft extension that covers key use cases.\n\nAlthough there\
    \ is much work still to do on exceptions, interrupts, and their interaction with\
    \ other features, this work creates a robust foundation that future work can build\
    \ on.\n\n#### Acknowledgments\n\nWe thank Richard Grisenthwaite (Arm EVP, Chief\
    \ Architect, and Fellow), Martin Weidmann (Director of Product Management, Arm\
    \ Architecture and Technology Group), and Will Deacon (Google) for detailed discussions\
    \ about the Arm architecture. We thank Ben Laurie and Sarah de Haas (Google) for\
    \ their support. We thank Jonathan Woodruff and others at the CL for their insightful\
    \ discussions.\n\nThis work was funded in part by Google. This work was funded\
    \ in part by Arm. This work was funded in part by an AUFF starter grant (Pichon-Pharabod).\
    \ This work was funded in part by two Amazon Research Awards (Pichon-Pharabod;\
    \ Sewell and Simner). This work was funded in part by UK Research and Innovation\
    \ (UKRI) under the UK government's Horizon Europe funding guarantee for ERC-AdG-2022,\
    \ EP/Y035976/1 SAFER. This project has received funding from the European Research\
    \ Council (ERC) under the European Union's Horizon 2020 research and innovation\
    \ programme (grant agreement No 789108, ERC-AdG-2017 ELVER). This work is supported\
    \ by ERC-2024-POC grant ELVER-CHECK, 101189371. Funded by the European Union.\
    \ Views and opinions expressed are however those of the author(s) only and do\
    \ not necessarily reflect those of the European Union or the European Research\
    \ Council Executive Agency. Neither the European Union nor the granting authority\
    \ can be held responsible for them. This work was supported in part by the Innovate\
    \ UK project Digital Security by Design (DSbD) Technology Platform Prototype,\
    \ 105694. The authors would like to thank the Isaac Newton Institute for Mathematical\
    \ Sciences, Cambridge, for support and hospitality during the programme Big Specification,\
    \ where work on this paper was undertaken. This work was supported by EPSRC grant\
    \ EP/Z000580/1. This work was funded in part by a Royal Society University Research\
    \ Fellowship. One of the authors has received funding from the UK Advanced Research\
    \ and Innovation Agency (ARIA) as part of the project Qbs4Safety: Core Representation\
    \ Underlying Safeguarded AI.\n\nPrecise exceptions in relaxed architectures ISCA\
    \ '25, June 21–25, 2025, Tokyo, Japan\n\n#### References\n\n- <span id=\"page-12-3\"\
    ></span>[1] A. Adir, H. Attiya, and G. Shurek. 2003. Information-Flow Models for\
    \ Shared Memory with an Application to the PowerPC Architecture. IEEE Trans. Parallel\
    \ Distrib. Syst. 14, 5 (2003), 502–515. [doi:10.1109/TPDS.2003.1199067](https://doi.org/10.1109/TPDS.2003.1199067)\n\
    - [2] Jade Alglave. 2010. A Shared Memory Poetics. Ph. D. Dissertation. Université\
    \ Paris 7 – Denis Diderot.\n- <span id=\"page-12-4\"></span>[3] Jade Alglave,\
    \ Will Deacon, Richard Grisenthwaite, Antoine Hacquard, and Luc Maranget. 2021.\
    \ Armed Cats: Formal Concurrency Modelling at Arm. ACM Trans. Program. Lang. Syst.\
    \ 43, 2 (2021), 8:1–8:54. [doi:10.1145/3458926](https://doi.org/10.1145/3458926)\n\
    - <span id=\"page-12-11\"></span>[4] Jade Alglave, Richard Grisenthwaite, Artem\
    \ Khyzha, Luc Maranget, and Nikos Nikoleris. 2024. Puss In Boots: on formalising\
    \ Arm's Virtual Memory System Architecture (extended version). (May 2024).<https://inria.hal.science/hal-04567296>\
    \ working paper or preprint.\n- <span id=\"page-12-12\"></span>[5] Jade Alglave\
    \ and Luc Maranget. [n. d.]. The herdtools7 tool suite. [diy.inria.fr,](diy.inria.fr)\
    \ [https://github.com/herd/herdtools7/.](https://github.com/herd/herdtools7/)\
    \ Accessed 2023-08-30.\n- <span id=\"page-12-34\"></span>[6] Jade Alglave, Luc\
    \ Maranget, Paul E. McKenney, Andrea Parri, and Alan S. Stern. 2018. Frightening\
    \ Small Children and Disconcerting Grown-ups: Concurrency in the Linux Kernel.\
    \ In Proceedings of the Twenty-Third International Conference on Architectural\
    \ Support for Programming Languages and Operating Systems, ASPLOS 2018, Williamsburg,\
    \ VA, USA, March 24-28, 2018, Xipeng Shen, James Tuck, Ricardo Bianchini, and\
    \ Vivek Sarkar (Eds.). ACM, 405–418. [doi:10.1145/3173162.3177156](https://doi.org/10.1145/3173162.3177156)\n\
    - <span id=\"page-12-5\"></span>[7] Jade Alglave, Luc Maranget, Susmit Sarkar,\
    \ and Peter Sewell. 2010. Fences in Weak Memory Models. In Computer Aided Verification,\
    \ 22nd International Conference, CAV 2010, Edinburgh, UK, July 15-19, 2010. Proceedings\
    \ (Lecture Notes in Computer Science, Vol. 6174), Tayssir Touili, Byron Cook,\
    \ and Paul B. Jackson (Eds.). Springer, 258–272. [doi:10.1007/978-3-642-14295-6\\\
    \\_25](https://doi.org/10.1007/978-3-642-14295-6_25)\n- <span id=\"page-12-21\"\
    ></span>[8] J. Alglave, L. Maranget, S. Sarkar, and P. Sewell. 2011. Litmus: Running\
    \ Tests Against Hardware. In Proc. TACAS. [doi:10.1007/978-3-642-19835-9\\\\_5](https://doi.org/10.1007/978-3-642-19835-9_5)\n\
    - <span id=\"page-12-6\"></span>[9] Jade Alglave, Luc Maranget, and Michael Tautschnig.\
    \ 2014. Herding Cats: Modelling, Simulation, Testing, and Data Mining for Weak\
    \ Memory. ACM Trans. Program. Lang. Syst. 36, 2 (2014), 7:1–7:74. [doi:10.1145/2627752](https://doi.org/10.1145/2627752)\n\
    - <span id=\"page-12-15\"></span>[10] Arm. 2024. Arm Architecture Reference Manual:\
    \ for A-profile architecture. [https://developer.arm.com/documentation/ddi0487/latest.](https://developer.arm.com/documentation/ddi0487/latest)\
    \ Accessed 2024-05-11. Issue K.a. 14777 pages..\n- <span id=\"page-12-14\"></span>[11]\
    \ Arm. 2024. Arm Generic Interrupt Controller Architecture Specification, GIC\
    \ architecture version 3 and version 4. Technical Report. Arm. IHI 0069H.b (ID041224).\n\
    - <span id=\"page-12-31\"></span>[12] Alasdair Armstrong, Thomas Bauereiss, Brian\
    \ Campbell, Alastair Reid, Kathryn E. Gray, Robert M. Norton, Prashanth Mundkur,\
    \ Mark Wassell, Jon French, Christopher Pulte, Shaked Flur, Ian Stark, Neel Krishnaswami,\
    \ and Peter Sewell. 2019. ISA Semantics for ARMv8-A, RISC-V, and CHERI-MIPS. In\
    \ Proceedings of the 46th ACM SIGPLAN Symposium on Principles of Programming Languages.\
    \ [doi:10.1145/3290384](https://doi.org/10.1145/3290384) Proc. ACM Program. Lang.\
    \ 3, POPL, Article 71.\n- <span id=\"page-12-1\"></span>[13] Alasdair Armstrong,\
    \ Brian Campbell, Ben Simner, Christopher Pulte, and Peter Sewell. 2021. Isla:\
    \ Integrating full-scale ISA semantics and axiomatic concurrency models. In Proc.\
    \ 33rd International Conference on Computer-Aided Verification (Lecture Notes\
    \ in Computer Science, Vol. 12759). Springer, 303–316. [doi:10.1007/978-](https://doi.org/10.1007/978-3-030-81685-8_14)\
    \ [3-030-81685-8\\\\_14](https://doi.org/10.1007/978-3-030-81685-8_14)\n- <span\
    \ id=\"page-12-2\"></span>[14] Ellen Arvidsson, Elias Castegren, Sylvan Clebsch,\
    \ Sophia Drossopoulou, James Noble, Matthew J. Parkinson, and Tobias Wrigstad.\
    \ 2023. Reference Capabilities for Flexible Memory Management. Proc. ACM Program.\
    \ Lang. 7, OOPSLA2 (2023), 1363–1393. [doi:10.1145/3622846](https://doi.org/10.1145/3622846)\n\
    - <span id=\"page-12-23\"></span>[15] Mark Batty, Kayvan Memarian, Kyndylan Nienhuis,\
    \ Jean Pichon-Pharabod, and Peter Sewell. 2015. The Problem of Programming Language\
    \ Concurrency Semantics. In Programming Languages and Systems - 24th European\
    \ Symposium on Programming, ESOP 2015, Held as Part of the European Joint Conferences\
    \ on Theory and Practice of Software, ETAPS 2015, London, UK, April 11-18, 2015.\
    \ Proceedings (Lecture Notes in Computer Science, Vol. 9032), Jan Vitek (Ed.).\
    \ Springer, 283–307. [doi:10.1007/978-3-662-46669-8\\\\_12](https://doi.org/10.1007/978-3-662-46669-8_12)\n\
    - <span id=\"page-12-25\"></span>[16] Mark John Batty. 2015. The C11 and C++11\
    \ concurrency model. Ph. D. Dissertation. University of Cambridge, UK. [https://ethos.bl.uk/OrderDetails.do?uin=uk.bl.](https://ethos.bl.uk/OrderDetails.do?uin=uk.bl.ethos.708458)\
    \ [ethos.708458](https://ethos.bl.uk/OrderDetails.do?uin=uk.bl.ethos.708458)\n\
    - <span id=\"page-12-30\"></span>[17] Thomas Bauereiss, Brian Campbell, Alasdair\
    \ Armstrong, Alastair Reid, Kathryn E. Gray, Anthony Fox, Peter Sewell, and Arm\
    \ Limited. 2024. Sail Armv9.4-A instruction-set architecture (ISA) model. [https://github.com/rems-project/sail](https://github.com/rems-project/sail-arm)[arm.](https://github.com/rems-project/sail-arm)\
    \ Accessed 2024-05-11..\n- <span id=\"page-12-35\"></span>[18] Mike Burrows. 2004.\
    \ How to Implement Unnecessary Mutexes. Springer New York, New York, NY, 51–57.\
    \ [doi:10.1007/0-387-21821-1\\\\_7](https://doi.org/10.1007/0-387-21821-1_7)\n\
    - <span id=\"page-12-26\"></span>[19] Soham Chakraborty. 2019. Correct Compilation\
    \ of Relaxed Memory Concurrency. Ph. D. Dissertation. Kaiserslautern University\
    \ of Technology, Germany. [https:](https://kluedo.ub.rptu.de/frontdoor/index/index/docId/5697)\
    \ [//kluedo.ub.rptu.de/frontdoor/index/index/docId/5697](https://kluedo.ub.rptu.de/frontdoor/index/index/docId/5697)\n\
    - <span id=\"page-12-33\"></span>[20] Luke Cheeseman, Matthew J. Parkinson, Sylvan\
    \ Clebsch, Marios Kogias, Sophia Drossopoulou, David Chisnall, Tobias Wrigstad,\
    \ and Paul Liétar. 2023. When Concurrency Matters: Behaviour-Oriented Concurrency.\
    \ Proc. ACM Program. Lang. 7, OOPSLA2 (October 2023). [https://www.microsoft.com/en-us/research/](https://www.microsoft.com/en-us/research/publication/when-concurrency-matters-behaviour-oriented-concurrency/)\
    \ [publication/when-concurrency-matters-behaviour-oriented-concurrency/](https://www.microsoft.com/en-us/research/publication/when-concurrency-matters-behaviour-oriented-concurrency/)\n\
    - <span id=\"page-12-7\"></span>[21] William W. Collier. 1992. Reasoning about\
    \ parallel architectures. Prentice Hall.\n- <span id=\"page-12-36\"></span>[22]\
    \ Mingyao Yang Dave Dice, Hui Huang. 2001. Asymmetric Dekker Synchronization.\
    \ [http://web.archive.org/web/20070214114205/http://blogs.sun.com/dave/](http://web.archive.org/web/20070214114205/http://blogs.sun.com/dave/resource/Asymmetric-Dekker-Synchronization.txt)\
    \ [resource/Asymmetric-Dekker-Synchronization.txt](http://web.archive.org/web/20070214114205/http://blogs.sun.com/dave/resource/Asymmetric-Dekker-Synchronization.txt)\n\
    - <span id=\"page-12-19\"></span>[23] Hernán Ponce de León and Johannes Kinder.\
    \ 2021. Cats vs. Spectre: An Axiomatic Approach to Modeling Speculative Execution\
    \ Attacks. CoRR abs/2108.13818 (2021). arXiv[:2108.13818 https://arxiv.org/abs/2108.13818](https://arxiv.org/abs/2108.13818)\n\
    - <span id=\"page-12-20\"></span>[24] Hernán Ponce de León and Johannes Kinder.\
    \ 2022. Cats vs. Spectre: An Axiomatic Approach to Modeling Speculative Execution\
    \ Attacks. In 43rd IEEE Symposium on Security and Privacy, SP 2022, San Francisco,\
    \ CA, USA, May 22-26, 2022. IEEE, 235–248. [doi:10.1109/SP46214.2022.9833774](https://doi.org/10.1109/SP46214.2022.9833774)\n\
    - <span id=\"page-12-29\"></span>[25] Will Deacon, Jade Alglave, Nikos Nikoleris,\
    \ and Artem Khyzha. 2023. The ARMv8 Application Level Memory Model. [https://github.com/herd/herdtools7/](https://github.com/herd/herdtools7/blob/master/herd/libdir/aarch64.cat)\
    \ [blob/master/herd/libdir/aarch64.cat](https://github.com/herd/herdtools7/blob/master/herd/libdir/aarch64.cat)\
    \ (accessed 2019-07-01). Accessed 2024-11-19.\n- <span id=\"page-12-37\"></span>[26]\
    \ Dave Dice. 2006. Biased Locking in Hotspot. Oracle Blog, Wayback Machine. [http://web.archive.org/web/20150320095550/https://blogs.oracle.com/](http://web.archive.org/web/20150320095550/https://blogs.oracle.com/dave/entry/biased_locking_in_hotspot)\
    \ [dave/entry/biased\\\\_locking\\\\_in\\\\_hotspot](http://web.archive.org/web/20150320095550/https://blogs.oracle.com/dave/entry/biased_locking_in_hotspot)\n\
    - <span id=\"page-12-38\"></span>[27] David Dice, Mark S. Moir, and William N.\
    \ Scherer III. 2010. United States Patent US 7814488B1 Quickly Reacquirable Locks.\
    \ United Statess Patent Office.\n- <span id=\"page-12-8\"></span>[28] Shaked Flur,\
    \ Kathryn E. Gray, Christopher Pulte, Susmit Sarkar, Ali Sezgin, Luc Maranget,\
    \ Will Deacon, and Peter Sewell. 2016. Modelling the ARMv8 architecture, operationally:\
    \ concurrency and ISA. In Proceedings of the 43rd ACM SIGPLAN-SIGACT Symposium\
    \ on Principles of Programming Languages (St. Petersburg, FL, USA). 608–621. [doi:10.1145/2837614.2837615](https://doi.org/10.1145/2837614.2837615)\n\
    - <span id=\"page-12-16\"></span>[29] Shaked Flur, Susmit Sarkar, Christopher\
    \ Pulte, Kyndylan Nienhuis, Luc Maranget, Kathryn E. Gray, Ali Sezgin, Mark Batty,\
    \ and Peter Sewell. 2017. Mixed-size concurrency: ARM, POWER, C/C++11, and SC.\
    \ In Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming\
    \ Languages, POPL 2017, Paris, France, January 18-20, 2017, Giuseppe Castagna\
    \ and Andrew D. Gordon (Eds.). ACM, 429–442. [doi:10.1145/3009837.3009839](https://doi.org/10.1145/3009837.3009839)\n\
    - <span id=\"page-12-17\"></span>[30] Kourosh Gharachorloo. 1995. Memory Consistency\
    \ Models for Shared-Memory Multiprocessors. Ph. D. Dissertation. Stanford University.\n\
    - <span id=\"page-12-18\"></span>[31] Kourosh Gharachorloo, Daniel Lenoski, James\
    \ Laudon, Phillip B. Gibbons, Anoop Gupta, and John L. Hennessy. 1990. Memory\
    \ Consistency and Event Ordering in Scalable Shared-Memory Multiprocessors. In\
    \ Proceedings of the 17th Annual International Symposium on Computer Architecture,\
    \ Seattle, WA, USA, June 1990, Jean-Loup Baer, Larry Snyder, and James R. Goodman\
    \ (Eds.). ACM, 15–26. [doi:10.](https://doi.org/10.1145/325164.325102) [1145/325164.325102](https://doi.org/10.1145/325164.325102)\n\
    - <span id=\"page-12-9\"></span>[32] Kathryn E. Gray, Gabriel Kerneis, Dominic\
    \ P. Mulligan, Christopher Pulte, Susmit Sarkar, and Peter Sewell. 2015. An integrated\
    \ concurrency and core-ISA architectural envelope definition, and test oracle,\
    \ for IBM POWER multiprocessors. In Proceedings of the 48th International Symposium\
    \ on Microarchitecture (Waikiki). 635–646. [doi:10.1145/2830772.2830775](https://doi.org/10.1145/2830772.2830775)\n\
    - <span id=\"page-12-32\"></span>[33] Siddharth Gupta, Yuanlong Li, Qingxuan Kang,\
    \ Abhishek Bhattacharjee, Babak Falsafi, Yunho Oh, and Mathias Payer. 2023. Imprecise\
    \ Store Exceptions. In Proceedings of the 50th Annual International Symposium\
    \ on Computer Architecture, ISCA 2023, Orlando, FL, USA, June 17-21, 2023, Yan\
    \ Solihin and Mark A. Heinrich (Eds.). ACM, 52:1–52:15. [doi:10.1145/3579371.3589087](https://doi.org/10.1145/3579371.3589087)\n\
    - <span id=\"page-12-0\"></span>[34] John L. Hennessy and David A. Patterson.\
    \ 2012. Computer Architecture: A Quantitative Approach (5 ed.). Morgan Kaufmann,\
    \ Amsterdam.\n- <span id=\"page-12-13\"></span>[35] Naorin Hossain, Caroline Trippel,\
    \ and Margaret Martonosi. 2020. TransForm: Formally Specifying Transistency Models\
    \ and Synthesizing Enhanced Litmus Tests. CoRR abs/2008.03578 (2020). arXiv[:2008.03578\
    \ https://arxiv.org/abs/2008.](https://arxiv.org/abs/2008.03578) [03578](https://arxiv.org/abs/2008.03578)\n\
    - <span id=\"page-12-22\"></span>[36] Luc Maranget. 2024. Personal communication.\n\
    - <span id=\"page-12-10\"></span>[37] Intel. 2002. A Formal Specification of Intel\
    \ Itanium Processor Family Memory Ordering. [developer.intel.com/design/itanium/downloads/251429.htm.](developer.intel.com/design/itanium/downloads/251429.htm)\n\
    - <span id=\"page-12-27\"></span>[38] Radha Jagadeesan, Alan Jeffrey, and James\
    \ Riely. 2020. Pomsets with preconditions: a simple model of relaxed memory. Proc.\
    \ ACM Program. Lang. 4, OOPSLA (2020), 194:1–194:30. [doi:10.1145/3428262](https://doi.org/10.1145/3428262)\n\
    - <span id=\"page-12-28\"></span>[39] Jeehoon Kang, Chung-Kil Hur, Ori Lahav,\
    \ Viktor Vafeiadis, and Derek Dreyer. 2017. A promising semantics for relaxed-memory\
    \ concurrency. In Proceedings of the 44th ACM SIGPLAN Symposium on Principles\
    \ of Programming Languages (Paris, France) (POPL '17). Association for Computing\
    \ Machinery, New York, NY, USA, 175–189. [doi:10.1145/3009837.3009850](https://doi.org/10.1145/3009837.3009850)\n\
    - <span id=\"page-12-39\"></span>[40] Kiyokuni Kawachiya. 2005. Java Locks: Analysis\
    \ and Acceleration. Ph. D. Dissertation. Keio University.\n- <span id=\"page-12-40\"\
    ></span>[41] Kiyokuni Kawachiya, Akira Koseki, and Tamiya Onodera. 2002. Lock\
    \ reservation: Java locks can mostly do without atomic operations. In Proceedings\
    \ of the 17th ACM SIGPLAN Conference on Object-Oriented Programming, Systems,\
    \ Languages, and Applications (Seattle, Washington, USA) (OOPSLA '02). Association\
    \ for Computing Machinery, New York, NY, USA, 130–141. [doi:10.1145/582419.582433](https://doi.org/10.1145/582419.582433)\n\
    - <span id=\"page-12-24\"></span>[42] Michalis Kokologiannakis, Ori Lahav, Konstantinos\
    \ Sagonas, and Viktor Vafeiadis. 2017. Effective stateless model checking for\
    \ C/C++ concurrency. Proc. ACM Program. Lang. 2, POPL, Article 17 (dec 2017),\
    \ 32 pages. [doi:10.1145/3158105](https://doi.org/10.1145/3158105)\n- [43] Michalis\
    \ Kokologiannakis, Azalea Raad, and Viktor Vafeiadis. 2019. Model checking for\
    \ weakly consistent libraries. In Proceedings of the 40th ACM SIGPLAN Conference\
    \ on Programming Language Design and Implementation (Phoenix, AZ, USA) (PLDI 2019).\
    \ Association for Computing Machinery, New York, NY, USA,\n\n<span id=\"page-13-0\"\
    ></span>96–110. [doi:10.1145/3314221.3314609](https://doi.org/10.1145/3314221.3314609)\n\
    \n- <span id=\"page-13-16\"></span>[44] Michalis Kokologiannakis and Viktor Vafeiadis.\
    \ 2021. GenMC: A Model Checker for Weak Memory Models. In Computer Aided Verification,\
    \ Alexandra Silva and K. Rustan M. Leino (Eds.). Springer International Publishing,\
    \ Cham, 427–440. [doi:10.1007/978-3-030-81685-8\\\\_20](https://doi.org/10.1007/978-3-030-81685-8_20)\n\
    - <span id=\"page-13-24\"></span>[45] Daniel Kroening, Lihao Liang, Tom Melham,\
    \ Peter Schrammel, and Michael Tautschnig. 2015. Effective Verification of Low-Level\
    \ Software with Nested Interrupts. In Proceedings of the 2015 Design, Automation\
    \ & Test in Europe Conference & Exhibition, DATE 2015, Grenoble, France, March\
    \ 9-13, 2015, Wolfgang Nebel and David Atienza (Eds.). EDA Consortium, 229–234.\
    \ [http://www.cs.ox.ac.uk/tom.](http://www.cs.ox.ac.uk/tom.melham/pub/Kroening-2015-EVL.pdf)\
    \ [melham/pub/Kroening-2015-EVL.pdf](http://www.cs.ox.ac.uk/tom.melham/pub/Kroening-2015-EVL.pdf)\n\
    - <span id=\"page-13-15\"></span>[46] Ori Lahav, Viktor Vafeiadis, Jeehoon Kang,\
    \ Chung-Kil Hur, and Derek Dreyer. 2017. Repairing sequential consistency in C/C++11.\
    \ In Proceedings of the 38th ACM SIGPLAN Conference on Programming Language Design\
    \ and Implementation (Barcelona, Spain) (PLDI 2017). Association for Computing\
    \ Machinery, New York, NY, USA, 618–632. [doi:10.1145/3062341.3062352](https://doi.org/10.1145/3062341.3062352)\n\
    - <span id=\"page-13-25\"></span>[47] Lihao Liang, Tom Melham, Daniel Kroening,\
    \ Peter Schrammel, and Michael Tautschnig. 2017. Effective Verification for Low-Level\
    \ Software with Competing Interrupts. ACM Transactions on Embedded Computing Systems\
    \ 17, 2 (December 2017), 36:1–36:26. [doi:10.1145/3147432](https://doi.org/10.1145/3147432)\n\
    - <span id=\"page-13-12\"></span>[48] Daniel Lustig, Geet Sethi, Margaret Martonosi,\
    \ and Abhishek Bhattacharjee. 2016. COATCheck: Verifying Memory Ordering at the\
    \ Hardware-OS Interface. In Proceedings of the Twenty-First International Conference\
    \ on Architectural Support for Programming Languages and Operating Systems, ASPLOS\
    \ 2016, Atlanta, GA, USA, April 2-6, 2016, Tom Conte and Yuanyuan Zhou (Eds.).\
    \ ACM, 233–247. [doi:10.1145/2872362.2872399](https://doi.org/10.1145/2872362.2872399)\n\
    - <span id=\"page-13-22\"></span>[49] Patricio Chilano Mateo. 2021. JEP 374: Deprecate\
    \ and Disable Biased Locking. JDK Enhancement Proposal.<https://openjdk.org/jeps/374>\n\
    - <span id=\"page-13-20\"></span>[50] Paul E. McKenney. 2023. Is Parallel Programming\
    \ Hard, And, If So, What Can You Do About It? [https://mirrors.edge.kernel.org/pub/linux/kernel/people/paulmck/](https://mirrors.edge.kernel.org/pub/linux/kernel/people/paulmck/perfbook/perfbook.html)\
    \ [perfbook/perfbook.html](https://mirrors.edge.kernel.org/pub/linux/kernel/people/paulmck/perfbook/perfbook.html)\n\
    - <span id=\"page-13-19\"></span>[51] Paul E. McKenney. 2024. RCU Concepts. [https://www.kernel.org/doc/](https://www.kernel.org/doc/Documentation/RCU/rcu.txt)\
    \ [Documentation/RCU/rcu.txt](https://www.kernel.org/doc/Documentation/RCU/rcu.txt)\
    \ Accessed 2024-11-19.\n- <span id=\"page-13-1\"></span>[52] Paul E McKenney and\
    \ John D Slingwine. 1998. Read-copy update: Using execution history to solve concurrency\
    \ problems. In Parallel and Distributed Computing and Systems, Vol. 509518. 509–518.\n\
    - <span id=\"page-13-11\"></span>[53] Nicholas Mosier, Hanna Lachnitt, Hamed Nemati,\
    \ and Caroline Trippel. 2022. Axiomatic hardware-software contracts for security.\
    \ In ISCA '22: The 49th Annual International Symposium on Computer Architecture,\
    \ New York, New York, USA, June 18 - 22, 2022, Valentina Salapura, Mohamed Zahran,\
    \ Fred Chong, and Lingjia Tang (Eds.). ACM, 72–86. [doi:10.1145/3470496.3527412](https://doi.org/10.1145/3470496.3527412)\n\
    - <span id=\"page-13-21\"></span>[54] Matthew J. Parkinson. 2024. Some things\
    \ I wish I hadn't seen. presented at The Future of Weak Memory 2024.\n- <span\
    \ id=\"page-13-17\"></span>[55] Jean Pichon-Pharabod and Peter Sewell. 2016. A\
    \ concurrency semantics for relaxed atomics that permits optimisation and avoids\
    \ thin-air executions. In Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium\
    \ on Principles of Programming Languages, POPL 2016, St. Petersburg, FL, USA,\
    \ January 20 - 22, 2016, Rastislav Bodík and Rupak Majumdar (Eds.). ACM, 622–633.\
    \ [doi:10.1145/](https://doi.org/10.1145/2837614.2837616) [2837614.2837616](https://doi.org/10.1145/2837614.2837616)\n\
    - <span id=\"page-13-18\"></span>[56] William W. Pugh. 1999. Fixing the Java Memory\
    \ Model. In Proceedings of the ACM 1999 Conference on Java Grande, JAVA '99, San\
    \ Francisco, CA, USA, June 12-14, 1999, Geoffrey C. Fox, Klaus E. Schauser, and\
    \ Marc Snir (Eds.). ACM, 89–98. [doi:10.1145/304065.304106](https://doi.org/10.1145/304065.304106)\n\
    - <span id=\"page-13-2\"></span>[57] Christopher Pulte. 2018. The Semantics of\
    \ Multicopy Atomic ARMv8 and RISC-V. Ph. D. Dissertation. University of Cambridge.\
    \ [https://www.repository.cam.ac.uk/](https://www.repository.cam.ac.uk/handle/1810/292229)\
    \ [handle/1810/292229.](https://www.repository.cam.ac.uk/handle/1810/292229)\n\
    - <span id=\"page-13-3\"></span>[58] Christopher Pulte, Shaked Flur, Will Deacon,\
    \ Jon French, Susmit Sarkar, and Peter Sewell. 2018. Simplifying ARM concurrency:\
    \ multicopy-atomic axiomatic and operational models for ARMv8. Proc. ACM Program.\
    \ Lang. 2, POPL (2018), 19:1–19:29. [doi:10.1145/3158107](https://doi.org/10.1145/3158107)\n\
    - <span id=\"page-13-23\"></span>[59] Kenneth Russell and David Detlefs. 2006.\
    \ Eliminating synchronization-related atomic operations with biased locking and\
    \ bulk rebiasing. In Proceedings of the 21st Annual ACM SIGPLAN Conference on\
    \ Object-Oriented Programming Systems, Languages, and Applications (Portland,\
    \ Oregon, USA) (OOPSLA '06). Association for Computing Machinery, New York, NY,\
    \ USA, 263–272. [doi:10.1145/1167473.](https://doi.org/10.1145/1167473.1167496)\
    \ [1167496](https://doi.org/10.1145/1167473.1167496)\n- <span id=\"page-13-4\"\
    ></span>[60] Susmit Sarkar, Kayvan Memarian, Scott Owens, Mark Batty, Peter Sewell,\
    \ Luc Maranget, Jade Alglave, and Derek Williams. 2012. Synchronising C/C++ and\
    \ POWER. In ACM SIGPLAN Conference on Programming Language Design and Implementation,\
    \ PLDI '12, Beijing, China - June 11 - 16, 2012, Jan Vitek, Haibo Lin, and Frank\
    \ Tip (Eds.). ACM, 311–322. [doi:10.1145/2254064.2254102](https://doi.org/10.1145/2254064.2254102)\n\
    - <span id=\"page-13-10\"></span>[61] Susmit Sarkar, Peter Sewell, Jade Alglave,\
    \ Luc Maranget, and Derek Williams. 2011. Understanding POWER multiprocessors.\
    \ In Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design\
    \ and Implementation, PLDI 2011, San Jose, CA, USA, June 4-8, 2011, Mary W. Hall\
    \ and David A. Padua (Eds.). ACM, 175–186. [doi:10.1145/1993498.1993520](https://doi.org/10.1145/1993498.1993520)\n\
    - <span id=\"page-13-5\"></span>[62] Susmit Sarkar, Peter Sewell, Francesco Zappa\
    \ Nardelli, Scott Owens, Tom Ridge, Thomas Braibant, Magnus O. Myreen, and Jade\
    \ Alglave. 2009. The semantics of\n\nx86-CC multiprocessor machine code. In Proceedings\
    \ of the 36th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages,\
    \ POPL 2009, Savannah, GA, USA, January 21-23, 2009, Zhong Shao and Benjamin C.\
    \ Pierce (Eds.). ACM, 379–391. [doi:10.1145/1480881.1480929](https://doi.org/10.1145/1480881.1480929)\n\
    \n- <span id=\"page-13-14\"></span>[63] Peter Sewell, Christopher Pulte, Shaked\
    \ Flur, Mark Batty, Luc Maranget, and Alasdair Armstrong. 2022. Multicore Semantics:\
    \ Making Sense of Relaxed Memory (MPhil slides).<https://www.cl.cam.ac.uk/~pes20/slides-acs-2022.pdf>\n\
    - <span id=\"page-13-6\"></span>[64] P. Sewell, S. Sarkar, S. Owens, F. Zappa\
    \ Nardelli, and M. O. Myreen. 2010. x86-TSO: A Rigorous and Usable Programmer's\
    \ Model for x86 Multiprocessors. Commun. ACM 53, 7 (July 2010), 89–97. [doi:10.1145/1785414.1785443](https://doi.org/10.1145/1785414.1785443)\n\
    - <span id=\"page-13-13\"></span>[65] Ben Simner, Alasdair Armstrong, Thomas Bauereiss,\
    \ Brian Campbell, Ohad Kammar, Jean Pichon-Pharabod, and Peter Sewell. 2024. Relaxed\
    \ exception semantics for Arm-A (extended version). CoRR abs/2412.15140 (2024).\
    \ [doi:10.](https://doi.org/10.48550/ARXIV.2412.15140) [48550/ARXIV.2412.15140](https://doi.org/10.48550/ARXIV.2412.15140)\
    \ arXiv[:2412.15140](https://arxiv.org/abs/2412.15140)\n- <span id=\"page-13-9\"\
    ></span>[66] Ben Simner, Alasdair Armstrong, Jean Pichon-Pharabod, Christopher\
    \ Pulte, Richard Grisenthwaite, and Peter Sewell. 2022. Relaxed virtual memory\
    \ in Armv8-A. In Proceedings of the 31st European Symposium on Programming (Lecture\
    \ Notes in Computer Science, Vol. 13240). Springer, 143–173. [doi:10.1007/978-3-](https://doi.org/10.1007/978-3-030-99336-8_6)\
    \ [030-99336-8\\\\_6](https://doi.org/10.1007/978-3-030-99336-8_6)\n- <span id=\"\
    page-13-8\"></span>[67] Ben Simner, Shaked Flur, Christopher Pulte, Alasdair Armstrong,\
    \ Jean Pichon-Pharabod, Luc Maranget, and Peter Sewell. 2020. ARMv8-A System Semantics:\
    \ Instruction Fetch in Relaxed Architectures. In Programming Languages and Systems\
    \ - 29th European Symposium on Programming, ESOP 2020, Held as Part of the European\
    \ Joint Conferences on Theory and Practice of Software, ETAPS 2020, Dublin, Ireland,\
    \ April 25-30, 2020, Proceedings (Lecture Notes in Computer Science, Vol. 12075),\
    \ Peter Müller (Ed.). Springer, 626–655. [doi:10.1007/978-3-030-44914-8\\\\_23](https://doi.org/10.1007/978-3-030-44914-8_23)\n\
    - <span id=\"page-13-7\"></span>[68] P. S. Sindhu, J.-M. Frailong, and M. Cekleov.\
    \ 1991. Formal Specification of Memory Models. In Scalable Shared Memory Multiprocessors.\
    \ Kluwer, 25–42. [doi:10.1007/978-1-4615-3604-8\\\\_2](https://doi.org/10.1007/978-1-4615-3604-8_2)"
  references:
  - '- <span id="page-12-3"></span>[1] A. Adir, H. Attiya, and G. Shurek. 2003. Information-Flow
    Models for Shared Memory with an Application to the PowerPC Architecture. IEEE
    Trans. Parallel Distrib. Syst. 14, 5 (2003), 502–515. [doi:10.1109/TPDS.2003.1199067](https://doi.org/10.1109/TPDS.2003.1199067)'
  - '- [2] Jade Alglave. 2010. A Shared Memory Poetics. Ph. D. Dissertation. Université
    Paris 7 – Denis Diderot.'
  - '- <span id="page-12-4"></span>[3] Jade Alglave, Will Deacon, Richard Grisenthwaite,
    Antoine Hacquard, and Luc Maranget. 2021. Armed Cats: Formal Concurrency Modelling
    at Arm. ACM Trans. Program. Lang. Syst. 43, 2 (2021), 8:1–8:54. [doi:10.1145/3458926](https://doi.org/10.1145/3458926)'
  - '- <span id="page-12-11"></span>[4] Jade Alglave, Richard Grisenthwaite, Artem
    Khyzha, Luc Maranget, and Nikos Nikoleris. 2024. Puss In Boots: on formalising
    Arm''s Virtual Memory System Architecture (extended version). (May 2024).<https://inria.hal.science/hal-04567296>
    working paper or preprint.'
  - '- <span id="page-12-12"></span>[5] Jade Alglave and Luc Maranget. [n. d.]. The
    herdtools7 tool suite. [diy.inria.fr,](diy.inria.fr) [https://github.com/herd/herdtools7/.](https://github.com/herd/herdtools7/)
    Accessed 2023-08-30.'
  - '- <span id="page-12-34"></span>[6] Jade Alglave, Luc Maranget, Paul E. McKenney,
    Andrea Parri, and Alan S. Stern. 2018. Frightening Small Children and Disconcerting
    Grown-ups: Concurrency in the Linux Kernel. In Proceedings of the Twenty-Third
    International Conference on Architectural Support for Programming Languages and
    Operating Systems, ASPLOS 2018, Williamsburg, VA, USA, March 24-28, 2018, Xipeng
    Shen, James Tuck, Ricardo Bianchini, and Vivek Sarkar (Eds.). ACM, 405–418. [doi:10.1145/3173162.3177156](https://doi.org/10.1145/3173162.3177156)'
  - '- <span id="page-12-5"></span>[7] Jade Alglave, Luc Maranget, Susmit Sarkar,
    and Peter Sewell. 2010. Fences in Weak Memory Models. In Computer Aided Verification,
    22nd International Conference, CAV 2010, Edinburgh, UK, July 15-19, 2010. Proceedings
    (Lecture Notes in Computer Science, Vol. 6174), Tayssir Touili, Byron Cook, and
    Paul B. Jackson (Eds.). Springer, 258–272. [doi:10.1007/978-3-642-14295-6\\_25](https://doi.org/10.1007/978-3-642-14295-6_25)'
  - '- <span id="page-12-21"></span>[8] J. Alglave, L. Maranget, S. Sarkar, and P.
    Sewell. 2011. Litmus: Running Tests Against Hardware. In Proc. TACAS. [doi:10.1007/978-3-642-19835-9\\_5](https://doi.org/10.1007/978-3-642-19835-9_5)'
  - '- <span id="page-12-6"></span>[9] Jade Alglave, Luc Maranget, and Michael Tautschnig.
    2014. Herding Cats: Modelling, Simulation, Testing, and Data Mining for Weak Memory.
    ACM Trans. Program. Lang. Syst. 36, 2 (2014), 7:1–7:74. [doi:10.1145/2627752](https://doi.org/10.1145/2627752)'
  - '- <span id="page-12-15"></span>[10] Arm. 2024. Arm Architecture Reference Manual:
    for A-profile architecture. [https://developer.arm.com/documentation/ddi0487/latest.](https://developer.arm.com/documentation/ddi0487/latest)
    Accessed 2024-05-11. Issue K.a. 14777 pages..'
  - '- <span id="page-12-14"></span>[11] Arm. 2024. Arm Generic Interrupt Controller
    Architecture Specification, GIC architecture version 3 and version 4. Technical
    Report. Arm. IHI 0069H.b (ID041224).'
  - '- <span id="page-12-31"></span>[12] Alasdair Armstrong, Thomas Bauereiss, Brian
    Campbell, Alastair Reid, Kathryn E. Gray, Robert M. Norton, Prashanth Mundkur,
    Mark Wassell, Jon French, Christopher Pulte, Shaked Flur, Ian Stark, Neel Krishnaswami,
    and Peter Sewell. 2019. ISA Semantics for ARMv8-A, RISC-V, and CHERI-MIPS. In
    Proceedings of the 46th ACM SIGPLAN Symposium on Principles of Programming Languages.
    [doi:10.1145/3290384](https://doi.org/10.1145/3290384) Proc. ACM Program. Lang.
    3, POPL, Article 71.'
  - '- <span id="page-12-1"></span>[13] Alasdair Armstrong, Brian Campbell, Ben Simner,
    Christopher Pulte, and Peter Sewell. 2021. Isla: Integrating full-scale ISA semantics
    and axiomatic concurrency models. In Proc. 33rd International Conference on Computer-Aided
    Verification (Lecture Notes in Computer Science, Vol. 12759). Springer, 303–316.
    [doi:10.1007/978-](https://doi.org/10.1007/978-3-030-81685-8_14) [3-030-81685-8\\_14](https://doi.org/10.1007/978-3-030-81685-8_14)'
  - '- <span id="page-12-2"></span>[14] Ellen Arvidsson, Elias Castegren, Sylvan Clebsch,
    Sophia Drossopoulou, James Noble, Matthew J. Parkinson, and Tobias Wrigstad. 2023.
    Reference Capabilities for Flexible Memory Management. Proc. ACM Program. Lang.
    7, OOPSLA2 (2023), 1363–1393. [doi:10.1145/3622846](https://doi.org/10.1145/3622846)'
  - '- <span id="page-12-23"></span>[15] Mark Batty, Kayvan Memarian, Kyndylan Nienhuis,
    Jean Pichon-Pharabod, and Peter Sewell. 2015. The Problem of Programming Language
    Concurrency Semantics. In Programming Languages and Systems - 24th European Symposium
    on Programming, ESOP 2015, Held as Part of the European Joint Conferences on Theory
    and Practice of Software, ETAPS 2015, London, UK, April 11-18, 2015. Proceedings
    (Lecture Notes in Computer Science, Vol. 9032), Jan Vitek (Ed.). Springer, 283–307.
    [doi:10.1007/978-3-662-46669-8\\_12](https://doi.org/10.1007/978-3-662-46669-8_12)'
  - '- <span id="page-12-25"></span>[16] Mark John Batty. 2015. The C11 and C++11
    concurrency model. Ph. D. Dissertation. University of Cambridge, UK. [https://ethos.bl.uk/OrderDetails.do?uin=uk.bl.](https://ethos.bl.uk/OrderDetails.do?uin=uk.bl.ethos.708458)
    [ethos.708458](https://ethos.bl.uk/OrderDetails.do?uin=uk.bl.ethos.708458)'
  - '- <span id="page-12-30"></span>[17] Thomas Bauereiss, Brian Campbell, Alasdair
    Armstrong, Alastair Reid, Kathryn E. Gray, Anthony Fox, Peter Sewell, and Arm
    Limited. 2024. Sail Armv9.4-A instruction-set architecture (ISA) model. [https://github.com/rems-project/sail](https://github.com/rems-project/sail-arm)[arm.](https://github.com/rems-project/sail-arm)
    Accessed 2024-05-11..'
  - '- <span id="page-12-35"></span>[18] Mike Burrows. 2004. How to Implement Unnecessary
    Mutexes. Springer New York, New York, NY, 51–57. [doi:10.1007/0-387-21821-1\\_7](https://doi.org/10.1007/0-387-21821-1_7)'
  - '- <span id="page-12-26"></span>[19] Soham Chakraborty. 2019. Correct Compilation
    of Relaxed Memory Concurrency. Ph. D. Dissertation. Kaiserslautern University
    of Technology, Germany. [https:](https://kluedo.ub.rptu.de/frontdoor/index/index/docId/5697)
    [//kluedo.ub.rptu.de/frontdoor/index/index/docId/5697](https://kluedo.ub.rptu.de/frontdoor/index/index/docId/5697)'
  - '- <span id="page-12-33"></span>[20] Luke Cheeseman, Matthew J. Parkinson, Sylvan
    Clebsch, Marios Kogias, Sophia Drossopoulou, David Chisnall, Tobias Wrigstad,
    and Paul Liétar. 2023. When Concurrency Matters: Behaviour-Oriented Concurrency.
    Proc. ACM Program. Lang. 7, OOPSLA2 (October 2023). [https://www.microsoft.com/en-us/research/](https://www.microsoft.com/en-us/research/publication/when-concurrency-matters-behaviour-oriented-concurrency/)
    [publication/when-concurrency-matters-behaviour-oriented-concurrency/](https://www.microsoft.com/en-us/research/publication/when-concurrency-matters-behaviour-oriented-concurrency/)'
  - '- <span id="page-12-7"></span>[21] William W. Collier. 1992. Reasoning about
    parallel architectures. Prentice Hall.'
  - '- <span id="page-12-36"></span>[22] Mingyao Yang Dave Dice, Hui Huang. 2001.
    Asymmetric Dekker Synchronization. [http://web.archive.org/web/20070214114205/http://blogs.sun.com/dave/](http://web.archive.org/web/20070214114205/http://blogs.sun.com/dave/resource/Asymmetric-Dekker-Synchronization.txt)
    [resource/Asymmetric-Dekker-Synchronization.txt](http://web.archive.org/web/20070214114205/http://blogs.sun.com/dave/resource/Asymmetric-Dekker-Synchronization.txt)'
  - '- <span id="page-12-19"></span>[23] Hernán Ponce de León and Johannes Kinder.
    2021. Cats vs. Spectre: An Axiomatic Approach to Modeling Speculative Execution
    Attacks. CoRR abs/2108.13818 (2021). arXiv[:2108.13818 https://arxiv.org/abs/2108.13818](https://arxiv.org/abs/2108.13818)'
  - '- <span id="page-12-20"></span>[24] Hernán Ponce de León and Johannes Kinder.
    2022. Cats vs. Spectre: An Axiomatic Approach to Modeling Speculative Execution
    Attacks. In 43rd IEEE Symposium on Security and Privacy, SP 2022, San Francisco,
    CA, USA, May 22-26, 2022. IEEE, 235–248. [doi:10.1109/SP46214.2022.9833774](https://doi.org/10.1109/SP46214.2022.9833774)'
  - '- <span id="page-12-29"></span>[25] Will Deacon, Jade Alglave, Nikos Nikoleris,
    and Artem Khyzha. 2023. The ARMv8 Application Level Memory Model. [https://github.com/herd/herdtools7/](https://github.com/herd/herdtools7/blob/master/herd/libdir/aarch64.cat)
    [blob/master/herd/libdir/aarch64.cat](https://github.com/herd/herdtools7/blob/master/herd/libdir/aarch64.cat)
    (accessed 2019-07-01). Accessed 2024-11-19.'
  - '- <span id="page-12-37"></span>[26] Dave Dice. 2006. Biased Locking in Hotspot.
    Oracle Blog, Wayback Machine. [http://web.archive.org/web/20150320095550/https://blogs.oracle.com/](http://web.archive.org/web/20150320095550/https://blogs.oracle.com/dave/entry/biased_locking_in_hotspot)
    [dave/entry/biased\\_locking\\_in\\_hotspot](http://web.archive.org/web/20150320095550/https://blogs.oracle.com/dave/entry/biased_locking_in_hotspot)'
  - '- <span id="page-12-38"></span>[27] David Dice, Mark S. Moir, and William N.
    Scherer III. 2010. United States Patent US 7814488B1 Quickly Reacquirable Locks.
    United Statess Patent Office.'
  - '- <span id="page-12-8"></span>[28] Shaked Flur, Kathryn E. Gray, Christopher
    Pulte, Susmit Sarkar, Ali Sezgin, Luc Maranget, Will Deacon, and Peter Sewell.
    2016. Modelling the ARMv8 architecture, operationally: concurrency and ISA. In
    Proceedings of the 43rd ACM SIGPLAN-SIGACT Symposium on Principles of Programming
    Languages (St. Petersburg, FL, USA). 608–621. [doi:10.1145/2837614.2837615](https://doi.org/10.1145/2837614.2837615)'
  - '- <span id="page-12-16"></span>[29] Shaked Flur, Susmit Sarkar, Christopher Pulte,
    Kyndylan Nienhuis, Luc Maranget, Kathryn E. Gray, Ali Sezgin, Mark Batty, and
    Peter Sewell. 2017. Mixed-size concurrency: ARM, POWER, C/C++11, and SC. In Proceedings
    of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages, POPL
    2017, Paris, France, January 18-20, 2017, Giuseppe Castagna and Andrew D. Gordon
    (Eds.). ACM, 429–442. [doi:10.1145/3009837.3009839](https://doi.org/10.1145/3009837.3009839)'
  - '- <span id="page-12-17"></span>[30] Kourosh Gharachorloo. 1995. Memory Consistency
    Models for Shared-Memory Multiprocessors. Ph. D. Dissertation. Stanford University.'
  - '- <span id="page-12-18"></span>[31] Kourosh Gharachorloo, Daniel Lenoski, James
    Laudon, Phillip B. Gibbons, Anoop Gupta, and John L. Hennessy. 1990. Memory Consistency
    and Event Ordering in Scalable Shared-Memory Multiprocessors. In Proceedings of
    the 17th Annual International Symposium on Computer Architecture, Seattle, WA,
    USA, June 1990, Jean-Loup Baer, Larry Snyder, and James R. Goodman (Eds.). ACM,
    15–26. [doi:10.](https://doi.org/10.1145/325164.325102) [1145/325164.325102](https://doi.org/10.1145/325164.325102)'
  - '- <span id="page-12-9"></span>[32] Kathryn E. Gray, Gabriel Kerneis, Dominic
    P. Mulligan, Christopher Pulte, Susmit Sarkar, and Peter Sewell. 2015. An integrated
    concurrency and core-ISA architectural envelope definition, and test oracle, for
    IBM POWER multiprocessors. In Proceedings of the 48th International Symposium
    on Microarchitecture (Waikiki). 635–646. [doi:10.1145/2830772.2830775](https://doi.org/10.1145/2830772.2830775)'
  - '- <span id="page-12-32"></span>[33] Siddharth Gupta, Yuanlong Li, Qingxuan Kang,
    Abhishek Bhattacharjee, Babak Falsafi, Yunho Oh, and Mathias Payer. 2023. Imprecise
    Store Exceptions. In Proceedings of the 50th Annual International Symposium on
    Computer Architecture, ISCA 2023, Orlando, FL, USA, June 17-21, 2023, Yan Solihin
    and Mark A. Heinrich (Eds.). ACM, 52:1–52:15. [doi:10.1145/3579371.3589087](https://doi.org/10.1145/3579371.3589087)'
  - '- <span id="page-12-0"></span>[34] John L. Hennessy and David A. Patterson. 2012.
    Computer Architecture: A Quantitative Approach (5 ed.). Morgan Kaufmann, Amsterdam.'
  - '- <span id="page-12-13"></span>[35] Naorin Hossain, Caroline Trippel, and Margaret
    Martonosi. 2020. TransForm: Formally Specifying Transistency Models and Synthesizing
    Enhanced Litmus Tests. CoRR abs/2008.03578 (2020). arXiv[:2008.03578 https://arxiv.org/abs/2008.](https://arxiv.org/abs/2008.03578)
    [03578](https://arxiv.org/abs/2008.03578)'
  - '- <span id="page-12-22"></span>[36] Luc Maranget. 2024. Personal communication.'
  - '- <span id="page-12-10"></span>[37] Intel. 2002. A Formal Specification of Intel
    Itanium Processor Family Memory Ordering. [developer.intel.com/design/itanium/downloads/251429.htm.](developer.intel.com/design/itanium/downloads/251429.htm)'
  - '- <span id="page-12-27"></span>[38] Radha Jagadeesan, Alan Jeffrey, and James
    Riely. 2020. Pomsets with preconditions: a simple model of relaxed memory. Proc.
    ACM Program. Lang. 4, OOPSLA (2020), 194:1–194:30. [doi:10.1145/3428262](https://doi.org/10.1145/3428262)'
  - '- <span id="page-12-28"></span>[39] Jeehoon Kang, Chung-Kil Hur, Ori Lahav, Viktor
    Vafeiadis, and Derek Dreyer. 2017. A promising semantics for relaxed-memory concurrency.
    In Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming
    Languages (Paris, France) (POPL ''17). Association for Computing Machinery, New
    York, NY, USA, 175–189. [doi:10.1145/3009837.3009850](https://doi.org/10.1145/3009837.3009850)'
  - '- <span id="page-12-39"></span>[40] Kiyokuni Kawachiya. 2005. Java Locks: Analysis
    and Acceleration. Ph. D. Dissertation. Keio University.'
  - '- <span id="page-12-40"></span>[41] Kiyokuni Kawachiya, Akira Koseki, and Tamiya
    Onodera. 2002. Lock reservation: Java locks can mostly do without atomic operations.
    In Proceedings of the 17th ACM SIGPLAN Conference on Object-Oriented Programming,
    Systems, Languages, and Applications (Seattle, Washington, USA) (OOPSLA ''02).
    Association for Computing Machinery, New York, NY, USA, 130–141. [doi:10.1145/582419.582433](https://doi.org/10.1145/582419.582433)'
  - '- <span id="page-12-24"></span>[42] Michalis Kokologiannakis, Ori Lahav, Konstantinos
    Sagonas, and Viktor Vafeiadis. 2017. Effective stateless model checking for C/C++
    concurrency. Proc. ACM Program. Lang. 2, POPL, Article 17 (dec 2017), 32 pages.
    [doi:10.1145/3158105](https://doi.org/10.1145/3158105)'
  - '- [43] Michalis Kokologiannakis, Azalea Raad, and Viktor Vafeiadis. 2019. Model
    checking for weakly consistent libraries. In Proceedings of the 40th ACM SIGPLAN
    Conference on Programming Language Design and Implementation (Phoenix, AZ, USA)
    (PLDI 2019). Association for Computing Machinery, New York, NY, USA,'
  - <span id="page-13-0"></span>96–110. [doi:10.1145/3314221.3314609](https://doi.org/10.1145/3314221.3314609)
  - '- <span id="page-13-16"></span>[44] Michalis Kokologiannakis and Viktor Vafeiadis.
    2021. GenMC: A Model Checker for Weak Memory Models. In Computer Aided Verification,
    Alexandra Silva and K. Rustan M. Leino (Eds.). Springer International Publishing,
    Cham, 427–440. [doi:10.1007/978-3-030-81685-8\\_20](https://doi.org/10.1007/978-3-030-81685-8_20)'
  - '- <span id="page-13-24"></span>[45] Daniel Kroening, Lihao Liang, Tom Melham,
    Peter Schrammel, and Michael Tautschnig. 2015. Effective Verification of Low-Level
    Software with Nested Interrupts. In Proceedings of the 2015 Design, Automation
    & Test in Europe Conference & Exhibition, DATE 2015, Grenoble, France, March 9-13,
    2015, Wolfgang Nebel and David Atienza (Eds.). EDA Consortium, 229–234. [http://www.cs.ox.ac.uk/tom.](http://www.cs.ox.ac.uk/tom.melham/pub/Kroening-2015-EVL.pdf)
    [melham/pub/Kroening-2015-EVL.pdf](http://www.cs.ox.ac.uk/tom.melham/pub/Kroening-2015-EVL.pdf)'
  - '- <span id="page-13-15"></span>[46] Ori Lahav, Viktor Vafeiadis, Jeehoon Kang,
    Chung-Kil Hur, and Derek Dreyer. 2017. Repairing sequential consistency in C/C++11.
    In Proceedings of the 38th ACM SIGPLAN Conference on Programming Language Design
    and Implementation (Barcelona, Spain) (PLDI 2017). Association for Computing Machinery,
    New York, NY, USA, 618–632. [doi:10.1145/3062341.3062352](https://doi.org/10.1145/3062341.3062352)'
  - '- <span id="page-13-25"></span>[47] Lihao Liang, Tom Melham, Daniel Kroening,
    Peter Schrammel, and Michael Tautschnig. 2017. Effective Verification for Low-Level
    Software with Competing Interrupts. ACM Transactions on Embedded Computing Systems
    17, 2 (December 2017), 36:1–36:26. [doi:10.1145/3147432](https://doi.org/10.1145/3147432)'
  - '- <span id="page-13-12"></span>[48] Daniel Lustig, Geet Sethi, Margaret Martonosi,
    and Abhishek Bhattacharjee. 2016. COATCheck: Verifying Memory Ordering at the
    Hardware-OS Interface. In Proceedings of the Twenty-First International Conference
    on Architectural Support for Programming Languages and Operating Systems, ASPLOS
    2016, Atlanta, GA, USA, April 2-6, 2016, Tom Conte and Yuanyuan Zhou (Eds.). ACM,
    233–247. [doi:10.1145/2872362.2872399](https://doi.org/10.1145/2872362.2872399)'
  - '- <span id="page-13-22"></span>[49] Patricio Chilano Mateo. 2021. JEP 374: Deprecate
    and Disable Biased Locking. JDK Enhancement Proposal.<https://openjdk.org/jeps/374>'
  - '- <span id="page-13-20"></span>[50] Paul E. McKenney. 2023. Is Parallel Programming
    Hard, And, If So, What Can You Do About It? [https://mirrors.edge.kernel.org/pub/linux/kernel/people/paulmck/](https://mirrors.edge.kernel.org/pub/linux/kernel/people/paulmck/perfbook/perfbook.html)
    [perfbook/perfbook.html](https://mirrors.edge.kernel.org/pub/linux/kernel/people/paulmck/perfbook/perfbook.html)'
  - '- <span id="page-13-19"></span>[51] Paul E. McKenney. 2024. RCU Concepts. [https://www.kernel.org/doc/](https://www.kernel.org/doc/Documentation/RCU/rcu.txt)
    [Documentation/RCU/rcu.txt](https://www.kernel.org/doc/Documentation/RCU/rcu.txt)
    Accessed 2024-11-19.'
  - '- <span id="page-13-1"></span>[52] Paul E McKenney and John D Slingwine. 1998.
    Read-copy update: Using execution history to solve concurrency problems. In Parallel
    and Distributed Computing and Systems, Vol. 509518. 509–518.'
  - '- <span id="page-13-11"></span>[53] Nicholas Mosier, Hanna Lachnitt, Hamed Nemati,
    and Caroline Trippel. 2022. Axiomatic hardware-software contracts for security.
    In ISCA ''22: The 49th Annual International Symposium on Computer Architecture,
    New York, New York, USA, June 18 - 22, 2022, Valentina Salapura, Mohamed Zahran,
    Fred Chong, and Lingjia Tang (Eds.). ACM, 72–86. [doi:10.1145/3470496.3527412](https://doi.org/10.1145/3470496.3527412)'
  - '- <span id="page-13-21"></span>[54] Matthew J. Parkinson. 2024. Some things I
    wish I hadn''t seen. presented at The Future of Weak Memory 2024.'
  - '- <span id="page-13-17"></span>[55] Jean Pichon-Pharabod and Peter Sewell. 2016.
    A concurrency semantics for relaxed atomics that permits optimisation and avoids
    thin-air executions. In Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium
    on Principles of Programming Languages, POPL 2016, St. Petersburg, FL, USA, January
    20 - 22, 2016, Rastislav Bodík and Rupak Majumdar (Eds.). ACM, 622–633. [doi:10.1145/](https://doi.org/10.1145/2837614.2837616)
    [2837614.2837616](https://doi.org/10.1145/2837614.2837616)'
  - '- <span id="page-13-18"></span>[56] William W. Pugh. 1999. Fixing the Java Memory
    Model. In Proceedings of the ACM 1999 Conference on Java Grande, JAVA ''99, San
    Francisco, CA, USA, June 12-14, 1999, Geoffrey C. Fox, Klaus E. Schauser, and
    Marc Snir (Eds.). ACM, 89–98. [doi:10.1145/304065.304106](https://doi.org/10.1145/304065.304106)'
  - '- <span id="page-13-2"></span>[57] Christopher Pulte. 2018. The Semantics of
    Multicopy Atomic ARMv8 and RISC-V. Ph. D. Dissertation. University of Cambridge.
    [https://www.repository.cam.ac.uk/](https://www.repository.cam.ac.uk/handle/1810/292229)
    [handle/1810/292229.](https://www.repository.cam.ac.uk/handle/1810/292229)'
  - '- <span id="page-13-3"></span>[58] Christopher Pulte, Shaked Flur, Will Deacon,
    Jon French, Susmit Sarkar, and Peter Sewell. 2018. Simplifying ARM concurrency:
    multicopy-atomic axiomatic and operational models for ARMv8. Proc. ACM Program.
    Lang. 2, POPL (2018), 19:1–19:29. [doi:10.1145/3158107](https://doi.org/10.1145/3158107)'
  - '- <span id="page-13-23"></span>[59] Kenneth Russell and David Detlefs. 2006.
    Eliminating synchronization-related atomic operations with biased locking and
    bulk rebiasing. In Proceedings of the 21st Annual ACM SIGPLAN Conference on Object-Oriented
    Programming Systems, Languages, and Applications (Portland, Oregon, USA) (OOPSLA
    ''06). Association for Computing Machinery, New York, NY, USA, 263–272. [doi:10.1145/1167473.](https://doi.org/10.1145/1167473.1167496)
    [1167496](https://doi.org/10.1145/1167473.1167496)'
  - '- <span id="page-13-4"></span>[60] Susmit Sarkar, Kayvan Memarian, Scott Owens,
    Mark Batty, Peter Sewell, Luc Maranget, Jade Alglave, and Derek Williams. 2012.
    Synchronising C/C++ and POWER. In ACM SIGPLAN Conference on Programming Language
    Design and Implementation, PLDI ''12, Beijing, China - June 11 - 16, 2012, Jan
    Vitek, Haibo Lin, and Frank Tip (Eds.). ACM, 311–322. [doi:10.1145/2254064.2254102](https://doi.org/10.1145/2254064.2254102)'
  - '- <span id="page-13-10"></span>[61] Susmit Sarkar, Peter Sewell, Jade Alglave,
    Luc Maranget, and Derek Williams. 2011. Understanding POWER multiprocessors. In
    Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design
    and Implementation, PLDI 2011, San Jose, CA, USA, June 4-8, 2011, Mary W. Hall
    and David A. Padua (Eds.). ACM, 175–186. [doi:10.1145/1993498.1993520](https://doi.org/10.1145/1993498.1993520)'
  - '- <span id="page-13-5"></span>[62] Susmit Sarkar, Peter Sewell, Francesco Zappa
    Nardelli, Scott Owens, Tom Ridge, Thomas Braibant, Magnus O. Myreen, and Jade
    Alglave. 2009. The semantics of'
  - x86-CC multiprocessor machine code. In Proceedings of the 36th ACM SIGPLAN-SIGACT
    Symposium on Principles of Programming Languages, POPL 2009, Savannah, GA, USA,
    January 21-23, 2009, Zhong Shao and Benjamin C. Pierce (Eds.). ACM, 379–391. [doi:10.1145/1480881.1480929](https://doi.org/10.1145/1480881.1480929)
  - '- <span id="page-13-14"></span>[63] Peter Sewell, Christopher Pulte, Shaked Flur,
    Mark Batty, Luc Maranget, and Alasdair Armstrong. 2022. Multicore Semantics: Making
    Sense of Relaxed Memory (MPhil slides).<https://www.cl.cam.ac.uk/~pes20/slides-acs-2022.pdf>'
  - '- <span id="page-13-6"></span>[64] P. Sewell, S. Sarkar, S. Owens, F. Zappa Nardelli,
    and M. O. Myreen. 2010. x86-TSO: A Rigorous and Usable Programmer''s Model for
    x86 Multiprocessors. Commun. ACM 53, 7 (July 2010), 89–97. [doi:10.1145/1785414.1785443](https://doi.org/10.1145/1785414.1785443)'
  - '- <span id="page-13-13"></span>[65] Ben Simner, Alasdair Armstrong, Thomas Bauereiss,
    Brian Campbell, Ohad Kammar, Jean Pichon-Pharabod, and Peter Sewell. 2024. Relaxed
    exception semantics for Arm-A (extended version). CoRR abs/2412.15140 (2024).
    [doi:10.](https://doi.org/10.48550/ARXIV.2412.15140) [48550/ARXIV.2412.15140](https://doi.org/10.48550/ARXIV.2412.15140)
    arXiv[:2412.15140](https://arxiv.org/abs/2412.15140)'
  - '- <span id="page-13-9"></span>[66] Ben Simner, Alasdair Armstrong, Jean Pichon-Pharabod,
    Christopher Pulte, Richard Grisenthwaite, and Peter Sewell. 2022. Relaxed virtual
    memory in Armv8-A. In Proceedings of the 31st European Symposium on Programming
    (Lecture Notes in Computer Science, Vol. 13240). Springer, 143–173. [doi:10.1007/978-3-](https://doi.org/10.1007/978-3-030-99336-8_6)
    [030-99336-8\\_6](https://doi.org/10.1007/978-3-030-99336-8_6)'
  - '- <span id="page-13-8"></span>[67] Ben Simner, Shaked Flur, Christopher Pulte,
    Alasdair Armstrong, Jean Pichon-Pharabod, Luc Maranget, and Peter Sewell. 2020.
    ARMv8-A System Semantics: Instruction Fetch in Relaxed Architectures. In Programming
    Languages and Systems - 29th European Symposium on Programming, ESOP 2020, Held
    as Part of the European Joint Conferences on Theory and Practice of Software,
    ETAPS 2020, Dublin, Ireland, April 25-30, 2020, Proceedings (Lecture Notes in
    Computer Science, Vol. 12075), Peter Müller (Ed.). Springer, 626–655. [doi:10.1007/978-3-030-44914-8\\_23](https://doi.org/10.1007/978-3-030-44914-8_23)'
  - '- <span id="page-13-7"></span>[68] P. S. Sindhu, J.-M. Frailong, and M. Cekleov.
    1991. Formal Specification of Memory Models. In Scalable Shared Memory Multiprocessors.
    Kluwer, 25–42. [doi:10.1007/978-1-4615-3604-8\\_2](https://doi.org/10.1007/978-1-4615-3604-8_2)'
- id: abstract_ray_tracing_is_a_rendering_technique_to_simulate_the_way_light_interacts_with_objects_to_create_realistic_images_it_has_become_prominent_thanks_to_the_latest_hardware_support_on_graphics_processing_units_gpus_i_e_the_ray_tracing_rt_unit_specially_designed_to_accelerate_ray_tracing_operations_despite_such_hardware_advances_ray_tracing_remains_a_performance_bottleneck_for_high_performance_graphics_workloads_such_as_real_time_path_tracing_pt_which_is_an_application_of_ray_tracing_where_multiple_bouncing_rays_are_traced_per_pixel_the_key_reasons_are_a_the_costly_bounding_volume_hierarchy_bvh_traversal_operation_and_b_low_single_instruction_multiple_thread_simt_efficiency_as_the_rays_in_the_same_warp_deviate_inevitably_in_their_traversal_paths
  title: Abstract
  abstract: 'Ray Tracing is a rendering technique to simulate the way light interacts
    with objects to create realistic images. It has become prominent thanks to the
    latest hardware support on Graphics Processing Units (GPUs), i.e., the Ray-Tracing
    (RT) unit, specially designed to accelerate ray tracing operations. Despite such
    hardware advances, ray tracing remains a performance bottleneck for high-performance
    graphics workloads, such as real-time path tracing (PT), which is an application
    of ray tracing where multiple bouncing rays are traced per pixel. The key reasons
    are (a) the costly Bounding Volume Hierarchy (BVH) traversal operation, and (b)
    low Single-Instruction-Multiple-Thread (SIMT) efficiency as the rays in the same
    warp deviate inevitably in their traversal paths.

    In this work, we propose a novel architecture design for cooperative BVH traversal
    that exploits the parallelism present in the BVH traversal process. The key idea
    of our CoopRT scheme is to make use of the idle threads, either completely inactive
    when the ray tracing instruction is executed or partially idle due to early completion,
    to help the long running threads in the same warp. Specifically, we enable idle
    threads in a GPU warp to utilize their readily available traversal hardware to
    help traverse the BVH tree for the busy threads, therefore helping them finish
    their traversal much faster. This approach is implemented purely in hardware,
    requiring no changes to the programming model. We present our architecture design
    and show that it only involves small changes to the existing RT unit.

    We evaluated CoopRT in Vulkan-sim, a cycle-level simulator, and observed up to
    5.11x speedup over the baseline, with a geometric mean of 2.15x speedup at the
    cost of a moderate area overhead of 3.0% of the warp buffer in the RT unit. Using
    the energy-delay product, our CoopRT achieves an average of 2.29x improvement
    over the baseline.'
  keywords: Ray tracing, GPU, 3D graphics
  document: '![](_page_0_Picture_0.jpeg)


    ![](_page_0_Picture_1.jpeg)


    [Yavuz Selim Tozlu](https://orcid.org/0009-0001-4153-2674) North Carolina State
    University Raleigh, North Carolina, USA ystozlu@ncsu.edu


    ### Abstract


    Ray Tracing is a rendering technique to simulate the way light interacts with
    objects to create realistic images. It has become prominent thanks to the latest
    hardware support on Graphics Processing Units (GPUs), i.e., the Ray-Tracing (RT)
    unit, specially designed to accelerate ray tracing operations. Despite such hardware
    advances, ray tracing remains a performance bottleneck for high-performance graphics
    workloads, such as real-time path tracing (PT), which is an application of ray
    tracing where multiple bouncing rays are traced per pixel. The key reasons are
    (a) the costly Bounding Volume Hierarchy (BVH) traversal operation, and (b) low
    Single-Instruction-Multiple-Thread (SIMT) efficiency as the rays in the same warp
    deviate inevitably in their traversal paths.


    In this work, we propose a novel architecture design for cooperative BVH traversal
    that exploits the parallelism present in the BVH traversal process. The key idea
    of our CoopRT scheme is to make use of the idle threads, either completely inactive
    when the ray tracing instruction is executed or partially idle due to early completion,
    to help the long running threads in the same warp. Specifically, we enable idle
    threads in a GPU warp to utilize their readily available traversal hardware to
    help traverse the BVH tree for the busy threads, therefore helping them finish
    their traversal much faster. This approach is implemented purely in hardware,
    requiring no changes to the programming model. We present our architecture design
    and show that it only involves small changes to the existing RT unit.


    We evaluated CoopRT in Vulkan-sim, a cycle-level simulator, and observed up to
    5.11x speedup over the baseline, with a geometric mean of 2.15x speedup at the
    cost of a moderate area overhead of 3.0% of the warp buffer in the RT unit. Using
    the energy-delay product, our CoopRT achieves an average of 2.29x improvement
    over the baseline.


    ## CCS Concepts


    • Computing methodologies → Graphics systems and interfaces; • Computer systems
    organization → Multicore architectures; Single instruction, multiple data.


    ISCA ''25, Tokyo, Japan


    <https://doi.org/10.1145/3695053.3731118>


    # [Huiyang Zhou](https://orcid.org/0000-0003-2133-0722)


    North Carolina State University Raleigh, North Carolina, USA hzhou@ncsu.edu


    ## Keywords


    Ray tracing, GPU, 3D graphics


    #### ACM Reference Format:


    Yavuz Selim Tozlu and Huiyang Zhou. 2025. CoopRT: Accelerating BVH Traversal for
    Ray Tracing via Cooperative Threads. In Proceedings of the 52nd Annual International
    Symposium on Computer Architecture (ISCA ''25), June 21–25, 2025, Tokyo, Japan.
    ACM, New York, NY, USA, [14](#page-13-0) pages. [https:](https://doi.org/10.1145/3695053.3731118)
    [//doi.org/10.1145/3695053.3731118](https://doi.org/10.1145/3695053.3731118)


    ## 1 Introduction


    Ray tracing (RT) is a 3D rendering technique that can achieve lifelike graphics
    by simulating how light rays travel through 3D scenes. It has been used in offline
    rendering for movies and animations [\[16\]](#page-12-0). With the special hardware
    support available in GPUs now, ray tracing is also being used in real-time applications
    such as video games [\[1\]](#page-11-0)[\[17\]](#page-12-1). Modern game engines
    adopt various ray tracing algorithms, such as ray-traced shadows and reflections
    [\[6\]](#page-11-1)[\[7\]](#page-11-2), which can be used in conjunction with
    traditional rasterization techniques to enhance visual realism. Beyond computer
    graphics, ray tracing is also used in various fields such as wireless communication
    for channel estimation, propagation modeling, and 3D imaging [\[20\]](#page-12-2)[\[27\]](#page-12-3)[\[28\]](#page-12-4)[\[39\]](#page-12-5)[\[43\]](#page-12-6).


    In practice, ray tracing involves traversing a tree-like structure called the
    Bounding Volume Hierarchy (BVH) tree [\[25\]](#page-12-7). A 3D scene is encoded
    as a hierarchical arrangement of Axis-Aligned Bounding Boxes (AABBs), where objects
    are grouped based on their proximity and size. These groups are enclosed in AABBs,
    and the collection of AABBs forms the BVH tree. Rays are modeled as 3D vectors,
    which traverse the BVH tree and test intersection against the AABBs to find the
    closest-hit primitive, a basic geometric shape such as a triangle, quad, or sphere.


    The BVH tree can become large depending on the scene''s complexity and often exceeds
    the capacity of on-chip caches. A traversal involves reading BVH nodes, performing
    intersections, and processing the subsequent nodes based on the intersection results.
    In large scenes, these memory accesses often miss in the caches and require expensive
    DRAM accesses, during which the threads stall and wait for the data to return.
    In addition, each thread traces a ray with different directions, causing the rays
    to diverge and traverse different portions of the BVH tree. As a result, the traversal
    process can be slow and memory-bound, making it a primary bottleneck in ray tracing
    performance.


    Latest consumer-grade GPUs incorporate a specialized hardware unit called the
    RT unit to accelerate ray tracing and enable realtime performance [\[3\]](#page-11-3)[\[4\]](#page-11-4)[\[5\]](#page-11-5).
    The RT unit is triggered when a warp encounters the specialized trace\_ray instruction.
    The semantics of this instruction is that each thread in the warp traces a ray
    by


    Permission to make digital or hard copies of all or part of this work for personal
    or classroom use is granted without fee provided that copies are not made or distributed
    for profit or commercial advantage and that copies bear this notice and the full
    citation on the first page. Copyrights for components of this work owned by others
    than the author(s) must be honored. Abstracting with credit is permitted. To copy
    otherwise, or republish, to post on servers or to redistribute to lists, requires
    prior specific permission and/or a fee. Request permissions from permissions@acm.org.


    <sup>©</sup> 2025 Copyright held by the owner/author(s). Publication rights licensed
    to ACM. ACM ISBN 979-8-4007-1261-6/25/06


    <span id="page-1-0"></span>![](_page_1_Figure_1.jpeg)


    Figure 1: Pipeline stalls from different instructions, RT: trace\_ray instructions,
    MEM: load/store instructions from CUDA cores, ALU: compute instructions from CUDA
    cores, SFU: special function instructions from CUDA cores. Configuration: 256x256
    resolution, path tracing, 1 sample-per-pixel.


    traversing the BVH tree and performing intersection tests to find the closest-hit
    primitive, if one exists. A warp of 32 threads traces up to 32 rays if all the
    threads are active.


    Despite the hardware support for ray tracing, performing realtime ray tracing,
    especially path tracing, is still very challenging. The key reasons are (1) for
    a high-resolution image, the number of rays to be traced is very high; (2) BVH
    traversals are memory-bound, dominated by reading tree nodes, with little computation
    needed mainly for intersection tests and coordinate transformations; and (3) there
    are high degrees of divergence as different rays follow different traversal paths
    and bounce in different directions.


    In Fig. [1,](#page-1-0) we present the contribution of pipeline stalls from different
    types of instructions across various scenes in a ray tracing application. It can
    be seen that most of the stalls are due to the trace\_ray instructions.


    To reveal the performance issues with the trace\_ray instructions, we look into
    path tracing (PT), which is a specific application of ray tracing to render a
    2D image of a 3D scene [\[13\]](#page-12-8)[\[14\]](#page-12-9). With PT, a primary
    ray is cast for each pixel, bouncing through the scene for a set number of times
    or until it hits a light source or escapes the scene. Each bounce contributes
    to the final color of the pixel and involves a trace\_ray instruction (more details
    in Section [3\)](#page-2-0). As the rays bounce through the scene with different
    paths, their traversing behaviors diverge. Given the SIMD nature of a warp, such
    divergence results in two levels of resource under-utilization. Firstly, as a
    ray misses/escapes the scene, the corresponding thread becomes inactive for subsequent
    bounces or subsequent trace\_ray instructions. As a result, when a trace\_ray
    instruction is dispatched to the RT unit, some of the threads may be completely
    inactive or idle. As the active threads continue tracing the bouncing rays through
    the scene, the number of idle threads in a warp typically increases as the number
    of bounces increases. Fig. [2](#page-1-1) shows how the ratio of active threads
    in a warp on average changes over time for various scenes on the baseline RT unit.
    We can see that all the warps have 100% SIMT efficiency (i.e., 100% active threads)
    when tracing the primary rays at the beginning. After only a few bounces, the
    efficiency starts to drop significantly. Secondly, among active threads, some
    may finish their traversal earlier than others, thereby being partially idle during
    the trace\_ray instruction execution.


    With the observation that there exists significant divergence and the resulting
    resource under-utilization during ray tracing, we


    <span id="page-1-1"></span>![](_page_1_Figure_9.jpeg)


    Figure 2: Percent of threads that are busy in RT unit.


    propose a novel way to overcome this fundamental performance challenge. Our idea
    is to parallelize BVH traversal by employing the abundant idle threads. The key
    insight is for a single ray, in order to determine its closest-hit in the BVH
    tree, the traversal can be effectively parallelized without error. In other words,
    the workload of one active thread in a warp in the RT unit performing the BVH
    tree traversal can be parallelized and distributed among multiple threads. Considering
    that each thread already has dedicated traversal hardware in the RT unit, if we
    enable the idle threads to access the active threads'' traversal stacks, the idle
    threads can traverse the BVH as usual, i.e., processing the nodes, and pushing
    new nodes into their own stacks based on the intersection test results. In our
    proposed scheme, the cooperation of threads is done completely in the RT unit
    hardware and is transparent to the software. Therefore, the cooperative execution
    only changes how a trace\_ray instruction is executed in the RT unit and does
    not affect rest of the GPU hardware or software.


    In summary, this paper makes the following contributions,


    - We propose a cooperative BVH traversal algorithm that utilizes existing GPU
    hardware to parallelize BVH traversal and accelerate ray tracing with no changes
    to the programming model.

    - We propose the architecture design for cooperative BVH traversal.

    - We model the cooperative BVH traversal in Vulkan-sim, a cycle-level simulator,
    to evaluate its performance and show that our CoopRT scheme achieves up to 5.11x
    with a geometric mean of 2.15x speedup.


    ### 2 Background


    ### 2.1 Ray Tracing and BVH Traversal


    Ray tracing is a 3D computer graphics method that can produce highly realistic
    visuals. It simulates light rays through a 3D scene and calculates how much illumination
    the objects would receive. Rays are modeled as 3D vectors with an origin, usually
    the camera, and a direction through the pixels on the 2D image plane. These rays
    are tested against the primitives in the scene to find the closesthit or detect
    if there is any-hit at all. This can be used to accurately compute global illumination,
    shadows, reflections, or any lighting effect in a scene. Therefore, ray tracing
    is commonly used together with rasterization to augment the lighting effects,
    and still maintain real-time performance.


    Ray tracing can also be used exclusively to render 2D images of 3D scenes with
    a technique known as path tracing (PT) [\[30\]](#page-12-10). In PT, primary rays
    originating from the camera are traced until they either miss the scene or hit
    an object. Upon hitting an object, the


    direction of reflection is calculated, and subsequent bouncing rays are traced.
    This bouncing process continues until a preset number of times determined by the
    programmer, 16 in this study, or until the rays hit a light source or miss the
    scene. Each bounce contributes to the pixels'' final color.


    Objects in a 3D scene are usually modeled as a collection of triangles. While
    it is possible to test rays against every triangle in the scene to find the closest-hit
    object, this quickly becomes impractical due to the large number of triangles
    in a scene. To accelerate ray tracing, BVH is utilized to encode the 3D scene
    in a tree-like structure of AABBs, where the root node is an AABB that bounds
    the whole scene [\[23\]](#page-12-11)[\[25\]](#page-12-7)[\[29\]](#page-12-12)[\[36\]](#page-12-13).
    When traversing a BVH tree, if a ray does not intersect an AABB, then it is guaranteed
    that the ray will not intersect any primitives inside the AABB, therefore they
    can be skipped.


    BVH can be built on the CPU or the GPU using various methods such as the surface
    area heuristic [\[32\]](#page-12-14). The traversal starts from the root node
    and typically follows a depth-first search (DFS). In DFS, a traversal stack is
    used to track nodes. The traversal stack stores the addresses of the nodes that
    will be accessed afterwards. A BVH tree node can be either an internal node or
    a leaf node. Internal nodes have child nodes, and they contain the coordinates
    and addresses of their children. Leaf nodes are primitives such as triangles or
    quads, and they contain the vertex coordinates of the primitive. Traversal involves
    popping nodes from the stack and fetching them from memory. For internal nodes,
    their children are tested for intersections and pushed onto the stack if a hit
    is detected. For leaf nodes, the primitives are directly tested for intersections.
    Traversal continues until the stack is empty, or any-hit is found, based on the
    criteria defined by the programmer according to the application''s needs.


    High-performance ray tracing applications are typically run on GPUs. Modern graphics
    application programming interfaces (API), such as Vulkan [\[41\]](#page-12-15)
    or DirectX, feature sophisticated ray tracing pipelines that programmers can use
    to develop ray tracing applications. The ray tracing pipeline introduces programmable
    shader stages such as raygen, closest-hit, miss, and optionally intersection and
    any-hit. Rays are generated in the raygen shader using the trace\_ray instruction.
    If a hit primitive is found, closest-hit shader is invoked to carry out reflection/illumination
    calculations, otherwise; the miss shader is invoked. Custom geometry, such as
    spheres, can be defined using the intersection shader. The any-hit shader is executed
    for each potential intersection, allowing for effects like transparency by conditionally
    accepting or discarding hits along the ray''s path.


    In GPU ray tracing applications, the BVH is built by the GPU driver. In this work,
    the open-source ray tracing library Embree 3.14 [\[2\]](#page-11-6) is used to
    build BVH trees for Vulkan-sim.


    ### 2.2 GPU Architecture


    GPUs are throughput oriented massively parallel computers. They consist of thousands
    of hardware threads that can run in parallel [\[31\]](#page-12-16). Fig. [3](#page-3-0)
    shows a generic GPU architecture which we consider as the baseline. A GPU contains
    an array of Streaming Multiprocessors (SM), as in Nvidia terminology. Each SM
    has a collection of execution lanes, register files, warp schedulers, and dedicated


    L1 caches. Shader programs are executed with a thread hierarchy of thread blocks
    (TBs), each composed of warps, with a warp of 32 threads that execute in a lock-step
    manner. TBs are assigned to SMs by the Gigathread Engine [\[40\]](#page-12-17).
    GPUs hide memory latencies by employing fine-grained multithreading. When a warp
    stalls, the warp scheduler issues instructions from another non-stalling warp
    instead. As long as there are enough warps, long instruction execution latencies,
    such as memory latency, can be hidden by continuously scheduling non-stalling
    warps. Modern GPUs can have anywhere from fewer than 10 (Mobile) to over 100 (Desktop)
    SMs, depending on the use case. Typically, there is an L2 cache shared among all
    SMs, connected via a crossbar. For off-chip memory, high-bandwidth DRAM modules
    are used with on-chip memory controllers.


    ### 2.3 Hardware Support for Ray Tracing


    Latest GPUs feature specialized hardware called the RT Unit/Core to efficiently
    perform ray tracing [\[4\]](#page-11-4)[\[5\]](#page-11-5). Vulkan-sim is a GPU
    architectural simulator that models an RT unit, which we use to model our proposed
    CoopRT in this study. The RT unit can be viewed as a specialized execution lane
    operating at warp granularity. When a warp encounters the trace\_ray instruction,
    the instruction is steered to the RT unit, where the hardware performs BVH traversals.
    Specifically, each thread of the warp is assigned a ray and traverses the BVH
    tree to find either the closest-hit or any-hit primitive. As shown in Fig. [3,](#page-3-0)
    an RT unit has a warp buffer which keeps the ray data and traversal stack for
    each thread. At each cycle, a warp from the warp buffer is selected, and a memory
    access request from that warp is served. When the data returns, an intersection
    test is performed by the operation units. Based on the result, either the node''s
    children are pushed to the traversal stack (upon a hit), or if it is a primitive,
    the closest-hit value is updated if needed. The perthread traversal stack stores
    the addresses of the nodes instead of the node data itself. While the RT unit
    does not reduce the latency of memory requests, it accelerates the traversal by
    streamlining the address calculations and intersection tests.


    ### <span id="page-2-0"></span>3 Thread Activity In Ray Tracing


    The latency of ray tracing applications is usually dominated by the CISC-like
    trace\_ray instruction, which traverses the BVH tree, performs intersection tests
    and writes the results back to memory. Listing [1](#page-3-1) shows the anatomy
    of a raygen shader. When compiled, the traceRay() function expands to a block
    of code that includes the performance-dominating trace\_ray instruction, followed
    by control flow instructions that branch off to the closest-hit, any-hit, intersection,
    or miss shaders. As the listing shows, each iteration of the loop processes one
    bounce and invokes a trace\_ray instruction. Like any GPU shaders, the raygen
    shader is executed in the SIMT manner. Due to such SIMT execution, there are two
    sources of hardware resource under-utilization: inactive threads and early finishing
    threads. With rays bouncing further into the scene every iteration, more threads
    become inactive as they miss the scene or hit a light source and exit the loop.
    However, although some threads become inactive, as long as there is at least one
    active thread in the warp, the loop keeps repeating. Inactive threads are masked
    off in hardware and they do not perform any traversal, leading to unused hardware.


    <span id="page-3-0"></span>![](_page_3_Figure_2.jpeg)


    Figure 3: Diagram of the GPU model used in this study. Red blocks indicate the
    modified components. Redrawn from [\[37\]](#page-12-18).


    <span id="page-3-2"></span>![](_page_3_Figure_4.jpeg)


    Figure 4: Thread status distribution.


    In addition, the execution latency of the trace\_ray instruction itself is variable
    among threads, similar to load instructions where some threads hit in the cache,
    while others miss. In the case of trace\_ray instruction, this latency variation
    is much more severe, as some threads might quickly find the closest-hit primitive
    or just miss the scene, whereas other threads might spend a long time traversing
    the BVH to find their closest-hit primitive. We refer to the threads in a warp
    that finish earlier than the others as early finishing threads.


    To quantify the importance of inactive and early finishing threads, we simulate
    the baseline GPU in Vulkan-sim and gather thread status data at fixed intervals
    and average them. Fig. [4](#page-3-2) shows the distribution of thread status
    across different 3D scenes. From the figure, we can see a high number of threads
    spend most of their time being inactive, or finishing early and waiting.


    Note that the divergence behavior in ray tracing (i.e., the raygen shader and
    the trace\_ray instruction) differs from typical GPGPU applications, where the
    programs have relatively more balanced if-then-else paths. In comparison, as shown
    in Listing [1,](#page-3-1) the majority of the execution time is spent inside
    the loop, leaving little to no work for threads that exit the loop. Fig. [5](#page-3-3)
    shows a simplified Control Flow Graph for the program in Listing [1.](#page-3-1)
    Existing SIMT control flow handling techniques could be beneficial for this code
    to some extent [\[18\]](#page-12-19)[\[19\]](#page-12-20)[\[21\]](#page-12-21)[\[22\]](#page-12-22)[\[42\]](#page-12-23).
    Dynamic Warp Formation[\[22\]](#page-12-22) and Thread Block Compaction[\[21\]](#page-12-21)
    could be used to create new warps when threads diverge to blocks 0, 1 and . Similarly,
    multi-path execution[\[19\]](#page-12-20) could execute these blocks in parallel.
    However, when threads diverge at block to and , these techniques become insufficient
    because the latency of is negligible compared to block , which contains the \_
    instruction. Therefore, the existing techniques can mitigate some of the divergence
    in ray tracing, but none of them address the main bottleneck, which is the BVH
    traversal process, i.e., block .


    Different from the existing schemes, CoopRT offers a novel way to exploit the
    inactive or early finishing threads to parallelize and


    <span id="page-3-3"></span>![](_page_3_Figure_10.jpeg)


    Figure 5: A Simplified Control Flow Diagram of Listing [1.](#page-3-1) 0, 1 are
    closest-hit shaders, and is a miss shader. is the loop iteration block.


    accelerate the BVH traversal process, therefore improving the resource utilization
    and significantly reducing the latency of the trace\_ray instruction and the raygen
    shader. More generally, as each trace\_ray instruction essentially performs 32
    DFS operations and the raygen shader can be viewed as a sequence of dependent
    DFS operations, CoopRT provides a novel way to accelerate such DFS operations,
    which has more profound impacts when the RT unit is repurposed for accelerating
    graph algorithms [\[11\]](#page-12-24)[\[26\]](#page-12-25)[\[44\]](#page-12-26).


    <span id="page-3-1"></span>


    | Listing 1: Simplified raygen shader for path tracing. |  |  |  |  |  |

    |-------------------------------------------------------|--|--|--|--|--|

    | //Calculate ray origin and direction                  |  |  |  |  |  |

    | //using pixel coordinates                             |  |  |  |  |  |

    | fo r ( i n t<br>i = 0 ; i <NUM_BOUNCES ; i + +) {     |  |  |  |  |  |

    | traceRay(ray.orig, ray.dir)                           |  |  |  |  |  |

    | i f<br>mi s s e d<br>   <br>! s c a t t e r e d       |  |  |  |  |  |

    | break ;                                               |  |  |  |  |  |

    | //Calculate new origin and direction                  |  |  |  |  |  |

    | //using the hit data                                  |  |  |  |  |  |

    | }                                                     |  |  |  |  |  |


    //Calculate and store pixel color


    ### 4 Cooperative BVH Traversal


    In this section, we first explain how the baseline BVH traversal works. Then,
    we propose our cooperative traversal by parallelizing the traversal process.


    ### 4.1 Baseline BVH Traversal


    The baseline RT unit in Vulkan-sim traverses a BVH using the DFS algorithm, as
    shown in Algorithm [1](#page-4-0) [\[37\]](#page-12-18).


    Algorithm 1: BVH Traversal using DFS to find the closesthit primitive


    ```

    Input: , _

    1 if  intersects _ then

    2 .ℎ(_);

    3 while !. do

    4  ← .();

    5 if . == _ then

    6 for  = 0 to 5 do // 6-ary tree

    7 ℎ[] ← _(, .ℎ []);

    8 if ℎ[] < _ℎ then

    9 .ℎ(.ℎ []);

    10 else // leaf node

    11 ℎ ← _(, );

    12 _ℎ ← (_ℎ, ℎ);

    ```

    The BVH tree in Algorithm [1](#page-4-0) is assumed to be a 6-ary tree, meaning
    each node can have up to 6 child nodes, following the convention used in the MESA
    graphics library and Vulkan-sim. The nodes in the BVH can be internal nodes, which
    are nodes that have up to 6 children, or leaf nodes which are primitives like
    triangles or quads. An internal node contains information such as the coordinates
    of the AABBs of its child nodes, as well as the address offsets of the child nodes.
    Leaf nodes contain the vertex coordinates of the primitive.


    Tree traversals are initiated when a warp issues a trace\_ray instruction to the
    RT unit. Each thread of the warp traverses one ray using the ray properties passed
    with the trace\_ray instruction. The root node is an AABB that encompasses the
    entire scene and is checked for intersection first (line 1). If the root node
    is hit, its address is pushed onto the traversal stack (line 2). Then, until the
    stack is empty, the node address at the top of the stack (TOS) is popped, read
    from memory, and if it is an internal node, its children are checked for intersections
    and their addresses are pushed onto the stack if hit (lines 4-12). If the popped
    node is a leaf node, i.e. a primitive, the variable min\_thit, representing the
    hit distance of the current closest-hit primitive, is updated if necessary (lines
    10-12). An important observation is that an entire tree/sub-tree often does not
    need to be traversed, as some nodes may be farther than the current closest-hit
    primitive. Consequently, those nodes and their children can be skipped. Upon finding
    a primitive hit, threads store the hit information to memory. Typically, subsequent
    instructions


    read this information to calculate reflections based on the material and geometry,
    and trace subsequent bouncing rays.


    From the description above, we can see that the baseline traversal is very similar
    to a generic DFS traversal. Each thread processes the traversal stack one node
    at a time, i.e., top of the stack, while multiple node addresses are available
    in the stack. This observation motivates our proposed Cooperative BVH traversal,
    which makes use of the node addresses in the traversal stack to parallelize and
    accelerate DFS operations.


    ### 4.2 Cooperative BVH Traversal


    Our cooperative traversal enables idle (or inactive) threads to be helper threads
    by tapping into the traversal stacks of busy threads (main threads) within the
    same warp. We define an idle or helper thread as one whose traversal stack is
    empty, and a busy or main thread as one with a non-empty traversal stack. Algorithm
    [2](#page-5-0) shows how an idle thread behaves during traversal. An idle thread
    (with its ID as tid) first searches for a busy thread within the same warp (lines
    2-6). Upon finding a busy thread to help (line 4), the top node is popped from
    the main thread''s stack and pushed to the helper thread''s stack (line 5). The
    helper thread saves the thread ID of the main thread (line 6), which is used to
    look-up and update the correct closest hit distance, i.e., min\_thit. It then
    proceeds to traverse the tree as usual until its stack is empty (lines 12-20),
    at which point it will look for another busy thread to help. The traversal finishes
    when all threads in the same warp have emptied their stacks and updated the destination
    registers to indicate whether a hit was found, after which the trace\_ray instruction
    retires. This cooperative traversal is functionally correct, i.e., the closest-hit
    primitive will be correctly identified, as long as helper and main threads update
    the right min\_thit value whenever a closer hit is found.


    Fig. [6](#page-5-1) illustrates an example of cooperative traversal. In the baseline,
    the entire tree is traversed by a single thread. After checking the AABB of root
    node for intersection and finding a hit, its address is pushed onto the stack
    to initiate the traversal. Then, the root node address is popped and fetched from
    memory. When the node data arrives, the children are tested for intersection,
    and found that they are both hit, therefore both the child node addresses are
    pushed onto the stack. Next, the thread pops the left child address and starts
    traversing the left subtree before checking the right subtree. With cooperative
    traversal, let us assume there is one helper thread. After the main thread pops
    the root''s left child address, the helper thread pops the main thread''s stack
    and gets the root''s right child address. Therefore, the helper thread would traverse
    the right subtree of the root. With both the main and the helper threads, the
    two subtrees are traversed in parallel. Note that it is possible that the helper
    thread pops a different node address from the main thread, depending on when the
    helper thread is available to pop an address from the main thread''s traversal
    stack. In this case, different subtrees would be traversed in parallel. Whenever
    a thread empties its traversal stack, it would become a helper thread and try
    to take an address from a busy thread''s stack. As such, the degree of parallelization
    is not affected by which address is taken by a helper thread. Although both the
    main and helper threads find triangle hits (red circles in Fig. [6\)](#page-5-1),
    only one of them is identified as


    <span id="page-5-1"></span>ISCA ''25, June 21–25, 2025, Tokyo, Japan Yavuz Selim
    Tozlu and Huiyang Zhou


    ![](_page_5_Figure_1.jpeg)


    Figure 6: Example BVH tree traversal comparing baseline and cooperative traversal.


    the closest-hit primitive, as the threads compare with the current closest-hit,
    min\_thit, of the main thread before updating it. As a result, the correctness
    of traversal is maintained.


    While we focus on cooperative traversal for DFS, it can be extended to breadth-first-search
    (BFS) as BFS is also inherently parallelizable. Compared to DFS, BFS would use
    a queue (FIFO) rather than a stack (LIFO) to track nodes. In that case, helper
    threads would steal nodes from the front of the queue and start their traversal.
    In general, as long as a tree/graph traversal algorithm is parallelizable and
    uses a stack/queue to track nodes, cooperative traversal can be directly applied.


    An important design consideration in cooperative traversal is deciding the range
    of the threads who can help each other. In Algorithm [2,](#page-5-0) all 32 threads
    in a warp are allowed to help each other, which maximizes cooperation and performance,
    but at the cost of more complex hardware. We investigate more restrictive configurations
    where only threads within the same subwarp are allowed to help each other, in
    order to reduce hardware overhead. We explore area and performance impact of subwarp
    sizes of 4, 8 or 16 threads in Section [7.](#page-7-0)


    ### <span id="page-5-2"></span>5 CoopRT Architecture


    ### 5.1 Overview of the Architecture


    To support our cooperative BVH traversal, we modify the warp buffer and the accompanying
    logic in the RT unit. Fig. [7](#page-6-0) shows the high level block diagram of
    our proposed implementation, with the added per-thread structures highlighted
    using red and added per-RT unit structures highlighted using purple. At every
    cycle, the warp scheduler in the RT unit picks a non-stalling warp from the warp
    buffers . The memory scheduler iterates through the threads in the scheduled warp,
    checking each thread''s status to determine if it has any remaining memory requests.
    The node addresses from the TOSes of these threads are coalesced to remove redundant
    cache or memory accesses. One of these unique addresses is selected and added
    to the memory access queue, which breaks the requests into small chunks before
    sending them to memory hierarchy . The threads that generated this request pop
    their TOSes and save the popped addresses in registers, which will be needed when
    the


    <span id="page-5-0"></span>


    | Algorithm 2: Cooperative BVH Traversal to find the            |

    |---------------------------------------------------------------|

    | closest-hit primitive                                         |

    | Input: 𝑟𝑎𝑦𝑠,<br>𝑚𝑖𝑛_𝑡ℎ𝑖𝑡𝑠,<br>𝑠𝑡𝑎𝑐𝑘𝑠,<br>𝑚𝑡𝑖𝑑𝑠,<br>𝑟𝑜𝑜𝑡_𝑛𝑜𝑑𝑒  |

    | 1 𝑚𝑡𝑖𝑑<br>[𝑡𝑖𝑑] ←<br>𝑡𝑖𝑑<br>// Initialize mtid to thread id   |

    | 2 if 𝑠𝑡𝑎𝑐𝑘𝑠[𝑡𝑖𝑑].𝑒𝑚𝑝𝑡𝑦<br>then                                |

    | for 𝑖<br>← 0 to 𝑤𝑎𝑟𝑝_𝑠𝑖𝑧𝑒<br>do<br>3                          |

    | if !𝑠𝑡𝑎𝑐𝑘𝑠[𝑖].𝑒𝑚𝑝𝑡𝑦<br>then<br>4                              |

    | 𝑠𝑡𝑎𝑐𝑘𝑠[𝑡𝑖𝑑].𝑝𝑢𝑠ℎ(𝑠𝑡𝑎𝑐𝑘𝑠[𝑖].𝑝𝑜𝑝());<br>5                       |

    | 𝑚𝑡𝑖𝑑<br>[𝑡𝑖𝑑] ←<br>𝑚𝑡𝑖𝑑𝑠[𝑖]<br>// Save main thread id to<br>6 |

    | mtid                                                          |

    | 𝑏𝑟𝑒𝑎𝑘;<br>7                                                   |

    |                                                               |

    | 8 𝑠𝑡𝑎𝑐𝑘<br>≡ 𝑠𝑡𝑎𝑐𝑘𝑠[𝑡𝑖𝑑]<br>// Let stack refer to stacks[tid] |

    | 9 𝑚𝑡𝑖𝑑<br>≡ 𝑚𝑡𝑖𝑑<br>[𝑡𝑖𝑑]<br>// Let mtid refer to mtid[tid]   |

    | 10 𝑟𝑎𝑦<br>≡ 𝑟𝑎𝑦𝑠[𝑚𝑡𝑖𝑑]<br>// Let ray refer to rays[mtid]      |

    | 11 𝑚𝑖𝑛_𝑡ℎ𝑖𝑡<br>≡ 𝑚𝑖𝑛_𝑡ℎ𝑖𝑡𝑠[𝑚𝑡𝑖𝑑]                              |

    | 12 while !𝑠𝑡𝑎𝑐𝑘.𝑒𝑚𝑝𝑡𝑦<br>do                                   |

    | 𝑛𝑜𝑑𝑒<br>← 𝑠𝑡𝑎𝑐𝑘.𝑝𝑜𝑝();<br>13                                  |

    | if 𝑛𝑜𝑑𝑒.𝑡𝑦𝑝𝑒<br>== 𝑖𝑛𝑡𝑒𝑟𝑛𝑎𝑙_𝑛𝑜𝑑𝑒<br>then<br>14                |

    | for 𝑖<br>← 0 to 5 do<br>15                                    |

    | 𝑡ℎ𝑖𝑡[𝑖] ←<br>𝑖𝑛𝑡𝑒𝑟𝑠𝑒𝑐𝑡𝑖𝑜𝑛_𝑡𝑒𝑠𝑡(𝑟𝑎𝑦, 𝑛𝑜𝑑𝑒.𝑐ℎ𝑖𝑙𝑑<br>[𝑖]);<br>16 |

    | if 𝑡ℎ𝑖𝑡[𝑖]<br><<br>𝑚𝑖𝑛_𝑡ℎ𝑖𝑡<br>then<br>17                     |

    | 𝑠𝑡𝑎𝑐𝑘.𝑝𝑢𝑠ℎ(𝑛𝑜𝑑𝑒.𝑐ℎ𝑖𝑙𝑑<br>[𝑖]);<br>18                          |

    |                                                               |

    | else<br>19                                                    |

    | 𝑡ℎ𝑖𝑡<br>← 𝑖𝑛𝑡𝑒𝑟𝑠𝑒𝑐𝑡𝑖𝑜𝑛_𝑡𝑒𝑠𝑡(𝑟𝑎𝑦, 𝑡𝑟𝑖𝑎𝑛𝑔𝑙𝑒);<br>20             |

    | 𝑚𝑖𝑛_𝑡ℎ𝑖𝑡<br>← min(𝑚𝑖𝑛_𝑡ℎ𝑖𝑡, 𝑡ℎ𝑖𝑡);<br>21                      |

    |                                                               |


    memory responses come in. In parallel, the Load Balancing Unit (LBU) looks for
    a thread that needs help, and another thread that can offer help within the scheduled
    warp . If a main and a helper thread are picked, it pushes the node at the TOS
    of the main thread to helper thread''s stack by controlling the per-thread multiplexors
    . If the Helper ID that LBU finds matches the thread''s ID, then the multiplexors
    select the TOS coming from LBU, which is then pushed to this thread''s traversal
    stack. Since LBU moves only one node at a cycle, the number of pushes is set to
    1; whereas the math units may push up to 6 nodes at once depending on how many
    child nodes are hit.


    Responses from memory hierarchy are inserted to the Response FIFO, and fed into
    the Math units where coordinate transformations, ray-box and ray-triangle intersection
    tests are carried out . The address of the response is checked against the saved
    TOS registers to determine which thread or threads originated the request. We
    assume there is one math unit associated with each thread to ensure there are
    no stalls for intersection tests, similar to [\[37\]](#page-12-18). Depending
    on the intersection test results, child nodes are pushed to traversal stacks of
    the associated threads. If a primitive hit is found, the closest hit information,
    min\_thit, may be updated (Section 5.3) and a store request for the primitive
    data is inserted to the store queue which can then be read by the closest-hit
    or any-hit shaders [\[37\]](#page-12-18).


    <span id="page-6-0"></span>![](_page_6_Figure_2.jpeg)


    Figure 7: Overview of the modified RT unit. Orange blocks indicate existing hardware,
    red blocks are newly added per-thread structures, and purple blocks are newly
    added per-RT unit (therefore per-SM) structures. main\_tid is a new 5-bit field
    added to Thread Status in the warp buffer.


    <span id="page-6-1"></span>![](_page_6_Figure_4.jpeg)


    Figure 8: Load Balancing Unit.


    ## 5.2 Load Balancing Unit


    LBU is a per SM unit and is responsible for assigning idle threads as helpers
    to a busy thread, and moving node addresses from the main to helper threads. Fig.
    [8](#page-6-1) shows details of the LBU. The priority encoder (PE) on the right
    of Fig. [8](#page-6-1) determines which thread needs help, and outputs its thread
    ID. A thread needs help if its traversal stack is not empty, and its TOS is not
    being processed in that cycle. Both active and inactive threads in the warp can
    be helped, which means a helper thread can also be helped by another thread as
    long as its traversal stack is not empty. The main thread ID is used to control
    the multiplexor which outputs the TOS of main thread. The PE on the left determines
    which thread is available to help, and outputs its thread ID. The empty signal
    means the corresponding thread''s traversal stack is empty at the cycle. If both
    a main and a helper thread are found, main\_tid of the main thread is saved in
    the helper thread''s main\_tid field in the warp buffer using the per-thread multiplexor
    . All the threads traversing the same ray, including the helper(s) and main, use
    the main\_tid field to get the right ray properties and min\_thit value. main\_tid
    is initialized to tid when the trace\_ray instruction first enters the RT unit.
    As threads are assigned as helpers, they save the main thread''s main\_tid field
    in their own main\_tid field.


    ## 5.3 Synchronization Between Main and Helper Threads


    When multiple threads traverse the same ray, to ensure functional correctness,
    they need to update the same min\_thit register as they find primitive hits. The
    min\_thit field in the warp buffer stores the hit distance of the current closest-hit
    primitive, and it is updated only when a closer primitive is found. With CoopRT,
    all helper threads update the min\_thit field of the main thread, which ensures
    functional correctness. This is achieved via the logic shown in Fig. [7,](#page-6-0)
    which is a per-thread structure . Three signals are ANDed together for each thread:
    math\_rdy signal that indicates the math unit''s output is ready, the main\_tid==tid
    signal where main\_tid is the main thread''s ID saved by the helper thread and
    tid is the thread ID of the thread whose min\_thit is being updated, and the thit
    value itself. Output of all the AND gates (one AND per thread) are ORed to allow
    the valid thit value that will be written to the main thread''s min\_thit field
    if it is smaller than the current one. The OR gate behaves like a multiplexor,
    because it is logically impossible for more than one thread to find a primitive
    hit for a given ray at the same cycle. The reason is that the responses from the
    response FIFO are popped one per cycle, and the math unit latency is constant.
    If the bandwidth of the response FIFO is increased to be more than one response
    per cycle, we can let each helper update their own min\_thit field first and then
    borrow atomic instruction support (e.g., atomicMin) to update the main threads''
    min\_thit at the store buffer when retiring. Since the FIFO throughput is not
    a performance bottleneck, we do not investigate this design option further.


    It is possible that a main thread empties its traversal stack, i.e. finishes traversal,
    before a helper thread. This does not pose a problem as the trace\_ray instruction
    will not retire until all threads in the warp have emptied their stacks.


    Overall, the newly added support shown in Fig. [7](#page-6-0) provides (a) a mechanism
    to identify helper and main thread pairs, up to one pair a cycle by the LBU, (b)
    the data path to read from the main thread''s traversal stack and write to the
    helper thread''s traversal stack (a bus design can also be sufficient, as only
    one main-helper pair is selected per cycle), and (c) the data path from the math
    units'' outputs, i.e., the thit from a helper thread, to update the main thread''s
    min\_thit if it is smaller than the min\_thit. This is achieved with a crossbar.
    If we allow any thread in a warp to help each other, it is a 32x32 crossbar. If
    we limit the scope to a subwarp (e.g., size 8), then we can replace the 32x32
    crossbar with multiple small crossbars (e.g., 4 8x8 crossbars). The logic is simplified
    by the observation that only up to one helper thread would update the main thread''s
    min\_thit at a cycle. The reason is that node addresses are unique, the memory
    response IO throughput is one per cycle, and different helper threads traverse
    different portions of the BVH tree. As a result, it is impossible for more than
    one thread to ever access the same primitive for a given ray, and when different
    threads access different primitives, their thit values will be available at different
    cycles.


    ### 6 Experimental Methodology


    ### 6.1 Modeling CoopRT in Vulkan-sim


    We modify Vulkan-sim 2.0 [\[37\]](#page-12-18) to model CoopRT and evaluate its
    performance. Vulkan-sim is built on top of GPGPUsim [\[31\]](#page-12-16), which
    is made up of a functional and a timing simulator. Vulkansim performs the actual
    BVH traversal process in the functional simulator, and passes a list of BVH node
    addresses for each thread to the timing simulator, which in turn simulates the
    memory accesses. The timing simulator picks a warp from the warp buffer at every
    cycle, reads the node addresses from the top of the lists passed from the functional
    simulator, merges duplicate addresses and sends one of the unique addresses to
    memory hierarchy. To implement CoopRT, we check for idle threads in the scheduled
    warp each cycle. If any are found, the node at the top of the list of a busy thread
    is moved to the idle thread''s node list.


    The functional simulator assumes a single thread traverses the BVH tree in DFS
    fashion for a given ray, and therefore generates the list of nodes accordingly.
    This means some nodes in the tree get eliminated during traversal because they
    are farther than the current closest-hit primitive, and they are not added to
    the list. However, when multiple threads traverse the BVH together, it is impossible
    to know beforehand which nodes will be eliminated, because that depends on runtime
    information such as how many threads traverse the tree and which parts of it they
    are processing, which is not available in the functional simulator.


    We resolve this issue by not doing any node eliminations in the functional simulator,
    and instead, passing the thit values of each node to the timing simulator. In
    the timing simulator, we keep track of the min\_thit value for each thread, which
    is initialized to positive infinity. Before processing a new node in the list,
    we compare its thit value to the min\_thit value. If thit is greater than or equal
    to min\_thit, we discard it.


    To estimate the impact on power consumption, we use GpuWattch [\[33\]](#page-12-27)
    shipped with Vulkan-sim.


    We use one of the default configurations available in the Vulkansim repository,
    namely the SM75\_RTX2060 configuration. Table [1.](#page-7-1) shows the key settings
    in this configuration.


    ### 6.2 Benchmark Suite


    We use the Lumibench[\[35\]](#page-12-28) ray tracing benchmark suite for evaluation.
    Lumibench features 16 3D scenes with various geometric


    <span id="page-7-1"></span>


    | # Streaming Multiprocessors(SM) | 30                                 |

    |---------------------------------|------------------------------------|

    | Max. TBs per SM                 | 32                                 |

    | Warp Size                       | 32                                 |

    | Instruction Cache               | 128KB, 20 cycles                   |

    | L1 Data Cache                   | 64KB, Fully assoc. LRU, 20 cycles  |

    | L2 Cache                        | 3MB, 16-way assoc. LRU, 160 cycles |

    | Core, Interconnect, L2 Clock    | 1365 MHz                           |

    | Memory Clock                    | 3500 MHz                           |

    | # of RT Units per SM            | 1                                  |

    | RT Unit Warp Buffer Size        | 4                                  |


    Table 1: Vulkan-sim baseline hardware configuration.


    complexities and lighting conditions. Table [2.](#page-8-0) shows a summary of
    the 3D scenes.


    The highest resolution we could simulate without simulations timing out or running
    out of memory is 256x256. Among all the scenes, we could only simulate 13 scenes
    at this resolution. For the remaining ones, we run the scenes car and robot at
    the resolution of 128x128 because they either time out or consume too much memory
    at the resolution of 256x256. The scene park would not finish after 3 days of
    simulation and time out at the resolution of 128x128. In all the scenes, we use
    1-sample-per-pixel, meaning one primary ray for each pixel.


    At the resolution of 256x256, there are 2048 thread blocks (TB) and each TB has
    one warp, which is the default thread block size in Vulkan-sim. 2048 TBs are enough
    to fill up the entire GPU that we use in this study, which has 30 SMs.


    ## <span id="page-7-0"></span>7 Results


    ## 7.1 CoopRT Performance and Memory Bandwidth Utilization


    Fig. [9](#page-8-1) shows the normalized speedup and power of CoopRT with PT shaders.
    We observe up to 5.11x speedup, with a geometric mean of 2.15x. Scenes with low
    SIMT efficiency and long BVH traversals, such as crnvl, fox and party, benefit
    the most from cooperative traversal. Although spnza has the highest number of
    BVH nodes visited among all the scenes, it also has relatively higher SIMT efficiency,
    meaning there are fewer idle threads, likely because it is a closed scene with
    minimal exposed sky. On average, power consumption is increased by 2.02x, and
    energy is decreased to 0.94x.


    To provide more insight to the source of performance gains, we calculate average
    thread utilization in RT unit using the readily available AerialVision stats [\[10\]](#page-11-7).
    At every 500 GPU cycles, we collect the number of busy threads in RT unit, i.e.,
    the threads with nonempty traversal stacks, and divide them by the number of total
    threads to get the thread utilization per sample. We then average all the samples
    to obtain overall utilization for each scene. Fig. [10](#page-8-2) shows the overall
    thread utilization for baseline and CoopRT.


    From the Figs. [9](#page-8-1) and [10,](#page-8-2) we can see that the speedups
    are proportional to the thread utilization improvements, rather than the actual
    final utilization. The three scenes, crnvl, fox, and party have the highest improvement
    in utilization, which is why they achieve the highest speedups. In other words,
    CoopRT overcomes the divergent nature of ray tracing by exploiting the parallelism
    of BVH traversal. The more divergent a scene, the higher speedup is achieved.


    <span id="page-8-0"></span>


    | Scene         |       |       |       |       |       |       |         |         |

    |---------------|-------|-------|-------|-------|-------|-------|---------|---------|

    | Label         | wknd  | ship  | bunny | spnza | chsnt | bath  | ref     | crnvl   |

    | Tree Size(MB) | 0.2   | 0.5   | 12.2  | 22    | 25.5  | 104.2 | 37.1    | 37.3    |

    | Depth         | 7     | 12    | 11    | 16    | 12    | 16    | 13      | 16      |

    | Scene         |       |       |       |       |       |       |         |         |

    | Label         | fox   | party | sprng | lands | frst  | park  | car     | robot   |

    | Tree Size(MB) | 597.8 | 143.8 | 164.3 | 279.2 | 348.6 | 501.9 | 1,233.6 | 1,721.3
    |

    | Depth         | 15    | 14    | 14    | 12    | 14    | 14    | 16      | 18      |


    Table 2: Benchmark scenes from LumiBench [\[35\]](#page-12-28). Scene stats taken
    from [\[15\]](#page-12-29).


    <span id="page-8-1"></span>![](_page_8_Figure_4.jpeg)


    <span id="page-8-2"></span>![](_page_8_Figure_5.jpeg)


    Fig. [11](#page-9-0) presents the trace\_ray instruction execution timeline of
    an example warp (in the bath scene) to illustrate how the thread utilization increases
    with CoopRT. In Fig. [11a,](#page-9-0) there are 13 inactive threads, and several
    threads that finish their traversal early and idle, yielding an average utilization
    rate of 30.5%. Fig. [11b](#page-9-0) shows how the timeline changes with CoopRT.
    Inactive threads steal work from active threads, and spend most of their time
    doing traversals. In addition, active threads who finish early also steal work
    from other threads. Ultimately, average utilization increases to 94.6%.


    CoopRT also entails substantial improvement in memory bandwidth utilization due
    to the increased number of threads doing traversals in parallel. Fig. [12](#page-9-1)
    shows the L2 cache and DRAM bandwidth utilization normalized to baseline. We observe
    up to 5.7x and 5.5x increase in L2 and DRAM bandwidth respectively. This increase
    is primarily due to low bandwidth use of the baseline RT unit, as there are fewer
    busy threads in the baseline.


    Another important factor is the number of maximum number of warps allowed in the
    RT unit. By default, as shown in Table [1.](#page-7-1), the RTX2060 configuration
    allows at most 4 warps to exist simultaneously in the RT unit. However, 4 warps
    are not enough to fully utilize the memory bandwidth. Simply increasing the number
    of


    warp buffers in the RT unit is costly, as all the fields in the warp buffer add
    up to hundreds of bits of storage per thread. We discuss the area overhead with
    detail in Section [7.5.](#page-9-2) To evaluate the impact of warp buffer size
    on performance, we simulate different warp buffer sizes with and without CoopRT.
    Fig. [13](#page-10-0) shows the normalized speedups for different warp buffer
    sizes. Compared to the baseline, we see geometric means of 1.45x, 1.64x, 1.64x
    for warp buffer sizes of 8, 16, 32 without CoopRT. As the warp buffer size increases,
    inter-warp parallelism and memory bandwidth utilization increase, yielding higher
    throughput and performance. Increasing the warp buffer size from 4 to 8 provides
    the greatest performance boost, with further increases yielding diminishing returns.
    For this particular hardware configuration, 8 or 16 buffer entries seem like the
    sweet spot for performance and area trade off. When CoopRT is enabled, the impact
    of warp buffer size becomes less significant. We see geometric means of 2.15x,
    2.13x, 2.06x, 1.99x for warp buffer sizes of 4, 8, 16 and 32 over the baseline.
    This is because CoopRT already saturates the memory bandwidth utilization. CoopRT
    with just 4 warp buffer entries achieves greater speedup than the baseline 32
    entry warp buffer configuration. Moreover, CoopRT reduces the latency of the longest-running
    or slowest warps, which would determine the frame rate in real-time rendering,
    compared to the schemes using large warp buffers. Fig. [14](#page-10-1) shows
    the latency of the longest running warp in each scene normalized to baseline.
    From the figure, we can see that CoopRT achieves higher throughput and better
    latency via intra-warp parallelism. On average, CoopRT achieves 0.46x the latency
    of the baseline, compared to 0.62x achieved by the large warp buffer scheme.


    <span id="page-9-0"></span>![](_page_9_Figure_1.jpeg)


    (b) CoopRT


    Figure 11: RT unit trace\_ray instruction execution timelines. Dark and light
    gray bars represent active and originally inactive threads, respectively. A continuous
    bar indicates a non-empty traversal stack. bath scene, 256x256 resolution path
    tracing.


    <span id="page-9-1"></span>![](_page_9_Figure_4.jpeg)


    Figure 12: Normalized L2 ↔ Interconnect, and DRAM bandwidth with CoopRT over the
    baseline.


    To quantify and compare the energy efficiency of CoopRT against large warp buffers,
    we calculate the energy-delay products (EDP) [\[24\]](#page-12-30)[\[12\]](#page-12-31)
    of each approach, and plot the EDP improvements in Fig. [15.](#page-10-2) Geometric
    means are 1.54x, 1.75x, 1.75x and 2.29x for warp buffer sizes of 8, 16, 32 without
    CoopRT and 4 with CoopRT respectively. While neither approach introduces substantial
    energy consumption, CoopRT with the warp buffer size of 4 achieves higher performance
    by better utilizing the existing hardware, thus achieving better EDP with much
    smaller area overhead.


    ## 7.2 Memory Contention Under CoopRT


    When CoopRT is enabled, the RT unit generates more memory requests in a shorter
    amount of time. This could lead to contention in the memory hierarchy. To analyze
    such contention, we collect the L1 and L2 miss rates for each scene, as shown
    in Fig. [16.](#page-10-3) From the figure, we observe that CoopRT results in:
    (1) increased L1 cache miss rates indicating more contention on L1; (2) a higher
    number of L2 accesses but similar L2 miss rates meaning more reuses at the L2
    (as some of the original L1 reuses now happen at L2); (3) GPU latency hiding capability
    tolerating additional L1 misses; and (4) overlapping misses (i.e. memory level
    parallelism) and memory bandwidth utilization being more important than the number
    of misses alone.


    ## 7.3 Ambient Occlusion and Shadow Shaders


    In addition to PT shaders, Lumibench features ambient occlusion (AO) and shadow
    (SH) shaders that use the ray tracing pipeline to produce realistic lighting effects.
    Unlike PT, these shaders are typically used together with rasterization, and are
    readily employed in real-time applications such as video games. AO and SH shaders
    are much more lightweight than PT, as they do not aim to render the entire 3D
    scene, but rather perform lighting calculations. Similar to PT, AO and SH shaders
    also begin with primary rays generated from the camera, but instead of bouncing
    the primary ray through the scene, they find the closest object that the primary
    rays hit. Then, from that intersection point, they trace a small number of shadow
    rays to determine how much light reaches that point. The key difference from PT
    is that these shadow rays are much more localized and coherent (i.e., non-divergent).
    Therefore, they are relatively faster to trace and there is less room for improvement.
    Fig. [17](#page-10-4) shows how CoopRT performs with AO and SH shaders. As expected,
    the speedups are smaller compared to PT. This is because, as mentioned before,
    AO and SH rays do not diverge nearly as much as PT, leaving less speedup opportunity
    for CoopRT than PT rays. Despite this, CoopRT achieves an average of 1.42x and
    1.28x speedup for AO and SH, respectively.


    # 7.4 Mobile GPU Configuration


    To demonstrate the robustness of CoopRT, we also evaluate the performance on a
    mobile GPU configuration included in Vulkan-sim, which has 8 SMs and 4 memory
    channels. Fig. [18](#page-10-5) shows the speedup, power and energy results under
    this configuration. We see CoopRT achieves an average of 1.8x speedup, 1.71x power
    and 0.95x energy relative to the baseline. The speedups are mainly bottlenecked
    by the memory bandwidth limitation of this configuration. When CoopRT is enabled,
    DRAM utilization increases from 44.0% to 85.3% on average.


    ## <span id="page-9-2"></span>7.5 Area Overhead


    We implement the hardware model proposed in Section [5](#page-5-2) to estimate
    the area cost of CoopRT. We wrote the RTL for all of the newly introduced blocks,
    and synthesized using FreePDK45 [\[38\]](#page-12-32) and Synopsys Design Compiler.
    While the proposed logic is purely combinational that consists of PEs, multiplexors
    and logic gates, we also take into account the extra fields introduced to warp
    buffers.


    The total number of combinational cells in this design is 16,122, occupying an
    area of 13,347 µm2. For reference, each sequential cell (e.g., a D flip-flop)
    takes up 6 µm2 in this design kit. This means that the area occupied by the combinational
    logic is equivalent to approximately 2,200 flip-flops. By comparison, in the baseline
    RT unit, just the RayProperties, TraversalStack and min\_thit fields


    <span id="page-10-0"></span>![](_page_10_Figure_2.jpeg)


    Figure 13: Normalized speedups for different RT warp buffer sizes with and w/o
    CoopRT. Baseline is 4-entry warp buffer without CoopRT. 1 warp per thread block,
    32 thread blocks in one SM at a time. Missing data points are due to consistently
    crashing or timing out simulations.


    <span id="page-10-1"></span>![](_page_10_Figure_4.jpeg)


    Figure 14: Latency of the slowest warps, normalized to baseline (4 buffer entries
    without CoopRT). Lower the better.


    <span id="page-10-2"></span>![](_page_10_Figure_6.jpeg)


    Figure 15: Normalized improvement in EDP for different RT warp buffer sizes. Baseline
    is 4-entry warp buffer without CoopRT.


    <span id="page-10-3"></span>![](_page_10_Figure_8.jpeg)


    <span id="page-10-4"></span>![](_page_10_Figure_9.jpeg)


    Figure 17: Speedups of CoopRT for AO and SH shaders normalized to baseline.


    in a warp buffer require a total of 768 bits of storage per thread, assuming a
    16-entry traversal stack. Here, RayProperties includes the ray''s origin, direction,
    and a max\_thit value. The extra fields in the warp buffer include the 5-bit main\_tid
    field per thread, and a stack empty flag per thread. Assuming 4 warp buffer entries,
    and 32 threads per warp, the warp buffer takes up 98,304 (=4\*32\*768) bits of
    storage.


    This means the CoopRT hardware takes up less than 3.0% (( 2200+4\*32\*(5+1))/98304)
    of the warp buffer area. This comparison also shows that CoopRT is much more area
    efficient than simply


    <span id="page-10-5"></span>![](_page_10_Figure_13.jpeg)


    Figure 18: Speed, power and energy of CoopRT on a mobile GPU, normalized to baseline.


    increasing the number of warp buffers as each warp buffer entry takes 24,576 (=32\*768)
    bits.


    One way to reduce the area consumption is to implement a subwarp scheme where
    only the threads within the same subwarp are allowed to help each other. A subwarp
    is a smaller, fixed-size group of threads in a warp. This slightly reduces the
    hardware cost, as the per-thread structures shown in Fig. [7](#page-6-0) do not
    need to have 32 inputs coming from 32 threads. The smaller the subwarp size, the
    less area consumed. However, this also places a constraint on which threads can
    help each other, therefore reducing the amount of parallelism and performance.


    One important design decision to make with the subwarp scheme is if all subwarps
    are processed together in one cycle to find a pair of helper-main threads for
    each subwarp, or if just one subwarp is picked and processed every cycle. The
    former approach reduces area consumption by reducing the number and sizes of the
    perthread OR gate and multiplexor in Fig. [7.](#page-6-0) It also replaces the
    PEs with a smaller pair for each subwarp. The latter approach rescales the number
    of gates and multiplexors to the subwarp size, potentially eliminating significant
    amount of hardware. However, it also requires additional hardware for subwarp
    scheduling. At each cycle, the subwarp scheduler would have to look through the
    subwarps, and pick a suitable one that has a main-helper pair candidate. In terms
    of performance, both approaches would perform similarly, as the latency of a trace\_ray
    instruction is on the order of thousands of cycles, which is long enough to hide
    any subwarp scheduling latency.


    Using the first approach described before, we synthesize CoopRT hardware with
    subwarp sizes of 4, 8 and 16 to estimate area savings, and also simulate in Vulkan-sim
    to understand the performance trade off. Table [3.](#page-11-8) shows the area
    results for different subwarp sizes. By decreasing the subwarp size to 4, almost
    10% of the area can be saved. Fig. [19](#page-11-9) shows how the performance
    changes with subwarp size. As expected, reducing the subwarp size leads to a decline
    in performance. Average speedups are 1.72x, 1.97x, 2.09x and 2.15x for 4, 8, 16
    and 32 respectively. It is worth noting that both area


    <span id="page-11-8"></span>


    | Subwarp size | # of cells | Total     | Percent      |

    |--------------|------------|-----------|--------------|

    |              |            | area(µm2) | change(Area) |

    | 32           | 16122      | 13347     | 0            |

    | 16           | 15867      | 13104     | 1.8          |

    | 8            | 15511      | 12661     | 5.1          |

    | 4            | 15167      | 12055     | 9.7          |


    ![](_page_11_Figure_2.jpeg)


    <span id="page-11-9"></span>![](_page_11_Figure_3.jpeg)


    Figure 19: Speedups of CoopRT for subwarp sizes of 4, 8, 16 and 32 normalized
    to baseline.


    consumption and performance drop the most when changing the subwarp size from
    8 to 4.


    ## 8 Related Work


    ## 8.1 Ray Tracing on GPUs


    Due to its parallel nature, ray tracing has been implemented and studied on GPUs.
    Early research on GPU ray tracing utilized GPGPU programming models such as CUDA,
    due to the lack of hardware and API support. Aila et al. [\[9\]](#page-11-10)
    implement a GPU ray tracer to assess the performance and bottlenecks of ray traversal
    on GPUs. They explore replacing early terminated rays with new ones, wider BVH
    trees, and work queues to improve SIMD efficiency. In another work [\[8\]](#page-11-11),
    Aila et al. focus on incoherent (divergent) rays and explore a treelet based BVH
    traversal scheme. The BVH tree is statically split into smaller trees called treelets
    to shrink the working set and reduce the memory footprint and latency. Wald [\[42\]](#page-12-23)
    proposes active thread compaction to mitigate divergence in PT. At the beginning
    of each ray bounce, active threads across multiple warps are compacted together
    to form fewer but more efficient warps, which is similar to the idea in [\[21\]](#page-12-21).
    Therefore, it may address the inactive thread problem to some degree (as it needs
    to compact different numbers of TBs or warps for each bounce), but not early finishing
    threads.


    ### 8.2 Hardware Accelerated Ray Tracing


    The recent introduction of specialized RT units in commodity GPUs sparked architectural
    research for hardware acceleration of ray tracing. Lufei et al.[\[34\]](#page-12-33)
    propose an intersection prediction algorithm. A dedicated hardware cache is used
    to store the intersection results of previous rays. Future rays can look up the
    cache by calculating a hash using the ray properties and predict intersections
    without traversing the BVH tree. Although effective with localized rays that AO
    and SH shaders generate, its effectiveness with PT is unknown. Saed et al. [\[37\]](#page-12-18)
    extend GPGPUsim 4.0 [\[31\]](#page-12-16) by incorporating an RT unit which is
    capable of simulating Vulkan ray tracing shaders. Inspired by Aila [\[8\]](#page-11-11),
    Chou et al. [\[15\]](#page-12-29) propose a treelet based BVH traversal and hardware
    prefetcher for ray tracing. Prefetching is a viable solution for memory latency-bound
    workloads, such as PT. In this context, the treelet prefetcher proves to be useful,
    though it requires complex hardware, and a custom BVH organization. CoopRT can
    be combined with a prefetcher, such as the Treelet prefetcher, although the benefits
    would need more careful consideration. The reason is that CoopRT increases parallelism
    and may saturate the memory bandwidth. In this case, the bandwidth left for prefetching
    would be limited. In a system where bandwidth is abundant, CoopRT can benefit
    from prefetching due to reduced memory access latency.


    ## 9 Conclusion


    In this work, we propose a novel cooperative BVH traversal scheme, CoopRT, to
    accelerate GPU ray tracing. We capitalize on two important insights: low SIMT
    efficiency of ray tracing workloads, and the inherent parallelism of BVH traversal.


    Ray tracing applications traverse millions of rays per frame, and during the traversal,
    a large number of rays terminate early either because they miss the scene or hit
    a light source. This causes the threads to idle, while other threads continue
    their traversals. However, BVH traversal can be parallelized without losing functional
    correctness. Therefore, we propose to utilize the idle threads to help the busy
    threads finish their traversals faster by letting the idle threads steal BVH nodes
    from busy threads'' traversal stacks. We evaluate the performance of CoopRT in
    Vulkan-sim [\[37\]](#page-12-18) by extending the baseline RT unit. We simulate
    CoopRT across 13 scenes in Lumibench [\[35\]](#page-12-28), and show that CoopRT
    achieves up to 5.11x speedup, with an average of 2.15x compared to the baseline
    RT unit. We also propose a hardware model for CoopRT and implement it in RTL to
    estimate its area overhead. We show that CoopRT takes less than 3.0% of the warp
    buffer area in the RT unit.


    ### Acknowledgments


    We thank the anonymous reviewers for their valuable comments. The work is funded
    in part by NSF grants PHY-2325080 (with a subcontract to NC State University from
    Duke University), and OMA-2120757 (with a subcontract to NC State University from
    the University of Maryland).


    ## References


    - <span id="page-11-0"></span>[1] [n. d.]. Cyberpunk 2077: Technology Preview
    Of New Ray Tracing Overdrive Mode Out Now. [https://www.nvidia.com/en-us/geforce/news/cyberpunk-2077](https://www.nvidia.com/en-us/geforce/news/cyberpunk-2077-ray-tracing-overdrive-update-launches-april-11/)
    [ray-tracing-overdrive-update-launches-april-11/](https://www.nvidia.com/en-us/geforce/news/cyberpunk-2077-ray-tracing-overdrive-update-launches-april-11/)

    - <span id="page-11-6"></span>[2] [n. d.]. Intel Embree. <https://www.embree.org/>

    - <span id="page-11-3"></span>[3] [n. d.]. Intel® Arc™ Graphics Developer Guide
    for Real-Time Ray Tracing in... [https://www.intel.com/content/www/us/en/developer/articles/guide/real](https://www.intel.com/content/www/us/en/developer/articles/guide/real-time-ray-tracing-in-games.html)[time-ray-tracing-in-games.html](https://www.intel.com/content/www/us/en/developer/articles/guide/real-time-ray-tracing-in-games.html)

    - <span id="page-11-4"></span>[4] [n. d.]. NVIDIA ADA GPU ARCHITECTURE. [https://images.nvidia.com/aem](https://images.nvidia.com/aem-dam/Solutions/geforce/ada/nvidia-ada-gpu-architecture.pdf)[dam/Solutions/geforce/ada/nvidia-ada-gpu-architecture.pdf](https://images.nvidia.com/aem-dam/Solutions/geforce/ada/nvidia-ada-gpu-architecture.pdf)

    - <span id="page-11-5"></span>[5] [n. d.]. NVIDIA AMPERE GA102 GPU ARCHITECTURE.
    [https://www.nvidia.](https://www.nvidia.com/content/PDF/nvidia-ampere-ga-102-gpu-architecture-whitepaper-v2.pdf)
    [com/content/PDF/nvidia-ampere-ga-102-gpu-architecture-whitepaper-v2.pdf](https://www.nvidia.com/content/PDF/nvidia-ampere-ga-102-gpu-architecture-whitepaper-v2.pdf)

    - <span id="page-11-1"></span>[6] [n. d.]. Real-Time Ray Tracing. [https://dev.epicgames.com/documentation/en](https://dev.epicgames.com/documentation/en-us/unreal-engine/hardware-ray-tracing-tips-and-tricks-in-unreal-engine)[us/unreal-engine/hardware-ray-tracing-tips-and-tricks-in-unreal-engine](https://dev.epicgames.com/documentation/en-us/unreal-engine/hardware-ray-tracing-tips-and-tricks-in-unreal-engine)

    - <span id="page-11-2"></span>[7] [n. d.]. Real-time Raytracing for Interactive
    Global Illumination Workflows in Frostbite. <https://www.gdcvault.com/play/1024801/>

    - <span id="page-11-11"></span>[8] Timo Aila and Tero Karras. 2010. Architecture
    considerations for tracing incoherent rays. In Proceedings of the Conference on
    High Performance Graphics (HPG ''10). Eurographics Association, Goslar, DEU, 113–122.

    - <span id="page-11-10"></span>[9] Timo Aila and Samuli Laine. 2009. Understanding
    the efficiency of ray traversal on GPUs. In Proceedings of the Conference on High
    Performance Graphics 2009 (HPG ''09). Association for Computing Machinery, New
    York, NY, USA, 145–149. <https://doi.org/10.1145/1572769.1572792>

    - <span id="page-11-7"></span>[10] Aaron Ariel, Wilson W. L. Fung, Andrew E. Turner,
    and Tor M. Aamodt. 2010. Visualizing complex dynamics in many-core accelerator
    architectures. In 2010


    IEEE International Symposium on Performance Analysis of Systems & Software (ISPASS).
    164–174. <https://doi.org/10.1109/ISPASS.2010.5452029>


    - <span id="page-12-24"></span>[11] Aaron Barnes, Fangjia Shen, and Timothy G.
    Rogers. 2024. Extending GPU Ray-Tracing Units for Hierarchical Search Acceleration.
    In Proceedings of the 57th Annual IEEE/ACM International Symposium on Microarchitecture
    (MICRO ''24). Association for Computing Machinery, New York, NY, USA.

    - <span id="page-12-31"></span>[12] D.M. Brooks, P. Bose, S.E. Schuster, H. Jacobson,
    P.N. Kudva, A. Buyuktosunoglu, J. Wellman, V. Zyuban, M. Gupta, and P.W. Cook.
    2000. Power-aware microarchitecture: design and modeling challenges for next-generation
    microprocessors. IEEE Micro 20, 6 (Nov. 2000), 26–44. <https://doi.org/10.1109/40.888701>
    Conference Name: IEEE Micro.

    - <span id="page-12-8"></span>[13] Brian Caulfield. 2018. What''s the Difference
    Between Ray Tracing and Rasterization? [https://blogs.nvidia.com/blog/whats-difference-between-ray-tracing](https://blogs.nvidia.com/blog/whats-difference-between-ray-tracing-rasterization/)[rasterization/](https://blogs.nvidia.com/blog/whats-difference-between-ray-tracing-rasterization/)

    - <span id="page-12-9"></span>[14] Brian Caulfield. 2022. What Is Path Tracing?
    [https://blogs.nvidia.com/blog/what](https://blogs.nvidia.com/blog/what-is-path-tracing/)[is-path-tracing/](https://blogs.nvidia.com/blog/what-is-path-tracing/)

    - <span id="page-12-29"></span>[15] Yuan Hsi Chou, Tyler Nowicki, and Tor M. Aamodt.
    2023. Treelet Prefetching For Ray Tracing. In Proceedings of the 56th Annual IEEE/ACM
    International Symposium on Microarchitecture (MICRO ''23). Association for Computing
    Machinery, New York, NY, USA, 742–755.

    - <span id="page-12-0"></span>[16] Per H. Christensen, Julian Fong, David M. Laur,
    and Dana Batali. 2006. Ray Tracing for the Movie ''Cars''. In 2006 IEEE Symposium
    on Interactive Ray Tracing. 1–6. <https://doi.org/10.1109/RT.2006.280208>

    - <span id="page-12-1"></span>[17] Per H. Christensen and Wojciech Jarosz. 2016.
    The Path to Path-Traced Movies. Foundations and Trends® in Computer Graphics and
    Vision 10, 2 (2016), 103–175. <https://doi.org/10.1561/0600000073>

    - <span id="page-12-19"></span>[18] Sana Damani, Mark Stephenson, Ram Rangan,
    Daniel Johnson, Rishkul Kulkami, and Stephen W. Keckler. 2022. GPU Subwarp Interleaving.
    In 2022 IEEE International Symposium on High-Performance Computer Architecture
    (HPCA). 1184–1197. <https://doi.org/10.1109/HPCA53966.2022.00090> ISSN: 2378-203X.

    - <span id="page-12-20"></span>[19] Ahmed ElTantawy, Jessica Wenjie Ma, Mike O''Connor,
    and Tor M. Aamodt. 2014. A scalable multi-path microarchitecture for efficient
    GPU control flow. In 2014 IEEE 20th International Symposium on High Performance
    Computer Architecture (HPCA). 248–259. <https://doi.org/10.1109/HPCA.2014.6835936>
    ISSN: 2378-203X.

    - <span id="page-12-2"></span>[20] Robert Felbecker, Leszek Raschkowski, Wilhelm
    Keusgen, and Michael Peter. 2012. Electromagnetic wave propagation in the millimeter
    wave band using the NVIDIA OptiX GPU ray tracing engine. In 2012 6th European
    Conference on Antennas and Propagation (EUCAP). 488–492. <https://doi.org/10.1109/EuCAP.2012.6206198>
    ISSN: 2164-3342.

    - <span id="page-12-21"></span>[21] Wilson W. L. Fung and Tor M. Aamodt. 2011.
    Thread block compaction for efficient SIMT control flow. In Proceedings of the
    2011 IEEE 17th International Symposium on High Performance Computer Architecture
    (HPCA ''11). IEEE Computer Society, USA, 25–36.

    - <span id="page-12-22"></span>[22] Wilson W. L. Fung, Ivan Sham, George Yuan,
    and Tor M. Aamodt. 2007. Dynamic Warp Formation and Scheduling for Efficient GPU
    Control Flow. In Proceedings of the 40th Annual IEEE/ACM International Symposium
    on Microarchitecture (MICRO 40). IEEE Computer Society, USA, 407–420. [https://doi.org/10.1109/MICRO.2007.](https://doi.org/10.1109/MICRO.2007.12)
    [12](https://doi.org/10.1109/MICRO.2007.12)

    - <span id="page-12-11"></span>[23] Jeffrey Goldsmith and John Salmon. 1987. Automatic
    Creation of Object Hierarchies for Ray Tracing. IEEE Computer Graphics and Applications
    7, 5 (May 1987), 14–20. <https://doi.org/10.1109/MCG.1987.276983> Conference Name:
    IEEE Computer Graphics and Applications.

    - <span id="page-12-30"></span>[24] R. Gonzalez and M. Horowitz. 1996. Energy
    dissipation in general purpose microprocessors. IEEE Journal of Solid-State Circuits
    31, 9 (Sept. 1996), 1277–1284. <https://doi.org/10.1109/4.535411> Conference Name:
    IEEE Journal of Solid-State Circuits.

    - <span id="page-12-7"></span>[25] Johannes Gunther, Stefan Popov, Hans-Peter
    Seidel, and Philipp Slusallek. 2007. Realtime Ray Tracing on GPU with BVH-based
    Packet Traversal. In 2007 IEEE Symposium on Interactive Ray Tracing. 113–118.
    [https://doi.org/10.1109/RT.2007.](https://doi.org/10.1109/RT.2007.4342598) [4342598](https://doi.org/10.1109/RT.2007.4342598)

    - <span id="page-12-25"></span>[26] Dongho Ha, Lufei Liu, Yuan Hsi Chou, Seokjin
    Go, Won Woo Ro, Hung-Wei Tseng, and Tor M. Aamodt. 2024. Generalizing Ray Tracing
    Accelerators for Tree Traversals on GPUs. In Proceedings of the 57th Annual IEEE/ACM
    International Symposium on Microarchitecture (MICRO ''24). Association for Computing
    Machinery, New York, NY, USA.

    - <span id="page-12-3"></span>[27] Danping He, Bo Ai, Ke Guan, Longhe Wang, Zhangdui
    Zhong, and Thomas Kürner. 2019. The Design and Applications of High-Performance
    Ray-Tracing Simulation Platform for 5G and Beyond Wireless Communications: A Tutorial.
    IEEE Communications Surveys & Tutorials 21, 1 (2019), 10–27. [https://doi.org/10.](https://doi.org/10.1109/COMST.2018.2865724)
    [1109/COMST.2018.2865724](https://doi.org/10.1109/COMST.2018.2865724) Conference
    Name: IEEE Communications Surveys & Tutorials.

    - <span id="page-12-4"></span>[28] Jakob Hoydis, Faycal Ait Aoudia, Sebastian
    Cammerer, Merlin Nimier-David, Nikolaus Binder, Guillermo Marcus, and Alexander
    Keller. 2023. Sionna RT: Differentiable Ray Tracing for Radio Propagation Modeling.
    In 2023 IEEE Globecom Workshops (GC Wkshps). 317–321. [https://doi.org/10.1109/GCWkshps58843.2023.](https://doi.org/10.1109/GCWkshps58843.2023.10465179)
    [10465179](https://doi.org/10.1109/GCWkshps58843.2023.10465179)

    - <span id="page-12-12"></span>[29] Thiago Ize, Ingo Wald, and Steven G. Parker.
    2007. Asynchronous BVH construction for ray tracing dynamic scenes on parallel
    multi-core architectures. In


    Proceedings of the 7th Eurographics conference on Parallel Graphics and Visualization
    (EGPGV ''07). Eurographics Association, Goslar, DEU, 101–108.


    - <span id="page-12-10"></span>[30] James T. Kajiya. 1986. The rendering equation.
    SIGGRAPH Comput. Graph. 20, 4 (Aug. 1986), 143–150. <https://doi.org/10.1145/15886.15902>

    - <span id="page-12-16"></span>[31] Mahmoud Khairy, Zhesheng Shen, Tor M. Aamodt,
    and Timothy G. Rogers. 2020. Accel-Sim: An Extensible Simulation Framework for
    Validated GPU Modeling. In 2020 ACM/IEEE 47th Annual International Symposium on
    Computer Architecture (ISCA). 473–486. <https://doi.org/10.1109/ISCA45697.2020.00047>

    - <span id="page-12-14"></span>[32] C. Lauterbach, M. Garland, S. Sengupta, D.
    Luebke, and D. Manocha. 2009. Fast BVH Construction on GPUs. Computer Graphics
    Forum 28, 2 (2009), 375–384. <https://doi.org/10.1111/j.1467-8659.2009.01377.x>
    \_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-8659.2009.01377.x.

    - <span id="page-12-27"></span>[33] Jingwen Leng, Tayler Hetherington, Ahmed ElTantawy,
    Syed Gilani, Nam Sung Kim, Tor M. Aamodt, and Vijay Janapa Reddi. 2013. GPUWattch:
    enabling energy optimizations in GPGPUs. ACM SIGARCH Computer Architecture News
    41, 3 (June 2013), 487–498. <https://doi.org/10.1145/2508148.2485964>

    - <span id="page-12-33"></span>[34] Lufei Liu, Wesley Chang, Francois Demoullin,
    Yuan Hsi Chou, Mohammadreza Saed, David Pankratz, Tyler Nowicki, and Tor M. Aamodt.
    2021. Intersection Prediction for Accelerated GPU Ray Tracing. In MICRO-54: 54th
    Annual IEEE/ACM International Symposium on Microarchitecture (MICRO ''21). Association
    for Computing Machinery, New York, NY, USA, 709–723. [https://doi.org/10.1145/3466752.](https://doi.org/10.1145/3466752.3480097)
    [3480097](https://doi.org/10.1145/3466752.3480097)

    - <span id="page-12-28"></span>[35] Lufei Liu, Mohammadreza Saed, Yuan Hsi Chou,
    Davit Grigoryan, Tyler Nowicki, and Tor M. Aamodt. 2023. LumiBench: A Benchmark
    Suite for Hardware Ray Tracing. In 2023 IEEE International Symposium on Workload
    Characterization (IISWC). 1–14. <https://doi.org/10.1109/IISWC59245.2023.00011>
    ISSN: 2835-2238.

    - <span id="page-12-13"></span>[36] J. David MacDonald and Kellogg S. Booth. 1990.
    Heuristics for ray tracing using space subdivision. The Visual Computer 6, 3 (May
    1990), 153–166. [https:](https://doi.org/10.1007/BF01911006) [//doi.org/10.1007/BF01911006](https://doi.org/10.1007/BF01911006)

    - <span id="page-12-18"></span>[37] Mohammadreza Saed, Yuan Hsi Chou, Lufei Liu,
    Tyler Nowicki, and Tor M. Aamodt. 2022. Vulkan-Sim: A GPU Architecture Simulator
    for Ray Tracing. In 2022 55th IEEE/ACM International Symposium on Microarchitecture
    (MICRO). 263–281. <https://doi.org/10.1109/MICRO56248.2022.00027>

    - <span id="page-12-32"></span>[38] James E. Stine, Ivan Castellanos, Michael
    Wood, Jeff Henson, Fred Love, W. Rhett Davis, Paul D. Franzon, Michael Bucher,
    Sunil Basavarajaiah, Julie Oh, and Ravi Jenkal. 2007. FreePDK: An Open-Source
    Variation-Aware Design Kit. In 2007 IEEE International Conference on Microelectronic
    Systems Education (MSE''07). 173–174. <https://doi.org/10.1109/MSE.2007.44>

    - <span id="page-12-5"></span>[39] Jundong Tan, Zhuo Su, and Yunliang Long. 2015.
    A Full 3-D GPU-based Beam-Tracing Method for Complex Indoor Environments Propagation
    Modeling. IEEE Transactions on Antennas and Propagation 63, 6 (June 2015), 2705–2718.
    [https:](https://doi.org/10.1109/TAP.2015.2415036) [//doi.org/10.1109/TAP.2015.2415036](https://doi.org/10.1109/TAP.2015.2415036)
    Conference Name: IEEE Transactions on Antennas and Propagation.

    - <span id="page-12-17"></span>[40] Aditya Ukarande, Suryakant Patidar, and Ram
    Rangan. 2021. Locality-Aware CTA Scheduling for Gaming Applications. ACM Transactions
    on Architecture and Code Optimization 19, 1 (Dec. 2021), 1:1–1:26. <https://doi.org/10.1145/3477497>

    - <span id="page-12-23"></span><span id="page-12-15"></span>[41] Vulkan. 2024.
    Home | Vulkan | Cross platform 3D Graphics. <https://vulkan.org/> [42] Ingo Wald.
    2011. Active thread compaction for GPU path tracing. In Proceedings of the ACM
    SIGGRAPH Symposium on High Performance Graphics (HPG ''11). Association for Computing
    Machinery, New York, NY, USA, 51–58. [https://doi.](https://doi.org/10.1145/2018323.2018331)

    - <span id="page-12-6"></span>[org/10.1145/2018323.2018331](https://doi.org/10.1145/2018323.2018331)
    [43] Kathryn Williams, Luis Tirado, Zhongliang Chen, Borja Gonzalez-Valdes, José
    Ángel Martínez, and Carey M. Rappaport. 2015. Ray Tracing for Simulation of Millimeter-Wave
    Whole Body Imaging Systems. IEEE Transactions on Antennas and Propagation 63,
    12 (Dec. 2015), 5913–5918. [https://doi.org/10.1109/TAP.2015.](https://doi.org/10.1109/TAP.2015.2486801)

    - <span id="page-12-26"></span>[2486801](https://doi.org/10.1109/TAP.2015.2486801)
    Conference Name: IEEE Transactions on Antennas and Propagation. [44] Yuhao Zhu.
    2022. RTNN: accelerating neighbor search using hardware ray tracing. In Proceedings
    of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming
    (PPoPP ''22). Association for Computing Machinery, New York, NY, USA, 76–89. <https://doi.org/10.1145/3503221.3508409>


    # A Artifact Appendix


    ## A.1 Abstract


    This artifact provides the source code for the modified Vulkan-sim which has newly
    added configuration options to enable CoopRT. In addition, Python and shell scripts
    are provided to run all the necessary simulations and generate the plots and figures
    in this paper. To streamline the artifact evaluation process, we prepared a docker
    image with all the software dependencies installed. As simulations take a long
    time, we also included the raw simulation outputs that we generated and used for
    this paper.


    ## <span id="page-13-0"></span>A.2 Artifact check-list (meta-information)


    - Program: Vulkan-sim, RayTracingInVulkan

    - Compilation: gcc/g++, ninja, meson, cmake, nvcc

    - Run-time environment: Ubuntu 20.04

    - Hardware: 32+GB RAM

    - Metrics: Number of cycles, average power consumption

    - Output: Vulkan-sim simulation outputs, figures.

    - How much disk space required (approximately)?: 30GB

    - How much time is needed to prepare workflow (approximately)?: About 1 hour

    - How much time is needed to complete experiments (approximately)?: 5 minutes
    to generate figures. One week for Vulkan-sim simulations, if ran in parallel.

    - Publicly available?: Yes

    - Code licenses (if publicly available)?: Yes

    - Archived (provide DOI)?: <https://doi.org/10.5281/zenodo.15103378>


    # A.3 Description


    A.3.1 How to access. We provide the Docker image which has everything required
    to run the simulations and generate the figures. You can download it from Zenodo
    using the archived link.


    A.3.2 Hardware dependencies. Only requirement is 32+GB of memory.


    A.3.3 Software dependencies. Only a Docker installation is required. All software
    dependencies are installed in the Docker image.


    # A.4 Installation


    Download the docker image from Zenodo, and start a container using the commands
    below,


    docker load < cooprt-isca2025-ae.tar.gz docker run -it cooprt-isca2025-ae:1.0
    /bin/bash


    # A.5 Experiment workflow


    Inside the container, we provide a shell script cooprt.sh that has all the simulation
    commands needed to generate results. However, due to the large number of simulations,
    we do not recommend running the shell script directly, as it runs the simulations
    sequentially. Instead, depending on the resources, simulations should be run in
    parallel. The shell script simply serves as a reference for simulation commands.
    Workflow for launching parallel jobs depends on the system being used, therefore
    we cannot provide a one-for-all script to launch parallel simulations. To launch
    a simulation in the container,


    cd /home/root/vulkan-sim-root


    source embree-3.13.4.x86\_64.linux/embree-vars.sh


    source vulkan-sim/setup\_environment


    cd RayTracingInVulkan/build/linux/bin


    ./RayTracer --scene 20 --width 256 \


    --height 256 > ship\_pt.log


    We also provide all of the raw simulation logs and the Python scripts that we
    used to plot the figures in this paper. To generate the figures, following command
    can be used inside the container,


    python3 figure1.py


    This will generate fig1.png using the simulation logs under cooprt\_raw\_simulation\_results.
    Other figures can be generated in a similar fashion.


    # A.6 Evaluation and expected results


    Running the Python scripts will generate the figures using the existing simulation
    logs. To reproduce or replace the simulation logs, the simulation commands in
    the shell script can be used.'
  references:
  - '- <span id="page-11-0"></span>[1] [n. d.]. Cyberpunk 2077: Technology Preview
    Of New Ray Tracing Overdrive Mode Out Now. [https://www.nvidia.com/en-us/geforce/news/cyberpunk-2077](https://www.nvidia.com/en-us/geforce/news/cyberpunk-2077-ray-tracing-overdrive-update-launches-april-11/)
    [ray-tracing-overdrive-update-launches-april-11/](https://www.nvidia.com/en-us/geforce/news/cyberpunk-2077-ray-tracing-overdrive-update-launches-april-11/)'
  - '- <span id="page-11-6"></span>[2] [n. d.]. Intel Embree. <https://www.embree.org/>'
  - '- <span id="page-11-3"></span>[3] [n. d.]. Intel® Arc™ Graphics Developer Guide
    for Real-Time Ray Tracing in... [https://www.intel.com/content/www/us/en/developer/articles/guide/real](https://www.intel.com/content/www/us/en/developer/articles/guide/real-time-ray-tracing-in-games.html)[time-ray-tracing-in-games.html](https://www.intel.com/content/www/us/en/developer/articles/guide/real-time-ray-tracing-in-games.html)'
  - '- <span id="page-11-4"></span>[4] [n. d.]. NVIDIA ADA GPU ARCHITECTURE. [https://images.nvidia.com/aem](https://images.nvidia.com/aem-dam/Solutions/geforce/ada/nvidia-ada-gpu-architecture.pdf)[dam/Solutions/geforce/ada/nvidia-ada-gpu-architecture.pdf](https://images.nvidia.com/aem-dam/Solutions/geforce/ada/nvidia-ada-gpu-architecture.pdf)'
  - '- <span id="page-11-5"></span>[5] [n. d.]. NVIDIA AMPERE GA102 GPU ARCHITECTURE.
    [https://www.nvidia.](https://www.nvidia.com/content/PDF/nvidia-ampere-ga-102-gpu-architecture-whitepaper-v2.pdf)
    [com/content/PDF/nvidia-ampere-ga-102-gpu-architecture-whitepaper-v2.pdf](https://www.nvidia.com/content/PDF/nvidia-ampere-ga-102-gpu-architecture-whitepaper-v2.pdf)'
  - '- <span id="page-11-1"></span>[6] [n. d.]. Real-Time Ray Tracing. [https://dev.epicgames.com/documentation/en](https://dev.epicgames.com/documentation/en-us/unreal-engine/hardware-ray-tracing-tips-and-tricks-in-unreal-engine)[us/unreal-engine/hardware-ray-tracing-tips-and-tricks-in-unreal-engine](https://dev.epicgames.com/documentation/en-us/unreal-engine/hardware-ray-tracing-tips-and-tricks-in-unreal-engine)'
  - '- <span id="page-11-2"></span>[7] [n. d.]. Real-time Raytracing for Interactive
    Global Illumination Workflows in Frostbite. <https://www.gdcvault.com/play/1024801/>'
  - '- <span id="page-11-11"></span>[8] Timo Aila and Tero Karras. 2010. Architecture
    considerations for tracing incoherent rays. In Proceedings of the Conference on
    High Performance Graphics (HPG ''10). Eurographics Association, Goslar, DEU, 113–122.'
  - '- <span id="page-11-10"></span>[9] Timo Aila and Samuli Laine. 2009. Understanding
    the efficiency of ray traversal on GPUs. In Proceedings of the Conference on High
    Performance Graphics 2009 (HPG ''09). Association for Computing Machinery, New
    York, NY, USA, 145–149. <https://doi.org/10.1145/1572769.1572792>'
  - '- <span id="page-11-7"></span>[10] Aaron Ariel, Wilson W. L. Fung, Andrew E.
    Turner, and Tor M. Aamodt. 2010. Visualizing complex dynamics in many-core accelerator
    architectures. In 2010'
  - IEEE International Symposium on Performance Analysis of Systems & Software (ISPASS).
    164–174. <https://doi.org/10.1109/ISPASS.2010.5452029>
  - '- <span id="page-12-24"></span>[11] Aaron Barnes, Fangjia Shen, and Timothy G.
    Rogers. 2024. Extending GPU Ray-Tracing Units for Hierarchical Search Acceleration.
    In Proceedings of the 57th Annual IEEE/ACM International Symposium on Microarchitecture
    (MICRO ''24). Association for Computing Machinery, New York, NY, USA.'
  - '- <span id="page-12-31"></span>[12] D.M. Brooks, P. Bose, S.E. Schuster, H. Jacobson,
    P.N. Kudva, A. Buyuktosunoglu, J. Wellman, V. Zyuban, M. Gupta, and P.W. Cook.
    2000. Power-aware microarchitecture: design and modeling challenges for next-generation
    microprocessors. IEEE Micro 20, 6 (Nov. 2000), 26–44. <https://doi.org/10.1109/40.888701>
    Conference Name: IEEE Micro.'
  - '- <span id="page-12-8"></span>[13] Brian Caulfield. 2018. What''s the Difference
    Between Ray Tracing and Rasterization? [https://blogs.nvidia.com/blog/whats-difference-between-ray-tracing](https://blogs.nvidia.com/blog/whats-difference-between-ray-tracing-rasterization/)[rasterization/](https://blogs.nvidia.com/blog/whats-difference-between-ray-tracing-rasterization/)'
  - '- <span id="page-12-9"></span>[14] Brian Caulfield. 2022. What Is Path Tracing?
    [https://blogs.nvidia.com/blog/what](https://blogs.nvidia.com/blog/what-is-path-tracing/)[is-path-tracing/](https://blogs.nvidia.com/blog/what-is-path-tracing/)'
  - '- <span id="page-12-29"></span>[15] Yuan Hsi Chou, Tyler Nowicki, and Tor M.
    Aamodt. 2023. Treelet Prefetching For Ray Tracing. In Proceedings of the 56th
    Annual IEEE/ACM International Symposium on Microarchitecture (MICRO ''23). Association
    for Computing Machinery, New York, NY, USA, 742–755.'
  - '- <span id="page-12-0"></span>[16] Per H. Christensen, Julian Fong, David M.
    Laur, and Dana Batali. 2006. Ray Tracing for the Movie ''Cars''. In 2006 IEEE
    Symposium on Interactive Ray Tracing. 1–6. <https://doi.org/10.1109/RT.2006.280208>'
  - '- <span id="page-12-1"></span>[17] Per H. Christensen and Wojciech Jarosz. 2016.
    The Path to Path-Traced Movies. Foundations and Trends® in Computer Graphics and
    Vision 10, 2 (2016), 103–175. <https://doi.org/10.1561/0600000073>'
  - '- <span id="page-12-19"></span>[18] Sana Damani, Mark Stephenson, Ram Rangan,
    Daniel Johnson, Rishkul Kulkami, and Stephen W. Keckler. 2022. GPU Subwarp Interleaving.
    In 2022 IEEE International Symposium on High-Performance Computer Architecture
    (HPCA). 1184–1197. <https://doi.org/10.1109/HPCA53966.2022.00090> ISSN: 2378-203X.'
  - '- <span id="page-12-20"></span>[19] Ahmed ElTantawy, Jessica Wenjie Ma, Mike
    O''Connor, and Tor M. Aamodt. 2014. A scalable multi-path microarchitecture for
    efficient GPU control flow. In 2014 IEEE 20th International Symposium on High
    Performance Computer Architecture (HPCA). 248–259. <https://doi.org/10.1109/HPCA.2014.6835936>
    ISSN: 2378-203X.'
  - '- <span id="page-12-2"></span>[20] Robert Felbecker, Leszek Raschkowski, Wilhelm
    Keusgen, and Michael Peter. 2012. Electromagnetic wave propagation in the millimeter
    wave band using the NVIDIA OptiX GPU ray tracing engine. In 2012 6th European
    Conference on Antennas and Propagation (EUCAP). 488–492. <https://doi.org/10.1109/EuCAP.2012.6206198>
    ISSN: 2164-3342.'
  - '- <span id="page-12-21"></span>[21] Wilson W. L. Fung and Tor M. Aamodt. 2011.
    Thread block compaction for efficient SIMT control flow. In Proceedings of the
    2011 IEEE 17th International Symposium on High Performance Computer Architecture
    (HPCA ''11). IEEE Computer Society, USA, 25–36.'
  - '- <span id="page-12-22"></span>[22] Wilson W. L. Fung, Ivan Sham, George Yuan,
    and Tor M. Aamodt. 2007. Dynamic Warp Formation and Scheduling for Efficient GPU
    Control Flow. In Proceedings of the 40th Annual IEEE/ACM International Symposium
    on Microarchitecture (MICRO 40). IEEE Computer Society, USA, 407–420. [https://doi.org/10.1109/MICRO.2007.](https://doi.org/10.1109/MICRO.2007.12)
    [12](https://doi.org/10.1109/MICRO.2007.12)'
  - '- <span id="page-12-11"></span>[23] Jeffrey Goldsmith and John Salmon. 1987.
    Automatic Creation of Object Hierarchies for Ray Tracing. IEEE Computer Graphics
    and Applications 7, 5 (May 1987), 14–20. <https://doi.org/10.1109/MCG.1987.276983>
    Conference Name: IEEE Computer Graphics and Applications.'
  - '- <span id="page-12-30"></span>[24] R. Gonzalez and M. Horowitz. 1996. Energy
    dissipation in general purpose microprocessors. IEEE Journal of Solid-State Circuits
    31, 9 (Sept. 1996), 1277–1284. <https://doi.org/10.1109/4.535411> Conference Name:
    IEEE Journal of Solid-State Circuits.'
  - '- <span id="page-12-7"></span>[25] Johannes Gunther, Stefan Popov, Hans-Peter
    Seidel, and Philipp Slusallek. 2007. Realtime Ray Tracing on GPU with BVH-based
    Packet Traversal. In 2007 IEEE Symposium on Interactive Ray Tracing. 113–118.
    [https://doi.org/10.1109/RT.2007.](https://doi.org/10.1109/RT.2007.4342598) [4342598](https://doi.org/10.1109/RT.2007.4342598)'
  - '- <span id="page-12-25"></span>[26] Dongho Ha, Lufei Liu, Yuan Hsi Chou, Seokjin
    Go, Won Woo Ro, Hung-Wei Tseng, and Tor M. Aamodt. 2024. Generalizing Ray Tracing
    Accelerators for Tree Traversals on GPUs. In Proceedings of the 57th Annual IEEE/ACM
    International Symposium on Microarchitecture (MICRO ''24). Association for Computing
    Machinery, New York, NY, USA.'
  - '- <span id="page-12-3"></span>[27] Danping He, Bo Ai, Ke Guan, Longhe Wang, Zhangdui
    Zhong, and Thomas Kürner. 2019. The Design and Applications of High-Performance
    Ray-Tracing Simulation Platform for 5G and Beyond Wireless Communications: A Tutorial.
    IEEE Communications Surveys & Tutorials 21, 1 (2019), 10–27. [https://doi.org/10.](https://doi.org/10.1109/COMST.2018.2865724)
    [1109/COMST.2018.2865724](https://doi.org/10.1109/COMST.2018.2865724) Conference
    Name: IEEE Communications Surveys & Tutorials.'
  - '- <span id="page-12-4"></span>[28] Jakob Hoydis, Faycal Ait Aoudia, Sebastian
    Cammerer, Merlin Nimier-David, Nikolaus Binder, Guillermo Marcus, and Alexander
    Keller. 2023. Sionna RT: Differentiable Ray Tracing for Radio Propagation Modeling.
    In 2023 IEEE Globecom Workshops (GC Wkshps). 317–321. [https://doi.org/10.1109/GCWkshps58843.2023.](https://doi.org/10.1109/GCWkshps58843.2023.10465179)
    [10465179](https://doi.org/10.1109/GCWkshps58843.2023.10465179)'
  - '- <span id="page-12-12"></span>[29] Thiago Ize, Ingo Wald, and Steven G. Parker.
    2007. Asynchronous BVH construction for ray tracing dynamic scenes on parallel
    multi-core architectures. In'
  - Proceedings of the 7th Eurographics conference on Parallel Graphics and Visualization
    (EGPGV '07). Eurographics Association, Goslar, DEU, 101–108.
  - '- <span id="page-12-10"></span>[30] James T. Kajiya. 1986. The rendering equation.
    SIGGRAPH Comput. Graph. 20, 4 (Aug. 1986), 143–150. <https://doi.org/10.1145/15886.15902>'
  - '- <span id="page-12-16"></span>[31] Mahmoud Khairy, Zhesheng Shen, Tor M. Aamodt,
    and Timothy G. Rogers. 2020. Accel-Sim: An Extensible Simulation Framework for
    Validated GPU Modeling. In 2020 ACM/IEEE 47th Annual International Symposium on
    Computer Architecture (ISCA). 473–486. <https://doi.org/10.1109/ISCA45697.2020.00047>'
  - '- <span id="page-12-14"></span>[32] C. Lauterbach, M. Garland, S. Sengupta, D.
    Luebke, and D. Manocha. 2009. Fast BVH Construction on GPUs. Computer Graphics
    Forum 28, 2 (2009), 375–384. <https://doi.org/10.1111/j.1467-8659.2009.01377.x>
    \_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-8659.2009.01377.x.'
  - '- <span id="page-12-27"></span>[33] Jingwen Leng, Tayler Hetherington, Ahmed
    ElTantawy, Syed Gilani, Nam Sung Kim, Tor M. Aamodt, and Vijay Janapa Reddi. 2013.
    GPUWattch: enabling energy optimizations in GPGPUs. ACM SIGARCH Computer Architecture
    News 41, 3 (June 2013), 487–498. <https://doi.org/10.1145/2508148.2485964>'
  - '- <span id="page-12-33"></span>[34] Lufei Liu, Wesley Chang, Francois Demoullin,
    Yuan Hsi Chou, Mohammadreza Saed, David Pankratz, Tyler Nowicki, and Tor M. Aamodt.
    2021. Intersection Prediction for Accelerated GPU Ray Tracing. In MICRO-54: 54th
    Annual IEEE/ACM International Symposium on Microarchitecture (MICRO ''21). Association
    for Computing Machinery, New York, NY, USA, 709–723. [https://doi.org/10.1145/3466752.](https://doi.org/10.1145/3466752.3480097)
    [3480097](https://doi.org/10.1145/3466752.3480097)'
  - '- <span id="page-12-28"></span>[35] Lufei Liu, Mohammadreza Saed, Yuan Hsi Chou,
    Davit Grigoryan, Tyler Nowicki, and Tor M. Aamodt. 2023. LumiBench: A Benchmark
    Suite for Hardware Ray Tracing. In 2023 IEEE International Symposium on Workload
    Characterization (IISWC). 1–14. <https://doi.org/10.1109/IISWC59245.2023.00011>
    ISSN: 2835-2238.'
  - '- <span id="page-12-13"></span>[36] J. David MacDonald and Kellogg S. Booth.
    1990. Heuristics for ray tracing using space subdivision. The Visual Computer
    6, 3 (May 1990), 153–166. [https:](https://doi.org/10.1007/BF01911006) [//doi.org/10.1007/BF01911006](https://doi.org/10.1007/BF01911006)'
  - '- <span id="page-12-18"></span>[37] Mohammadreza Saed, Yuan Hsi Chou, Lufei Liu,
    Tyler Nowicki, and Tor M. Aamodt. 2022. Vulkan-Sim: A GPU Architecture Simulator
    for Ray Tracing. In 2022 55th IEEE/ACM International Symposium on Microarchitecture
    (MICRO). 263–281. <https://doi.org/10.1109/MICRO56248.2022.00027>'
  - '- <span id="page-12-32"></span>[38] James E. Stine, Ivan Castellanos, Michael
    Wood, Jeff Henson, Fred Love, W. Rhett Davis, Paul D. Franzon, Michael Bucher,
    Sunil Basavarajaiah, Julie Oh, and Ravi Jenkal. 2007. FreePDK: An Open-Source
    Variation-Aware Design Kit. In 2007 IEEE International Conference on Microelectronic
    Systems Education (MSE''07). 173–174. <https://doi.org/10.1109/MSE.2007.44>'
  - '- <span id="page-12-5"></span>[39] Jundong Tan, Zhuo Su, and Yunliang Long. 2015.
    A Full 3-D GPU-based Beam-Tracing Method for Complex Indoor Environments Propagation
    Modeling. IEEE Transactions on Antennas and Propagation 63, 6 (June 2015), 2705–2718.
    [https:](https://doi.org/10.1109/TAP.2015.2415036) [//doi.org/10.1109/TAP.2015.2415036](https://doi.org/10.1109/TAP.2015.2415036)
    Conference Name: IEEE Transactions on Antennas and Propagation.'
  - '- <span id="page-12-17"></span>[40] Aditya Ukarande, Suryakant Patidar, and Ram
    Rangan. 2021. Locality-Aware CTA Scheduling for Gaming Applications. ACM Transactions
    on Architecture and Code Optimization 19, 1 (Dec. 2021), 1:1–1:26. <https://doi.org/10.1145/3477497>'
  - '- <span id="page-12-23"></span><span id="page-12-15"></span>[41] Vulkan. 2024.
    Home | Vulkan | Cross platform 3D Graphics. <https://vulkan.org/> [42] Ingo Wald.
    2011. Active thread compaction for GPU path tracing. In Proceedings of the ACM
    SIGGRAPH Symposium on High Performance Graphics (HPG ''11). Association for Computing
    Machinery, New York, NY, USA, 51–58. [https://doi.](https://doi.org/10.1145/2018323.2018331)'
  - '- <span id="page-12-6"></span>[org/10.1145/2018323.2018331](https://doi.org/10.1145/2018323.2018331)
    [43] Kathryn Williams, Luis Tirado, Zhongliang Chen, Borja Gonzalez-Valdes, José
    Ángel Martínez, and Carey M. Rappaport. 2015. Ray Tracing for Simulation of Millimeter-Wave
    Whole Body Imaging Systems. IEEE Transactions on Antennas and Propagation 63,
    12 (Dec. 2015), 5913–5918. [https://doi.org/10.1109/TAP.2015.](https://doi.org/10.1109/TAP.2015.2486801)'
  - '- <span id="page-12-26"></span>[2486801](https://doi.org/10.1109/TAP.2015.2486801)
    Conference Name: IEEE Transactions on Antennas and Propagation. [44] Yuhao Zhu.
    2022. RTNN: accelerating neighbor search using hardware ray tracing. In Proceedings
    of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming
    (PPoPP ''22). Association for Computing Machinery, New York, NY, USA, 76–89. <https://doi.org/10.1145/3503221.3508409>'
