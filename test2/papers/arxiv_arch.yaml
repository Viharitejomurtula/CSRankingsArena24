papers:
- title: 'NeuroVM: Dynamic Neuromorphic Hardware Virtualization'
  abstract: 'This paper introduces a novel approach in neuromorphic computing, integrating

    heterogeneous hardware nodes into a unified, massively parallel architecture.

    Our system transcends traditional single-node constraints, harnessing the

    neural structure and functionality of the human brain to efficiently process

    complex tasks. We present an architecture that dynamically virtualizes

    neuromorphic resources, enabling adaptable allocation and reconfiguration for

    various applications. Our evaluation, using diverse applications and

    performance metrics, provides significant insights into the system''s

    adaptability and efficiency. We observed scalable throughput increases across

    configurations of 1, 2, and 4 Virtual Machines (VMs), reaching up to 5.1

    Gibibits per second (Gib/s) for different data transfer sizes. This scalability

    demonstrates the system''s capacity to handle tasks that require substantial

    amounts of data. The energy consumption of our virtualized accelerator

    environment increased nearly linearly with the addition of more NeuroVM

    accelerators, ranging from 25 to 45 millijoules (mJ) as the number of

    accelerators increased from 1 to 20. Further, our investigation of

    reconfiguration overheads revealed that partial reconfigurations significantly

    reduce the time spent on reconfigurations compared to full reconfigurations,

    particularly when there are more virtual machines, as indicated by the

    logarithmic scale of time measurements.'
  url: http://arxiv.org/abs/2410.00295v1
  keywords: ''
  document: '# NeuroVM: Dynamic Neuromorphic Hardware Virtualization


    Murat Isik Stanford University Stanford, USA Email: mrtisik@stanford.edu


    Jonathan Naoukin University of Texas at Austin Austin, USA Email: jnaoukin@utexas.edu


    I. Can Dikmen Temsa Research & Development Center Adana, Turkey Email: can.dikmen@temsa.com


    #### ABSTRACT


    This paper introduces a novel approach in neuromorphic computing, integrating
    heterogeneous hardware nodes into a unified, massively parallel architecture.
    Our system transcends traditional single-node constraints, harnessing the neural
    structure and functionality of the human brain to efficiently process complex
    tasks. We present an architecture that dynamically virtualizes neuromorphic resources,
    enabling adaptable allocation and reconfiguration for various applications. Our
    evaluation, using diverse applications and performance metrics, provides significant
    insights into the system''s adaptability and efficiency. We observed scalable
    throughput increases across configurations of 1, 2, and 4 Virtual Machines (VMs),
    reaching up to 5.1 Gibibits per second (Gib/s) for different data transfer sizes.
    This scalability demonstrates the system''s capacity to handle tasks that require
    substantial amounts of data. The energy consumption of our virtualized accelerator
    environment increased nearly linearly with the addition of more NeuroVM accelerators,
    ranging from 25 to 45 millijoules (mJ) as the number of accelerators increased
    from 1 to 20. Further, our investigation of reconfiguration overheads revealed
    that partial reconfigurations significantly reduce the time spent on reconfigurations
    compared to full reconfigurations, particularly when there are more virtual machines,
    as indicated by the logarithmic scale of time measurements.


    #### I. INTRODUCTION


    Over the past decade, the landscape of computation architectures has changed significantly,
    moving away from traditional CPU-based systems toward specialized, parallel architectures
    such as GPUs and FPGAs. The advancements have improved processing power, energy
    efficiency, and the ability to adapt to a variety of workloads. However, with
    the increasing complexity of computational tasks, particularly in artificial intelligence
    and machine learning, new paradigms for computing are needed [\[1\]](#page-5-0)–[\[5\]](#page-5-1).


    Neuromorphic computing, inspired by the structures and functions of the human
    brain, offers a transformational solution. Synaptic connections and neural networks
    are emulated by neuromorphic systems in order to facilitate complex cognitive
    tasks. In addition to their suitability for artificial intelligence, robotics,
    and scientific computations, these systems rely on parallel processing and adaptive
    learning principles [\[6\]](#page-5-2), [\[7\]](#page-5-3). Multi-node neuromorphic
    computing systems leverage the capabilities of neuromorphic computing by integrating
    heterogeneous hardware. A single node cannot address computations beyond its capabilities
    when multiple architectures are integrated [\[8\]](#page-5-4)–[\[13\]](#page-5-5).
    It is challenging to develop scalable and portable software technologies for large-scale
    neuromorphic systems as well as to explore virtualization within them. FP-GAs
    have been rapidly adopted across a wide range of applications due to their versatility.
    The time-consuming nature of feature extraction algorithms makes them unsuitable
    for realtime applications. To address this issue, dedicated hardware is used,
    such as FPGAs, which allow complex operations to be performed in parallel. As
    a result of FPGA virtualization, the hardware is abstracted, the interface is
    decoupled, and the complexity of the framework is hidden [\[14\]](#page-5-6)–[\[18\]](#page-5-7).
    FPGA virtualization methodologies and definitions have evolved with changing application
    requirements. An innovative neuromorphic architecture leveraging dynamic virtualization
    is proposed in this paper to tackle these challenges. Through exploring the integration,
    virtualization, and optimization of neuromorphic hardware nodes, we aim to realize
    the full potential of neuromorphic computing for a wide range of scientific and
    computational applications. Our contributions include:


    - Proposing a novel neuromorphic architecture that integrates multiple hardware
    nodes through dynamic virtualization, enhancing the system''s ability to handle
    complex computations and adapt to varying workloads.

    - Conducting an extensive analysis of key performance indicators such as throughput,
    energy efficiency, and resource utilization, and performing a comparative study
    with traditional single-node neuromorphic systems to highlight the advantages
    of our multi-node, virtualized architecture.

    - Outlining future research directions, focusing on integrating specialized accelerators
    with the neuromorphic fabric and exploring security implications in virtualized
    neuromorphic environments.


    This paper investigates the union of diverse neuromorphic hardware nodes within
    a parallel framework. Section 2 reviews the history and concept of virtualization
    in neuromorphic computing. Section 3 explores the architectural design and the
    dynamic virtualization essential for resource management. Section 4 assesses the
    system''s performance using different VM configurations. Section 5 concludes by
    summarizing the study''s primary insights. Finally, Section 6 envisions future
    work on integrating specialized accelerators and securing virtualized neuromorphic
    environments.


    ## II. BACKGROUND


    Neuromorphic computing, inspired by the human brain, represents a major advancement
    in artificial cognition. These systems mimic neural networks and synaptic connections,
    crucial for AI, robotics, and complex data analysis. However, single-node hardware
    configurations limit their potential. Transitioning to integrated, parallel systems
    is natural but challenging, especially regarding software compatibility and adaptability
    with expansive neuromorphic setups.


    Our goal is to create a unified system architecture to efficiently manage tasks
    across neuromorphic hardware nodes, each contributing unique capabilities. The
    system is designed to be flexible and self-optimizing in response to varying workloads
    as other researchers set up a direction [\[19\]](#page-5-8), [\[20\]](#page-5-9).
    We are exploring neuromorphic virtualization for dynamic reconfiguration and resource
    sharing, aiming to enhance the adaptability, efficiency, and performance of neuromorphic
    systems to move forward from their results [\[21\]](#page-5-10)–[\[23\]](#page-5-11).


    Virtualization has become fundamental in computing, optimally allocating capabilities
    between hardware and OS. Conceived by IBM in the 1960s to partition mainframes
    into multiple virtual instances, virtualization has evolved to improve efficiency
    and reduce costs. The hypervisor, or virtual machine monitor (VMM), abstracts
    physical resources from the OS, enabling multiple OSs to run simultaneously on
    a single hardware platform [\[24\]](#page-5-12)–[\[26\]](#page-5-13). This enhances
    resource utilization and strengthens security, reliability, and resilience. With
    cloud computing''s rise, virtualization bridges hardware and software applications,
    creating a cohesive operational ecosystem. Various virtualization techniques,
    including full, OS-layer, hardware, para, application, and resource virtualization,
    offer distinct benefits in resource sharing, isolation, and efficiency.


    FPGA virtualization is not new; various microkernel-based approaches have been
    explored, enabling mapping and exchange of hardware accelerators to VMs. However,
    these systems focused mainly on resource utilization, neglecting energy efficiency
    and diverse guest OS requirements.


    The FPGA virtualization layer L4ReC was developed to enable shared use of reconfigurable
    resources by multiple guest OS while addressing embedded systems'' constraints.
    It tackles key challenges such as system isolation—ensuring performance and data
    isolation to prevent interference and security breaches. L4ReC integrates FPGA
    virtualization into the L4Re micro-hypervisor, creating virtual FPGAs (vFPGAs)
    that allow VMs to access hardware accelerators regardless of location. A crucial
    feature is the dynamic mapping and scheduling of hardware threads, prioritizing
    real-time needs. Early results on Xilinx Ultrascale MPSoC show improved FPGA resource
    utilization and energy efficiency, especially in battery-powered devices. L4ReC
    represents a significant step forward in optimizing reconfigurable resources in
    embedded systems [\[27\]](#page-5-14).


    Authors [\[28\]](#page-5-15) explore Linux''s foundational concepts and technologies
    underpinning the Virtio-FPGA solution. They provide background on the Virtio standard,
    the Linux FPGA Manager component of the Linux kernel, and the VFIO pass-through
    for ARM in QEMU and Device Tree Overlays technologies. FPGA overlay architectures
    on computational storage devices enable programmable near-storage processing.
    These architectures consist of reconfigurable operators, crossbar stream switches,
    and on-board DRAM, facilitating data movements between operators and the FPGA''s
    DRAM. The storage interfaces in these architectures allow direct access to storage
    units, adopting the NVMe standard for fast and parallel access. Software support
    for these systems includes an abstraction layer simplifying near-storage processing,
    exposing FPGA overlay architecture operators as executable files within an OS.
    In I/O virtualization, software-based virtualization presents virtual device instances
    for device sharing across VMs. Full virtualization uses a trap-and-emulate approach,
    while paravirtualization creates VM-friendly virtual device interfaces.


    #### III. METHODOLOGY


    Neuromorphic hardware, evolving from basic silicon neurons to advanced neuromorphic
    chips, offers significant advantages in dynamic resource management and virtualization.
    The integration of FPGAs with neuromorphic systems brings new capabilities in
    virtualization and resource utilization. This section explores methodologies and
    implications of virtualizing neuromorphic hardware, focusing on design, application,
    and system architecture. Initial efforts aimed to replicate brain neural structures
    in silicon, progressing to sophisticated neuromorphic chips for various fields.
    FPGAs are ideal for neuromorphic systems due to their flexibility and reconfigurability,
    aligning with efficient computing models. Virtualization enables dynamic resource
    management, meeting variable computational demands by reconfiguring resources.
    Studies have explored design methodologies for dynamic management and reconfiguration
    of neuromorphic systems [\[19\]](#page-5-8), [\[29\]](#page-5-16)–[\[31\]](#page-5-17).


    Neuromorphic hardware, designed to emulate biological neurons, faces growing computational
    demands, especially for Spiking Neural Network (SNN) inference. Neuromorphic Hardware
    Virtualization tackles these challenges through dynamic resource allocation and
    reconfiguration, optimizing performance while minimizing power and hardware use.
    Neurons communicate via spikes, reducing logic load, which is essential for NeuroVM.
    This framework combines hardware design, security, and neuromorphic computing
    to create adaptable and secure systems for various tasks.


    The virtualization of neuromorphic hardware is key to our research, offering greater
    efficiency and flexibility. By optimizing task profiling, memory, and interconnects,
    along with a sophisticated kernel controller, we aim to advance neuromorphic computing.
    As shown in [Figure 1,](#page-2-0) neurocores—modular


    | Functional Hierarchy     | Key Technology                             | Neuromorphic<br>Virtualization<br>Solu   |
    Digital Virtualization Solution            |

    |--------------------------|--------------------------------------------|------------------------------------------|--------------------------------------------|

    |                          |                                            | tion                                     |                                            |

    | Resource Pool Management | Integrated<br>neuromorphic<br>resources,   | Use<br>neuromorphic-specific             |
    Use traditional virtualization manage      |

    |                          | centralized<br>management,<br>dynamic      | management
    tools for resource pools.     | ment tools like VMware, and Hyper-V.       |

    |                          | allocation,<br>monitoring,<br>maintenance, |                                          |                                            |

    |                          | and unified scheduling.                    |                                          |                                            |

    | Virtualization Layer     | Neuromorphic virtualization, VM man        | Employ
    neuromorphic-specific virtual     | Utilize hardware-assisted or software      |

    |                          | agement, and container management.         | ization
    technologies for resource ab     | assisted virtualization technologies.      |

    |                          |                                            | straction.                               |                                            |

    | Resource Isolation Layer | Hardware isolation technology, soft        | Implement
    neuromorphic-specific iso      | Use established isolation technologies     |

    |                          | ware isolation technology, or a combi      | lation
    technologies to prevent resource  | like Intel VT-x, and AMD-V.                |

    |                          | nation of the two.                         | contention.                              |                                            |

    | Scheduling Layer         | Neuromorphic resource scheduling al        | Developing<br>and<br>use<br>neuromorphic
    | Apply<br>conventional<br>scheduling<br>and |

    |                          | gorithm, load balancing, resource pre      | aware
    scheduling algorithms for effi     | load balancing algorithms.                 |

    |                          | diction.                                   | cient
    resource allocation.               |                                            |

    | Application Layer        | Neuromorphic task scheduling algo          | Utilize
    algorithms optimized for neuro   | Leverage<br>general-purpose<br>computing   |

    |                          | rithm, dynamic migration technology,       | morphic
    computing tasks and dynamic      | algorithms and migration technologies      |

    |                          | application deployment, and manage         | resource
    management.                     | like live VM migration.                    |

    |                          | ment.                                      |                                          |                                            |


    TABLE I: Comparison of Neuromorphic Virtualization vs. Digital Virtualization


    <span id="page-2-0"></span>![](_page_2_Figure_2.jpeg)


    Fig. 1: Schematic of Neurocore Interconnectivity in Neuromorphic Hardware


    <span id="page-2-1"></span>Fig. 2: FPGA Virtualization Concept


    processing units that emulate brain circuits are interconnected to enable parallel
    processing, which is crucial for high-speed, energy-efficient computations in
    neural network simulations.Neuromorphic computing necessitates hardware platforms
    capable of emulating neural architectures. FPGAs are well-suited for this due
    to their reconfigurable nature and parallel processing capabilities. FPGA virtualization
    abstracts physical FPGA resources to enable multiple applications on a single
    chip. This section details methodologies and strategies in FPGA virtualization,
    emphasizing Dynamic Function Exchange (DFX) for adaptability and efficiency, conceptually
    illustrated in [Figure 2.](#page-2-1) DFX is an FPGA feature facilitating runtime
    reconfiguration of hardware functions without disrupting system operation. DFX
    allows selective activation and deactivation of hardware modules, enabling the
    FPGA to adapt to new tasks on-the-fly. This is crucial in neuromorphic computing,
    where dynamic reconfiguration in response to neural network demands is essential.


    Our approach leverages DFX to create a virtualized neuromorphic hardware environment,
    enabling real-time reconfiguration of neural network models. A dedicated controller
    manages the dynamic loading and unloading of DFX modules, partitioning hardware
    resources for concurrent tasks while ensuring data isolation and security. Virtualization
    improves adaptability and performance through resource sharing. For I/O, software
    mechanisms such as full and paravirtualization facilitate device sharing across
    multiple VMs, with paravirtualization reducing overhead. Our methodology includes
    an advanced task profiling system that assesses tasks based on processing demands
    and parallel execution suitability. This guides task allocation to appropriate
    neuromorphic nodes, optimizing resource utilization and computational throughput.
    We design specialized memory hierarchies and interconnect architectures tailored
    to neuromorphic hardware, aiming to establish a high-bandwidth, low-latency communication
    framework for efficient data exchange across the neuromorphic network. The kernel
    controller driver manages data influx and controls neuromorphic processor cores.
    Inspired by existing paradigms like the Coyote driver, our design virtualizes
    neuromorphic processors, replacing traditional VFPGAs with bespoke designs. The
    driver interfaces with user space through C++ constructs, ensuring seamless hardware
    interaction.


    FPGA virtualization segments FPGA capabilities to mimic


    multiple discrete processing units, crucial for resource management similar to
    virtual machines. Our project uses the Vitis platform for FPGA development, leveraging
    C++ and embedded systems expertise to implement PS-PL relationships within the
    architecture. We focus on high-level synthesis in Vitis to meet neuromorphic virtualization
    needs. Table [II](#page-3-0) summarizes the resource utilization for the Zynq
    UltraScale+ XCZU7EV MPSoC, detailing used and available resources such as Logic
    Cells, Memory, DSP Slices, and I/O Pins, along with utilization percentages for
    reference. Our system addresses resource contention by employing a dynamic scheduling
    algorithm that prioritizes tasks based on real-time requirements and resource
    availability. Each VM operates within a defined allocation of neuromorphic resources,
    and when contention occurs, the system dynamically reallocates resources to ensure
    tasks maintain performance. Additionally, the architecture incorporates performance
    isolation mechanisms to reduce the impact of contention on overall system efficiency.


    TABLE II: Resource utilization summary


    <span id="page-3-0"></span>


    | Zynq UltraScale+ XCZU7EV |             |           |               |  |

    |--------------------------|-------------|-----------|---------------|--|

    | Resource                 | Utilization | Available | % Utilization |  |

    | LUT                      | 151,200     | 504,000   | 30            |  |

    | Memory                   | 11.4MB      | 38MB      | 30            |  |

    | IO                       | 139         | 464       | 29.19         |  |

    | DSP                      | 518         | 1,728     | 29.94         |  |


    [Figure 3](#page-4-0) shows the task management and processing architecture in
    a neuromorphic computing system. The ''Driver'' handles system inputs and outputs,
    interfacing with internal processing units. Below it, the ''Service Scheduler''
    efficiently allocates tasks to ''Neuroprocessors'' optimized for neural network
    simulations. The ''Proc'' blocks represent individual processing units executing
    tasks, while the ''FPGA Manager'' oversees on-the-fly hardware reconfiguration.
    The system includes buffering to manage peak processing times and ensures smooth
    task execution across multiple dedicated hardware units. In our system, communication
    between nodes is facilitated by a highbandwidth, low-latency interconnect architecture
    specifically designed for neuromorphic hardware. We mitigate potential communication
    bottlenecks by optimizing task profiling and dynamic resource allocation. Parallel
    processing is ensured by the system''s modular neurocores, which are interconnected
    to ensure efficient data exchange. Additionally, we incorporate advanced scheduling
    strategies to minimize data transfer delays, especially when handling intensive,
    real-time tasks. As a result, our architecture supports scalability and performance
    in distributed, multi-node environments.


    #### IV. EVALUATION


    The key performance indicators we measured were throughput, energy efficiency,
    resource utilization, and reconfiguration overhead. Scalability, adaptability,
    and energy consumption are among the architecture''s strengths, demonstrating
    its advantages over previous designs.


    [Figure 4](#page-4-1) (a) illustrates the relationship between transfer size and
    throughput in Gibibits per second (Gib/s) for 1, 2, and 4 VMs. VM configurations
    are represented by different colors and markers, revealing how throughput scales
    as transfer sizes increase. These three lines provide insight into the efficiency
    and scalability of the VMs under varying data transfer loads, with the black dashed
    line representing one VM, the red solid line representing two VMs, and the blue
    dotted line representing four VMs. This analysis is performed to understand the
    performance dynamics for neuromorphic computing designs. [Figure 4](#page-4-1)
    (b) depicts the percentage of resource utilization against different transfer
    sizes for 1, 2, and 4 VMs. VM utilization efficiency is shown by different colored
    lines and markers. This graph shows the results for neuromorphic VM efficiency
    by showing how resource utilization varies with throughput size. The utilization
    increases with larger throughput sizes, especially in more VM-based configurations,
    highlighting the importance of VM density in neuromorphic computing environments.
    [Figure 5](#page-4-2) illustrates the energy consumption trends in a virtualized
    accelerator environment. According to the results, as the number of NeuroVM accelerators
    increases, the energy demand also increases. Understanding this trend is of great
    importance, especially in the context of neuromorphic computing, where energy
    management is an important performance metric. The almost linear increase in energy
    consumption with more accelerators reveals the need to optimize resource allocation
    to balance computational power and energy efficiency.


    [Figure 6](#page-5-18) presents a comparative analysis of the implementation overhead
    for full and partial reconfigurations in a NeuroVM environment. Full reconfigurations
    are shown with a red line with circle markers, while partial reconfigurations
    are shown with a blue line with square markers. In particular, partial reconfiguration
    shows a significantly reduced time overhead compared to full reconfiguration,
    especially as the number of virtual machines increases. In neuromorphic virtual
    machines, where reconfiguration times play a critical role in overall system efficiency
    and responsiveness, this observation is crucial for optimizing performance. Partial
    reconfiguration strategies can benefit dynamic computing environments that require
    rapid adaptation. Through this comprehensive evaluation, we aimed to demonstrate
    the robustness, versatility, and efficiency of our neuromorphic computing architecture.
    Our design is designed to address a variety of applications and leverage the benefits
    of virtualization, and showcases the potential of virtualized neuromorphic hardware
    in shared computing situations.


    ## V. CONCLUSION


    We evaluated an innovative neuromorphic computing architecture that demonstrates
    the potential of high-performance computing to be a unique field. Throughput,
    energy efficiency, resource utilization, and reconfiguration overhead are tested
    for our multi-node, virtualized neuromorphic architecture. According to the accompanying
    figures, this architecture offers significant advantages over traditional single-node
    neuromorphic systems. Partially reconfiguring virtual machines significantly reduces
    the overhead compared to a full reconfiguration,


    <span id="page-4-0"></span>![](_page_4_Figure_0.jpeg)


    Fig. 3: Task Management and Processing Architecture in Neuromorphic Systems


    <span id="page-4-1"></span>![](_page_4_Figure_2.jpeg)


    Fig. 4: Comparative analysis of NeuroVM performance.


    especially as the number of virtual machines increases. As a result of this approach,
    computational interruptions are minimized and tasks with tight deadlines are not
    delayed by


    <span id="page-4-2"></span>![](_page_4_Figure_5.jpeg)


    Fig. 5: NeuroVM Energy Consumption.


    reconfigurations. It will be important to compare our system with other multi-node
    neuromorphic architectures.


    ### VI. FUTURE WORKS


    We plan to compare our approach with multi-node neuromorphic systems to determine
    its advantages and limitations. This work will include benchmarking performance
    metrics such as throughput, latency, power consumption, and reconfiguration overheads
    on various systems. Our future efforts will focus on several goals to improve
    our proposed neuromorphic architecture. The primary goal will be the integration
    of specialized accelerators to increase computational efficiency and expand the
    ability to handle complex, intensive tasks such as advanced machine learning and
    real-time analytics. Additionally, we plan to address security challenges in virtualized
    neuromorphic environments by developing protocols to


    <span id="page-5-18"></span>![](_page_5_Figure_0.jpeg)


    Fig. 6: NeuroVM implementation overhead with full and partial reconfigurations.


    mitigate threats and ensure system integrity. We will continue to optimize the
    architecture to meet the specific needs of a variety of applications, from scientific
    simulations to industrial processing.


    #### REFERENCES


    - <span id="page-5-0"></span>[1] C. Bobda, J. M. Mbongue, P. Chow, M. Ewais, N.
    Tarafdar, J. C. Vega, K. Eguro, D. Koch, S. Handagala, M. Leeser *et al.*, "The
    future of fpga acceleration in datacenters and the cloud," *ACM Transactions on
    Reconfigurable Technology and Systems (TRETS)*, vol. 15, no. 3, pp. 1–42, 2022.

    - [2] E. Nurvitadhi, J. Sim, D. Sheffield, A. Mishra, S. Krishnan, and D. Marr,
    "Accelerating recurrent neural networks in analytics servers: Comparison of fpga,
    cpu, gpu, and asic," in *2016 26th International Conference on Field Programmable
    Logic and Applications (FPL)*. IEEE, 2016, pp. 1–4.

    - [3] C. M. Wyant, C. R. Cullinan, and T. R. Frattesi, "Computing performance
    benchmarks among cpu, gpu, and fpga," *Computing*, 2012.

    - [4] M. Isik, "A survey of spiking neural network accelerator on fpga," *arXiv
    preprint arXiv:2307.03910*, 2023.

    - <span id="page-5-1"></span>[5] B. Vogginger, A. Rostami, V. Jain, S. Arfa, A.
    Hantsch, D. Kappel, M. Schafer, U. Faltings, H. A. Gonzalez, C. Liu *et al.*,
    "Neuromorphic hardware for sustainable ai data centers," *arXiv preprint arXiv:2402.02521*,
    2024.

    - <span id="page-5-2"></span>[6] M. Isik, N. Howard, S. Miziev, and W. Pawlak,
    "Advancing neuromorphic computing: Mixed-signal design techniques leveraging brain
    code units and fundamental code units," in *2024 International Joint Conference
    on Neural Networks (IJCNN)*. IEEE, 2024, pp. 1–8.

    - <span id="page-5-3"></span>[7] M. Isik, K. Tiwari, M. B. Eryilmaz, and I. Dikmen,
    "Accelerating sensor fusion in neuromorphic computing: A case study on loihi-2,"
    *arXiv preprint arXiv:2408.16096*, 2024.

    - <span id="page-5-4"></span>[8] S. Qin, F. Wang, Y. Liu, Q. Wan, X. Wang, Y.
    Xu, and et al., "A lightstimulated synaptic device based on graphene hybrid phototransistor,"
    *2D Mater.*, vol. 4, no. 3, p. 035022, 2017.

    - [9] Z. Du, D. D. Ben-Dayan Rubin, Y. Chen, L. He, T. Chen, L. Zhang, C. Wu,
    and O. Temam, "Neuromorphic accelerators: A comparison between neuroscience and
    machine-learning approaches," in *Proceedings of the 48th International Symposium
    on Microarchitecture*, 2015, pp. 494–507.

    - [10] A. Balaji, S. Song, A. Das, N. Dutt, J. Krichmar, N. Kandasamy, and F.
    Catthoor, "A framework to explore workload-specific performance and lifetime trade-offs
    in neuromorphic computing," *IEEE Computer Architecture Letters*, vol. 18, no.
    2, pp. 149–152, 2019.

    - [11] O. Moreira, A. Yousefzadeh, F. Chersi, A. Kapoor, R.-J. Zwartenkot, P.
    Qiao, G. Cinserin, M. A. Khoei, M. Lindwer, and J. Tapson, "Neuronflow: A hybrid
    neuromorphic–dataflow processor architecture for ai workloads," in *2020 2nd IEEE
    International Conference on Artificial Intelligence Circuits and Systems (AICAS)*.
    IEEE, 2020, pp. 01–05.

    - [12] M. Isik, K. Inadagbo, and H. Aktas, "Design optimization for highperformance
    computing using fpga," in *Annual International Conference on Information Management
    and Big Data*. Springer, 2023, pp. 142– 156.

    - <span id="page-5-5"></span>[13] G. Fanuli, "Allowing prototyping of applications
    running on heterogeneous hw through a multi-tenant platform based on cloud microservices,"
    Ph.D. dissertation, Politecnico di Torino, 2024.

    - <span id="page-5-6"></span>[14] A. Vaishnav, K. D. Pham, and D. Koch, "A survey
    on fpga virtualization," in *2018 28th International Conference on Field Programmable
    Logic and Applications (FPL)*. IEEE, 2018, pp. 131–1317.

    - [15] M. H. Quraishi, E. B. Tavakoli, and F. Ren, "A survey of system architectures
    and techniques for fpga virtualization," *IEEE Transactions on Parallel and Distributed
    Systems*, vol. 32, no. 9, pp. 2216–2230, 2021.

    - [16] S. Zeng, G. Dai, H. Sun, K. Zhong, G. Ge, K. Guo, Y. Wang, and H. Yang,
    "Enabling efficient and flexible fpga virtualization for deep learning in the
    cloud," in *2020 IEEE 28th Annual International Symposium on Field-Programmable
    Custom Computing Machines (FCCM)*. IEEE, 2020, pp. 102–110.

    - [17] S. Yazdanshenas and V. Betz, "Quantifying and mitigating the costs of fpga
    virtualization," in *2017 27th International Conference on Field Programmable
    Logic and Applications (FPL)*. IEEE, 2017, pp. 1–7.

    - <span id="page-5-7"></span>[18] Y. Wang, "Low-power design of advanced image
    processing algorithms under fpga in real-time applications," in *2024 IEEE 4th
    International Conference on Power, Electronics and Computer Applications (ICPECA)*.
    IEEE, 2024, pp. 1080–1084.

    - <span id="page-5-8"></span>[19] M. Nilsson, O. Schelén, A. Lindgren, U. Bodin,
    C. Paniagua, J. Delsing, and F. Sandin, "Integration of neuromorphic ai in event-driven
    distributed digitized systems: Concepts and research directions," *Frontiers in
    Neuroscience*, vol. 17, p. 1074439, 2023.

    - <span id="page-5-9"></span>[20] P. K. Huynh, M. L. Varshika, A. Paul, M. Isik,
    A. Balaji, and A. Das, "Implementing spiking neural networks on neuromorphic architectures:
    A review," *arXiv preprint arXiv:2202.08897*, 2022.

    - <span id="page-5-10"></span>[21] L. Concha Salor and V. Monzon Baeza, "Harnessing
    the potential of emerging technologies to break down barriers in tactical communications,"
    *Telecom*, vol. 4, no. 4, pp. 709–731, 2023. [Online]. Available:<https://www.mdpi.com/2673-4001/4/4/32>

    - [22] M. Pfeiffer and T. Pfeil, "Deep learning with spiking neurons: Opportunities
    and challenges," *Frontiers in neuroscience*, vol. 12, p. 774, 2018.

    - <span id="page-5-11"></span>[23] M. Isik and K. Inadagbo, "Astrocyte-integrated
    dynamic function exchange in spiking neural networks," in *International Conference
    on Engineering of Computer-Based Systems*. Springer, 2023, pp. 263– 273.

    - <span id="page-5-12"></span>[24] R. Schmid, M. Plauth, L. Wenzel, F. Eberhardt,
    and A. Polze, "Accessible near-storage computing with fpgas," in *Proceedings
    of the Fifteenth European Conference on Computer Systems*, 2020, pp. 1–12.

    - [25] X. Li and D. L. Maskell, "Time-multiplexed fpga overlay architectures:
    A survey," *ACM Transactions on Design Automation of Electronic Systems (TODAES)*,
    vol. 24, no. 5, pp. 1–19, 2019.

    - <span id="page-5-13"></span>[26] A. Brant and G. G. Lemieux, "Zuma: An open
    fpga overlay architecture," in *2012 IEEE 20th international symposium on fieldprogrammable
    custom computing machines*. IEEE, 2012, pp. 93–96.

    - <span id="page-5-14"></span>[27] C. Wulf and D. Göhringer, "Virtualization of
    embedded reconfigurable systems," in *2022 32nd International Conference on Field-Programmable
    Logic and Applications (FPL)*. IEEE, 2022, pp. 460– 461.

    - <span id="page-5-15"></span>[28] S. Bandara, A. Sanaullah, Z. Tahir, U. Drepper,
    and M. Herbordt, "Enabling virtio driver support on fpgas," in *2022 IEEE/ACM
    International Workshop on Heterogeneous High-performance Reconfigurable Computing
    (H2RC)*. IEEE, 2022, pp. 1–8.

    - <span id="page-5-16"></span>[29] E. Johnson, C. Pronovost, and J. Criswell,
    "Hardening hypervisors with ombro," in *2022 USENIX Annual Technical Conference
    (USENIX ATC 22)*, 2022, pp. 415–436.

    - [30] B. Pachideh, C. Zielke, S. Nitzsche, and J. Becker, "Towards hardwaresoftware
    self-adaptive acceleration of spiking neural networks on reconfigurable digital
    hardware," in *2023 IEEE 36th International System-on-Chip Conference (SOCC)*.
    IEEE, 2023, pp. 1–6.

    - <span id="page-5-17"></span>[31] C. U. Kumar, P. Saravanan, N. Thiyagarajan,
    and V. Raj, "Digital implementation of neural network by partial reconfiguration,"
    in *Neuromorphic Computing Systems for Industry 4.0*. IGI Global, 2023, pp. 226–260.'
- title: "A Reconfigurable Approximate Computing RISC-V Platform for\n  Fault-Tolerant\
    \ Applications"
  abstract: 'The demand for energy-efficient and high performance embedded systems
    drives

    the evolution of new hardware architectures, including concepts like

    approximate computing. This paper presents a novel reconfigurable embedded

    platform named "phoeniX", using the standard RISC-V ISA, maximizing energy

    efficiency while maintaining acceptable application-level accuracy. The

    platform enables the integration of approximate circuits at the core level with

    diverse structures, accuracies, and timings without requiring modifications to

    the core, particularly in the control logic. The platform introduces novel

    control features, allowing configurable trade-offs between accuracy and energy

    consumption based on specific application requirements. To evaluate the

    effectiveness of the platform, experiments were conducted on a set of

    applications, such as image processing and Dhrystone benchmark. The core with

    its original execution engine, occupies 0.024mm2 of area, with average power

    consumption of 4.23mW at 1.1V operating voltage, average energy-efficiency of

    7.85pJ per operation at 620MHz frequency in 45nm CMOS technology. The

    configurable platform with a highly optimized 3-stage pipelined RV32I(E)M

    architecture, possesses a DMIPS/MHz of 1.89, and a CPI of 1.13, showcasing

    remarkable capabilities for an embedded processor.'
  url: http://arxiv.org/abs/2410.00622v1
  keywords: Approximate Computing, Low Power Design, RISC-V, Embedded Processor, Energy-Efficient
    Computation, Very Large Scale Integration
  document: '# A Reconfigurable Approximate Computing RISC-V Platform for Fault-Tolerant
    Applications


    A. Delavari, F. Ghoreishy, H. S. Shahhoseini, S. Mirzakuchaki


    *School of Electrical Engineering*


    *Iran University of Science and Technology*


    Tehran, Iran


    {arvin delavari, faraz ghoreishy}@elec.iust.ac.ir, {shahhoseini, m kuchaki}@iust.ac.ir


    *Abstract*—The demand for energy-efficient and highperformance embedded systems
    drives the evolution of new hardware architectures, including concepts like approximate
    computing. This paper presents a novel reconfigurable embedded platform named
    "phoeniX", using the standard RISC-V ISA, maximizing energy efficiency while maintaining
    acceptable application-level accuracy. The platform enables the integration of
    approximate circuits at the core level with diverse structures, accuracies, and
    timings without requiring modifications to the core, particularly in the control
    logic. The platform introduces novel control features, allowing configurable trade-offs
    between accuracy and energy consumption based on specific application requirements.
    To evaluate the effectiveness of the platform, experiments were conducted on a
    set of applications, such as image processing and Dhrystone benchmark. The core
    with its original execution engine, occupies 0.024mm² of area, with average power
    consumption of 4.23mW at 1.1V operating voltage, average energy-efficiency of
    7.85pJ per operation at 620MHz frequency in 45nm CMOS technology. The configurable
    platform with a highly optimized 3-stage pipelined RV32I(E)M architecture, possesses
    a DMIPS/MHz of 1.89, and a CPI of 1.13, showcasing remarkable capabilities for
    an embedded processor.


    *Index Terms*—Approximate Computing, Low Power Design, RISC-V, Embedded Processor,
    Energy-Efficient Computation, Very Large Scale Integration


    ## I. INTRODUCTION


    Approximate computing is an emerging paradigm for energy-efficient and high-performance
    designs. It includes various sets of computation techniques that return a possibly
    inaccurate result rather than a guaranteed accurate result. This approach is useful
    in applications where an approximation is sufficient for its purpose. Approximation
    techniques are employed in applications such as image processing [\[1\]](#page-7-0),
    neural networks [\[2\]](#page-7-1), machine learning and PIM methods [\[3\]](#page-7-2)
    due to their inherent capabilities of adhering inaccurate results. By utilizing
    approximation, these applications can handle large computational workloads, maintaining
    acceptable accuracy.


    The proposed platform is crafted in a way to be a global and high-quality foundation
    for integration of approximate computing techniques, with the RISC-V architecture.
    The general idea of the scheme is integration of approximate arithmetic circuits
    into a highly optimized 3-stage pipelined processor with minimum internal fragmentation
    [\[4\]](#page-7-3), constructing an embedded core, suitable for error-tolerant
    applications.


    The modular architecture of the platform enables designers to add and test approximate
    arithmetic circuits on the core level, without any need for changes in other parts
    of the processor such as control logic and etc. With this capability, designers
    can enhance performance by adding new features, and develop different architectural
    techniques. This project has been open-sourced[1](#page-0-0) with technical documentation,
    helping designers to fully understand the structure of the platform.


    In this work, a processor is introduced and described as a novel approximate computing
    development platform using standard RISC-V instruction set architecture and conventions.
    The major contributions of this paper are as follows:


    - Introduction of the phoeniX core, a modular and reconfigurable, 32-bit optimized
    3-stage pipelined RV32I(E)M embedded processor platform for approximate computing
    applications with specialized features, designed in Verilog HDL.

    - Introduction of a customized control logic for accurate/approximate execution
    units with error-configurablity feature, using RISC-V Control Status Registers
    (CSRs), and dynamic internal circuit switching using CSRs for all execution units
    integrated within the core.

    - Design and implementation of an accuracy controllable, very fast and low-power
    Carry Select Adder circuit as the default (demo) adder circuit of the platform.

    - Design and implementation of an error configurable, high-speed and low-power
    approximate multiplier circuit as the default (demo) multiplier circuit of the
    platform.

    - Design and implementation of a dynamic accuracy controllable non-restoring divider
    unit, with accurate division capability at the base error level.


    The rest of the paper is organized as follows: In Section [II](#page-0-1) investigations
    on related works are discussed. In section [III](#page-2-0) the architecture of
    the platform is described in details. In Section [IV,](#page-3-0) the execution
    engine of the core is explained and Section [V](#page-4-0) is considered for experimental
    results on an image processing application. Hardware implementation results and
    comparison are included in Section [VI](#page-5-0) and finally, the paper is concluded
    in Section [VII.](#page-7-4)


    ## II. RELATED WORKS


    <span id="page-0-1"></span>Over the last few years, approximate computing has
    enhanced the energy efficiency of many applications. There has


    <span id="page-0-0"></span><sup>1</sup><https://github.com/phoeniX-Digital-Design/phoeniX>


    ![](_page_1_Figure_0.jpeg)


    <span id="page-1-0"></span>Fig. 1. The phoeniX RISC-V processor block diagram


    been recent exploration into the utilization of RISC-V in combination with approximate
    computing methods. Software adaptation has resulted in the proposal of an ISA
    extension, along with a control mechanism for multi-level accuracy, which is discussed
    in [\[5\]](#page-7-5). Software approximation methods can be effective in certain
    cases, it is important to recognize that approximate hardware techniques are often
    highlighted and evaluated independently, and are typically implemented in customized
    application-specific settings. AxPIKE, an ISA simulator that enables incorporation
    of hardware approximation at the instruction level and assess their impact on
    result, is introduced at [\[6\]](#page-8-0). By implanting approximation in multiplication
    and memory access instructions of various applications, the authors demonstrate
    how the resulting statistics are used to evaluate quality trade-offs with respect
    to energy. The study in [\[7\]](#page-8-1) by Felzmann et al. characterized an
    extension for a RISC-V architecture that orchestrates diverse circuit-level approximation.
    The authors aimed to connect software and hardware approximation methods for better
    outcomes.


    In [\[8\]](#page-8-2), a customized core based on RI5CY [\[9\]](#page-8-3) is
    implemented, by integrating an approximate multiplier within. MARLIN [\[10\]](#page-8-4)
    is also a framework that enables the deployment of approximate neural networks
    (NNs) on energy-constrained devices like microcontrollers. It integrates a customizable
    multiplier architecture with runtime selection of 256 approximation levels into
    a PULP microcontroller [\[11\]](#page-8-5), allowing for energy-efficient execution
    of NNs.


    Investigations were conducted recently on advantages of a Multi-Processor System-on-Chip
    (MPSoC) incorporating a blend of both exact and approximate cores. The integration
    of approximate computing in company with accurate computation capabilities in
    an MPSoC holds potential for leveraging the benefits offered by approximation,
    while maintaining the precision provided by exact computing. In [\[12\]](#page-8-6),
    a multi-core system with exact and approximate cores named AxE is introduced.
    AxE is a heterogeneous RISC-V MPSoC with exact and approximate cores using PicoRV32
    [\[13\]](#page-8-7), that allows exploring hardware approximation for any application
    by using separate software instructions for approximate and exact computation.


    Our approach presents a different perspective on the integration of RISC-V processors
    and approximate computing. The proposed design incorporates a single RV32I(E)M
    core where the output of each execution unit switches between accurate and approximate
    circuits by user''s request. Parameters are included within the RTL source in
    order to switch between I and E extension, and enabling M extension of RISC-V
    standard ISA. Each execution unit of the platform can host four arithmetic circuits
    and is capable of switching between these circuits. Also, support for error control
    is provided if applicable in these modules.


    This method combines the precision of accurate circuits with the efficiency and
    potential energy savings offered by approximate circuits. While the MPSoC approach
    presented in [\[12\]](#page-8-6) provides accurate and approximate computation
    in separate cores, the proposed design takes the benefits of putting approximation
    alongside exact computation capabilities in one standard core, with maintaining
    dynamic configurability features. This will lead to lower occupied area, energy
    saving, less complexity in software development and performance boost in a single
    core. Alternatively, it can be also placed back in an MPSoC platform where each
    node consisting of this core can use different execution units while benefiting
    from a similar control logic which is applicable in both hetero- and homogeneous
    topologies.


    ## III. ARCHITECTURE


    <span id="page-2-0"></span>The proposed processor, is a 32-bit in-order scalar
    core which the high-level datapath is shown in Fig. [1.](#page-1-0) It features
    a modular and extensive design, which is highly beneficial for designers, as it
    facilitates testing and research on computer architecture techniques. A key aspect
    of this processor is the removal of the traditional "Control Unit" found in most
    RISC processors. Instead, each module in the processor can generate its own control
    signals based on the fields of the instruction it receives after the decode stage.
    This micro-architecture is regarded as distributed control logic in pipeline processors
    [\[4\]](#page-7-3), and it helps us with the modularity of the core.


    Each module in the execution stage consists of three primary components: the "Self-Control
    Logic", "Approximation Control Logic" and the "Operational Logic". These components
    work in an organized flow inside the module to ensure the correct execution of
    instructions within the processor. The control logic uses the decoded fields coming
    from decode stage to create control signals in a distributed control manner. After
    the generation of control signals, operations will be taken in the modules according
    to the instruction fields. In this architecture, designers can make changes directly
    in the operational logic without any need to make changes in the self-control
    and approximation control part, which is a beneficial feature in reconfigurable
    architectures. In the traditional method of CPU design, a centralized control
    unit is responsible for generation and propagation of control signals and addition
    or replacement of execution units by designers would require a modification in
    the control unit. The structure of execution units and their three main mentioned
    sections, Control Logic, Approximation Control and Operational Logic can be seen
    in Fig. [2.](#page-2-1)


    ![](_page_2_Figure_4.jpeg)


    <span id="page-2-1"></span>Fig. 2. Proposed processor''s execution units structure


    Predefined approximate/accurate arithmetic circuit designs are included in the
    execution units, but top modules (Arithmetic Logic Unit, Multiplier Unit, Divider
    Unit) are designed in a way that designers can easily include and integrate their
    own design within the execution units as well. These execution units have reserved
    blocks for four modules which can be accurate or approximate, and if they are
    approximate, accuracy control logic is supported and handled by execution unit
    through the RISC-V CSRs. Our design combines the precision of accurate circuits
    with the efficiency and potential energy savings offered by approximate circuits.
    The design gains benefits of having approximate arithmetic units alongside exact
    computation modules capabilities in one standard core. As shown in Fig. [3.](#page-2-2),
    the execution unit result will be selected by a multiplexer at the end of the
    execution unit hardware which is determined by the value in the relative CSR.
    Unused circuits are dynamically switched off by leveraging the circuit selection
    field in the CSRs. This helps to prevent redundant processing of input data, eliminating
    unnecessary dynamic and static power consumption by the unused circuits within
    the core which will lead to higher power-efficiency. This structure allows the
    user or supervisor to dynamically switch between circuits during runtime according
    to power consumption constraints (e.g., in battery operated embedded devices.).
    The key contribution of the implemented design is enabling dynamic energy control
    utilizing controllable error levels and approximation, where runtime energy control
    can be managed by user or the supervisor privilege access, in fault-tolerant applications
    where approximation methods are allowed in computation process.


    ![](_page_2_Figure_8.jpeg)


    <span id="page-2-2"></span>Fig. 3. Circuit result selection logic in core''s execution
    units according to the respective CSR of each module


    The execution engine has three main modules: Arithmetic Logic Unit, Multiplier
    Unit and Divider Unit. These modules are implemented in a way which gives designers
    the ability to change or add execution circuits in the function units, without
    any need to change the control logic of the modules. For each execution unit,
    there is a dedicated special purpose register named alucsr, mulcsr and divcsr.
    These control status registers (CSRs) are designed in a way to provide the core''s
    special features for approximate computing. The structure of the fields within
    the mentioned registers are shown in Fig. [4.](#page-3-1)


    Each entry in these registers is related to a specific feature, aiming to assist
    users and designers in demonstrating advancements in their respective fields of
    study and work. The ultimate objective of these features, which contributes to
    the


    | alucsr          | [31:16]       | [15:12] | [11:8]                                                                    |
    [7:3] | [2:1] | [0] |

    |-----------------|---------------|---------|---------------------------------------------------------------------------|-------|-------|-----|

    |                 |               |         |                                                                           |       |       |     |

    | mulcsr          | [31:16]       | [15:12] | [11:8]                                                                    |
    [7:3] | [2:1] | [0] |

    |                 |               |         |                                                                           |       |       |     |

    | divcsr          | [31:16]       | [15:12] | [11:8]                                                                    |
    [7:3] | [2:1] | [0] |

    |                 |               |         |                                                                           |       |       |     |

    | Register Field: | Error Control |         | Custom Field II Custom Field I Trunc.
    Control Circuit Select Enable Appx. |       |       |     |


    <span id="page-3-1"></span>Fig. 4. Proposed Platform''s Execution Engine Control
    Status Registers General Format


    overall goal of the project, is to enhance user accessibility in programming and
    provide designers with greater flexibility in implementations. Definition of each
    field and bits in the mentioned special purpose registers are as follows:


    - Bit 0: Enabling approximate computations.

    - Bits 1 to 2: Gives the programmer the ability of selection from four arithmetic
    circuits in the execution unit which can be accurate or approximate as shown in
    Fig. [3.](#page-2-2) One notable advantage of this feature is the design philosophy
    behind this project, which ensures that control logic, forwarding bases, and pipeline
    signals remain unchanged even in the presence of varying timing and clock cycles
    in each circuit.

    - Bits 3 to 7: Custom field 1 for designer''s need of control and accessibility
    features.

    - Bits 8 to 11: Custom field 2 for designer''s need of control and accessibility
    features.

    - Bits 12 to 15: Truncation control level for special approximate arithmetic units
    which may work with truncation logic.

    - Bits 16 to 31: Error control bits for accuracy controllable and error configurable
    approximate designs, which also include this platform''s default execution units.


    These three special purpose registers are mapped as 0x800 (alucsr), 0x801(mulcsr)
    and 0x802 (divcsr) in the core''s CSR indexing range [\[14\]](#page-8-8). Fig.
    [5](#page-3-2) presents a sample RISC-V assembly code to show the programming
    convention of the platform using an approximate arithmetic circuit where the error
    level is configurable in the circuit. The accurate result of the procedure with
    input 10, is 3628800. Using the approximation factor set in the mulcsr in Fig[.5.](#page-3-2)
    the processor results 3587840, which has an error of 1.128%.


    ![](_page_3_Figure_10.jpeg)


    <span id="page-3-2"></span>Fig. 5. Sample "Factorial" RISC-V assembly code utilizing
    *mulcsr* to select the approximate circuit and setting error level


    One significant feature of the following format of program-


    ming convention in this platform is that unlike many other proposed designs, there
    is no need to add any instruction to RISC-V original compiler tool-chain [\[15\]](#page-8-9).
    Unlike [\[5\]](#page-7-5) and [\[12\]](#page-8-6) approaches which was done by
    adding custom instructions to the original compiler, no additional modifications
    in software toolchain is required in the proposed method. This also prevents recompilation
    of existing source code for approximate execution on this platform.


    ## IV. EXECUTION ENGINE


    <span id="page-3-0"></span>This processor supports RV32I(E)M instruction set of
    RISC-V standard extensions. Both I and E extensions are designed for integer arithmetical
    and logical instructions [\[16\]](#page-8-10) which are configurable in this core,
    as designer may set the input parameter of the module in the RTL source.


    ## *A. Adder/Subtractor*


    For the main adder/subtractor circuit (demo circuit) of the core, we have implemented
    a fast and low power carry select adder, with reduced critical path. The proposed
    32-bit adder circuit, is an error-configurable design with 8-bit error control
    signals (64 configurations) which will suite this module with various applications.
    The CSA architecture proposed in this platform is constructed by 4-bit ripple
    carry adders which are error controllable, and the error configuration feature
    in the adder is concluded from the inner ripple carry adders. This adder can also
    perform accurate addition when the respected field of alucsr is equal to 0x0F,
    or the approximate enabling bit turned to zero. The accurate/approximate computation
    feature in the default adder aids the design with saving power and area, leaving
    the core with no need of additional arithmetic circuits supporting accurate and
    approximate addition exclusively. Proposed adder has improved area by 11.3%, power
    by 21.1% (regarding average power consumption in all 64 configurations which is
    144.92µW) and critical path by 34.1% in comparison with an accurate CSA. Detailed
    hardware efficiency metrics for the adder is reported in Table [I,](#page-3-3)
    where ACC stands for conventional accurate carry select adder, and APX stands
    out for the proposed accurate/approximate adder. The AVG term indicates the average
    of all possible configurations which in this case, are 64 error levels of the
    proposed adder.


    <span id="page-3-3"></span>TABLE I HARDWARE EFFICIENCY COMPARISON OF PROPOSED
    ADDER AND CONVENTIONAL CARRY SELECT ADDER IN 45NM CMOS


    | 32-bit Carry Select Adder / 8-bit Error Control |            |        |            |       |       |      |      |  |

    |-------------------------------------------------|------------|--------|------------|-------|-------|------|------|--|

    |                                                 | Area (µm²) |        | Delay
    (ns) |       |       |      |      |  |

    | ACC                                             | APX        | ACC    | APX
    (AVG)  | Min   | Max   | ACC  | APX  |  |

    | 337.3                                           | 299.2      | 183.62 | 144.92     |
    136.1 | 152.0 | 2.02 | 1.33 |  |


    This circuit is specifically designed for addition and subtraction instructions
    of I and E extension of RISC-V instruction set architecture. For address generation
    and control flow instructions, precise addition using an accurate adder is essential
    as approximation is not permissible.


    ## *B. Multiplier*


    The second default (demo) circuit integrated in the core is a low-power, high-speed
    error configurable approximate 8 bit multiplier which features reduced area, low
    energy and power consumption, and optimized critical path delay. The proposed
    multiplier in an 8-bit configuration showcases an improvement of 54.4% in area
    occupation, 66% in power consumption (considering the average power consumption
    in all dynamic configurations which is 75.49µW) and 51.5% in critical path delay,
    in comparison with a conventional accurate Wallace multiplier in which details
    are shown in Table [II.](#page-4-1) The lowest power consumption in the dynamic
    configuration can be achieved by the highest error level (error field of CSR equal
    to 0x00) with 67.43µW and the highest power is related to the highest quality
    in terms of error metrics (error field of CSR equal to 0x7E) with 81.05µW.


    <span id="page-4-1"></span>TABLE II HARDWARE EFFICIENCY COMPARISON OF 8-BIT PROPOSED
    MULTIPLIER AND CONVENTIONAL WALLACE TREE MULTIPLIER IN 45NM CMOS


    | 8-bit Error Controllable Approximate Multiplier |            |            |           |       |       |            |      |  |

    |-------------------------------------------------|------------|------------|-----------|-------|-------|------------|------|--|

    |                                                 | Area (µm²) | Power (µW) |           |       |       |
    Delay (ns) |      |  |

    | ACC                                             | APX        | ACC        |
    APX (AVG) | Min   | Max   | ACC        | APX  |  |

    | 434.7                                           | 198.7      | 224.31     |
    75.49     | 67.43 | 81.05 | 1.32       | 0.64 |  |


    For integration with the core, a 32-bit multiplier in hierarchical arrangement
    of 8-bit modules is implemented. The method used for creation of the 32-bit architecture
    uses the original 8-bit multiplier over multiple cycles to perform 16 bit multiplication.
    This hardware is replicated four times as illustrated in Fig. [6](#page-4-2) to
    perform a 32-bit multiplication.


    ![](_page_4_Figure_5.jpeg)


    <span id="page-4-2"></span>Fig. 6. Hierarchical 32-bit multiplication from original
    8-bit multipliers


    The approximate multiplier in the core has significant results on Error Rate (ER),
    Mean Relative Error Distance (MRED) and Normalized Mean Error Distance (NMED)
    [\[17\]](#page-8-11) [\[18\]](#page-8-12) which are some of the favored metrics
    for accuracy efficiency criteria measurement in approximate arithmetic circuits,
    that are described in Table [III.](#page-4-3)


    The results for described error analysis metrics of the proposed multiplier in
    an 8-bit configuration are shown in Table [IV.](#page-4-4) This table only depicts
    selected error levels of the multiplier, which is set by the related CSR in the
    processor.


    # *C. Divider*


    In the last phase of core''s default execution engine development, a sample non-restoring
    32-bit divider with an 8-bit error


    TABLE III DESCRIPTION AND EQUATIONS FOR EACH ERROR METRIC


    <span id="page-4-3"></span>


    | Metric | Equation                                                        | Description                                                                      |  |  |

    |--------|-----------------------------------------------------------------|----------------------------------------------------------------------------------|--|--|

    | ER     | #RF<br>22N                                                      | Ratio
    of the number of<br>false results (#RF ) to<br>number of possible outputs. |  |  |

    | MRED   | RA<br>i −RX<br>2N<br> <br>1<br>P2<br>i<br>22N<br>i=1<br>RA<br>i | Average
    of REDi<br>(Relative Error Distance)<br>for every possible result.       |  |  |

    | NMED   | 2N<br>1<br>P2<br>i=1  RA<br>i −RX<br> <br>i<br>22N<br>(2N −1)2  | Average
    of EDi (Error<br>Distance) divided by most<br>considerable exect result. |  |  |


    <span id="page-4-4"></span>TABLE IV NMED, MRED AND ER FOR 8-BIT VERSION OF THE
    PROPOSED APPROXIMATE DYNAMIC ERROR CONFIGURABLE MULTIPLIER


    | Error<br>Level | Error Control bits<br>(Field of CSR) | NMED<br>(%) | MRED<br>(%)
    | ER<br>(%) |

    |----------------|--------------------------------------|-------------|-------------|-----------|

    | 6              | 0x7E                                 | 0.25        | 0.85        |
    36.16     |

    | 5              | 0x7C                                 | 0.26        | 0.97        |
    39.73     |

    | 4              | 0x78                                 | 0.28        | 1.33        |
    44.33     |

    | 3              | 0x70                                 | 0.34        | 2.11        |
    50.33     |

    | 2              | 0x60                                 | 0.48        | 3.59        |
    57.02     |

    | 1              | 0x40                                 | 0.76        | 5.89        |
    61.8      |

    | 0              | 0x00                                 | 1.25        | 8.94        |
    65.07     |


    configuration range is implemented within the core, in order to finalize the functional
    units used in the standard RISC-V M-Extension. A similar error control mechanism
    for the ALU and multiplier unit is also utilized for this module, which computes
    accurate division at the zero error level. This control signal is delivered through
    the specific control register (divscr) designated for the divider unit.


    # <span id="page-4-0"></span>V. EXPERIMENTAL RESULTS IN IMAGE PROCESSING APPLICATION


    The assessment is based on the analysis of an application in image processing,
    using the proposed processor and its accurate-approximate execution engine. In
    this research, a widely used image sharpening algorithm, using the 5×5 kernel
    in [\[19\]](#page-8-13) was considered. The selected case for the evaluation is
    a 512×512 8-bit grayscale image. The focus of approximation was exclusively on
    the multiplication in the convolution process, with all other operations (such
    as addition, subtraction, and division) being accurately executed.


    The evaluation of processed image quality was carried out using the peak signal-to-noise
    ratio (PSNR) which is defined in terms of the mean squared error (MSE). In this
    study, both MSE and PSNR are defined according to the following equations:


    $$MSE = \frac{1}{nm} \sum\_{i=0}^{n-1} \sum\_{j=0}^{m-1} [X(i,j) - Y(i,j)]^2,\tag{1}$$


    $$PSNR = 10 \cdot \log\_{10}(\frac{MAX^2}{MSE}),\tag{2}$$


    where X(i, j) and Y (i, j) represent the expected and obtained values, n and m
    correspond to the image dimensions, while MAX denotes 255, as the maximum value
    of each pixel in the 8-bit grayscale image.


    The highest PSNR, which indicates highest quality, was reached through core''s
    default multiplier at 6th error level. Table [V](#page-6-0) presents the PSNR
    results and power consumption for different error levels which are set by *Error
    Control Field* of mulcsr. The default multiplier error is reduced from error zero
    to six as shown in Table [V.](#page-6-0) On average, there is a 12% reduction
    in power consumption when compared to the precise multiplier used in the same
    application.


    Other approximate multipliers were integrated in the platform, and the same program
    was executed. Table [VI](#page-6-1) shows a comparison between the results of
    this experiment. AxRMs [\[20\]](#page-8-14) and Ax8 [\[21\]](#page-8-15) do not
    benefit from dynamic reconfigurability and thus their error metrics are set at
    design time. And therefore, in scenarios where a trade-off between power and quality
    is critical, different circuits must be present in the related execution unit
    and the *Circuit Select Field* of the csr must be utilized. In situations where
    dynamic reconfigurabilty is necessary, the proposed platform''s accuracy-controllable
    execution units provide a wide range of flexibility at execution time.


    ## VI. IMPLEMENTATION AND COMPARISON


    <span id="page-5-0"></span>The proposed design has been implemented and evaluated
    on a Xilinx ZYNQ (XC7Z020) FPGA, and also undergone a complete ASIC synthesis
    and implementation flow. A comparison in terms of LUT number, frequency, pipeline
    stages and Dhrystone [\[22\]](#page-8-16) benchmark result, between this work
    and similar designs, reported by [\[23\]](#page-8-17) are shown in Table [VII.](#page-6-2)
    On a 100MHz clock frequency, which is not the maximum frequency that phoeniX is
    capable of, has the highest benchmark performance, with DMIPS/MHz of 1.73.


    The ASIC synthesis was done utilizing the NanGate 45nm Open Cell Library. The
    static timing analysis (STA) results show that the maximum delay in modules and
    pipeline stages is under 1.6ns. Setting the clock cycle based on the critical
    path provides enough margin for the maximum delay, ensuring timely data propagation
    through the pipeline. Adhering to this timing requirement allows the processor
    to operate at 620MHz, supporting efficient instruction execution and meeting operational
    specifications for embedded processors. The RISC-V Instruction Set Architecture
    is recognized for its high efficiency in low-power and minimal area design, attributed
    to its reduced instruction set and straightforward hardware implementation. A
    notable advantage of approximate computing is energy conservation. By leveraging
    the strengths of both paradigms, this core has emerged as an exemplary model for
    energy-efficient computing while upholding an acceptable level of performance.
    The platform exhibits notable metrics, with an average power consumption of 4.23mW
    at 1.1V at 620MHz frequency, in 0.024mm² of area when using the default execution
    engine, showcasing its efficiency for various error-tolerant applications. Fig.
    [7](#page-5-1) shows a comparison in term of maximum power consumption between
    phoeniX and other notable RISC cores, RI5CY [\[9\]](#page-8-3), PULP (1), PULP
    Cluster [\[11\]](#page-8-5), X-HEEP [\[24\]](#page-8-18), Ariane [\[25\]](#page-8-19),
    SHAKTI-C [\[26\]](#page-8-20), Rocket [\[27\]](#page-8-21), Boom v2 [\[28\]](#page-8-22),
    OpenRISC [\[29\]](#page-8-23) and Mr. Wolf [\[30\]](#page-8-24). According to
    different technology nodes in these processors and by applying scaling factors,
    this comparison showcases remarkable power efficiency offered by the proposed
    core, resulted from the reconfigurable accurate/approximate execution units. Technical
    details about some of these cores are explained later in this section.


    ![](_page_5_Figure_7.jpeg)


    <span id="page-5-1"></span>Fig. 7. Power and frequency comparison between RISC-V
    cores


    Table [VIII](#page-6-3) presents energy consumption per instruction, in sample
    software programs, compiled using GCC with standard optimization options for each
    program. Compilation options for each program executed on processor for post-synthesis
    power simulations are also reported in Table [VIII.](#page-6-3) It is concluded
    from the table that the proposed platform is an energy-efficient base for embedded
    designs.


    Table [IX](#page-6-4) is an energy efficiency comparison between RISC-V cores
    in pJ per operation metric. The proposed design with an average of 7.85 pJ/op
    is the most efficient design in the study with other reported values in term of
    energy per instruction. Mr. Wolf [\[30\]](#page-8-24) and PULP cluster [\[11\]](#page-8-5)
    are second and third best designs, noting the fact that technology nodes differ
    in this comparison. Total energy consumption in each application and average result
    of 7.85 pJ/op was measured through post-synthesis power simulations alongside
    switching activity files extracted from the set of applications executed on the
    processor. All measurements for the proposed design were done under same circumstances
    (including voltage, frequency and compiler options) described in Table [VIII\)](#page-6-3).


    Table [X](#page-7-6) provides an architectural and technical analysis and comparison
    between renowned processors. The comparison focuses on overall performance analysis
    and hardware efficiency in which the proposed processor was considered with accurate
    execution engine in order to conduct a comparison with other processors which
    do not have support for approximation by default. In this research, processors
    are categorized as application class, and embedded or microcontroller class according
    to their architectural specialties. The proposed design is architecturally categorized
    as an embedded core, but as it is shown in Table [X,](#page-7-6) it has a frequency
    rate higher than most embedded processors, and on the edge of the transition between
    embedded computing and high-performance computing. The optimized path of the core
    which is less than 24 NAND gates, leads to a remarkable result in term of critical
    path and overall performance. In fact,


    TABLE V PSNR AND POWER FOR IMAGE SHARPENING ALGORITHM USING PROCESSOR''S DEFAULT
    ERROR CONFIGURABLE MULTIPLIER


    <span id="page-6-0"></span>


    | Parameter  | Error Level 6 | Error Level 5 | Error Level 4 | Error Level 3 |
    Error Level 2 | Error Level 1 | Error Level 0 |

    |------------|---------------|---------------|---------------|---------------|---------------|---------------|---------------|

    | PSNR (dB)  | 46.32         | 43.99         | 41.78         | 37.51         |
    31.27         | 26.52         | 21.82         |

    | Power (mW) | 4.5544        | 4.5403        | 4.5092        | 4.4247        |
    4.3412        | 4.2792        | 4.2376        |


    | TABLE VI |  |

    |----------|--|

    |          |  |


    PSNR RESULT COMPARISON OF DIFFERENT MULTIPLIERS USING PROPOSED PLATFORM


    <span id="page-6-1"></span>


    | Designs   | Error Level 6 | Error Level 5 | AxRM1 | AxRM2 | AxRM3 | Ax8-1 |
    Ax8-2 |

    |-----------|---------------|---------------|-------|-------|-------|-------|-------|

    | Reference | Default       | Default       | [20]  | [20]  | [20]  | [21]  |
    [21]  |

    | PSNR (dB) | 46.32         | 43.99         | 48.62 | 32.31 | 22.51 | 53.68 |
    23.54 |


    <span id="page-6-2"></span>TABLE VII LUT NUMBER, FREQUNCY AND DMIPS/MHZ COMPARISON
    ON FPGA


    | Results on Xilinx 7 series FPGA |      |            |                 |           |  |  |

    |---------------------------------|------|------------|-----------------|-----------|--|--|

    | Design                          | LUTs | fclk (MHz) | Pipeline Stages | DMIPS/MHz
    |  |  |

    | phoeniX                         | 1665 | 100        | 3               | 1.73      |  |  |

    | Freedom                         | 2692 | 32.5       | 5               | 1.61      |  |  |

    | RI5CY                           | 6748 | 50         | 5               | 1.1       |  |  |

    | PicoRV32                        | 1765 | 115        | 0               | 0.13      |  |  |


    <span id="page-6-3"></span>TABLE VIII ENERGY PER INSTRUCTION [PJ] FOR STANDARD
    SAMPLE CODES RUNNING ON THE PROPOSED PLATFORM


    | Power Efficiency1<br>(pJ/op) | Error Control Field<br>of mulcsr |  |  |

    |------------------------------|----------------------------------|--|--|

    | 7.3423                       | NULL (Accurate)                  |  |  |

    | 7.8190                       | 0x00                             |  |  |

    | 7.8634                       | 0x7E                             |  |  |

    | 7.7609                       | 0x78                             |  |  |

    | 7.9925                       | 0x60                             |  |  |

    | 8.1020                       | 0x60                             |  |  |

    |                              |                                  |  |  |


    <sup>1</sup> Processor running @1.1V, 620MHz,


    \* Compiled with riscv64-unknown-gcc 8.3.0


    <sup>a</sup> Using recommended ARM compiler flags and settings in [\[22\]](#page-8-16)


    † GCC-8.3.0 Flags: -c -O2 -mabi=ilp32 -march=rv32im


    <span id="page-6-4"></span>TABLE IX ENERGY EFFICIENCY [PJ/OP] COMPARISON IN SELECTED
    RISC-V PROCESSORS


    | RISC-V<br>Processor | Ref.     | Energy Efficiency<br>(pJ/op) | Tech. Node<br>(nm)
    |

    |---------------------|----------|------------------------------|--------------------|

    | Ariane              | [25]     | 51.8                         | 22                 |

    | Rocket              | [27]*    | 100                          | 45                 |

    | Mr. Wolf            | [30]*    | 12.5                         | 40                 |

    | Boom v2             | [28]*    | 133                          | 45                 |

    | SHAKTI              | [26]*    | 122                          | 22                 |

    | PicoRV32            | [34]     | 92.6                         | 130                |

    | PULP Cluster        | [11]     | 25.2                         | 65                 |

    | phoeniX             | Proposed | 7.85                         | 45                 |


    \* Reported by [\[25\]](#page-8-19)


    this project represents an optimized embedded class platform with significantly
    reduced area and power consumption, which can perform with a frequency and performance
    near application class and high-performance processors.


    Dominating industrial cores in microcontrollers are designed by ARM, known as
    Cortex-M series. These processors are usually fabricated on more mature nodes
    such has 180nm, 90nm and 40nm technologies. ARM Cortex-M series [\[31\]](#page-8-26)
    are architecturally close to the proposed core and other designs in the research,
    except for Cortex-M7 which is a superscalar 6 stage pipelined core. Cortex-A5
    is an application class, single issue 8-stage pipelined in-order processor, which
    in matter of frequency and performance level can be compared with the presented
    platform.


    Table [X](#page-7-6) also presents a comparison within these cores (including
    application class and embedded class) based on result of the Dhrystone benchmark.
    The proposed processor has a rate of 1.89 DMIPS/MHz, Dhrystones per Seconds per
    MHz of 3324 and a CPI of 1.13 at the operating frequency. It is important to note
    that some of this cores such as Ariane, Boom v2 and Rocket are application class
    and have a frequency rate of +1GHz, through a tape out process with newer technology
    nodes. SHAKTI [\[26\]](#page-8-20) was taped-out in 22nm technology with a maximum
    frequency level of 800MHz and is an application class core. Ariane, designed by
    ETH Zurich was crafted in 22nm FDSOI technology [\[25\]](#page-8-19) as an in-order
    (out-of-order execute, in-order commit) 6-stage pipelined processor. Rocket [\[27\]](#page-8-21)
    by UC Berkeley, is a 6-stage pipelined design in 45nm technology.


    In application class processors Ariane stands out as a low power design, powered
    by the 22nm node, with highest frequency rate of 1.7GHz. Rocket is the smallest
    core in this class with an area of 0.14mm² in 45nm technology. Boom has a benchmark
    result of 2.13 DMIPS/MHz due to the out of order execution design, showcasing
    best application performance in the benchmarking in its class. RI5CY [\[9\]](#page-8-3)
    by PULP Platform and CV32E40P [\[33\]](#page-8-27) by OpenHW Group, are embedded
    cores with similar microarchitecture to phoeniX, both implemented in 65nm CMOS
    technology with a 4-stage in-order pipeline design. Ibex [\[33\]](#page-8-27)
    by lowRISC, inspired from Zero-riscy [\[9\]](#page-8-3) is an optimized 2-stage
    in-order pipeline core, implemented in 65nm CMOS, with a configurable RV32IMC
    architecture, resulting a DMIPS/MHz rate of 0.9.


    <span id="page-7-6"></span>


    | Processor                              | Ref.     | Power (mW) | Node (nm) |
    Freq. (MHz) | Area (mm²) | DMIPS/MHz | ISA         | Architecture                   |

    |----------------------------------------|----------|------------|-----------|-------------|------------|-----------|-------------|--------------------------------|

    | Application Class and High Performance |          |            |           |             |            |           |             |                                |

    | Ariane                                 | [25]     | 52         | 22        |
    1700        | 0.30       | 1.65      | RV64IMC     | 6-stage, out-of-order pipeline
    |

    | SHAKTI-C                               | [26]     | 90         | 22        |
    800         | 0.29       | 1.68      | RV64G       | 5-stage, in-order pipeline     |

    | Rocket                                 | [27]     | 125        | 45        |
    1300        | 0.14       | 1.72      | RV32/64GC   | 6-stage, in-order pipeline     |

    | Boom v2                                | [28]     | 250        | 45        |
    1000        | 1.00       | 2.13      | RV64G       | 8-stage, out-of-order pipeline
    |

    | Cortex-A5                              | [32]     | ≥ 500      | 40        |
    1000        | 0.53       | 1.57      | ARMv7-A     | 8-stage, in-order pipeline     |

    | Embedded and Microcontroller Class     |          |            |           |             |            |           |             |                                |

    | OpenRISC                               | [29]     | 12         | 65        |
    362         | 0.064      | NR*       | RISC (Org.) | 4-stage, in-order pipeline     |

    | RI5CY                                  | [9]      | 3.77       | 65        |
    560         | 0.059      | 1.10      | RV32IM      | 4-stage, in-order pipeline     |

    | CV32E40P                               | [33]     | 1.57       | 65        |
    453         | 0.104      | NR*       | RV32IMC     | 4-stage, in-order pipeline     |

    | Ibex                                   | [33]     | 1.78       | 65        |
    394         | 0.053      | 0.90      | RV32IMC     | 2-stage, in-order pipeline     |

    | Cortex-M4                              | [31]     | 7.87       | 40        |
    240         | 0.028      | 1.67      | Armv7E-M    | 3-stage, in-order pipeline     |

    | PicoRV32                               | [34]     | 5.14       | 130       |
    250         | 0.23       | 0.50      | RV32IMC     | Multi-cycle RISC               |

    | phoeniX                                | Proposed | 4.23       | 45        |
    620         | 0.024      | 1.89      | RV32IEM     | 3-stage, in-order pipeline     |


    TABLE X ARCHITECTURAL AND PERFORMANCE COMPARISON OF PROCESSORS WITH DIFFERENT
    TECHNOLOGY NODES


    NR\*: Not Reported


    In conclusion, phoeniX stands out in benchmark result, with 1.89 DMIPS/MHz, and
    frequency rate of 620MHz, as the best in each category in embedded cores. The
    proposed processor also has the smallest area in comparison with other designs
    in this study, occupying only 0.024mm² in 45nm technology. CV32E40P has the lowest
    power consumption in 453MHz frequency. Ibex as a small 2-stage pipelined core
    is the second best in term of power consumption with 1.78mW. The pheoniX core
    is the third in this factor, with average power consumption of 4.23mW in 620MHz
    frequency, in 45nm CMOS technology, regarded as low-power and energy-efficient
    platform.


    ## VII. CONCLUSION


    <span id="page-7-4"></span>In conclusion, approximate computing holds significance
    as it enables quicker and more energy-efficient calculations by trading off a
    small amount of accuracy for performance gains. This strategy is especially beneficial
    in scenarios where precise outcomes are not always crucial, like in image processing
    or machine learning tasks. On the other hand, presence of low power processors
    in embedded systems is essential to extend and support functionality in environments
    with limited energy resources. The importance and necessity of a reconfigurable
    platform for implementing approximate circuits are concluded in these statements,
    aiming to achieve energy efficiency and performance enhancements while preserving
    application-level accuracies in fault-tolerant applications.


    This paper introduces a reconfigurable embedded platform, proposing a novel way
    of utilizing approximate computation in a RISC-V core, which is the key goal of
    the project. The project''s focus on modularity and the removal of traditional
    control unit, sets it apart from many other designs, providing an ideal base for
    computer architecture, digital design and fault-tolerant application researches.


    The platform meets the need for energy-efficient and high-performance embedded
    systems, through employing energy saving by approximation, while maintaining acceptable
    application-level accuracy. This platform enables integration of approximate arithmetic
    units with diverse structures and varying levels of accuracy within the core,
    by complying with signaling conventions of the platform. This allows for customizable
    trade-offs between speed, accuracy, and power consumption, based on the specific
    needs of the application. Additionally, its architecture supports the easy integration
    of additional units such as hardware accelerators, enhancing taskspecific performance.


    In further and future works and updates, phoeniX will be integrated into an NoC
    (Network on Chip) in order to be a part of an MPSoC. The proposed design gives
    the flexibility of dynamic accuracy and error level control in one core, which
    means the integration of the processor in an MPSoC platform can give us the benefits
    of both homogeneous and heterogeneous systems in various scenarios and applications.


    ## ACKNOWLEDGMENT


    The authors are grateful to A. M. H. Monazzah for their valuable insights, which
    greatly improved this paper.


    ## REFERENCES


    - <span id="page-7-0"></span>[1] G. Anusha and P. Deepa, "Design of approximate
    adders and multipliers for error tolerant image processing," *Microprocessors
    and Microsystems*, vol. 72, p. 102940, 2020.

    - <span id="page-7-1"></span>[2] S. Shakibhamedan, N. Amirafshar, A. S. Baroughi,
    H. S. Shahhoseini and N. Taherinejad, "ACE-CNN: Approximate carry disregard multipliers
    for energy-efficient CNN-Based image classification," *in IEEE Transactions on
    Circuits and Systems I: Regular Papers*, 2024.

    - <span id="page-7-2"></span>[3] S. Bavikadi, P. R. Sutradhar, M. Indovina, A.
    Ganguly and S. M. P. Dinakarrao, "ReApprox-PIM: Reconfigurable approximate Look-Up-Table
    (LUT)-Based Processing-in-Memory (PIM) machine learning accelerator," *in IEEE
    Transactions on Computer-Aided Design of Integrated Circuits and Systems*, 2024.

    - <span id="page-7-3"></span>[4] Shen, John Paul, and Mikko H. Lipasti, "Modern
    processor design: fundamentals of superscalar processors," Waveland Press, 2013.

    - <span id="page-7-5"></span>[5] T. Trevisan J et al., "Approxrisc: An approximate
    computing infrastructure for RISC-V," *RISC-V Workshop in Barcelona*, May 2018,
    poster.

    - <span id="page-8-0"></span>[6] I. Felzmann, J. F. Filho and L. Wanner, "AxPIKE:
    Instruction-level Injection and Evaluation of Approximate Computing," *2021 Design,
    Automation & Test in Europe Conference & Exhibition (DATE)*, Grenoble, France,
    2021, pp. 491-494.

    - <span id="page-8-1"></span>[7] I. Felzmann, J. F. Filho, and L. Wanner, "Risk-5:
    Controlled approximations for RISC-V," *IEEE Transactions on Computer-Aided Design
    of Integrated Circuits and Systems*, vol. 39, no. 11, pp. 4052–4063, 2020.

    - <span id="page-8-2"></span>[8] A. Verma, P. Sharma and B. P. Das, "RISC-V Core
    with Approximate Multiplier for Error-Tolerant Applications," *2022 25th Euromicro
    Conference on Digital System Design (DSD)*, Maspalomas, Spain, 2022, pp. 239-246.

    - <span id="page-8-3"></span>[9] P. Davide Schiavone et al., "Slow and steady
    wins the race? A comparison of ultra-low-power RISC-V cores for Internet-of-Things
    applications," *2017 27th International Symposium on Power and Timing Modeling,
    Optimization and Simulation (PATMOS)*, Thessaloniki, Greece, 2017, pp. 1-8.

    - <span id="page-8-4"></span>[10] F. Guella, E. Valpreda, M. Caon, G. Masera and
    M. Martina, "MARLIN: A Co-Design Methodology for Approximate ReconfigurabLe Inference
    of Neural Networks at the Edge," *in IEEE Transactions on Circuits and Systems
    I: Regular Papers*, doi: 10.1109/TCSI.2024.3365952.

    - <span id="page-8-5"></span>[11] M. Gautschi et al., "Near-Threshold RISC-V Core
    With DSP Extensions for Scalable IoT Endpoint Devices," *in IEEE Transactions
    on Very Large Scale Integration (VLSI) Systems*, vol. 25, no. 10, pp. 2700-2713,
    Oct. 2017.

    - <span id="page-8-6"></span>[12] A. S. Baroughi, S. Huemer, H. S. Shahhoseini
    and N. TaheriNejad, "AxE: An approximate-exact multi-processor system-on-chip
    platform," *2022 25th Euromicro Conference on Digital System Design (DSD)*, pp.
    60-66, 2022.

    - <span id="page-8-7"></span>[13] C. Wolf, "PicoRV32- A Size-Optimized RISC-V
    CPU." [Online]. Available:<https://github.com/cliffordwolf/picorv32>

    - <span id="page-8-8"></span>[14] A. Waterman and K. Asanovic. (2021). "The RISC-V
    Instruction Set Manual—Volume II: Privileged Architecture," [Online]. Available:
    https://riscv.org/specifications/privileged-isa/

    - <span id="page-8-9"></span>[15] RISC-V Collaboration, "GNU toolchain for RISC-V,
    including GCC." [Online]. Available:<https://github.com/riscv-collab/riscv-gnu-toolchain>

    - <span id="page-8-10"></span>[16] E. Cui, T. Li and Q. Wei, "RISC-V Instruction
    Set Architecture Extensions: A Survey," *in IEEE Access*, vol. 11, pp. 24696-24711,
    2023.

    - <span id="page-8-11"></span>[17] A. G. M. Strollo, E. Napoli, D. D. Caro, N.
    Petra, and G. D. Meo, "Comparison and extension of approximate 4–2 compressors
    for lowpower approximate multipliers," *IEEE Trans. Circuits Syst. I, Reg. Papers*,
    vol. 67, no. 9, pp. 3021–3034, Sep. 2020.

    - <span id="page-8-12"></span>[18] P. Yin, C. Wang, H. Waris, W. Liu, Y. Han,
    and F. Lombardi, "Design and analysis of energy-efficient dynamic range approximate
    logarithmic multipliers for machine learning," *IEEE Trans. Sustain. Comput.*,
    vol. 6, no. 4, pp. 612–625, Oct. 2021.

    - <span id="page-8-13"></span>[19] E. Zacharelos, I. Nunziata, G. Saggese, A.
    G. M. Strollo and E. Napoli, "Approximate Recursive Multipliers Using Low Power
    Building Blocks," *in IEEE Transactions on Emerging Topics in Computing*, vol.
    10, no. 3, pp. 1315-1330, 1 July-Sept. 2022.

    - <span id="page-8-14"></span>[20] H. Waris, C. Wang, C. Xu and W. Liu, "AxRMs:
    Approximate Recursive Multipliers Using High-Performance Building Blocks," *in
    IEEE Transactions on Emerging Topics in Computing*, vol. 10, no. 2, pp. 1229-1235,
    1 April-June 2022

    - <span id="page-8-15"></span>[21] H. Waris, C. Wang, W. Liu, J. Han and F. Lombardi,
    "Hybrid Partial Product-Based High-Performance Approximate Recursive Multipliers,"
    *in IEEE Transactions on Emerging Topics in Computing*, vol. 10, no. 1, pp. 507-513,
    1 Jan.-March 2022

    - <span id="page-8-16"></span>[22] Dhrystone Benchmarking for Arm Cortex Processors.
    Accessed: May 20, 2019. [Online]. Available: [https://developer.arm.com/documentation/](https://developer.arm.com/documentation/105958/latest/)
    [105958/latest/](https://developer.arm.com/documentation/105958/latest/)

    - <span id="page-8-17"></span>[23] R. Holler, D. Haselberger, D. Ballek, P. R
    ¨ ossler, M. Krapfenbauer ¨ and M. Linauer, "Open-Source RISC-V processor IP cores
    for FPGAs − overview and evaluation," *2019 8th Mediterranean Conference on Embedded
    Computing (MECO)*, pp. 1-6, 2019.

    - <span id="page-8-18"></span>[24] Pasquale Davide Schiavone et al. "X-HEEP: An
    Open- Source, Configurable and Extendible RISC-V Microcontroller," *In: Proc.
    of Int. Conf. on Computing Frontiers*. CF ''23. New York, NY, USA: ACM, 2023,
    pp. 379–380.

    - <span id="page-8-19"></span>[25] F. Zaruba and L. Benini, "The Cost of Application-Class
    Processing: Energy and Performance Analysis of a Linux-Ready 1.7-GHz 64-Bit RISC-V
    Core in 22-nm FDSOI Technology," *in IEEE Transactions on Very Large Scale Integration
    (VLSI) Systems*, vol. 27, no. 11, pp. 2629- 2640, Nov. 2019.

    - <span id="page-8-20"></span>[26] Shakti C-Class. Accessed: Aug, 2023. [Online].
    Available: [https://gitlab.](https://gitlab.com/shaktiproject/cores/c-classl)
    [com/shaktiproject/cores/c-classl](https://gitlab.com/shaktiproject/cores/c-classl)

    - <span id="page-8-21"></span>[27] Y. Lee et al., "A 45nm 1.3GHz 16.7 double-precision
    GFLOPS/W RISC-V processor with vector accelerators," *ESSCIRC 2014 - 40th European
    Solid State Circuits Conference (ESSCIRC)*, Venice Lido, Italy, 2014, pp. 199-202.

    - <span id="page-8-22"></span>[28] C. Celio, P.-F. Chiu, B. Nikolic, D. A. Patterson,
    and K. Asanovi´c, "BOOM v2: An open-source out-of-order RISC-V core," *Dept. Elect.
    Eng. Comput. Sci., Univ. California, Berkeley*, Berkeley, CA, USA, Tech. Rep.
    UCB/EECS-2017-157, Sep. 2017. [Online]. Available: [http:](http://www2.eecs.berkeley.edu/Pubs/TechRpts/2017/EECS-2017-157.html)
    [//www2.eecs.berkeley.edu/Pubs/TechRpts/2017/EECS-2017-157.html](http://www2.eecs.berkeley.edu/Pubs/TechRpts/2017/EECS-2017-157.html)

    - <span id="page-8-23"></span>[29] M. Gautschi et al., "Tailoring instruction-set
    extensions for an ultra-low power tightly-coupled cluster of OpenRISC cores,"
    *in Proc. IFIP/IEEE Int. Conf. Very Large Scale Integr. (VLSI-SoC)*, Oct. 2015,
    pp. 25–30.

    - <span id="page-8-24"></span>[30] A. Pullini, D. Rossi, I. Loi, A. Di Mauro and
    L. Benini, "Mr. Wolf: A 1 GFLOP/s Energy-Proportional Parallel Ultra Low Power
    SoC for IOT Edge Processing," *ESSCIRC 2018 - IEEE 44th European Solid State Circuits
    Conference (ESSCIRC)*, Dresden, Germany, 2018, pp. 274-277.

    - <span id="page-8-26"></span>[31] ARM Limited, "https://www.arm.com/products/processors/cortex-m."

    - <span id="page-8-28"></span>[32] "ARM Cortex-A5 Processor Performance and Specification,"
    Available: [http://www.arm.com/products/processors/cortex-a/cortex-a5.php.](http://www.arm.com/products/processors/cortex-a/cortex-a5.php)

    - <span id="page-8-27"></span>[33] N. Gallmann, P. Vogel, P. D. Schiavone and
    L. Benini, "From Swift to Mighty: A Cost-Benefit Analysis of Ibex and CV32E40P
    Regarding Application Performance, Power and Area," *in Fifth Workshop on Computer
    Architecture Research with RISC-V*, 2021.

    - <span id="page-8-25"></span>[34] A. Djupdal, M. Sjalander, M. Jahre, and S.
    Aunet, "Minimizing the En- ¨ ergy Usage of Tiny RISC-V Cores," *in Seventh Workshop
    on Computer Architecture Research with RISC-V*, 2023.'
- title: Using a Performance Model to Implement a Superscalar CVA6
  abstract: 'A performance model of CVA6 RISC-V processor is built to evaluate performance

    related modifications before implementing them in RTL. Its accuracy is 99.2% on

    CoreMark. This model is used to evaluate a superscalar feature for CVA6. During

    design phase, the model helped detecting and fixing performance bugs. The

    superscalar feature resulted in a CVA6 performance improvement of 40% on

    CoreMark.'
  url: http://arxiv.org/abs/2410.01442v1
  keywords: ''
  document: '# Using a Performance Model to Implement a Superscalar CVA6


    Côme Allart1, 2, Jean-Roch Coulon¹, André Sintzoff¹, Olivier Potin², Jean-Baptiste
    Rigaud² ¹ *Thales DIS*, Meyreuil, France


    > <span id="page-0-4"></span><span id="page-0-2"></span>² *Mines Saint-Etienne,
    CEA, Leti, Centre CMP*, F-13541 Gardanne, France ¹ {come.allart, jean-roch.coulon,
    andre.sintzoff}@thalesgroup.com ² {come.allart, olivier.potin, rigaud}@emse.fr


    Abstract *A performance model of CVA6 RISC-V processor is built to evaluate performancerelated
    modifications before implementing them in RTL. Its accuracy is 99.2% on CoreMark.
    This model is used to evaluate a superscalar feature for CVA6. During design phase,
    the model helped detecting and fixing performance bugs. The superscalar feature
    resulted in a CVA6 performance improvement of 40% on CoreMark.*


    ### 1. Introduction


    <span id="page-0-3"></span>RISC-V is an open Instruction Set Architecture (ISA);
    organizations can build processors implementing this ISA without fees. CVA6 is
    an open-source RISC-V processor created by ETH Zurich [\[1\]](#page-6-0) and maintained
    by OpenHW Group [\[2\].](#page-6-1) It is highly-configurable to address industrial
    applications of many OpenHW Group members with a single codebase. This single-issue
    processor issues instructions inorder but executes them out-of-order. CVA6 codebase
    is a synthesizable Register-Transfer Level (RTL) model.


    There is a need to improve CVA6 performance to address more applications. For
    instance, [\[3\]](#page-6-2) implemented a superscalar CVA6 and reported a performance
    gain of 16% on Dhrystone benchmark. To investigate further performance improvements,
    we propose a model-driven methodology for architectural exploration and implementation.


    <span id="page-0-6"></span><span id="page-0-5"></span>Modeling only the control
    path eases modifications for design exploration while allowing performance modeling.
    Hence, detailed models such as Gem5 [\[4\]](#page-6-3) are not suitable. Cycle-accurate
    comparison with RTL is needed for model-driven implementation so mechanistic and
    empirical modeling [\[5\]](#page-6-4) are not suitable either. Despite the short
    *execution* time of such models, we focus on limiting the *development* time to
    perform microarchitecture exploration.


    We implemented a cycle-accurate model of the reference CVA6 in Python. We modified
    this model to perform architectural exploration and take design decisions. We
    then performed a model-driven implementation of a superscalar CVA6: we often used
    our model to verify that the expected performance is met, and to debug potential
    discrepancies. The *superscalar* feature is optional and can be enabled by RTL
    configuration.


    <span id="page-0-8"></span><span id="page-0-7"></span>CoreMark [\[6\]](#page-6-5)
    is a well-known benchmark with matrix operations, chained lists and state machines.
    An iteration runs about 266,000 dynamic instructions. The performance of the reference
    CVA6 is 3.10 CoreMark/MHz [\[7\]](#page-6-6) and 4.35 CoreMark/MHz for the superscalar
    version resulting from our modeldriven approach, improving performance by 40%.


    <span id="page-0-1"></span>Our model is open-source[¹](#page-0-0) and our superscalar
    CVA6 will be proposed for integration into CVA6 codebase.


    The paper is organized as follows. First, we detail our model. We then describe
    the main changes to build a superscalar CVA6. Finally, implementation results
    are presented.


    <span id="page-0-0"></span>[¹](#page-0-1)See <https://github.com/ThalesSiliconSecurity/cva6-perf-model>


    # 2. Modeling


    <span id="page-1-3"></span><span id="page-1-2"></span>Our model simulates performance
    but not behavior. Hence, it requires a RISC-V Formal Interface (RVFI) [\[8\]](#page-6-7)
    trace as an input, i.e. the list of dynamic instructions executed. It does not
    run faster than Verilator [\[9\]](#page-6-8) RTL simulation. Developed in Python,
    it is easier to modify as it is 24 times smaller than CVA6 RTL model. CVA6 hardware
    architecture is a 6-stage pipeline:


    - 1. Instruction fetch

    - 2. Program counter generation (branch prediction)

    - 3. Decode

    - 4. Issue (operands reading)

    - 5. Execute

    - 6. Commit


    Only pipeline stages 4 to 6 are modeled because they require most design decisions.
    These three stages are modeled with Python functions as shown in [Figure 1](#page-1-0).
    A clock cycle is modeled by running these three functions. They are run in reverse
    order to simulate sequential logic [\[10\].](#page-6-9)


    <span id="page-1-4"></span><span id="page-1-0"></span>![](_page_1_Figure_9.jpeg)


    <span id="page-1-5"></span>Figure 1: The pipeline of the model


    #### 2.a. Modeling instruction queue


    A list is built from the input RVFI trace where each dynamic instruction is stored
    as an object with the following fields:


    - the program counter, for branch prediction;

    - the instruction in hexadecimal, for decoding;

    - the disassembly, for human-readable display.


    This list, called *Instruction Queue*, is a FIFO because CVA6 is an in-order processor.


    #### <span id="page-1-1"></span>2.b. Modeling issue stage


    Issuing consists in taking an instruction from *Instruction Queue* and pushing
    it into *Scoreboard*. The latter keeps track of the issued-but-not-committed instructions,
    as it does in the RTL. try\_issue() verifies the absence of any of the three kinds
    of hazards [\[11\]](#page-6-10) beforehand.


    Data hazards between the instruction being issued and each scoreboard entry are
    checked. There are three sorts of data hazards. Read After Write (RAW) hazards
    occur when one of the source registers of is the destination register of . CVA6
    can forward operands: there is no need to stall if the result from is available
    albeit not committed. Write After Write (WAW) hazards happen when the destination
    register of is the same as . These hazards can be eliminated thanks to register
    renaming. CVA6 has no Write After Read (WAR) hazards because it issues instructions
    in-order.


    Structural hazards occur when *Scoreboard* is full and cannot accept new instructions.
    They are modeled on functional units (FUs) with a busy Boolean for each FU, indicating
    that it cannot accept new instructions. When an instruction is issued to FU ,
    it becomes busy because FUs can accept at most one instruction per cycle. If another
    FU uses the same write-back port as , it becomes busy as well. As a new cycle
    starts, FUs are freed. However, as the multiplier has a 2-stage internal pipeline;
    if it was busy the previous cycle, units sharing its write-back port become busy
    instead.


    Control hazards are handled by performing branch prediction on the last issued
    instruction. In case of a branch miss, issue stage stalls if the miss occurred
    less than 6 cycles ago. CVA6 has three branch prediction mechanisms: Return Address
    Stack (RAS), Branch History Table (BHT), and Branch Table Buffer (BTB). RAS and
    BHT model implementations are similar to their CVA6 counterparts. BTB is not part
    of the model because there are no indirect jumps in CoreMark; except returns,
    which are handled by the RAS.


    The corresponding try\_issue() function is called times per cycle to model a hypothetical
    -issue CVA6.


    #### 2.c. Modeling execute stage


    Each instruction is issued into *Scoreboard* with a cycle counter initialized
    to 0. try\_execute() increments all these counters every cycle because issued
    instructions are executed in parallel. When a counter reaches the instruction
    execution duration, its instruction is marked as *done*. Multiplications, loads
    and stores are 2-cycle long whereas other instructions take 1 cycle. This try\_execute()
    function is called once per clock cycle.


    #### 2.d. Modeling commit stage


    *done* instructions are removed from *Scoreboard* in-order and put into the *Retired*
    list. This try\_commit() function is called times per cycle to model a hypothetical
    -commit CVA6 but only the first commit port can commit a store.


    #### <span id="page-2-0"></span>2.e. Measuring model accuracy


    The model outputs an RVFI trace annotated with the commit cycle of each dynamic
    instruction. To validate our model, we need to compare this trace with one from
    the RTL. We modified the RVFI tracer in RTL to get commit cycle from Verilator
    simulation. To measure accuracy, we used the methodology from [\[7\]](#page-6-6),
    which we recall below.


    Let be the commit cycle of the th instruction. Let Δ = − −1 . For each instruction
    , this duration from the model Δ Model is compared with the one from Verilator
    Δ RTL . We define the accuracy as the ratio of matching instructions.


    $$\text{Accuracy} = \frac{\#\{i \mid \Delta t\_i^{\text{Model}} = \Delta t\_i^{\text{RTL}}\}}{\#\{i\}}$$


    We measured an accuracy of 99.2% on CoreMark 2nd iteration. We used a cv32a6\_imac\_sv0-based
    CVA6 configuration, enabling Zba, Zbb, Zbc and Zbs RISC-V extensions support and
    assuming a low cache latency of one cycle. When disabling the RAS in both the
    model and the RTL, the model accuracy increases to 99.8%. From this promising
    accuracy, we will exploit it to take implementation decisions.


    ### 2.f. First results from the model


    [\[7\]](#page-6-6) focused on issue width and the number of commit ports . They
    report no benefit to have > and little benefit to have > while increasing requires
    more hazards management logic so we chose = . Considering performance and pipeline
    widths, we chose a width of 2.


    The only structural hazard modeled is the shared write-back port between the ALU
    and the multiplier, which have different latencies. It is the only structural
    hazard that occurs on CoreMark in single-issue CVA6. We improved the modeling
    of structural hazards as explained in [Section 2.b.](#page-1-1) A second ALU was
    added with precedence for better FU usage.


    We discarded modifications which were predicted to not bring enough performance
    according to the model. For instance, we initially planned to connect the multiplier
    to the same write-back port as the FPU instead of the ALU. The hypothesis was
    that integer multiplications are rarely mixed with floating-point operations.
    However, they are often mixed with integer additions, for instance to multiplyaccumulate.
    As the model predicted a performance gain of less than 3% for a single-issue CVA6
    on the CoreMark, we decided to discard this modification.


    ### 3. Implementing


    To implement a 2-way superscalar CVA6, we split our development in three steps.
    Expected performance improvements below are relative to single-issue CVA6. The
    implementation step definition is comparable to [\[3\]](#page-6-2), minus branch
    prediction.


    - 64-bit instruction fetch: +1%

    - Dual issue (single ALU): +21%

    - <span id="page-3-1"></span>• Superscalar (two ALUs): +47%[²](#page-3-0)


    Our model provides 3 levels of comparison with RTL results from Verilator for
    debugging.


    - () Global performance: cycles spent.

    - () Local performance: cycle-annotated trace.

    - () Internal: print scoreboard and events each cycle.


    We evaluate performance () after each development step. If we are satisfied, we
    continue with the next step. Else, we compare the annotated trace () from Verilator
    with the one from our model to find which instruction sequences cause the difference.
    The issue can be either in our RTL implementation or in the model. To understand
    the difference, we compare the cycle-accurate scoreboard evolution () using Verilator
    waveforms and the model debug trace.


    The modified parts of CVA6 are shown in [Figure 2](#page-3-2), where colors indicate
    three levels of changes. Blue parts are affected by parameter value changes without
    significant code changes. Purple parts are modified for genericity or new features.
    Green modules are instantiated more times in dual- than in singleissue. These
    modifications can be enabled via SystemVerilog genericity features.


    ### <span id="page-3-2"></span>3.a. Frontend


    CVA6 *Frontend* gathers stages 1 and 2. Making instruction fetch from 32- to 64-bit
    required few changes as it was almost generic. We fixed *Re-aligner* 64-bit support.


    *Instruction Scan* detects control flows for branch prediction. Instructions are
    put into *Instruction Queue* before the decode stage. CVA6 conditionally supports
    compressed instructions so these items were already generic.


    ### 3.b. Dual issue


    We made it possible to pop up to two instructions from *Instruction Queue*. We
    duplicated *Compressed Decoder* and *Decoder*. We made *Issue Buffer* a 2-instruction
    FIFO.


    <span id="page-3-0"></span>[²](#page-3-1)Like the reference CVA6, the model had
    register renaming at the time of the experiments. This feature has since been
    removed from CVA6. The model has been updated since, removing register renaming
    and integrating feedback from the implementation phase. The improvement predicted
    by the latest version of the model is 41%.


    ![](_page_4_Figure_0.jpeg)


    Figure 2: Simplified version of CVA6 schematic [\[2\]](#page-6-1), with colors
    for superscalar-related modifications.


    *Issue Read Operands* reads twice more operands from registers and detects twice
    more data hazards with instructions in *Scoreboard*, allowing twice more operand
    forwarding. It detects hazards between the two instructions being issued.


    We implemented structural hazards detection in the same way as we did in our Python
    model in [Sec](#page-1-1)[tion 2.b.](#page-1-1)


    After this step, the superscalar made the embedded version of CVA6 slower. We
    found with the model () that *Scoreboard* was not efficiently used anymore, which
    became an issue due to embedded little *Scoreboard*. Indeed, we initially prevented
    from issuing as soon as less than two *Scoreboard* entries were free. We fixed
    this by detecting separately if *Scoreboard* is full and if there is exactly one
    free entry. To do this, we check if all odd index entries are occupied. We do
    the same for even index entries. As *Scoreboard* is a circular buffer, an *and*
    between these two values means that the scoreboard is full and an *or* means that
    a second instruction cannot be issued this cycle.


    *Issue* stage now emits two instructions to *Execute* stage. We added a multiplexer
    2–1 before each FU. It selects an issue port targeting this FU. No precedence
    rule is required because *Issue* stage already handles structural hazards.


    #### 3.c. Speculative Scoreboard


    At the end of the dual-issue implementation step, a performance improvement of
    12% was reached, instead of 21%. The cycle-annotated traces () showed that most
    differences followed branch hits. Indeed, in the model, we did not implement control
    hazards between instructions issued in the same cycle. To fill the gap, the instruction
    following a control flow should be issued the same cycle. Consequently, it must
    be discarded when the branch is resolved as missed. However, *Scoreboard* could
    only be completely flushed.


    *Scoreboard* is a FIFO with two pointers: the issue pointer to push instructions
    and the commit one to pop them. We wrote a new module to build the interval of
    instructions to discard, which are between the branch and the issue pointer. It
    takes two indexes (, ) ∈ [0; [ 2 and returns a -bit vector where the bit at index
    is set if ∈ [; [. *Scoreboard* is cycling, so > is valid and results in a non-zero
    vector.


    To discard instructions, simply removing them from *Scoreboard* does not work
    because the removed instruction is not cancelled in *Execute* stage: it can write
    back the wrong result into the entry of the correct instruction. Instead, a cancelled
    bit was added to each *Scoreboard* entry. When a branch is missed, the cancelled
    bits of next instructions are set. *Commit* stage acknowledges cancelled instructions
    without modifying the architectural state.


    This *speculative scoreboard* feature is optional.


    #### 3.d. Second ALU


    To not add an area-consuming write-back port, the added ALU uses the *FPU* write-back
    port. However, knowing when the FPU will yield a result is not trivial. As we
    do not need a FPU yet, we chose to not spend time analyzing it. As a consequence,
    superscalar CVA6 does not support FPU yet.


    ### 4. Results


    Performance, Power and Area (PPA) results are shown in [Table 1](#page-5-0). The
    reference is the same configuration as in [Section 2.e.](#page-2-0) For superscalar
    we enabled the *superscalar* and *speculative scoreboard* options. It results
    in a performance of 4.35 CoreMark/MHz; a gain of 40% which is greater than the
    related 11% area cost. The maximum frequency slightly reduced but the critical
    path is not in the modified parts.


    <span id="page-5-0"></span>


    | Criteria       | Reference | Superscalar | Variation |

    |----------------|-----------|-------------|-----------|

    | CoreMark/MHz   | 3.10      | 4.35        | +40.1%    |

    | Max. Frequency | 892 MHz   | 877 MHz     | −1.75%    |

    | Power          | 32.45 mW  | 34.84 mW    | +7.37%    |

    | Area           | 250 kGE   | 278 kGE     | +11.1%    |


    Table 1: Performance / Power / Area results


    All basic tests from the CVA6 repository pass with and without *superscalar* enabled.
    Linux boots on FPGA for single-issue and *superscalar* without *speculative scoreboard*.
    Booting Linux with *speculative scoreboard* is left for future work.


    We also ran Dhrystone on our superscalar CVA6 and measured a 24% gain; 1.5 times
    the previous version gain. This tends to validate our model-driven approach.


    ### 5. Discussion


    Our work is based on a model that is accurate and easy to modify. As a consequence,
    the model ignores data. It prevents the modeling of divisions and data cache.
    By extracting data from the RVFI trace, we could model them. Also, speculative
    instructions are not present in the RVFI trace. It prevents the modeling of instruction
    cache. Still, the model accuracy on CoreMark 2nd iteration is above 99% so we
    could take decisions based on it. However, as we focused on the CoreMark, the
    model accuracy could be lower on other benchmarks.


    We noticed that CoreMark has few WAW hazards compared to Dhrystone. By activating
    register renaming in our model, the gain related to the *superscalar* feature
    reaches 45% on CoreMark. Thus, we believe that register renaming could significantly
    improve CVA6 performance on Dhrystone.


    # 6. Conclusion


    A performance model of CVA6 was built in raw Python. An accuracy metric was defined.
    The measured model accuracy is above 99%. We extrapolated the model to estimate
    the performance gain of making CVA6 superscalar.


    A superscalar CVA6 was implemented and can be enabled by configuration. While
    implementing, the model was used as a reference with means of detailed comparison.
    It helped finding and fixing several performance bugs. Especially, we added an
    optional *speculative scoreboard* feature which allows partial scoreboard flush.


    Future works will focus on (i) booting Linux with this new feature and pushing
    these changes to the CVA6 repository [\[2\]](#page-6-1); (ii) implementing register
    renaming via the scoreboard to fix WAW hazards; (iii) running Post Quantum Cryptography
    (PQC) benchmarks using the model to see how it matches RTL and to evaluate potential
    optimizations.


    ## Acknowledgments


    These activities are supported by the TRISTAN project funded by the Key Digital
    Technologies Joint Undertaking (KDT JU) under grant agreements 101095947. The
    present action reflects only the authors'' view; the European Commission and the
    JU are not responsible for any use that may be made of the information it contains.


    # Bibliography


    - <span id="page-6-0"></span>[\[1\]](#page-0-2) F. Zaruba et al., "The Cost of
    Application-Class Processing: Energy and Performance Analysis of a Linux-Ready
    1.7-GHz 64-Bit RISC-V Core in 22-nm FDSOI Technology," *IEEE Transactions on VLSI
    Systems*, vol. 27, no. 11, pp. 2629–2640, Nov. 2019, doi: [10.1109/TVLSI.2019.2926114](https://doi.org/10.1109/TVLSI.2019.2926114).

    - <span id="page-6-1"></span>[\[2\]](#page-0-3) OpenHW Group, "CVA6 RISC-V CPU."
    GitHub, 2024.

    - <span id="page-6-2"></span>[\[3\]](#page-0-4) Z. Zheng, "Microarchitectural
    Enhancement Of Ariane." 2019.

    - <span id="page-6-3"></span>[\[4\]](#page-0-5) gem5, "The gem5 simulator system,"
    2011, [Online]. Available: <https://www.gem5.org/>

    - <span id="page-6-4"></span>[\[5\]](#page-0-6) M. Breughe et al., "A Mechanistic
    Performance Model for Superscalar In-Order Processors," 2012, doi: [10.1109/ISPASS.2012.6189202](https://doi.org/10.1109/ISPASS.2012.6189202).

    - <span id="page-6-5"></span>[\[6\]](#page-0-7) EEMBC, "CoreMark." 2024.

    - <span id="page-6-6"></span>[\[7\]](#page-0-8) C. Allart et al., "Performance
    Modeling of CVA6 with Cycle-Based Simulation," 2023, [Online]. Available: <https://riscv-europe.org/summit/2023/posters>

    - <span id="page-6-7"></span>[\[8\]](#page-1-2) "RISC-V Formal Interface (RVFI)."
    2020.

    - <span id="page-6-8"></span>[\[9\]](#page-1-3) Veripool, "Verilator." [Online].
    Available: <https://www.veripool.org/verilator/>

    - <span id="page-6-9"></span>[\[10\]](#page-1-4) S. Jiang et al., "UMOC: Unified
    Modular Ordering Constraints to Unify Cycle- and Register-Transfer-Level Modeling,"
    in *2021 58th ACM/IEEE Design Automation Conference (DAC)*, San Francisco, CA,
    USA: IEEE, Dec. 2021, pp. 883–888. doi: [10.1109/DAC18074.2021.9586130](https://doi.org/10.1109/DAC18074.2021.9586130).

    - <span id="page-6-10"></span>[\[11\]](#page-1-5) J. L. Hennessy and D. A. Patterson,
    *Computer architecture: a quantitative approach*, Sixth edition. Cambridge, MA:
    Morgan Kaufmann Publishers, 2019.'
- title: 'IMAGine: An In-Memory Accelerated GEMV Engine Overlay'
  abstract: 'Processor-in-Memory (PIM) overlays and new redesigned reconfigurable
    tile

    fabrics have been proposed to eliminate the von Neumann bottleneck and enable

    processing performance to scale with BRAM capacity. The performance of these

    FPGA-based PIM architectures has been limited due to a reduction of the BRAMs

    maximum clock frequencies and less than ideal scaling of processing elements

    with increased BRAM capacity. This paper presents IMAGine, an In-Memory

    Accelerated GEMV engine, a PIM-array accelerator that clocks at the maximum

    frequency of the BRAM and scales to 100% of the available BRAMs. Comparative

    analyses are presented showing execution speeds over existing PIM-based GEMV

    engines on FPGAs and achieving a 2.65x - 3.2x faster clock. An AMD Alveo U55

    implementation is presented that achieves a system clock speed of 737 MHz,

    providing 64K bit-serial multiply-accumulate (MAC) units for GEMV operation.

    This establishes IMAGine as the fastest PIM-based GEMV overlay, outperforming

    even the custom PIM-based FPGA accelerators reported to date. Additionally, it

    surpasses TPU v1-v2 and Alibaba Hanguang 800 in clock speed while offering an

    equal or greater number of MAC units.'
  url: http://arxiv.org/abs/2410.04367v1
  keywords: Processing-in-Memory, System Design, Block RAM, GEMV engine, Processor
    Array.
  document: '© 2024 IEEE. Personal use of this material is permitted. Permission from
    IEEE must be obtained for all other uses, in any current or future media, including
    reprinting/republishing this material for advertising or promotional purposes,
    creating new collective works, for resale or redistribution to servers or lists,
    or reuse of any copyrighted component of this work in other works.


    This work has been accepted at the 2024 34th International Conference on Field-Programmable
    Logic and Applications (FPL) and will appear in the proceedings and on the IEEE
    website soon.


    # IMAGine: An *I*n-*M*emory *A*ccelerated *G*EMV Eng*ine* Overlay


    MD Arafat Kabir<sup>∗</sup> , Tendayi Kamucheka<sup>∗</sup> , Nathaniel Fredricks<sup>∗</sup>
    ,


    Joel Mandebi† , Jason Bakos‡ , Miaoqing Huang<sup>∗</sup> , and David Andrews<sup>∗</sup>


    <sup>∗</sup>Department of Electrical Engineering and Computer Science, University
    of Arkansas,


    ‡Department of Computer Science and Engineering, University of South Carolina,


    †Advanced Micro Devices, Inc. (AMD)


    {makabir, tfkamuch, njfredri, mqhuang, dandrews}@uark.edu, jmandebi@amd.com, jbakos@cse.sc.edu,


    *Abstract*—Processor-in-Memory (PIM) overlays and alternative reconfigurable tile
    fabrics have been proposed to eliminate the von Neumann bottleneck and enable
    processing performance to scale with BRAM capacity. The performance of these FPGAbased
    PIM architectures has been limited due to a reduction of the BRAMs maximum clock
    frequencies and less than ideal scaling of processing elements with increased
    BRAM capacity. This paper presents IMAGine, an In-Memory Accelerated GEMV engine,
    a PIM-array accelerator that clocks at the maximum frequency of the BRAM and scales
    to 100% of the available BRAMs. Comparative analyses are presented showing execution
    speeds over existing PIM-based GEMV engines on FPGAs and achieving a 2.65× – 3.2×
    faster clock. An AMD Alveo U55 implementation is presented that achieves a system
    clock speed of 737 MHz, providing 64K bit-serial multiply-accumulate (MAC) units
    for GEMV operation. This establishes IMAGine as the fastest PIM-based GEMV overlay,
    outperforming even the custom PIM-based FPGA accelerators reported to date. Additionally,
    it surpasses TPU v1-v2 and Alibaba Hanguang 800 in clock speed while offering
    an equal or greater number of multiply-accumulate (MAC) units.


    *Index Terms*—Processing-in-Memory, System Design, Block RAM, GEMV engine, Processor
    Array.


    #### I. INTRODUCTION


    The exponential growth of Internet-of-Things (IoT) devices and social media applications
    has significantly changed the landscape of computing workloads. Modern workloads,
    such as scientific computation, graph processing, and machine learning, generate
    and process datasets that are expanding at a rate that outpaces Moore''s Law [\[1\]](#page-7-0).
    However, today''s processors remain constrained by the "Memory Wall" of the von
    Neumann architecture, which limits the ability to exploit the parallelism within
    these memory-intensive tasks. Processingin-memory (PIM) architectures are being
    pursued [\[2\]](#page-7-1)–[\[15\]](#page-7-2) to mitigate the memory wall and
    enable processing performance to scale with memory capacity.


    Modern Field Programmable Gate Arrays (FPGAs) with 100s of Mbits of SRAM distributed
    throughout the device in the form of disaggregated memory resources can provide
    several TB/s of internal bandwidth. This is an ideal programmable substrate for
    creating customized Processor In/Near Memory accelerators. Several PIM array-based
    accelerator designs [\[6\]](#page-7-3)– [\[13\]](#page-7-4) have been proposed
    to harness this massive internal bandwidth. However, results reported to date
    show achievable clock frequencies and compute densities are not sufficient to
    compete with their custom Application Specific Integrated Circuit (ASIC) counterparts.


    Such shortcomings have motivated redesigns of the separate Block-RAM (BRAM) and
    LUT resources into tightly integrated PIM tiles. While these redesigns have increased
    chip compute densities, the maximum achievable clock frequency remains no better
    than their overlay counterparts. Additionally, the adoption of a bigger FPGA with
    an increased resource capacity does not translate into a linear increase in compute
    performance.


    This paper presents IMAGine, a PIM array-based GEMV accelerator that clocks at
    the maximum frequency of the BRAM. The PIM tile array architecture of IMAGine
    has been designed to achieve linear scalability of the number of compute units
    with increased BRAM densities. Comparative studies are presented that show it
    is the fastest and most scalable PIM array-based GEMV accelerator reported to
    date. Run time results also show that IMAGine shatters some of the myths concerning
    performance limitations of PIM-array accelerators and FPGA overlays in general.
    Our contributions can be summarized as follows,


    - A set of aspirational but practical design goals for PIM array-based accelerators.
    We argue these goals need to be met to claim a "Scalable High-Performance PIM
    design" on FPGAs.

    - We present the design and implementation of IMAGine, an In-Memory Accelerated
    GEMV engine overlay, that breaks some existing myths around FPGA design, clocking
    faster than Google''s TPU v1-v2 with equal or more processing elements (PEs) using
    an off-the-shelf datacenter-grade FPGA.

    - We present a comparative study of IMAGine with existing PIM-array accelerators,
    establishing it as the fastest and most scalable FPGA PIM-based GEMV accelerator.


    IMAGine has been published at [\[16\]](#page-7-5) as open-source implementation
    and is freely available for study, use, modification, and distribution without
    restriction.


    This material is based upon work supported by the National Science Foundation
    under Grant No. 1955820.


    <span id="page-2-0"></span>TABLE I MAXIMUM FREQUENCY (MHZ) OF EXISTING FPGA-PIM
    DESIGNS


    | PIM Design        | Type   | Device              | fBRAM fP IM |     | Rel.
    | fSys Rel. |         |

    |-------------------|--------|---------------------|-------------|-----|------|-----------|---------|

    | CCB               | Custom | Stratix 10          | 1000        | 624 | 62%  |           |
    455 46% |

    | CoMeFa-A          | Custom | Arria 10            | 730         | 294 | 40%  |           |
    288 39% |

    | CoMeFa-D          | Custom | Arria 10            | 730         | 588 | 81%  |           |
    292 40% |

    | BRAMAC-2SA Custom |        | Arria 10            | 730         | 586 | 80%  |
    -         | -       |

    | BRAMAC-1DA Custom |        | Arria 10            | 730         | 500 | 68%  |
    -         | -       |

    | M4BRAM            | Custom | Arria 10            | 730         | 553 | 76%  |
    -         | -       |

    | SPAR-2            |        | Overlay UltraScale+ | 737         | 445 | 60%  |           |
    200 27% |

    | PiCaSO            |        | Overlay UltraScale+ | 737         | 737 | 100%
    | -         | -       |


    ## II. RELATED WORK


    # <span id="page-2-1"></span>*A. Custom-BRAM PIMs*


    Wang et al [\[6\]](#page-7-3) proposed the Compute-Capable BRAM (CCB) based on
    Neural Cache [\[17\]](#page-7-6). CCB exposes compute parallelism within a BRAM
    by converting each BRAM bitline into a bit-serial Processing Element (PE). CCB
    was used to build RIMA [\[6\]](#page-7-3) to accelerate recurrent neural networks
    (RNNs). RIMA achieved 1.25× and 3× higher performance compared to the Brainwave
    DL soft processor [\[18\]](#page-7-7) for 8-bit integer and block floating-point
    precisions, respectively.


    Arora et al [\[10\]](#page-7-8), [\[11\]](#page-7-9) proposed CoMeFa that uses
    bitserial PEs per SRAM bitline like CCB, but exploits the dualport nature of BRAMs
    to simultaneously read two operands. To evaluate the performance and energy benefits
    of CoMeFa RAMs, various microbenchmarks, including General Matrix-Vector Multiplication
    (GEMV) and General Matrix-Matrix Multiplication (GEMM) were studied in [\[11\]](#page-7-9).
    Augmenting an Intel Arria 10-like FPGA with CoMeFa RAMs delivered a geomean speedup
    of 2.55× across diverse applications.


    Chen et al proposed BRAMAC [\[12\]](#page-7-10) and M4BRAM [\[13\]](#page-7-4),
    which bypass MAC computation on the slow and powerhungry primary BRAM array by
    copying operands to a smaller "dummy array". BRAMAC requires 2-/4-/8-bit predefined
    weights and activations, limiting its use to quantized uniformprecision deep neural
    nets. M4BRAM overcomes some of these limitations by enabling variable activation
    precision between 2 and 8 bits with linearly scaled MAC latency. Combining BRAMAC-2SA/BRAMAC-1DA
    with Intel''s DLA [\[19\]](#page-7-11) resulted in an average speedup of 2.05×/1.7×
    for AlexNet and 1.33×/1.52× for ResNet-34. M4BRAM surpassed BRAMAC by an average
    of 1.43× across diverse benchmarks.


    # *B. BRAM-Overlay PIMs*


    To leverage the benefits of PIM architectures in contemporary FPGAs, PIM overlay
    architectures have been proposed. Panahi et al [\[7\]](#page-7-12)–[\[9\]](#page-7-13)
    proposed SPAR-2, a SIMD PIM-array overlay accelerator, connecting bit-serial PEs
    from the programmable fabric with BRAMs. SPAR-2 was implemented on Virtex-7 and
    Virtex UltraScale FPGAs with 10K PEs to accelerate several deep learning applications.
    It achieved up to 34.2× and 3.5× speedups compared to other custom HLSbased and
    RTL-based accelerators, respectively.


    Building upon the PIM overlay of SPAR-2, Kabir et al proposed PiCaSO [\[15\]](#page-7-2)
    with configurable pipeline stages along the datapath. PiCaSO introduced an intermediate
    muxing module


    <span id="page-2-2"></span>TABLE II DELAY (NS) BREAKDOWN OF 1-LEVEL LOGIC PATH
    IN AMD DEVICES


    |     | FF-C2Q1 LUT |      | FF-Setup |       |       | Total2 BRAM3 Net Budget
    | SB-Min4 |

    |-----|-------------|------|----------|-------|-------|-------------------------|---------|

    | V7  | 0.290       | 0.34 | 0.255    | 0.885 | 1.839 | 0.954                   |
    0.272   |

    | US+ | 0.087       | 0.15 | 0.098    | 0.335 | 1.356 | 1.021                   |
    0.102   |


    <sup>1</sup> Clock-to-Q delay of flip-flops


    <sup>2</sup> Total cell delay


    <sup>3</sup> BRAM pulse-width requirement, clock period for Fmax


    <sup>4</sup> Minimum net delay through a switchbox


    ![](_page_2_Figure_16.jpeg)


    <span id="page-2-3"></span>Fig. 1. Ideal scaling vs. actual TOPS of RIMA on Stratix
    10 GX2800


    to enable zero-copy in-block reduction and a "binary-hopping" pipelined NEWS network
    for array-level reduction. PiCaSO provided competitive performance and memory
    utilization efficiency compared to both CCB and CoMeFa custom-BRAM architectures.


    ## III. MOTIVATION AND DESIGN GOALS


    <span id="page-2-4"></span>Table [I](#page-2-0) summarizes the maximum frequencies
    of the PIM designs discussed in section [II.](#page-2-1) The relative frequency
    columns (Rel.) show that the clock frequency fP IM of all the PIM tiles are significantly
    slower compared to the maximum frequency for the device BRAMs (fBRAM), except
    for PiCaSO. Their fastest system frequencies (fSys) are 2.1× – 3.7× slower than
    the BRAM maximum frequencies (fBRAM). This slower frequency was attributed to
    the limitations of the soft logic and the routing resources of the FPGAs. It was
    also reported as unlikely that an FPGA accelerator at the system level would operate
    at a frequency surpassing the degraded frequency (fP IM) of these PIM designs,
    even in a more advanced node than the evaluation platforms [\[10\]](#page-7-8)–[\[13\]](#page-7-4).


    Further observation yielded that most of these systems could not utilize all available
    BRAMs as PIMs. This lower utilization combined with a lower clock frequency results
    in less efficient use of the available internal BRAM bandwidth of the devices
    and a lower system-level compute density. A final observation shows a troubling
    common pattern: as the utilization of BRAMs increases the achievable system-level
    clock frequency decreases [\[6\]](#page-7-3), [\[11\]](#page-7-9).


    These observations motivated our interest in understanding if these results were
    a new reality of BRAM PIM arrays or symptomatic of specific design and implementation
    choices.


    # *A. System Clock Speed Goal*


    In FPGAs, BRAMs are the single component with the longest latency [\[20\]](#page-7-14)–[\[22\]](#page-7-15).
    Thus, we propose using the maximum frequency (Fmax) of the BRAM as the target
    frequency for the PIM-array accelerators. To assess the practicality of this


    ![](_page_3_Figure_0.jpeg)


    <span id="page-3-0"></span>Fig. 2. System architecture of IMAGine illustrating
    the data and instruction flow (a) through the GEMV engine and (b) within GEMV
    tiles.


    design goal, we closely examined two AMD FPGA families: Virtex-7 and UltraScale+.
    We created a test design where all timing paths are one logic level deep and averaged
    all paths to obtain Table [II.](#page-2-2) The Total column sums the cell delays
    in the columns to its left. The BRAM column lists the clock period for BRAM Fmax.
    The SB-Min column displays the minimum delay of a net passing through a switchbox.
    Net Budget is derived by subtracting the Total column from the BRAM column. Comparing
    the net budget with the minimum net delay shows that, it is feasible to design
    at least two LUTs deep logic paths clocking at the BRAM Fmax.


    ### *B. Performance Scaling Goal*


    We posited that the peak-performance of a PIM design needs to scale linearly with
    the on-chip BRAM resource. The compute capacity in custom-BRAM-based PIM designs
    [\[6\]](#page-7-3), [\[10\]](#page-7-8)–[\[13\]](#page-7-4) scales linearly with
    BRAM count if all BRAM tiles are used in PIM mode. However, a significant sacrifice
    is imposed in the clock frequency that ends up limiting the achievable peak-performance
    on the device. Table [I](#page-2-0) fP IM column indicates that the custom-BRAM
    PIM designs are up to 2.5× slower than the BRAM Fmax. Fig. [1](#page-2-3) plots
    RIMA''s peakperformance from Table-II of [\[6\]](#page-7-3), computed using reported
    BRAM utilization and M-DPE clock frequency. The irregular trend is attributed
    to RIMA''s system-level architecture. If RIMA adhered to the proposed performance
    scaling goal, even at the degraded CCB frequency of 624 MHz, its peakperformance
    would align with the CCB Ideal TOPS line. The gap between these plots represents
    wasted compute capacity and memory bandwidth provided by CCB BRAMs.


    #### IV. IMAGINE ARCHITECTURE


    #### *A. System-Level Architecture*


    The top-level system is illustrated in Fig. [2\(](#page-3-0)a). It consists of
    (1) a 2D array of GEMV tiles, (2) a set of input registers, (3) a fanout tree
    connecting the input registers to the tile array, and (4) a column of shift-registers
    to read out the final result. The front-end processor sends instructions to the
    GEMV tiles through the input registers. The fanout tree is parameterized to be
    adjusted during implementation. The 2D tile array is implemented as a parameterized
    module that instantiates and connects the tiles. At the end of the GEMV operation,
    the output vector is stored in the column shift registers, which is


    ![](_page_3_Figure_8.jpeg)


    <span id="page-3-1"></span>Fig. 3. Architectures of (a) GEMV controller and (b)
    PiCaSO-IM, the adapted version of PiCaSO-F [\[15\]](#page-7-2).


    shifted up and read through the FIFO-out port, one element per cycle.


    #### *B. IMAGine GEMV Tile Architecture*


    Illustrated in Fig. [2\(](#page-3-0)b), the GEMV tile is the heart of IMAGine.
    It consists of (1) an FSM-based controller, (2) a 2D array of PIM blocks, and
    (3) a fanout tree between them. The controller receives the instruction written
    to the input registers at the top level, decodes it, and generates the sequence
    of control signals needed to execute the instruction. The fanout tree connects
    the control signals to all PEs in the PIM array and is parameterized for adjustment
    during implementation. The PIM array interfaces allow cascading with arrays in
    neighboring tiles on each side. During accumulation, partial results move from
    east to west through PIM arrays, ultimately accumulating in the left-most PE column
    of the left-most GEMV tile.


    #### *C. Tile Controller*


    Fig. [3\(](#page-3-1)a) shows the architecture of the tile controller. It takes
    a 30-bit instruction, which is executed by either the single-cycle or the multicycle
    driver, selected by the 2-state driver-selection FSM. The single-cycle driver
    can execute one instruction every cycle, while the multicycle driver takes several
    cycles to execute instructions like ADD, SUB, MULT, etc. including an additional
    cycle to load its parameters from the Op-Params module. All inputs and outputs
    are registered to localize timing paths within the controller. The combinatorial
    logic in the controller is grouped into meaningful steps and optional pipeline
    stages are added as illustrated by the dashed lines A, B, and C in Fig. [3\(](#page-3-1)a).
    Running synthesis, we ensured that each step could be implemented in one or two
    logic levels.


    #### <span id="page-3-2"></span>*D. PIM Module*


    We adopted PiCaSO [\[15\]](#page-7-2) as IMAGine''s PIM module for the following
    three reasons: (1) it is publicly available and open-source [\[23\]](#page-7-16),
    (2) it is a modifiable overlay that can be ported and studied on existing AMD
    devices, and (3) PiCaSO-F, a pipelined configuration of PiCaSO, can be clocked
    at the BRAM Fmax. The modifications highlighted in red in Fig. [3\(](#page-3-1)b)
    were applied to PiCaSO-F to build PiCaSO-IM for IMAGine. The original NEWS network
    was replaced with a simpler eastto-west data movement network. Block-ID-based
    selection


    <span id="page-4-0"></span>TABLE III UTILIZATION AND FREQUENCY OF 12×2 GEMV TILE
    COMPONENTS


    |             | Controller Rel. |      | Fanout | Rel.  | PIM Array | Rel.   |
    Tile |

    |-------------|-----------------|------|--------|-------|-----------|--------|------|

    | LUT         | 167             | 5.8% | 0      | 0.0%  | 2736      | 94.2%  |
    2903 |

    | FF          | 155             | 4.0% | 615    | 15.9% | 3096      | 80.1%  |
    3866 |

    | DSP         | 0               | -    | 0      | -     | 0         | -      |
    0    |

    | BRAM        | 0               | 0.0% | 0      | 0.0%  | 12.0      | 100.0% |
    12   |

    | Freq. (MHz) | 890             | 1.2× | 890    | 1.2×  | 737       | 1×     |
    737  |

    |             |                 |      |        |       |           |        |      |


    logic was included in PiCaSO-IM. IMAGine''s accumulation algorithm requires 3
    addresses to maximize the overlap of data movement and computation. As PiCaSO-F
    supports only 2 simultaneous addresses, we added a pointer register for the third
    address. If PiCaSO is realized as a custom-BRAM tile as proposed in [\[15\]](#page-7-2),
    these changes can be implemented in programmable logic fabric, keeping registerfile,
    OpMux, and ALU modules within the BRAM tile. We name such a custom-BRAM implementation
    of PiCaSO-IM as PiCaSO-CB.


    ## V. IMPLEMENTATION AND ANALYSIS


    In this section, we discuss the bottom-up implementation and analysis of IMAGine,
    targeting the design goals discussed in Section [III.](#page-2-4) In [\[15\]](#page-7-2),
    PiCaSO was studied on AMD Alveo U55C (xcu55c, -2 speed grade). We use the same
    device as our implementation platform to keep the results predictable. The BRAM
    Fmax on this device is 737 MHz [\[21\]](#page-7-17), which sets the target clock
    period to be 1.356 ns. All of the following studies were carried out using Vivado
    2022.2.


    # *A. GEMV Tile*


    The components of the GEMV tile were studied individually to verify if they met
    the design requirements. Each tile contains a 12×2 PIM array and 2 stages of pipeline
    in the fanout tree, which best fits the physical layout of the Alveo U55 FPGA
    as discussed later in this section. Table [III](#page-4-0) shows the utilization
    and performance of these components and their relative values compared to the
    entire GEMV tile.


    The controller together with the fanout network passed the timing constraints
    at a clock rate of 890 MHz. Because the PIM array contains the BRAM, it cannot
    run faster than the BRAM Fmax. It passed the timing at 737 MHz, the BRAM Fmax.
    As observed in Table [III,](#page-4-0) the logic utilization of the controller
    is around 5% of the entire tile and requires no DSPs, while around 90% of the
    logic resources are consumed by the PIM array. Thus, the controller and the fanout
    tree are not expected to bottleneck system frequency or utilization. The GEMV
    tile''s speed and scalability are fundamentally dependent on the PIM array, which
    is the desired outcome.


    # *B. Scalability Study*


    To evaluate the scalability of our architecture on different device families,
    we followed the approach in [\[15\]](#page-7-2). Along with Alveo U55, four representatives
    were selected from AMD''s Virtex-7 and UltraScale+ devices based on two criteria:
    BRAM capacity and LUT-to-BRAM ratio. Table [IV](#page-4-1) lists these devices
    with their BRAM capacity, LUT-to-BRAM ratio, and a short ID used in Fig. [4.](#page-4-2)
    The target clock frequency of the system


    <span id="page-4-1"></span>TABLE IV REPRESENTATIVES OF VIRTEX-7 AND ULTRASCALE+
    FAMILIES [\[15\]](#page-7-2)


    | Device          | Tech | BRAM# | Ratio1 | Max PE#2 | ID   |

    |-----------------|------|-------|--------|----------|------|

    | xcu55c-fsvh-2   | US+  | 2016  | 646    | 64K      | U55  |

    | xc7vx330tffg-2  | V7   | 750   | 272    | 24K      | V7-a |

    | xc7vx485tffg-2  | V7   | 1030  | 295    | 32K      | V7-b |

    | xc7v2000tfhg-2  | V7   | 1292  | 946    | 41K      | V7-c |

    | xc7vx1140tflg-2 | V7   | 1880  | 379    | 60K      | V7-d |

    | xcvu3p-ffvc-3   | US+  | 720   | 547    | 23K      | US-a |

    | xcvu23p-vsva-3  | US+  | 2112  | 488    | 67K      | US-b |

    | xcvu19p-fsvb-2  | US+  | 2160  | 1892   | 69K      | US-c |

    | xcvu29p-figd-3  | US+  | 2688  | 643    | 86K      | US-d |

    |                 |      |       |        |          |      |


    <sup>1</sup> LUT-to-BRAM ratio


    <sup>2</sup> Number of PEs utilizing all BRAMs as PIMs


    ![](_page_4_Figure_14.jpeg)


    <span id="page-4-2"></span>Fig. 4. Resource usage of IMAGine on representatives
    of Virtex-7 and Ultrascale+ families utilizing 100% BRAMs as PIM overlays.


    was set to 100 MHz on all devices to avoid timing issues and only focus on the
    logic utilization of the system at this point.


    Fig. [4](#page-4-2) shows a bar graph of post-implementation utilization numbers
    of IMAGine on the representative devices. As observed, IMAGine can utilize 100%
    of the available BRAMs as PIM overlays providing 64K PEs in U55, with only 25%
    logic and 6% control set utilization. This leaves sufficient logic resources to
    implement the fanout trees and pipeline stages if they are needed to achieve the
    target clock speed. In fact, IMAGine scaled up to 100% of available BRAM in all
    the representative devices for Virtex-7 and UltraScale+ families.


    In the Virtex-7 family, the device V7-a has the smallest number of BRAMs and the
    smallest LUT-to-BRAM ratio. IMAGine used around 60% logic resources to provide
    24K PEs in V7-a. In the UltraScale+ family, US-a and US-b have the smallest number
    of BRAMs and the smallest LUT-to-BRAM ratio, respectively. In these devices IMAGine
    provide 23K and 67K PEs, respectively, using roughly 30% logic resources. For
    devices with more BRAMs and a higher LUTto-BRAM ratio the logic utilization is
    very small: the logic utilization in US-c is less than 10% providing 69K PEs.
    Thus, IMAGine is scalable up to 100% BRAM capacity irrespective of the available
    logic resources in existing devices.


    # *C. System-Level Timing Optimizations*


    For the final implementation, the target clock was set to 1.356 ns to match the
    BRAM Fmax of Alveo U55. The goal of the study was to find out how close we can
    get to the target clock rate, and what are the practical challenges that limit
    us from achieving it. We ran the first iteration using the default settings of
    Vivado and achieved a setup slack of -0.52 ns. The critical paths were within
    the controller with a logic depth of 4, going through the pipeline stage A of
    the controller as shown in Fig. [3\(](#page-3-1)a). So, we enabled the pipeline
    stage A in the controller for the next iteration.


    ![](_page_5_Figure_0.jpeg)


    <span id="page-5-0"></span>Fig. 5. Avoiding unnecessary hard-block (CMAC) crossing
    by floorplanning (a) placement and net connections before floorplanning, (b) floorplan
    localizing logic and routing, (b) placement and net connections in the final design.


    At the end of the second iteration of implementation, we achieved a setup slack
    of -0.38 ns. The control signals between the controller and the PIM array were
    failing the timing due to their high fanout and long routes. Thus, we synthesize
    a fanout tree between the controller and the PIM array empirically choosing 2
    levels and a fanout of 4 for the next iteration.


    The design achieved a setup slack of -0.27 ns in the third iteration. The long
    routes crossing hard blocks, like an Ethernet port (CMAC) [\[24\]](#page-7-18),
    were failing the timing. The white lines in Fig. [5\(](#page-5-0)a) highlight
    some of those critical nets. To avoid placement results generating such paths,
    we created floorplanning blocks (Pblocks) [\[25\]](#page-7-19) as shown in Fig.
    [5\(](#page-5-0)b), to localize the logic placement and routing of a tile. This
    required defining a tile with 12×2 PIM array on Alveo U55. Fig. [5\(](#page-5-0)c)
    shows the placement and net connections in the final iteration. The logic and
    routing of each tile are localized on either side of the hard block. Only the
    inter-tile connections for eastto-west accumulation, highlighted in yellow lines,
    cross the CMAC block requiring minimal routing resources.


    The final design met the timing at 737 MHz clock, demonstrating the practical
    achievability of the proposed clocking goal. Utilizing 100% available BRAMs as
    PIMs, this design also achieved linear scaling of peak-performance. Surprisingly,
    this clock rate is faster than custom GEMM accelerator ASICs TPU v1-v2 [\[1\]](#page-7-0),
    [\[26\]](#page-7-20) and Alibaba Hanguang 800 [\[27\]](#page-7-21), that run at
    700 MHz. Both Alveo U55 and TPU v2 are manufactured at 16 nm and Hanguang 800
    at 12nm technology nodes. So, this clock improvement is not due to a technology
    node difference. On Alveo U55, IMAGine has an equal number of PEs compared to
    TPU v1 (64K), and 4× of TPU v2 (16K). However, IMAGine can only deliver up to
    0.33 TOPS at 8-bit precision, which is significantly smaller compared to TPU v1
    (92 TOPS) and v2 (46 TOPS), due its bit-serial architecture. These results dispel
    the myth that FPGA designs are always slower and have less compute density compared
    to ASICs.


    ## *D. Comparison With Other PIM-Array Accelerators*


    Table [V](#page-5-1) shows the utilization and system frequencies of existing
    GEMV engines and equivalent PIM-array accelerators. System-level utilizations
    and frequencies for BRAMAC and M4BRAM-based systems were not reported in [\[12\]](#page-7-10),
    [\[13\]](#page-7-4).


    <span id="page-5-1"></span>TABLE V UTILIZATION AND FREQUENCY OF PIM-BASED GEMV/GEMM
    ENGINES


    |               | LUT   | FF    | DSP   | BRAM   |     | fSys1 Rel. Freq |

    |---------------|-------|-------|-------|--------|-----|-----------------|

    | RIMA-Fast     | 60%   |       | 50%   | 55%    | 455 | 45.5%           |

    | RIMA-Large    | 89%   |       | 50%   | 93%    | 278 | 27.8%           |

    | CCB GEMV      | 27.9% |       | 90.1% | 91.8%  | 231 | 31.6%           |

    | CoMeFa-A GEMV | 27.9% |       | 90.1% | 91.8%  | 242 | 33.2%           |

    | CoMeFa-D GEMM | 25.5% |       | 92.4% | 86.7%  | 267 | 36.6%           |

    | SPAR-2 (US+)  | 11.3% | 2.4%  | 0.0%  | 14.5%  | 200 | 27.1%           |

    | SPAR-2 (V7)   | 28.5% | 7.0%  | 0.0%  | 30.4%  | 130 | 23.9%           |

    | IMAGine       | 35.6% | 24.8% | 0.0%  | 100.0% | 737 | 100.0%          |

    | IMAGine-CB2   | 10.1% | 7.2%  | 0.0%  | 100.0% | 737 | 100.0%          |


    <sup>1</sup> System frequency in MHz


    2 IMAGine with custom-BRAM PIM tile (PiCaSO-CB)


    RIMA [\[6\]](#page-7-3) was evaluated on a Stratix 10 GX2800 FPGA with a BRAM
    Fmax of 1 GHz [\[22\]](#page-7-15). Its fastest reported configuration (RIMA-Fast)
    runs at 455 MHz, which is 2.2× slower than its BRAM Fmax. The largest reported
    configuration (RIMA-Large) utilizes 93% of BRAMs and runs at 278 MHz, 4× slower
    than the BRAM Fmax. The GEMV/GEMM systems based on CCB and CoMeFa were evaluated
    on an Arria 10 GX900 with a BRAM Fmax of 730 MHz [\[11\]](#page-7-9). Though CoMeFa-based
    designs run slightly faster than the CCB-GEMV engine, they are still roughly 3×
    slower than the BRAM Fmax. Thus, CCB and CoMeFa-based GEMV/GEMM engine performance
    did not scale well at the system level.


    SPAR-2 [\[8\]](#page-7-22) utilized only 30% of the BRAMs while running 4× slower
    than BRAM Fmax on both platforms. Thus, its performance and scalability are even
    worse than CCB and CoMeFa-based systems. On the other hand, IMAGine has a system
    clock running at the BRAM Fmax while utilizing 100% device BRAM as PIMs. Outperforming
    all existing designs, IMAGine is the fastest PIM array-based GEMV engine implemented
    on any FPGA, running at a clock rate 2.65× – 3.2× faster than any existing design.
    This is an important proof of concept design that dispels earlier beliefs that
    PIM arrays and overlay accelerators cannot achieve BRAM Fmax clock frequencies
    at the system level [\[10\]](#page-7-8)–[\[13\]](#page-7-4).


    As observed in Table [V,](#page-5-1) RIMA and CCB/CoMeFa-based designs exhaust
    either the logic resources or the DSPs of the device even though their PIM blocks
    are implemented by customizing the BRAM tile itself. Even after being an overlay,
    IMAGine is achieving faster clock and better scalability using 0 DSPs and only
    one-third of the device logic resources due to its near-optimal architectural
    choices. Like SPAR-2, IMAGine does not use DSPs to implement the bit-serial PEs.
    With a custom-BRAM implementation of the PIM module, like PiCaSO-CB discussed
    in Section [IV-D,](#page-3-2) IMAGine would consume about 10% of device resources
    while being fully scalable and implementable even in resource-limited FPGAs.


    # *E. GEMV Execution Latency*


    Fig. [6](#page-6-0) plots the GEMV latency of PIM-array accelerators, with square-matrix
    dimensions on the x-axis and latency in log scale on the y-axis. The execution
    times in Fig. [6\(](#page-6-0)b) are computed by multiplying cycle latencies with
    the corresponding clock periods from Table [V](#page-5-1) system frequencies.


    ![](_page_6_Figure_0.jpeg)


    <span id="page-6-0"></span>Fig. 6. Cycle latency and execution time of GEMV operation
    on different PIM array-based FPGA accelerators


    We adopted the approach in [\[12\]](#page-7-10) to model the block-level cycle
    latencies of CCB, CoMeFa, BRAMAC, and SPAR-2 using their analytical models. IMAGine''s
    latency model was developed and validated by running a prototype on hardware.


    As observed in Fig. [6\(](#page-6-0)a), BRAMAC has the shortest cycle latency,
    due to their hybrid bit-serial & bit-parallel MAC2 algorithm. BRAMAC''s MAC latency
    grows linearly with operand bit-width, while it grows quadratically in the other
    bit-serial architectures. BRAMAC is designed specifically for low-precision (2,
    4, and 8-bit) quantized neural networks, rendering it unsuitable for general computing
    tasks like GEMV. BRAMAC did not report the system-level frequency which is why
    we could not plot its execution time.


    SPAR-2 has the longest latency across all precisions, due to its slow NEWS accumulation
    network, with latency increasing almost linearly with matrix dimension. CCB and
    CoMeFa-based GEMV engines have the shortest cycle latency among bit-serial architectures
    across all precisions. This is due to their fast reduction algorithm based on
    a popcountbased adder and pipelined adder tree. The cycle latency of IMAGine is
    significantly shorter compared to SPAR-2 but longer than CCB/CoMeFa-based implementations.
    However, IMAGine clocks at least 2× faster than any of the other GEMV engines.
    As a result, IMAGine outperforms all other GEMV engines in terms of overall execution
    time. This highlights the importance of the system clock speed over the cycle
    latency; despite the CCB/CoMeFa GEMV engines'' shorter cycle latency, their slower
    clock significantly degrades the execution time.


    Because IMAGine is utilizing only 30% of the logic resources in U55, the remaining
    resources can be used to further improve its performance. The IMAGine-slice4 curves
    in Fig. [6](#page-6-0) shows the latency of a variant of IMAGine with a 4-bit
    sliced accumulation network and a PE implementing Booth''s radix-4 multiplication
    (default is radix-2). This latency is estimated by adjusting the analytical model
    of IMAGine assuming no effect on the clock rate. In terms of cycle latency, it
    can run almost as fast as CCB/CoMeFa-based GEMV implementations, while significantly
    outperforming them in execution time.


    #### VI. CONCLUSIONS AND FUTURE WORK


    Processor In/Close to Memory (PIM) architectures have become popular frameworks
    replacing classic von Neumann architectures within domain-specific machine learning
    accelerators. This paper presented a study proposing the performance and scalability
    goals for PIM array-based accelerators on FPGAs. The design, implementation, and
    analysis of IMAGine was presented demonstrating how a PIM-array accelerator could
    achieve the BRAM Fmax as the system frequency. A scalability study was presented
    showing processing capacity scaling linearly with increasing BRAM density, even
    for devices with low LUT-to-BRAM ratios. An implementation with 64K PEs was run
    on Alveo U55, clocking faster than the Tensor Processing Unit (TPU v1-v2) and
    Alibaba Hanguang 800. This breaks the myth that FPGA overlays and fabrics must
    clock slower than ASIC designs.


    A comparative study with state-of-the-art PIM-array accelerators was presented
    showing IMAGine has 2.65× – 3.2× faster system frequency, and significantly outperforms
    them in execution time, establishing IMAGine as the fastest and most scalable
    PIM array-based GEMV engine reported to date.


    Our future work includes the completion of an MLIRbased compiler framework for
    hardware/software codesign and application-specific customization of IMAGine-like
    PIM array-based accelerators.


    #### REFERENCES


    - <span id="page-7-0"></span>[1] N. P. Jouppi, D. Hyun Yoon, M. Ashcraft, M. Gottscho,
    T. B. Jablin, G. Kurian, J. Laudon, S. Li, P. Ma, X. Ma, T. Norrie, N. Patil,
    S. Prasad, C. Young, Z. Zhou, and D. Patterson, "Ten lessons from three generations
    shaped google''s TPUv4i : Industrial product," in *2021 ACM/IEEE 48th Annual International
    Symposium on Computer Architecture (ISCA)*. IEEE, Jun. 2021, pp. 1–14.

    - <span id="page-7-1"></span>[2] J.-H. Kim, J. Lee, J. Lee, H.-J. Yoo, and J.-Y.
    Kim, "Z-PIM: An Energy-Efficient Sparsity Aware Processing-In-Memory Architecture
    with Fully-Variable Weight Precision," in *2020 IEEE Symposium on VLSI Circuits*.
    IEEE, Jun. 2020, pp. 1–2.

    - [3] B. Zhang, S. Yin, M. Kim, J. Saikia, S. Kwon, S. Myung, H. Kim, S. J. Kim,
    J.-S. Seo, and M. Seok, "PIMCA: A Programmable In-Memory Computing Accelerator
    for Energy-Efficient DNN Inference," *IEEE Journal of Solid-State Circuits*, vol.
    58, no. 5, pp. 1436–1449, May 2023.

    - [4] C.-F. Lee, C.-H. Lu, C.-E. Lee, H. Mori, H. Fujiwara, Y.-C. Shih, T.-L.
    Chou, Y.-D. Chih, and T.-Y. J. Chang, "A 12nm 121-TOPS/W 41.6-TOPS/mm2 All Digital
    Full Precision SRAM-based Computein-Memory with Configurable Bit-width For AI
    Edge Applications," in *2022 IEEE Symposium on VLSI Technology and Circuits (VLSI
    Technology and Circuits)*. IEEE, Jun. 2022, pp. 24–25.

    - [5] Y. Kwon, G. Kim, N. Kim, W. Shin, J. Won, H. Joo, H. Choi, B. An, G. Shin,
    D. Yun, J. Kim, C. Kim, I. Kim, J. Park, C. Park, Y. Song, B. Yang, H. Lee, S.
    Park, W. Lee, S. Lee, K. Kim, D. Kwon, C. Jeong, J. Kim, E. Lim, and J. Chun,
    "Memory-Centric Computing with SK Hynix''s Domain-Specific Memory," in *2023 IEEE
    Hot Chips 35 Symposium (HCS)*, 2023, pp. 1–26.

    - <span id="page-7-3"></span>[6] X. Wang, V. Goyal, J. Yu, V. Bertacco, A. Boutros,
    E. Nurvitadhi, C. Augustine, R. R. Iyer, and R. Das, "Compute-Capable Block RAMs
    for Efficient Deep Learning Acceleration on FPGAs," *2021 IEEE 29th Annual International
    Symposium on Field-Programmable Custom Computing Machines (FCCM)*, pp. 88–96,
    2021.

    - <span id="page-7-12"></span>[7] S. Basalama, A. Panahi, A.-T. Ishimwe, and D.
    Andrews, "SPAR-2: A SIMD Processor Array for Machine Learning in IoT Devices,"
    in *2020 3rd International Conference on Data Intelligence and Security (ICDIS)*.
    IEEE, 2020, pp. 141–147.

    - <span id="page-7-22"></span>[8] A. Panahi, S. Balsalama, A.-T. Ishimwe, J. M.
    Mbongue, and D. Andrews, "A Customizable Domain-Specific Memory-Centric FPGA Overlay
    for Machine Learning Applications," in *2021 31st International Conference on
    Field-Programmable Logic and Applications (FPL)*, Aug. 2021, pp. 24–27.

    - <span id="page-7-13"></span>[9] A. Panahi, "A memory-centric customizable domain-specific
    FPGA overlay for accelerating machine learning applications," Ph.D dissertation,
    University of Arkansas, 2022.

    - <span id="page-7-8"></span>[10] A. Arora, T. Anand, A. Borda, R. Sehgal, B.
    Hanindhito, J. Kulkarni, and L. K. John, "CoMeFa: Compute-in-Memory Blocks for
    FPGAs," in *2022 IEEE 30th Annual International Symposium on Field-Programmable
    Custom Computing Machines (FCCM)*, May 2022, pp. 1–9.

    - <span id="page-7-9"></span>[11] A. Arora, A. Bhamburkar, A. Borda, T. Anand,
    R. Sehgal, B. Hanindhito, P.-E. Gaillardon, J. Kulkarni, and L. K. John, "CoMeFa:
    Deploying Compute-in-Memory on FPGAs for Deep Learning Acceleration," *ACM Transactions
    on Reconfigurable Technology and Systems*, vol. 16, no. 3, pp. 1–34, Sep. 2023.

    - <span id="page-7-10"></span>[12] Y. Chen and M. S. Abdelfattah, "BRAMAC: Compute-in-BRAM
    Architectures for Multiply-Accumulate on FPGAs," in *2023 IEEE 31st Annual International
    Symposium on Field-Programmable Custom Computing Machines (FCCM)*. Marina Del
    Rey, CA, USA: IEEE, May 2023, pp. 52–62.

    - <span id="page-7-4"></span>[13] Y. Chen, J. Dotzel, and M. S. Abdelfattah, "M4BRAM:
    Mixed-Precision Matrix-Matrix Multiplication in FPGA Block RAMs," Nov. 2023,


    arXiv:2311.02758 [cs]. [Online]. Available: [http://arxiv.org/abs/2311.](http://arxiv.org/abs/2311.02758)
    [02758](http://arxiv.org/abs/2311.02758)


    - [14] M. A. Kabir, J. Hollis, A. Panahi, J. Bakos, M. Huang, and D. Andrews,
    "Making BRAMs Compute: Creating Scalable Computational Memory Fabric Overlays,"
    in *2023 IEEE 31st Annual International Symposium on Field-Programmable Custom
    Computing Machines (FCCM)*. IEEE, May 2023, pp. 224–224.

    - <span id="page-7-2"></span>[15] M. A. Kabir, E. Kabir, J. Hollis, E. Levy-Mackay,
    A. Panahi, J. Bakos, M. Huang, and D. Andrews, "FPGA Processor In Memory Architectures
    (PIMs): Overlay or Overhaul ?" in *2023 33rd International Conference on Field-Programmable
    Logic and Applications (FPL)*. Gothenburg, Sweden: IEEE, Sep. 2023, pp. 109–115.

    - <span id="page-7-5"></span>[16] M. A. Kabir, T. Kamucheka, N. Fredricks, J.
    Mandebi, J. Bakos, M. Huang, and D. Andrews, "IMAGine: An In-Memory Accelerated
    GEMV Engine Overlay." [Online]. Available: [https://github.com/](https://github.com/Arafat-Kabir/IMAGine)
    [Arafat-Kabir/IMAGine](https://github.com/Arafat-Kabir/IMAGine)

    - <span id="page-7-6"></span>[17] C. Eckert, X. Wang, J. Wang, A. Subramaniyan,
    R. Iyer, D. Sylvester, D. Blaauw, and R. Das, "Neural Cache: Bit-Serial in-Cache
    Acceleration of Deep Neural Networks," in *2018 ACM/IEEE 45Th annual international
    symposium on computer architecture (ISCA)*, 2018, pp. 383–396.

    - <span id="page-7-7"></span>[18] J. Fowers, K. Ovtcharov, M. Papamichael, T.
    Massengill, M. Liu, D. Lo, S. Alkalay, M. Haselman, L. Adams, M. Ghandi, S. Heil,
    P. Patel, A. Sapek, G. Weisz, L. Woods, S. Lanka, S. K. Reinhardt, A. M. Caulfield,
    E. S. Chung, and D. Burger, "A Configurable Cloud-Scale DNN Processor for Real-Time
    AI," in *2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture
    (ISCA)*, 2018, pp. 1–14.

    - <span id="page-7-11"></span>[19] U. Aydonat, S. O''Connell, D. Capalija, A.
    C. Ling, and G. R. Chiu, "An OpenCL™ Deep Learning Accelerator on Arria 10," in
    *Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable
    Gate Arrays*. Association for Computing Machinery, 2017, p. 55–64.

    - <span id="page-7-14"></span>[20] *Virtex-7 T and XT FPGAs Data Sheet: DC and
    AC Switching Characteristics*, AMD, 2021. [Online]. Available: [https://docs.xilinx.](https://docs.xilinx.com/v/u/en-US/ds183_Virtex_7_Data_Sheet)
    [com/v/u/en-US/ds183](https://docs.xilinx.com/v/u/en-US/ds183_Virtex_7_Data_Sheet)
    Virtex 7 Data Sheet

    - <span id="page-7-17"></span>[21] *Virtex UltraScale+ FPGA Data Sheet: DC and
    AC Switching Characteristics*, AMD, 2021. [Online]. Available: [https://docs.xilinx.](https://docs.xilinx.com/v/u/en-US/ds923-virtex-ultrascale-plus)
    [com/v/u/en-US/ds923-virtex-ultrascale-plus](https://docs.xilinx.com/v/u/en-US/ds923-virtex-ultrascale-plus)

    - <span id="page-7-15"></span>[22] *Intel® Stratix® 10 Device Datasheet*, Intel.
    [Online]. Available: [https://www.intel.com/content/www/us/en/docs/programmable/](https://www.intel.com/content/www/us/en/docs/programmable/683181/current/memory-block-specifications.html)
    [683181/current/memory-block-specifications.html](https://www.intel.com/content/www/us/en/docs/programmable/683181/current/memory-block-specifications.html)

    - <span id="page-7-16"></span>[23] M. A. Kabir, E. Kabir, J. Hollis, E. Levy-Mackay,
    A. Panahi, J. Bakos, M. Huang, and D. Andrews, "PiCaSO: A Scalable and Fast PIM
    Overlay." [Online]. Available:<https://github.com/Arafat-Kabir/PiCaSO>

    - <span id="page-7-18"></span>[24] *Alveo U55C Data Center Accelerator Card User
    Guide*, AMD. [Online]. Available:<https://docs.xilinx.com/r/en-US/ug1469-alveo-u55c>

    - <span id="page-7-19"></span>[25] *Vivado Design Suite User Guide: Design Analysis
    and Closure Techniques*, AMD. [Online]. Available: [https://docs.amd.com/r/en-US/](https://docs.amd.com/r/en-US/ug906-vivado-design-analysis/Using-Pblock-Based-Floorplanning)
    [ug906-vivado-design-analysis/Using-Pblock-Based-Floorplanning](https://docs.amd.com/r/en-US/ug906-vivado-design-analysis/Using-Pblock-Based-Floorplanning)

    - <span id="page-7-20"></span>[26] N. P. Jouppi, C. Young, N. Patil, D. Patterson,
    G. Agrawal, R. Bajwa, S. Bates, S. Bhatia, N. Boden, A. Borchers *et al.*, "In-datacenter
    performance analysis of a tensor processing unit," in *Proceedings of the 44th
    annual international symposium on computer architecture*, 2017, pp. 1–12.

    - <span id="page-7-21"></span>[27] Y. Jiao, L. Han, R. Jin, Y.-J. Su, C. Ho, L.
    Yin, Y. Li, L. Chen, Z. Chen, L. Liu, Z. He, Y. Yan, J. He, J. Mao, X. Zai, X.
    Wu, Y. Zhou, M. Gu, G. Zhu, R. Zhong, W. Lee, P. Chen, Y. Chen, W. Li, D. Xiao,
    Q. Yan, M. Zhuang, J. Chen, Y. Tian, Y. Lin, W. Wu, H. Li, and Z. Dou, "7.2 a
    12nm programmable convolution-efficient neural-processing-unit chip achieving
    825tops," in *2020 IEEE International Solid- State Circuits Conference - (ISSCC)*.
    IEEE, Feb. 2020, pp. 136–140.'
- title: "Salient Store: Enabling Smart Storage for Continuous Learning Edge\n  Servers"
  abstract: 'As continuous learning based video analytics continue to evolve, the
    role of

    efficient edge servers in efficiently managing vast and dynamic datasets is

    becoming increasingly crucial. Unlike their compute architecture, storage and

    archival system for these edge servers has often been under-emphasized. This is

    unfortunate as they contribute significantly to the data management and data

    movement, especially in a emerging complute landscape where date storage and

    data protection has become one of the key concerns. To mitigate this, we

    propose Salient Store that specifically focuses on the integration of

    Computational Storage Devices (CSDs) into edge servers to enhance data

    processing and management, particularly in continuous learning scenarios,

    prevalent in fields such as autonomous driving and urban mobility. Our

    research, gos beyond the compute domain, and identifies the gaps in current

    storage system designs. We proposes a framework that aligns more closely with

    the growing data demands. We present a detailed analysis of data movement

    challenges within the archival workflows and demonstrate how the strategic

    integration of CSDs can significantly optimize data compression, encryption, as

    well as other data management tasks, to improve overall system performance. By

    leveraging the parallel processing capabilities of FPGAs and the high internal

    bandwidth of SSDs, Salient Store reduces the communication latency and data

    volume by ~6.2x and ~6.1x, respectively. This paper provides a comprehensive

    overview of the potential of CSDs to revolutionize storage, making them not

    just data repositories but active participants in the computational process.'
  url: http://arxiv.org/abs/2410.05435v1
  keywords: ''
  document: '# <span id="page-0-0"></span>Salient Store: Enabling Smart Storage for
    Continuous Learning Edge Servers


    Cyan Subhra Mishra, Deeksha Chaudhary, Jack Sampson, Mahmut Taylan Knademir, Chita
    Das The Pennsylvania State University {cyan, dmc6955, jms1257, mtk2, cxd12}@psu.edu


    # Abstract


    As continuous learning based video analytics continue to evolve, the role of efficient
    edge servers in efficiently managing vast and dynamic datasets is becoming increasingly
    crucial. Unlike their compute architecture, storage and archival system for these
    edge servers has often been under-emphasized. This is unfortunate as they contribute
    significantly to the data management and data movement, especially in a emerging
    complute landscape where date storage and data protection has become one of the
    key concerns. To mitigate this, we propose Salient Store that specifically focuses
    on the integration of Computational Storage Devices (CSDs) into edge servers to
    enhance data processing and management, particularly in continuous learning scenarios,
    prevalent in fields such as autonomous driving and urban mobility. Our research,
    gos beyond the compute domain, and identifies the gaps in current storage system
    designs. We proposes a framework that aligns more closely with the growing data
    demands. We present a detailed analysis of data movement challenges within the
    archival workflows and demonstrate how the strategic integration of CSDs can significantly
    optimize data compression, encryption, as well as other data management tasks,
    to improve overall system performance. By leveraging the parallel processing capabilities
    of FPGAs and the high internal bandwidth of SSDs, Salient Store reduces the communication
    latency and data volume by ≈ 6.2× and ≈ 6.1×, respectively. This paper provides
    a comprehensive overview of the potential of CSDs to revolutionize storage, making
    them not just data repositories but active participants in the computational process.


    # 1 Introduction


    Video analytics, powered by deep neural networks (DNNs) has become the key component
    of multiple applications including but not limited to autonomous driving [\(Brown
    et al., 2023;](#page-17-0) [Fang](#page-19-0) [et al., 2023;](#page-19-0) [Huang
    et al., 2023\)](#page-19-1), urban mobility [\(Corporation;](#page-18-0) [Custom
    On-Device ML Models with](#page-18-1) [Learn2Compress\)](#page-18-1), surveillance
    and monitoring [\(Bozcan & Kayacan, 2020;](#page-17-1) [Dutta & Ekenna, 2019;](#page-18-2)
    [Pichierri et al., 2023\)](#page-21-0), video streaming and conferencing [\(Dasari
    et al., 2022b;](#page-18-3) [Cheng et al., 2024;](#page-18-4) [Sivaraman et al.,
    2024\)](#page-21-1), telemedicine [\(Wan et al., 2020\)](#page-22-0), and tourism
    [\(Zhu et al., 2024;](#page-23-0) [Pierdicca et al.,](#page-21-2) [2021;](#page-21-2)
    [Godovykh et al., 2022\)](#page-19-2). While some of these applications rely on
    collecting the video data and processing them offline, many need real-time analytics
    for the seamless integration, operation and effectiveness of the task at hand
    [\(Bramberger et al., 2004;](#page-17-2) [Apostolo et al., 2022;](#page-17-3)
    [Grulich &](#page-19-3) [Nawab, 2018\)](#page-19-3). Moreover, depending on the
    deployment, scenario and requirements, some of these applications also demand
    learning to keep up with the data drift [\(Bhardwaj et al., 2022;](#page-17-4)
    [Mishra](#page-20-0) [et al., 2024;](#page-20-0) [Kim et al., 2024;](#page-19-4)
    [Rebuffi et al., 2017\)](#page-21-3). However, regulations, resource limitations
    and privacy concerns often mandate these applications (both learning and inference)
    to be performed at the edge [\(Bhardwaj et al., 2022;](#page-17-4) [Mishra et
    al., 2024\)](#page-20-0). For example, many of the European cities restrict the
    traffic video data to be streamed to the cloud [\(www.dlapiperdataprotection.com;](#page-22-1)
    [Achieving Compliant](#page-17-5) [Data Residency and Security with Azure;](#page-17-5)
    [Bhardwaj et al., 2022\)](#page-17-4), which enforces performing video


    <span id="page-1-0"></span>![](_page_1_Figure_0.jpeg)


    Figure 1: Data flow pipeline of continuous learning edge servers with storage
    and data archival pipeline. The Shown storage pipeline is the preliminary focus
    of Salient Store .


    analytics and learning tasks related to urban mobility at the edge. This has lead
    to a significant development in the direction of enabling video analytics and
    learning with edge servers.


    Although efficient algorithms, compute orchestration and hardware have addressed
    the analytics part, the scaling of such a system becomes a problem primarily due
    to high energy consumption. Recent works try to solve this problem by augmenting
    these continuous learning edge servers with application-specific hardware targeted
    for intermittent computing which could run using solar power. However, all these
    works focus on the analytics part while overlooking one critical aspect: *what
    happens to all those video data after the analytics?*


    *Data Archival:* The answer is straightforward, especially for mission-critical
    public records like urban mobility and surveillance data: these need to be archived
    in a local storage to avoid undermining the benefits of edge computation, i.e.
    minimizing communication and preserving privacy. However, managing such data requires
    substantial storage infrastructure. For instance, storing a full day''s worth
    of 1080p video at 60fps requires (1920 × 1080 × 3 pixels × 4 bytes per pixel ×
    60 frames per second ×3600×24)117.32 TiB of raw video data, which compresses to
    approximately 60 GiB to 400 GiB of encoded data per day. Including redundancy,
    this requires an additional 33% to 100% more storage capacity. Furthermore, given
    the plug-and-play nature of storage media like HDDs and SSDs, securing this data
    becomes even more critical.


    The typical *archival process* involves three key phases: compression, encryption,
    and redundancy. Fig. [1](#page-1-0) illustrates the data flow in an edge server,
    where video data are first encoded (using H264 or similar codecs), then encrypted
    (with RSA or similar standards), and finally stored across a distributed set of
    disks to ensure redundancy (e.g., RAID 5). These processes create a complex data-flow
    pipeline, differentiating two streams of video data: one for real-time inference
    and training, and another for archival. This dual-stream processing consumes considerable
    system resources such as CPU, memory, and energy, further complicating the management
    for intermittently powered systems like Us. ás [\(Mishra et al., 2024\)](#page-20-0)
    where data integrity and security must be maintained during power disruptions
    [1](#page-0-0) . Table [1](#page-2-0) provides an estimation of resource utilization
    on commercial systems underscoring the fact that compressing, encrypting and reliably
    storing the data, especially for (intermittent) edge servers is a bigger challenge
    in contrast to classical cloud storage servers.


    *The Challenges:* In edge computing architectures, the lack of data reuse between
    the analytics and video data archival poses significant challenges. Classically,
    video data is streamed simultaneously to compute and storage systems for various
    processing tasks, such as inference, exemplar selection, and storage, thereby
    increasing I/O bandwidth and system processing demands. This processing complexity
    necessitates substantial compute and memory resources, escalating power consumption


    <sup>1</sup>Management and retrieval of data typically utilize a vector database
    like file-system, although this is beyond the scope of this discussion.


    <span id="page-2-0"></span>


    | Data  | Task        | Algorithm | % CPU Utilization | % DRAM Utilization |         |

    |-------|-------------|-----------|-------------------|--------------------|---------|

    |       |             |           | 16 Core Xeon      | Peak               | Average
    |

    | All   | Encryptions | RSA512    | 2.18              | 14.56              | 5.85    |

    | All   | Decryptions | RSA512    | 3.45              | 17.2               | 6.12    |

    | 3D PC | Compression | OctTree   | 26.78             | 78.2               | 32.54   |

    |       | Inflation   | OctTree   | 29.24             | 81.56              | 36.18   |

    | Video | Compression | ZStd      | 24.7              | 62.54              | 24.5    |

    |       | Inflation   | ZStd      | 22.6              | 79.18              | 29.43   |

    |       | Compression | H264      | 12.85             | 52.46              | 21.4    |

    |       | Inflation   | H264      | 14.2              | 69.46              | 26.18   |

    | All   | (un)RAID    | Unraid    | 11.25             | 29.4               | 19.24   |


    Table 1: Resource utilization while running different algorithms under classical
    data archival pipeline for multiple data modalities in an AWS h1.4xlarge storage-optimized
    instance.


    (e.g., systems with CPUs having a thermal design power of 145W and 64GB of DRAM
    as noted in Table [1\)](#page-2-0) and requiring larger form factors, which exceed
    the capabilities of intermittent systems and challenge sustainability goals. Therefore,
    there is an urgent need to *minimize compute and power requirements* in archival
    processes. Moreover, the divergence of analytics and archival pipelines from the
    outset suggests that optimizing both hardware, software and data-flow used in
    inference, learning and archival could significantly enhance throughput and energy
    efficiency. This can be achieved by using modern *neural compression algorithms*
    instead of the classical encoding algorithm. However, the neural compression algorithm
    needs to be *compute efficient* (reusing maximum analytics pipeline), have *high
    compression ratio* (need to compete with H264) and *feature rich* (could be decompressed
    and retrieved with a reasonable loss). Furthermore, it needs to take advantage
    of the *data similarity between frames* to further minimize the storage footprint,
    and thereby reducing the form factor and need of frequent disk swapping/ maintenance.


    The second challenge comes from privacy and security of sensitive data. Considering
    the cheap commodity use storage devices are often plug and play, they are often
    vulnerable for data leak, especially if they are deployed in public, like urban
    mobility setting. Unlike secure data centers, these federated, distributed and
    public deployment could be susceptible to direct physical attacks for data breach.
    Although modern encryption algorithms like RSA are secure, there is still a threat
    of *store now decrypt later*[2](#page-0-0) kind of attack [\(National Cybersecurity
    Center of Excellence \(NCCoE\), 2023\)](#page-20-1). To mitigate this, *quantum
    safe encryption algorithms needs to be used without hindering the throughput*.
    Furthermore, the design *needs to be programmable* to ensure encryption keys to
    be changed regularly for additional security.


    *Solution Space and Our Work:* This paper proposes Salient Store , a novel storage
    solution designed for continuous learning edge servers by incorporating a hardware-software
    co-design framework that allows for efficient data archival and storage. Salient
    Store utilizes the state-ofthe-art neural compression which partially uses the
    inference/ exemplar selection pipeline along with layered neural codecs to compress
    the video data. It also uses the motion vectors as a latent space to effectively
    use the inter-frame similarity, thereby further increasing the compression ratio.
    Salient Store also provides a hardware accelerated lattice-based quantum safe
    encryption mechanism. To tightly integrate these solutions to the storage space,
    while providing data security, Salient Store uses computational storage devices
    (CSDs) [\(AMD, b\)](#page-17-6) which reduce the energy consumption while keeping
    the compute pipeline unaltered. Our main contributions include:


    • We propose design of a hybrid storage pipeline equipped with computational storage
    drives (CSDs) where the different drives could communicate with each other in
    a peer-to-peer fashion. These CSDs synergistically orchestrate the archival related
    tasks between the storage controller CPU and the computational storage FPGAs.
    The hybrid storage system is capable of taking the computed


    <sup>2</sup>Given a powerful enough computer like quantum computers, RSA encrypted
    data can be decrypted, and therefore National Institute of Standards and Technology
    (NIST) called for proposals to develop post quantum cryptography algorithms [\(National
    Institute of Standards and Technology \(NIST\), 2024\)](#page-20-2), and defined
    a standard for the same. One of the successful submissions – which was later defined
    as a standard – uses lattice-based encryption algorithm [\(Micciancio & Regev,
    2009\)](#page-20-3) and will be the focus of our work.


    frame features and the motion vectors from the compute hardware to perform a novel
    layered neural compression.


    - We discuss the storage data-flow, the compute orchestration and mapping in the
    proposed system. This includes a hardware software co-design for compute-intensive
    applications along with mapping different functions to different hardware in the
    data pipeline. Furthermore, we add failure management support for the intermittent
    edge servers.

    - We go beyond the compute and look into future-proofing the storage server by
    equipping it with quantum safe lattice-based encryption technique. We maximize
    the hardware utilization by reusing compute kernels from the compression pipeline.
    We detail the design of the hardware accelerated encryption and maximize the resource
    reuse between the exemplar selection and encryption.

    - Finally we perform an in-depth exploration of this integration, supported by
    real-world data from domains such as autonomous driving and urban mobility, to
    illustrate its effectiveness in continuous learning scenarios. The proposed design
    provides ≈ 2.2× latency and ≈ 5.6× data movement benefits compared to the state-of-the-art,
    on a single storage and ≈ 4.8× latency benefit in a multi-node system.


    # 2 Background and Motivation


    #### 2.1 Storage for Continuous Learning Edge Servers


    Recent developments in continuous learning for video analytics [\(Bhardwaj et
    al., 2022;](#page-17-4) [Mishra](#page-20-0) [et al., 2024;](#page-20-0) [Kim
    et al., 2024\)](#page-19-4) has significantly boosted the capabilities and accuracy
    of learning systems. The major focus of these works have been building compute
    platforms with efficient scheduling [\(Bhardwaj et al., 2022;](#page-17-4) [Mishra
    et al., 2024\)](#page-20-0), and reconfigurable hardware design [\(Kim](#page-19-4)
    [et al., 2024;](#page-19-4) [Mishra et al., 2024\)](#page-20-0). This solves majority
    of the bottlenecks in a performance-driven classical cloud server platform. However,
    video analytics for many applications [\(Corporation;](#page-18-0) [Custom](#page-18-1)
    [On-Device ML Models with Learn2Compress;](#page-18-1) [Wright";](#page-22-2)
    ["premioinc"\)](#page-21-4) are moving towards the edge. Therefore, managing and
    storing the hefty volume of video data brings more challenge due to the energy,
    compute and form-factor limitations. Modern and upcoming applications like urban
    mobility and autonomous driving are predicted to be generating hundreds of exabytes
    of data [\(Urban Traffic](#page-22-3) [Dataset;](#page-22-3) [Corporation;](#page-18-0)
    [Wright";](#page-22-2) ["premioinc"\)](#page-21-4) per year while increasingly
    being deployed at the edge calling for a robust, secure, and efficient infrastructure
    for storing data at the edge while needing occasional human intervention for maintenance.


    *Using State-of-the-Art Video Data Storage? Maybe Not:* While storing video data
    as files works for small systems, in very large-scale systems they are typically
    stored using vector databases [\(Shen](#page-21-5) [et al., 2005;](#page-21-5)
    [Pan et al., 2024\)](#page-20-4). Vector databases typically extract features
    from the video data to form index and those indices are then sorted using various
    metrics like neighborhood, maximum similarity, etc. [\(Fonseca & Jorge, 2003;](#page-19-5)
    [Cao et al., 2013;](#page-17-7) [Tian et al., 2023;](#page-21-6) [Douze et al.,
    2024\)](#page-18-5) for faster retrieval. The vector index are used to point to
    the meta-data of the video file and then the actual video data is retrieved from
    the storage [\(Shen et al., 2005\)](#page-21-5). This approach helps context-based
    search like looking for particular objects, events, or attributes [\(Douze et
    al., 2024\)](#page-18-5). However, it is obvious that this approach, albeit good
    for streaming and retrieval, are not entirely space-efficient, and at times can
    increase the data volume by many folds [\(Douze et al., 2024\)](#page-18-5). In
    an edge server where we are only worried about storage and not retrieval[3](#page-0-0)
    , the vector database approach is *not* effective. Rather, storing the data in
    a compressed and encrypted format (with redundancies) is more efficient and therefore
    is the focus of our work.


    *Why Not the Usual Process?* Now that we know Classical approach of video data
    storage involves encoding and encrypting the video data before storing them in
    a redundant storage array [\(Huang](#page-19-6) [& Xu, 2014;](#page-19-6) [Fan
    et al., 2022;](#page-19-7) [Korkiakangas, 2014;](#page-20-5) [Yue et al., 2016\)](#page-22-4)
    which consumes significant resources (refer Table [1\)](#page-2-0). Even though
    there have been significant research in accelerating both compression [\(Collet
    & Kucherawy, 2018;](#page-18-6) [Chen et al., 2021\)](#page-18-7) and encryption
    [\(Milanov, 2009;](#page-20-6) [Rawat](#page-21-7) [et al., 2019;](#page-21-7)
    [Yang et al., 2015\)](#page-22-5), operating on large-scale video data often demands
    more resource than what edge servers could afford [\(Mishra et al., 2024\)](#page-20-0).
    Co-locating this compute along with the inference and training would definitely
    hinder the critical path.


    <sup>3</sup>We assume the data to be eventually available in the data repository,
    where they can be properly stored for efficient lookup. This can be done by periodically
    transporting the data by swapping out storage bays. Our goal is to maximize storage
    at edge so that the frequency of maintenance decreases.


    The main reason these algorithms consume significant resources is because of the
    amount of data they handle. Every single 1920 × 1080 raw frame prior to encoding
    carries ≈ 23MiB of data which, @60fps, will require processing ≈ 1.4GiB of data
    per second per imaging source. This pipeline assumes the use of classical data
    encoding algorithms like H264[4](#page-0-0) which requires all the frames to encode
    a video stream, albeit it only saves the essential information. Therefore, it
    does not use any of the computations that are used in the inference and learning
    pipeline. Then, the question is: is there a way we can reuse the computations
    used for the inference to help us in encoding the data? The answer is *neural
    codecs* [\(Ma et al., 2019;](#page-20-7) [Chen et al., 2017\)](#page-18-8).


    *Neural Codecs – DNNs for Compression:* Neural codecs represent a paradigm shift
    in video compression technology, leveraging the capabilities of deep learning
    to optimize both encoding and decoding processes. Unlike traditional codecs that
    rely on predefined algorithms to compress video data, neural codecs utilize an
    end-to-end trainable system based on neural networks. These networks are trained
    on extensive video datasets, allowing them to dynamically adapt compression strategies
    based on the content''s complexity and prevailing network conditions. The architectural
    backbone of neural codecs typically comprises an autoencoder, where the encoder
    compresses the video into a compact, lower-dimensional representation, and the
    decoder reconstructs it back into video format. These blocks can be stacked over
    each other to form layered codecs (like SHVC and SVC). This process benefits significantly
    from residual learning techniques, where each successive layer in the network
    aims to correct errors from the previous layers, thereby enhancing the reconstructed
    video quality incrementally with each additional decoding layer.


    Moreover, neural codecs excel in adaptability, offering robust performance across
    variable bandwidth and computational conditions, making them particularly suited
    for real-time streaming environments. These codecs can operate on generic computational
    hardware, such as GPUs and FPGAs, without the need for specialized video processing
    units, thus broadening their applicability across different device platforms.
    This adaptability also extends to content delivery dynamics, where neural codecs
    can adjust the streaming quality in real-time, responding adeptly to fluctuations
    in network throughput and variations in device capabilities. Such capabilities
    not only enhance user experience by minimizing buffering and maximizing video
    quality but also optimize bandwidth usage, presenting a cost-effective solution
    for content providers. These technical advancements position neural codecs as
    potential game-changers in the video streaming industry [\(NVIDIA Corporation,
    2024\)](#page-20-8), promising significant improvements in efficiency, scalability,
    and flexibility in various streaming scenarios.


    One major issue with neural codecs are their lack of utilization of inter-frame
    similarity. Classically, neural codecs compress each frame by treating them like
    images. However, video data often comes with a large amount of inter-frame similarity
    [\(Ying et al., 2022b;](#page-22-6) [Zhang et al., 2017;](#page-22-7) [Zhao et
    al., 2020,](#page-22-8) [2021;](#page-23-1) [Ying et al., 2022a\)](#page-22-9).
    This similarity could be exploited, like in classical encoding algorithms, to
    further increase the compression ratio while improving the feature quality. Furthermore,
    neural codecs offer us the flexibility to approximate the computation (using quantization)
    to further enhance efficiency. Since they also use the computation blocks of the
    standard neural network, they can be jointly trained along with the classifier
    network to perform as the feature extraction and encoding backbone, thereby maximizing
    the utilization of the inference pipeline, and reducing the computation needed
    for compression. This maximizes data and resource reuse, and the pipeline for
    inference and compression do not diverge from the get going.


    However, even with data reuse, performing compression using neural codecs would
    require extra compute resources, energy and latency, hindering the critical path,
    i.e., inference. However, *this issue could be alleviated by not using the compute
    resource in the critical path and preferably moving the compute to the storage
    where the data will eventually be stored*. This is where the modern and upcoming
    computational storage devices (CSDs) come to rescue: bringing computation closer
    to data while providing with maximum energy efficiency and programmability [\(Newsroom;](#page-20-9)
    [AMD, b\)](#page-17-6). However, storage systems are not typically built to cater
    towards the ML applications, and now that compression becomes a ML application
    with the use of stacked neural codecs, building the right storage stack along
    with computational storage devices becomes an important problem.


    *Evolution of Computational Storage:* The advent of CSDs represents a paradigm
    shift, bringing computation closer to storage. These devices, by integrating CPUs
    or FPGAs into the storage


    <sup>4</sup>We do *not* consider H265 here as currently in commercial systems
    H264 is the standard and typically enjoys hardware support. Moreover, H265 is
    an extension of H264 with additional features like coding tree units and intra-prediction
    directions which demand significantly more computation.


    medium, facilitate computation at the storage level. Initial applications of CSDs
    were confined to tasks like encryption/decryption, RAID, and compression. However,
    there has been a significant push towards enabling more complex workloads, including
    query processing on CSDs. Efforts to adapt CSDs for machine learning applications,
    albeit limited in scope to classical learning and data management, mark a crucial
    step forward. Yet, the expansion of these technologies to encompass large-scale
    storage stacks and the integration of CSDs into conventional storage systems,
    particularly for ML applications, remains a significant challenge and an open
    area of research.


    *Bridging the Gap in Storage System Design:* There is a discernible gap in the
    design and conceptualization of storage drives, systems and servers, especially
    in the context of ML applications. Historically, researchers have investigated
    these components individually, often focusing on highperformance computing, scientific
    computing, and database applications. However, the specific demands of ML applications
    on storage systems have largely been overlooked. storage stack architects often
    abstract the computational processes, neglecting considerations such as data movement
    cost, compute cost, and power requirements. Conversely, architects of storage
    drives, including CSDs, tend to overlook the broader application requirements,
    such as data prioritization, potential offloading of computational tasks to storage,
    and application-specific data compression and encryption strategies. Our research
    aims to *bridge* this gap, focusing specifically on the exigencies of continuous
    learning applications.


    #### 2.2 The Problem: Understanding the Data Flow


    Challenges in Data Movement: In both consumer applications and high-performance
    computing (HPC) programs, the process of data collection followed by analytics
    is a critical operation [\(Cao et al.,](#page-18-9) [2020;](#page-18-9) [Mailthody
    et al., 2019;](#page-20-10) [Chapman et al., 2019;](#page-18-10) [Li et al., 2023\)](#page-20-11).
    The efficiency of data movement significantly influences the performance and energy
    consumption of large-scale systems [\(Park et al.\)](#page-21-8). As earlier discussions
    highlighted, applications on edge servers need to store generated data on storage
    stacks. These data are can then be moved to a central facility where they can
    be further treated for efficient retrieval ans on demand streaming.


    Urban Mobility – A Case Study in Continuous Learning: To thoroughly understand
    this data flow, let us examine "urban mobility", a prevalent continuous learning
    task [\(Bhardwaj et al., 2022\)](#page-17-4). This task dynamically adapts to
    changing traffic patterns and environmental conditions. In urban mobility, high-volume
    data streams from multiple sources, such as high-resolution traffic cameras, are
    first compressed then analyzed for exemplar selection. This process involves "representation
    learning" [\(Rebuffi et al., 2017\)](#page-21-3), where data is transformed into
    "feature vectors" using deep neural networks, followed by unsupervised learning
    techniques like k-means clustering. The goal is to identify unique or new classes
    of data for training while archiving known classes. In exemplar selection, the
    entire dataset is analyzed to detect classes with unique features, i.e., the images
    that are much different from the training data distribution or new classes that
    were not included in the training data. This is typically achieved through representation
    learning [\(Rebuffi et al., 2017\)](#page-21-3) where the data is first converted
    into a feature vectors using the convolution layers of a large DNN model (or multiples
    of them), and then performing an unsupervised learning based classification, e.g.,
    k-means++ [\(Arthur](#page-17-8) [& Vassilvitskii, 2007;](#page-17-8) [Bahmani
    et al., 2012\)](#page-17-9), to cluster the data. It is noteworthy that, the process
    of neural compression as well as inference/representation learning use the feature
    extraction method. In this work, our goal is to use this to our advantage and
    maximize the compute and data reuse.


    *The critical insight to be gleaned here is reusing data and compute pipeline
    prior to its transfer to storage could markedly diminish the costs associated
    with data movement. This approach would notably minimize the consumption of bandwidth
    and latency and curtail the compute requirements, thereby ensuring that the compute
    server''s resources are judiciously employed only for indispensable computations.
    This strategic compute-reuse could help in optimizing the efficiency and efficacy
    of computational operations, especially in large scale data intensive and data
    driven applications.*


    # 2.3 Why Not More Compute at Storage Stacks?


    Integrating additional compute capabilities within storage stacks to offload certain
    computational tasks may seem an intuitive solution to the problem at hand. Theoretically,
    storage stacks could handle operations like unRAID and decryption, and extend
    their functionality to data inflation and feature extraction. This would ostensibly
    allow for the selective transfer of only pertinent exemplar data to compute servers,
    thereby optimizing data movement costs. However, this approach presents several
    challenges. Current storage stacks are outfitted with robust CPUs and memory systems,
    which are heavily tasked with state-of-the-art storage management algorithms,
    as evidenced by the resource utilization outlined in TABLE [1.](#page-2-0) These
    algorithms already consume substantial compute and memory resources, often to
    the extent of fully occupying the storage controller system, with a portion of
    resources being allocated for essential system stack operations. Furthermore,
    incorporating more advanced compute hardware, such as FPGAs, GPGPUs and other
    accelerators, would lead to underutilized I/O slots and memory, consequently escalating
    the cost of storage systems. Additionally, storage solutions that are optimized
    for I/O throughput tend to lose their specialized efficiency when repurposed for
    general compute tasks [\(Haynes et al., 2021\)](#page-19-8). While the previous
    research has suggested the idea of integrating analytics into storage systems,
    these discussions usually revolve around system architecture, scheduling, and
    data management, without delving into the requisite compute capabilities [\(Haynes
    et al., 2021;](#page-19-8) [Daum et al., 2021;](#page-18-11) [Tsai et al., 2020;](#page-22-10)
    [Xu et al., 2019\)](#page-22-11).


    #### 2.4 More Compute in Storage? Why Not?


    Exploring the realm of computational storage drives presents a tantalizing avenue
    for on-disk computing capabilities. Storage controllers, typically constrained
    by I/O bandwidth, are now being complemented by the vast internal bandwidth of
    solid-state drives (SSDs), making them prime candidates for near-data processing.
    Recent advances in both commercial [\(Newsroom;](#page-20-9) [AMD, b;](#page-17-6)
    [Mesnier,](#page-20-12) [2022;](#page-20-12) [ScaleFlux, a,](#page-21-9)[b;](#page-21-10)
    [Eideticom & Laboratory;](#page-19-9) [Laboratory\)](#page-20-13) and academic
    sectors [\(Torabzadehkashi](#page-22-12) [et al., 2019b;](#page-22-12) [Barbalace
    & Do, 2021;](#page-17-10) [Lukken & Trivedi, 2021;](#page-20-14) [Torabzadehkashi
    et al., 2019a;](#page-22-13) [Sala](#page-21-11)[mat et al., 2021,](#page-21-11)
    [2022;](#page-21-12) [Do et al., 2013\)](#page-18-12) advocate the use of computational
    storage drives (CSDs) across databases, high-performance computing (HPC), and
    analytics. AMD and Xilinx have introduced specialized tools and libraries designed
    to harness CSDs [\(AMD, a;](#page-17-11) [AMD & Xilinx\)](#page-17-12), enabling
    peer-to-peer PCIe transactions that bypass the CPU [\(AMD & Xilinx\)](#page-17-12).
    The emergence of Compute Express Link (CXL) technology [\(Sharma, 2022a](#page-21-13)[,b;](#page-21-14)
    [Jung, 2022\)](#page-19-10) further amplifies the potential of disaggregated storage
    and memory systems.


    Decoupling compute tasks needed for storing the data from the host CPU and embedding
    them directly within storage devices, particularly through CSDs, has demonstrated
    significant performance and energy benefits. Tasks traditionally performed at
    the storage controller level are now being offloaded to CSDs, often accelerated
    using FPGA primitives [\(Kim et al., 2021;](#page-19-11) [AMD & Xilinx;](#page-17-12)
    [AMD, a\)](#page-17-11). Moreover, CSDs hold the potential to undertake critical
    machine learning tasks like feature extraction and clustering, streamlining tasks
    like neural compression. The unique combination of FPGAs'' parallel processing
    prowess, the substantial internal bandwidth of SSDs, and their blockaccessible
    nature position CSDs as "ideal components" for evolving smart storage solutions
    tailored for machine learning.


    To cater towards this, in this work, we propose Salient Store – a mini computational
    storage server (we call it "edge storage server") stack for managing the data
    archival in edge servers. Salient Store provides adaptive data compression using
    neural codecs and further enhances the data security by providing an accelerated
    quantum safe data encryption policy, protecting these vulnerable edge storage
    servers against the store now decrypt later [\(National Cybersecurity Center of
    Excellence](#page-20-1) [\(NCCoE\), 2023\)](#page-20-1) attacks.


    # 3 Data Compression using Neural Codec


    In this section we go over the overall design and design choices for the neural
    codec design of Salient Store . We discuss the edge storage architecture followed
    by the choice of neural codec and their design.


    #### 3.1 Salient Store Storage Architecture:


    Salient Store edge storage architecture is crafted to optimize data-flow and computational
    efficiency in continuous learning edge servers. Salient Store employs a "hybrid
    model", integrating Computational Storage Drives (CSDs) with Field-Programmable
    Gate Arrays (FPGAs) [\(AMD, b\)](#page-17-6) and classical storage drives. This
    design, depicted in Fig. [2,](#page-7-0) gives the programmable computational
    capabilities of CSDs along with the cost-effectiveness and durability of the classical
    HDDs. It leverages the PCIe interface for efficient "peer-to-peer communication",
    substantially reducing communication latency and energy requirements. This setup
    alleviates the host CPU from handling frequent data movement interruptions. A
    storage platform entirely designed with CSDs is not pragmatic at the current time
    because of their exorbitant cost and power consumption [\(AMD, b;](#page-17-6)
    [Cao et al., 2020\)](#page-18-9) . However, a combination of CSDs with classical
    storage medium provides the most optimal solution and hence motivates our design.


    <span id="page-7-0"></span>![](_page_7_Figure_1.jpeg)


    Figure 2: High-level design of the Salient Store edge server - it consists of
    the accelerated video analytics compute along with computational storage and classical
    storage drives.


    *Data-flow Reorganization in* Salient Store : At the core of Salient Store ''s
    design is the "data-aware" reorganization of compute processes. Unlike conventional
    storage servers, Salient Store discerns between the "data path" and the "resource
    path" for system I/O calls, translating these requests into CSD-specific functions.
    These functions leverage FPGA kernels for efficient data processing.


    As discussed earlier,Salient Store edge storage implements a video archival by
    using neural compression followed by a quantum safe encryption (refer Fig. [1\)](#page-1-0).
    To maximize the compute reuse between the compute pipeline and the archival pipeline,
    Salient Store uses apart of the neural network of the inference engine to extract
    features, and then further performs the encoding using the FPGA in the CSD.


    Neural codecs present a new paradigm video compression technology, leveraging
    deep learning to surpass traditional codec efficiency [\(Ma et al., 2019;](#page-20-7)
    [Chen et al., 2017\)](#page-18-8). Traditional neural codecs, while innovative,
    typically encode and decode video streams in a monolithic fashion, which often
    results in suboptimal utilization of computational resources and inflexibility
    [\(Ma et al., 2019\)](#page-20-7). This inherent inefficiency stems from their
    design which does not allow incremental improvements in video quality and often
    leads to either over-utilization or under-utilization of bandwidth. To address
    these shortcomings, the advent of layered neural codecs marks a significant advancement
    [\(Dasari](#page-18-13) [et al., 2022a\)](#page-18-13). Layered neural codecs,
    by design, encode video into multiple, distinct layers of data, each enhancing
    the video quality incrementally. This allows for dynamic adaptation to network
    fluctuations and client-side computational capabilities, thereby optimizing the
    streaming experience. The capability to adjust the quality dynamically, by decoding
    additional layers as resources permit, ensures efficient bandwidth usage and enhances
    the overall Quality of Experience (QoE), making layered neural codecs a compelling
    choice for modern video streaming solutions.


    In an effort to optimize video compression beyond traditional and current neural
    codec capabilities, we introduce an enhanced layered neural codec that utilizes
    both intra-frame and inter-frame redundancies. Our approach extends the layered
    neural codecs by incorporating motion vectors as a latent space, akin to the macroblock
    techniques used in H.264, to maximize inter-frame compression efficiency. <span
    id="page-8-0"></span>Algorithm 1 Neural Encoding and Compression using the video
    data inference pipeline. 1: Input: Video frames sequence F = {f1, f2, . . . ,
    fn} 2: Output: Compressed frames C = {c1, c2, . . . , cn} 3: Initialize MobileNet
    Model M 4: Initialize Autoencoder Decoder D 5: Initialize Motion Vector Extractor
    V 6: procedure EXTRACTFEATURES(frame) 7: features ← M(frame) ▷ Extract features
    using MobileNet 8: return features 9: end procedure 10: procedure COMPRESSFEATURES(features)
    11: compressed ← D(features) ▷ Compress using autoencoder 12: return compressed
    13: end procedure 14: procedure CALCULATEMOTIONVECTORS(framecurrent, frameprevious)
    15: motion\_vectors ← V (framecurrent, frameprevious) 16: return motion\_vectors
    17: end procedure 18: procedure STACKCOMPRESSION(current\_compressed, motion\_vectors)
    19: return some compression algorithm using current\_compressed and motion\_vectors
    20: end procedure 21: for i ← 1 to n do 22: features<sup>i</sup> ← EXTRACTFEATURES(fi)
    23: c<sup>i</sup> ← COMPRESSFEATURES(featuresi) 24: if i > 1 then 25: m<sup>i</sup>
    ← CALCULATEMOTIONVECTORS(f<sup>i</sup> , fi−1) 26: c<sup>i</sup> ← STACKCOMPRESSION(c<sup>i</sup>
    , mi) 27: end if 28: C[i] ← c<sup>i</sup> 29: end for


    By exploiting the temporal correlations between consecutive frames, our codec
    can significantly reduce the required bitrate while maintaining high video quality.


    The primary innovation lies in leveraging motion vectors to identify and encode
    only the changes between frames, rather than re-encoding entire frames. This technique,
    combined with the use of anchor frames—similar to keyframes in traditional codecs—allows
    the codec to understand and predict frame sequences more effectively, reducing
    redundancy and enhancing compression. Let F<sup>t</sup> represent the frame at
    time t, and Ft−<sup>1</sup> be the anchor frame. The motion vector field M<sup>t</sup>
    between F<sup>t</sup> and Ft−<sup>1</sup> is computed to capture the displacement
    of pixels. Each frame F<sup>t</sup> is divided into blocks Bt,i, where i indexes
    the block within the frame. The residual frame R<sup>t</sup> for each frame is
    calculated as:


    $$R\_t = F\_t - \text{predict}(F\_{t-1}, M\_t)$$


    where predict(·) is a function that reconstructs F<sup>t</sup> from Ft−<sup>1</sup>
    using the motion vector field Mt. This prediction involves translating the blocks
    of Ft−<sup>1</sup> according to M<sup>t</sup> and serves as the predicted frame.
    The residual frame Rt, which contains only the differences not captured by the
    motion prediction, is then encoded using the layered neural network. Each layer
    of the neural network encodes progressively finer details of Rt. If Lk(Rt) represents
    the k-th layer''s encoding of the residual frame, the overall encoding of the
    frame can be expressed as:


    $$E\_t = \sum\_{k=1}^{K} L\_k(R\_t)$$


    where K is the total number of layers, and each L<sup>k</sup> encodes different
    levels of detail or different regions of the frame, based on the motion information
    and the prediction error. Algorithm [1](#page-8-0) shows the details of the layered
    neural codec.


    We implement this neural codec using the FPGA in the CSDs. FPGAs are ideal for
    this application due to their parallel processing capabilities and the ability
    to handle multiple data streams concurrently.


    |     | Algorithm 2 Training Auto-Encoder with Motion Vectors and Stacked Compression
    while freezing |

    |-----|----------------------------------------------------------------------------------------------|

    |     | the inference model.                                                                         |

    |     | 1: Input: Set of training video sequences V                                                  |

    |     | 2: Initialize: MobileNet M<br>▷ Weights frozen                                               |

    |     | 3: Initialize: Autoencoder A<br>▷ Trainable                                                  |

    |     | 4: Initialize: Motion Vector Extractor V                                                     |

    |     | 5: procedure EXTRACTMOTIONVECTORS(framecurrent, frameprevious)                               |

    | 6:  | motion_vectors ← V (framecurrent, frameprevious)                                             |

    | 7:  | return motion_vectors                                                                        |

    |     | 8: end procedure                                                                             |

    |     | 9: procedure FORWARDPASS(video)                                                              |

    | 10: | previous_features ← null                                                                     |

    | 11: | previous_compressed ← null                                                                   |

    | 12: | for each frame frame in video do                                                             |

    | 13: | features ← M(frame)<br>▷ Extract features using frozen MobileNet                             |

    | 14: | compressed ← A.encode(features)<br>▷ Compress features                                       |

    | 15: | if previous_compressed ̸= null then                                                          |

    | 16: | motion_vectors ← EXTRACTMOTIONVECTORS(frame, previous_frame)                                 |

    | 17: | stacked_input ← concatenate                                                                  |

    |     | (compressed, previous_compressed, motion_vectors)                                            |

    | 18: | compressed ← A.reencode(stacked_input)<br>▷ Stacked compression                              |

    | 19: | end if                                                                                       |

    | 20: | reconstructed ← A.decode(compressed)<br>▷ Decompress to reconstruct                          |

    | 21: | Calculate reconstruction loss between frame and reconstructed                                |

    | 22: | previous_frame ← frame                                                                       |

    | 23: | previous_compressed ← compressed                                                             |

    | 24: | previous_features ← features                                                                 |

    | 25: | end for                                                                                      |

    | 26: | Backpropagate loss and update weights of A only                                              |

    |     | 27: end procedure                                                                            |

    |     | 28: while not converged do                                                                   |

    | 29: | for each video in V do FORWARDPASS(video)                                                    |

    | 30: | end for                                                                                      |

    |     | 31: end while                                                                                |


    The implementation of layered codecs involve the following components.1. Motion
    Estimation: Utilize dedicated hardware blocks for calculating motion vectors between
    consecutive frames. This step can leverage FPGA''s DSP slices for fast cross-correlation
    or block matching algorithms. 2. Prediction and Residual Calculation: Implement
    pipelined architectures for the predict(·) function and subsequent residual calculation
    to minimize latency. 3. Layered Encoding: Each layer of the neural codec can be
    implemented using parallel processing units in FPGA, allowing simultaneous processing
    of different frame parts or different quality layers. 4.Data Flow Management:
    Design efficient data paths to handle the high throughput of video data and intermediate
    results between the FPGA blocks, ensuring that bandwidth and memory access bottlenecks
    are minimized. By optimizing each component for FPGA execution and taking advantage
    of the hardware''s ability to execute multiple operations in parallel, the proposed
    codec can achieve real-time performance even for high-resolution video streams.
    The use of FPGA not only accelerates the processing speed but also provides flexibility
    to adapt to various codec configurations.


    *Training the Neural Codecs to Utilize Inference Pipeline:* The enhancement of
    our layered neural codec involves a joint training regimen that integrates the
    model used in inferecne pipeline, specifically MobileNet, as a static feature
    extractor within the compression framework. This strategy capitalizes on the robust,
    pre-trained features of MobileNet, which are frozen during training to ensure
    their integrity and to leverage their proven capability in capturing essential
    visual features. The extraction of motion vectors between frames further enriches
    the feature space by incorporating temporal dynamics essential for effective compression.


    The codec''s autoencoder component, which is trainable, is then tasked with compressing
    these enriched features. This setup not only streamlines the encoding process
    by utilizing high-quality features but also significantly enhances compression
    efficiency by exploiting both intra-frame richness and inter-frame continuity.
    The training process is designed to optimize the autoencoder''s ability to compress
    and decompress video sequences efficiently, without altering the pretrained feature
    extractor, thereby providing a stable, high-performance baseline for feature representation.
    The mathematical formulation for this training process is centered around minimizing
    the reconstruction loss, L = P<sup>N</sup> <sup>t</sup>=1 <sup>∥</sup>F<sup>t</sup>
    <sup>−</sup> <sup>F</sup><sup>ˆ</sup> t∥ 2 2 ; where F<sup>t</sup> is the original
    frame at time t, and Fˆ <sup>t</sup> is the reconstructed frame, obtained by decoding
    the compressed representation that was encoded using features extracted via MobileNet
    and refined by the motion vector-informed autoencoder. The backpropagation is
    applied only to the layers of the autoencoder, ensuring that the feature extractor''s
    parameters remain intact. This approach not only preserves the integrity of the
    visual features derived from MobileNet but also tailors the compression mechanism
    to be highly adaptive to the content-specific characteristics captured by these
    features.


    # 4 Optimizing Lattice-Based Cryptography


    Followed by the video compression, in this section, we discuss the quantum safe
    encryption technique. Among quantum safe encryption algorithms, Lattice-based
    cryptography (LBC) [\(Micciancio & Regev,](#page-20-3) [2009;](#page-20-3) [Ajtai,
    1996\)](#page-17-13), grounded in hard lattice problems, offers robust resistance
    against quantum attacks. This quantum resilience is vital for protecting large
    data-sets on machine learning storage servers, particularly from ''store now,
    decrypt later'' threats.


    <span id="page-10-0"></span>


    | Algorithm 3 Lattice-Based Encryption Process |  |  |  |  |  |

    |----------------------------------------------|--|--|--|--|--|

    |----------------------------------------------|--|--|--|--|--|


    |                  | 1: procedure LATTICE_BASED_ENCRYPT(message, public_key) |                  |  |  |

    |------------------|---------------------------------------------------------|------------------|--|--|

    | 2:               | PM ← ConvertToPolynomial(message)                       |                  |  |  |

    | 3:               | (PA, PE) ← GenerateRandomPolynomials()                  |                  |  |  |

    | 4:               | C1 ← PolynomialMultiply(PA, public_key)                 |
    ▷ Utilizing HSPM |  |  |

    | 5:               | C2 ← PolynomialMultiply(PA, PM)                         |
    ▷ Employing SDMM |  |  |

    | 6:               | return (C1, C2)                                         |                  |  |  |

    | 7: end procedure |                                                         |                  |  |  |


    At the heart of LBC lies the Ring-Learning with Errors (R-LWE) algorithm, which
    translates plaintext messages into polynomial representations and intertwines
    them with random polynomials using complex multiplications and additions, as delineated
    in Algorithm [3.](#page-10-0) Note that, within the LBC algorithm, certain components,
    specifically polynomial multiplications, exhibit similarities to operations performed
    in CNNs. These similarities open up the possibility of reusing hardware designed
    for CNN operations to accelerate LBC computations. The most common algorithms
    used fod the said operations are High-Speed Schoolbook Polynomial Multiplication
    (HSPM) and Pipelined Systolic Dimension Modular Multiplier (SDMM), and we propose
    to accelrate the same using the FPGAs in the CSDs. The data locality due to the
    SSD and the high throughput due to the FPGA could facilitate swift polynomial
    processing and refined modular multiplications using DSP slices, thereby accelerating
    the encryption process.


    *Designing HSPM Accelerator on CSD FPGA:* The HSPM hardware is characterized by
    its fully parallelized design, incorporating 128 Multiply-Accumulate (MAC) units
    for handling polynomials of degree n = 256. Each MAC unit within the HSPM architecture
    is capable of conducting two parallel modular multiplications. This is achieved
    through the use of a single Digital Signal Processing (DSP) block that operates
    on signed data representation. Consequently, these units are referred to as Signed
    Double Modular Multiplication (SDMM) units. The HSPM accelerator''s architecture,
    as illustrated in Fig. [3a,](#page-11-0) comprises three pipelined stages. These
    stages include the data loading phase, the modular polynomial multiplication via
    the SDMM unit, and the final accumulation registers. Data is input into the HSPM
    in a serial-to-parallel conversion process, while the outputs, after undergoing
    addition operations, are retrieved serially through an address signal.


    Central to the Ring-Learning with Errors (R-LWE) based Public Key Encryption (PKE)
    is the equation d = a · b + c. In this context, the operand a can represent polynomials
    such as p, a, or c<sup>1</sup> from Algorithm 1, while b corresponds to e1, r2,
    and c to e2, e3, c2. During the data loading phase, the 256 coefficients of polynomial
    b are input serially into a 6-bit shift register. Simultaneously, the first coefficient
    a<sup>0</sup> of polynomial a is fed in parallel to all 128 SDMM units. Post-loading,
    each SDMM


    <span id="page-11-0"></span>![](_page_11_Figure_0.jpeg)


    (b) Pipeline SDMM Structure micro-architecture.


    Figure 3: Microarchitectural designs of HSPM and SDMM: Hardware modules for polynomial
    multiplication in LBC.


    unit commences the modular multiplication of each coefficient a<sup>i</sup> with
    the entirety of b''s coefficients in parallel. This process repeats for all coefficients
    a<sup>i</sup> . The resultant outputs from each SDMM calculation are accumulated
    every cycle. The final polynomial product p is sequentially read out by the address
    signal addrab, combined with the coefficient of c, thereby producing the final
    output d as d = a · b + c. This output is then transmitted serially over n clock
    cycles.


    *Designing SDMM Hardware on CSD FPGA:* The SDMM hardware is innovatively designed
    to perform two modular multiplications per DSP Slice. This is achieved through
    the implementation of signed Gaussian sampling [\(Liu et al., 2019\)](#page-20-15),
    for the error-vector e<sup>1</sup> and the secret-key r2. In this signed representation,
    the Gaussian distribution ranges from [0, ks) to [q − ks, 0), where k takes integer
    values 1, 2, 3, . . ., and is symmetrically distributed around m = 0. Consequently,
    there is no data in the range [0, q − 1), allowing the maximum value of the 13-bit
    samples to be represented efficiently by a 6-bit signed number. The modular multiplication
    utilizing these signed numbers is formulated as follows:


    $$b \otimes a \mod q = q - [(b - a) \mod q] \tag{1}$$


    Here, the most significant bit (MSB) signed-bit dictates the subtraction of the
    result from the modulus q, as illustrated in Fig. [3b.](#page-11-0) A single DSP
    unit includes a multiplier, with the lowest 18 bits of the


    DSP output representing the first product d<sup>0</sup> and the highest 18 bits
    indicating the second product d1, corresponding to b<sup>0</sup> and b<sup>1</sup>
    respectively, as depicted in Fig. [3b.](#page-11-0)


    Following the multiplication, the two 18-bit products undergo separate modular
    reductions. A Modular Reduction (MR) circuitry based on approximation [\(Kundi
    et al., 2020\)](#page-20-16). Our MR unit achieves consumes ≈ 82% less hardware
    compared to classical implementation. This process is constant time, requiring
    only a single subtraction of q. The MR hardware, highlighted in Fig. [3b,](#page-11-0)
    consists of a single shift block, a subtractor, and a adder, consuming much less
    hardware then the state-of-the-art [\(Fan et al., 2018\)](#page-19-12). The MR,
    in conjunction with the multiplier, delivers the output within 2 clock cycles.
    The SDMM also offers a simpler controller design for multiplication and in-place
    product term reduction after vector-vector multiplications. Notably, the SDMM
    can be easily reconfigured to support any modulus.


    # 5 Implementation and Evaluation


    To implement and evaluate Salient Store , we chose two different platforms – 1)
    Using a servergrade system with two Xilinx-Samsung computational storage drive
    [\(AMD, b\)](#page-17-6), and 2) Using amazon web services (AWS) F1 instances,
    which have AMD Alveo FPGA which can work as the compute capable of peer-to-peer
    communication and thereby enabling a computational storage platform. We use Vitis
    2022.2 along with Xilinx Runtime Library [\(AMD & Xilinx\)](#page-17-12) for programming
    the FPGAs in both computational storage drives and the Alveo card. The configuration
    of the server with Xilinx CSD has a 12-core Xeon bronze CPU with 128GB memory,
    2×3.84TB CSDs, and 2×2TB SSDs. Similarly, the AWS server has 24 cores, with 192GB
    memory, one Alveo FPGA and 2TB SSDs.


    <span id="page-12-0"></span>![](_page_12_Figure_4.jpeg)


    Figure 4: Latency analysis of Salient Store on the commercial Xilinx CSD on a
    workstation class machine (lower is better). Compute server indicates a software
    only classical storage solution without CSDs.


    <span id="page-12-1"></span>


    | Data Location           | kernel Execution        | SpeedUp |

    |-------------------------|-------------------------|---------|

    | CSD1                    | CPU                     | 1       |

    | CSD1                    | CSD1                    | 3.9     |

    | CSD1 (0.1X), CSD2(0.9X) | CSD1 (0.1X), CSD2(0.9X) | 4.46    |

    | CSD1 (0.3X), CSD2(0.7X) | CSD1 (0.3X), CSD2(0.7X) | 5.608   |

    | CSD1 (0.4X), CSD2(0.6X) | CSD1 (0.4X), CSD2(0.6X) | 6.67    |

    | CSD1 (0.5X), CSD2(0.5X) | CSD1 (0.5X), CSD2(0.5X) | 7.7     |


    Table 2: Effect of Data distribution on compute speed-up.


    <span id="page-13-0"></span>![](_page_13_Figure_0.jpeg)


    Figure 5: Performance of Salient Store on larger compute and storage nodes. This
    experiment to mimics a consolidated edge server catering to many video streams.
    Compute server indicates a software only classical storage solution without CSDs.


    #### 5.1 Evaluation of Encoding, Scaling and Accuracy


    In evaluating the Salient Store storage system, particularly for continuous learning
    scenarios video analytics applications, we selected data-sets that are not only
    dense but also necessitate continuous learning due to their dynamic nature. Autonomous
    driving and urban mobility applications, generating over 400TB of data annually
    [\(Wright";](#page-22-2) ["premioinc";](#page-21-4) [Bhardwaj et al., 2022\)](#page-17-4),
    predominantly comprise video and 3D point cloud data, making them ideal for our
    evaluation. The *Cityscapes* data-set [\(Cordts et al., 2015\)](#page-18-14) is
    a comprehensive collection of urban street scenes from 50 different cities, providing
    a rich source of annotated video data for semantic urban scene understanding.
    This data-set is particularly suited for evaluating the performance of Salient
    Store in dense, urban environments. Another video data-set, the *Waymo Open Data-set*
    [\(Sun et al., 2020\)](#page-21-15), is one of the largest and most diverse data-sets
    for autonomous driving. It offers high-resolution sensor data, including LIDAR
    and camera recordings, across a variety of urban and suburban landscapes. This
    data-set''s volume and diversity make it an excellent benchmark for assessing
    Salient Store ''s capabilities in handling large-scale, real-world data. For 3D
    point cloud data, we selected the *KITTI Vision Benchmark Suite* [\(Geiger et
    al., 2012\)](#page-19-13), a fundamental data-set in autonomous driving research.
    It includes a variety of data types and tasks, providing a real-world urban driving
    context that is essential for testing Salient Store ''s performance in processing
    and managing 3D spatial data. Additionally, the *nuScenes* data-set [\(Caesar
    et al., 2020\)](#page-17-14) by Aptiv, with its comprehensive sensor data and
    annotations covering diverse driving scenes, is instrumental in evaluating Salient
    Store ''s efficiency in multi-modal data processing typical in urban mobility
    scenarios. Beyond the visionbased data, we also incorporated the *Chime Audio*
    data-set [\(Foster et al., 2015\)](#page-19-14) into our evaluation. This data-set,
    consisting of audio recordings, offers a different modality to test the versatility
    of Salient Store in handling various types of continuous learning data beyond
    visual inputs. In our comparative analysis, we employ *VSS: A Video Storage System*
    [\(Haynes et al., 2021\)](#page-19-8) as a "baseline", given its innovative approach
    in optimizing video data management. VSS excels in decoupling high-level video
    operations from storage and retrieval processes, efficiently organizing data on
    disk, enhancing caching mechanisms, and reducing redundancies in multi-camera
    setups [\(Haynes et al.,](#page-19-8) [2021\)](#page-19-8). This system has demonstrated
    significant improvements in VDBMS read performance, up to 54%, and a reduction
    in storage costs by up to 45% [\(Haynes et al., 2021\)](#page-19-8). VSS''s emphasis
    on video data optimization makes it a pertinent benchmark for assessing the Salient
    Store storage system''s capabilities in managing large-scale machine learning
    data-sets.


    We first implemented Salient Store on a workstation-class machine with two Xilinx
    CSDs which colosely mimics the classical edge server setup. Fig. [4](#page-12-0)
    shows the latency while using the storage server and CSD for performing data writes,
    *normalized* to the latency of doing the same using the state of the art Alveo
    FPGAs. Salient Store on CSDs perform ≈ 1.99× better than the implementation on
    classical storage systems. To further mimic realwold scenarios of multiple cameras
    sending multiple streams with various stream rate, we *distributed* video data
    into across two CSDs in different ratio, as shown in TABLE [2.](#page-12-1) Trivially,
    a load-balanced scenario outperforms any of the biased data distribution scenario.
    However, it is evident that any compute offloaded to any of the CSDs in these
    scenarios offers benefits in terms of data processing latency.


    To underscore the impact of Salient Store at a much larger scale, where one can
    assume a consolidated edge server catering towards multiple streams as depicted
    in Ekya [\(Bhardwaj et al.,](#page-17-4) [2022\)](#page-17-4), we utilized an
    AWS EC2 F1 instance with Alveo FPGAs, along with EC2 P4 instance with A100 GPUs
    to implement the continuous learning end-to-end. As shown in Fig. [5a](#page-13-0)
    Salient Store


    <span id="page-14-0"></span>![](_page_14_Figure_0.jpeg)


    keeps up with the accuracy of the traditional compression system thanks to the
    robust layered coding algorithm. Additionally, as shown in Fig. [5b,](#page-13-0)
    Salient Store has 4.49× less latency than VSS where as it shows about a 6.18×
    speedup compared to the classical storage server. Furthermore, as depicted in
    Fig. [5c,](#page-13-0) SLAT reduces the data communication volume up to ≈ 5.63×
    compared to the classical approaches, thereby saving bandwidth (reduces bandwidth
    by ≈ 36%, public cloud has strict regulations on measuring the actual bandwidth,
    and hence we report the approximate result from the traffic pattern) and energy.


    <span id="page-14-1"></span>![](_page_14_Figure_2.jpeg)


    Figure 7: Proposed encryption vs the state-of-the-art normalized to the FPGA implementation
    of the lattice based encryption.


    Since majority of the storage systems are not limited to one storage server, but
    are spread across multiple servers, we scaled Salient Store by deploying it in
    a distributed fashion. We used 5× AWS EC2 F1 instances with Alveo FPGAs as storage
    nodes, along with one EC2 P4 instance with A100 GPUs as the compute server. We
    do not observe much change in the data volume. However, due to complex interaction
    between multiple storage nodes, especially where some of the data needed to arrive
    at different nodes via network, in Fig. [6,](#page-14-0) we observe a significant
    change in the latency compared to a single storage node. Although, a multi-node
    setup provides more parallelism, the speedup is sub-linear, and we observe ≈ 3×
    and ≈ 4.77× speedup against VSS and a classical storage server, respectively.


    #### 5.2 Evaluation of Video Data Recovery and Quality:


    To ensure proper video quality upon recovery, we perform a peak signal-to-noise
    ratio (PSNR) study of Salient Store with the classical encoding mechanisms H264
    [\(ITU-T, 2019a\)](#page-19-15) and H265 (HEVC) [\(ITU-T, 2019b\)](#page-19-16).
    Experiments on waymo [\(Sun et al., 2020\)](#page-21-15) dataset shows the PSNR
    of Salient Store compared to the classical H264 and HEVC encoding pipeline in
    Fig. [8.](#page-15-0) While


    Salient Store is consistently performing better than H264, HEVC thanks to it''s
    novel approach of fine grained computation at times outperforms our approach.
    A similar trend is observed in other datasets. With complex and high dimensional
    video data, HEVC computation complexity increases exponentially, and is more pronounced
    because of lack of hardware support. However, it is noteworthy that at higher
    bitrates, Salient Store consistently achieves high quality recovery with PSNR
    reaching ≈ 47dB. Furthermore, as we can see in Fig. [9,](#page-15-1) HEVC takes
    significantly more latency compared to Salient Store and therefore not entirely
    suitable for high frame-rate applications with resource constraints.


    <span id="page-15-0"></span>![](_page_15_Figure_1.jpeg)


    Figure 8: Compression and Recovery efficiency.


    <span id="page-15-1"></span>![](_page_15_Figure_3.jpeg)


    Figure 9: Encoding Latency using layered coding.


    #### 5.3 Evaluation of Lattice Based Encryption


    One of the major contribution of Salient Store is accelerating the lattice-based
    encryption with the help of FPGAs on the storage nodes. Fig. [7](#page-14-1) shows
    the comparison of our FPGA-accelerated lattice-based encryption against other
    state-of-the-art-techniques. RSA, the most popular encryption algorithm, when
    implemented using FPGA, outperforms our proposed hardware solution. However, our
    proposed solution offers ≈ 3.2× speedup compared to its software counterpart and
    a ≈ 2.5× speedup compared to the software based RSA algorithm. While we do agree
    that the lattice-based encryption has more overheads compared to the RSA algorithm,
    the benefit of being "quantum-safe" outweighs the minimal cost incurred on encryption.


    Fig. [10](#page-16-0) shows the impact of scaling Salient Store with respect to
    a single node system, i.e., a single storage server. As the number of storage
    servers increases, the network contention and data orchestration challenges exponentially
    increase. Furthermore, each server performs more remote accesses, increasing the
    contention even more. This leads to an exponential growth in latency with the
    increase in the number of storage servers used per application. It is recommended
    to contain most of the data belonging to the same application in the same storage
    server to minimize remove disk accesses over network.


    <span id="page-16-0"></span>![](_page_16_Figure_0.jpeg)


    <span id="page-16-1"></span>Figure 10: Change of data movement latency with respect
    to the number of storage servers.


    ![](_page_16_Figure_2.jpeg)


    Figure 11: Impact of increasing the number of CSDs in the system (the baseline
    system has a SSD-to-CSD ratio 8 to 1).


    Similarly, Fig. [11](#page-16-1) shows the impact of increasing the number of
    CSDs in a storage server. With increasing the number of CSDs per SSD (or other
    storage element) does show significant improvement because of increase in parallelism.
    However, a typical CSD is ≈ 15× expensive than a standard SSD and ≈ 25× expensive
    than a classical HDD. Integrating more number of CSDs into the standard storage
    server will significantly increase the cost of the server. Moreover, in large-scale
    systems where failure is common, replacing failed CSDs would further increase
    the cost. From our evaluation, we found an 8:1 ratio of SSD to CSD (capacity ratio)
    provides the best possible cost-to-acceleration benefit.


    # 6 Conclusions


    In this paper, we have explored the critical role of storage systems in the context
    of continuous learning video analytics edge server, emphasizing in particular
    the transformative potential of Computational Storage Devices (CSDs) in this domain.
    Our proposal, Salient Store , highlights the need for a paradigm shift in storage
    architecture to accommodate the dynamic and computationally intensive nature of
    modern ML applications. By reducing unnecessary data movement and enabling near-data
    processing, CSDs enhance the efficiency, performance, and sustainability of storage
    servers. Salient Store , with its intelligent data orchestration and acceleration,
    can provide up to 6.18× latency and 6.13× data movement reduction, compared to
    classical systems. This is particularly evident in continuous learning scenarios
    prevalent in applications such as autonomous driving and urban mobility, where
    the volume and complexity of data are immense.


    Looking ahead, the role of storage systems is poised to become even more pivotal
    as ML applications continue to evolve and generate larger, more complex datasets.
    The ongoing innovation in the design and optimization of CSDs will be crucial
    in meeting these challenges. In summary, our research presents a comprehensive
    vision for the future of storage systems in ML, where computational storage devices
    play a key role in advancing the, performance, efficiency, and capabilities of
    storage servers, thereby contributing significantly to the broader field of ML.


    # References


    - <span id="page-17-5"></span>Achieving Compliant Data Residency and Security
    with Azure. Achieving Compliant Data Residency and Security with Azure. [https://azure.microsoft.com/mediahandler/files/](https://azure.microsoft.com/mediahandler/files/resourcefiles/achieving-compliant-data-residency-and-security-with-azure/Achieving_Compliant_Data_Residency_and_Security_with_Azure.pdf)
    [resourcefiles/achieving-compliant-data-residency-and-security-with-azure/](https://azure.microsoft.com/mediahandler/files/resourcefiles/achieving-compliant-data-residency-and-security-with-azure/Achieving_Compliant_Data_Residency_and_Security_with_Azure.pdf)
    [Achieving\\_Compliant\\_Data\\_Residency\\_and\\_Security\\_with\\_Azure.pdf](https://azure.microsoft.com/mediahandler/files/resourcefiles/achieving-compliant-data-residency-and-security-with-azure/Achieving_Compliant_Data_Residency_and_Security_with_Azure.pdf).

    - <span id="page-17-13"></span>Miklós Ajtai. Generating hard instances of lattice
    problems. In *Proceedings of the twenty-eighth annual ACM symposium on Theory
    of computing*, pp. 99–108, 1996.

    - <span id="page-17-11"></span>AMD. Vitis unified software platform. [https://www.xilinx.com/products/design-tools/](https://www.xilinx.com/products/design-tools/vitis/vitis-platform.html)
    [vitis/vitis-platform.html](https://www.xilinx.com/products/design-tools/vitis/vitis-platform.html),
    a. (Accessed on 11/13/2023).

    - <span id="page-17-6"></span>AMD. Smartssd computational storage drive. [https://www.xilinx.com/applications/](https://www.xilinx.com/applications/data-center/computational-storage/smartssd.html)
    [data-center/computational-storage/smartssd.html](https://www.xilinx.com/applications/data-center/computational-storage/smartssd.html),
    b. (Accessed on 07/13/2023).

    - <span id="page-17-12"></span>AMD and Xilinx. Xilinx runtime library (xrt). [https://www.xilinx.com/products/](https://www.xilinx.com/products/design-tools/vitis/xrt.html)
    [design-tools/vitis/xrt.html](https://www.xilinx.com/products/design-tools/vitis/xrt.html).
    (Accessed on 11/20/2023).

    - <span id="page-17-3"></span>Guilherme H. Apostolo, Pablo Bauszat, Vinod Nigade,
    Henri E. Bal, and Lin Wang. Live video analytics as a service. In *Proceedings
    of the 2nd European Workshop on Machine Learning and Systems*, EuroMLSys ''22,
    pp. 37–44, New York, NY, USA, 2022. Association for Computing Machinery. ISBN
    9781450392549. doi: 10.1145/3517207.3526973. URL [https://doi.org/](https://doi.org/10.1145/3517207.3526973)
    [10.1145/3517207.3526973](https://doi.org/10.1145/3517207.3526973).

    - <span id="page-17-8"></span>David Arthur and Sergei Vassilvitskii. K-means++
    the advantages of careful seeding. In *Proceedings of the eighteenth annual ACM-SIAM
    symposium on Discrete algorithms*, pp. 1027–1035, 2007.

    - <span id="page-17-9"></span>Bahman Bahmani, Benjamin Moseley, Andrea Vattani,
    Ravi Kumar, and Sergei Vassilvitskii. Scalable k-means++. *arXiv preprint arXiv:1203.6402*,
    2012.

    - <span id="page-17-10"></span>Antonio Barbalace and Jaeyoung Do. Computational
    storage: Where are we today? In *CIDR*, 2021.

    - <span id="page-17-4"></span>Romil Bhardwaj, Zhengxu Xia, Ganesh Ananthanarayanan,
    Junchen Jiang, Yuanchao Shu, Nikolaos Karianakis, Kevin Hsieh, Paramvir Bahl,
    and Ion Stoica. Ekya: Continuous learning of video analytics models on edge compute
    servers. In *19th USENIX Symposium on Networked Systems Design and Implementation
    (NSDI 22)*, pp. 119–135, 2022.

    - <span id="page-17-1"></span>Ilker Bozcan and Erdal Kayacan. Au-air: A multi-modal
    unmanned aerial vehicle dataset for low altitude traffic surveillance, 2020. URL
    <https://arxiv.org/abs/2001.11737>.

    - <span id="page-17-2"></span>M. Bramberger, J. Brunner, B. Rinner, and H. Schwabach.
    Real-time video analysis on an embedded smart camera for traffic surveillance.
    In *Proceedings. RTAS 2004. 10th IEEE Real-Time and Embedded Technology and Applications
    Symposium, 2004.*, pp. 174–181, 2004. doi: 10.1109/ RTTAS.2004.1317262.

    - <span id="page-17-0"></span>Barry Brown, Mathias Broth, and Erik Vinkhuyzen.
    The halting problem: Video analysis of selfdriving cars in traffic. In *Proceedings
    of the 2023 CHI Conference on Human Factors in Computing Systems*, CHI ''23, New
    York, NY, USA, 2023. Association for Computing Machinery. ISBN 9781450394215.
    doi: 10.1145/3544548.3581045. URL [https://doi.org/10.1145/3544548.](https://doi.org/10.1145/3544548.3581045)
    [3581045](https://doi.org/10.1145/3544548.3581045).

    - <span id="page-17-14"></span>Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh
    Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and
    Oscar Beijbom. nuscenes: A multimodal dataset for autonomous driving. In *Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition*, pp. 11621–11631,
    2020.

    - <span id="page-17-7"></span>Jie Cao, Zhiang Wu, Junjie Wu, and Wenjie Liu. Towards
    information-theoretic k-means clustering for image indexing. *Signal Processing*,
    93(7):2026–2037, 2013.

    - <span id="page-18-9"></span>Wei Cao, Yang Liu, Zhushi Cheng, Ning Zheng, Wei
    Li, Wenjie Wu, Linqiang Ouyang, Peng Wang, Yijing Wang, Ray Kuan, et al. {POLARDB}
    meets computational storage: Efficiently support analytical workloads in {Cloud-Native}
    relational database. In *18th USENIX conference on file and storage technologies
    (FAST 20)*, pp. 29–41, 2020.

    - <span id="page-18-10"></span>Keith Chapman, Mehdi Nik, Behnam Robatmili, Shahrzad
    Mirkhani, and Maysam Lavasani. Computational storage for big data analytics. In
    *Proceedings of 10th International Workshop on Accelerating Analytics and Data
    Management Systems (ADMS''19)*, 2019.

    - <span id="page-18-7"></span>Jianyu Chen, Maurice Daverveldt, and Zaid Al-Ars.
    Fpga acceleration of zstd compression algorithm. In *2021 IEEE International Parallel
    and Distributed Processing Symposium Workshops (IPDPSW)*, pp. 188–191. IEEE, 2021.

    - <span id="page-18-8"></span>Tong Chen, Haojie Liu, Qiu Shen, Tao Yue, Xun Cao,
    and Zhan Ma. Deepcoder: A deep neural network based video compression. In *2017
    IEEE Visual Communications and Image Processing (VCIP)*, pp. 1–4. IEEE, 2017.

    - <span id="page-18-4"></span>Yihua Cheng, Ziyi Zhang, Hanchen Li, Anton Arapin,
    Yue Zhang, Qizheng Zhang, Yuhan Liu, Kuntai Du, Xu Zhang, Francis Y Yan, et al.
    {GRACE}:{Loss-Resilient}{Real-Time} video through neural codecs. In *21st USENIX
    Symposium on Networked Systems Design and Implementation (NSDI 24)*, pp. 509–531,
    2024.

    - <span id="page-18-6"></span>Yann Collet and Murray Kucherawy. Zstandard compression
    and the application/zstd media type. Technical report, 2018.

    - <span id="page-18-14"></span>Marius Cordts, Mohamed Omran, Sebastian Ramos,
    Timo Scharwächter, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth,
    and Bernt Schiele. The cityscapes dataset. In *CVPR Workshop on the Future of
    Datasets in Vision*, volume 2. sn, 2015.

    - <span id="page-18-0"></span>Intel Corporation. Smart city technologies and intelligent
    transportation systems are helping cities absorb growing populations, overcome
    congestion, and create sustainable futures. [https://www.](https://www.intel.com/content/www/us/en/transportation/urban-mobility.html)
    [intel.com/content/www/us/en/transportation/urban-mobility.html](https://www.intel.com/content/www/us/en/transportation/urban-mobility.html).
    (Accessed on 11/21/2022).

    - <span id="page-18-1"></span>Custom On-Device ML Models with Learn2Compress.
    Custom on-device ml models with learn2compress. [https://ai.googleblog.com/2018/05/custom-on-device-ml-models.](https://ai.googleblog.com/2018/05/custom-on-device-ml-models.html)
    [html](https://ai.googleblog.com/2018/05/custom-on-device-ml-models.html). (Accessed
    on 11/21/2022).

    - <span id="page-18-13"></span>Mallesham Dasari, Kumara Kahatapitiya, Samir R
    Das, Aruna Balasubramanian, and Dimitris Samaras. Swift: Adaptive video streaming
    with layered neural codecs. In *19th USENIX Symposium on Networked Systems Design
    and Implementation (NSDI 22)*, pp. 103–118, 2022a.

    - <span id="page-18-3"></span>Mallesham Dasari, Kumara Kahatapitiya, Samir R.
    Das, Aruna Balasubramanian, and Dimitris Samaras. Swift: Adaptive video streaming
    with layered neural codecs. In *19th USENIX Symposium on Networked Systems Design
    and Implementation (NSDI 22)*, pp. 103–118, Renton, WA, April 2022b. USENIX Association.
    ISBN 978-1-939133-27-4. URL [https://www.usenix.org/](https://www.usenix.org/conference/nsdi22/presentation/dasari)
    [conference/nsdi22/presentation/dasari](https://www.usenix.org/conference/nsdi22/presentation/dasari).

    - <span id="page-18-11"></span>Maureen Daum, Brandon Haynes, Dong He, Amrita Mazumdar,
    and Magdalena Balazinska. Tasm: A tile-based storage manager for video analytics.
    In *2021 IEEE 37th International Conference on Data Engineering (ICDE)*, pp. 1775–1786.
    IEEE, 2021.

    - <span id="page-18-12"></span>Jaeyoung Do, Yang-Suk Kee, Jignesh M Patel, Chanik
    Park, Kwanghyun Park, and David J DeWitt. Query processing on smart ssds: Opportunities
    and challenges. In *Proceedings of the 2013 ACM SIGMOD International Conference
    on Management of Data*, pp. 1221–1230, 2013.

    - <span id="page-18-5"></span>Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff
    Johnson, Gergely Szilvasy, Pierre-Emmanuel Mazaré, Maria Lomeli, Lucas Hosseini,
    and Hervé Jégou. The faiss library. *arXiv preprint arXiv:2401.08281*, 2024.

    - <span id="page-18-2"></span>Sourav Dutta and Chinwe Ekenna. Air-to-ground surveillance
    using predictive pursuit. In *2019 International Conference on Robotics and Automation
    (ICRA)*, pp. 8234–8240. IEEE Press, 2019. doi: 10.1109/ICRA.2019.8794073. URL
    <https://doi.org/10.1109/ICRA.2019.8794073>.

    - <span id="page-19-9"></span>Eideticom and Los Alamos National Laboratory. Line-rate
    compression on lustre/zfs-based parallel filesystems using noload computational
    storage processor. [https://www.eideticom.](https://www.eideticom.com/media/attachments/2020/06/03/noload-compression-zfs.pdf)
    [com/media/attachments/2020/06/03/noload-compression-zfs.pdf](https://www.eideticom.com/media/attachments/2020/06/03/noload-compression-zfs.pdf).
    (Accessed on 11/13/2023).

    - <span id="page-19-7"></span>Lizhou Fan, Zhanyuan Yin, Huizi Yu, and Anne J Gilliland.
    Using machine learning to enhance archival processing of social media archives.
    *Journal on Computing and Cultural Heritage (JOCCH)*, 15(3):1–23, 2022.

    - <span id="page-19-12"></span>Sailong Fan, Weiqiang Liu, James Howe, Ayesha Khalid,
    and Maire O''Neill. Lightweight hardware implementation of r-lwe lattice-based
    cryptography. In *2018 IEEE Asia Pacific Conference on Circuits and Systems (APCCAS)*,
    pp. 403–406. IEEE, 2018.

    - <span id="page-19-0"></span>Shaoheng Fang, Zi Wang, Yiqi Zhong, Junhao Ge, Siheng
    Chen, and Yanfeng Wang. Tbp-former: Learning temporal bird''s-eye-view pyramid
    for joint perception and prediction in vision-centric autonomous driving, 2023.
    URL <https://arxiv.org/abs/2303.09998>.

    - <span id="page-19-5"></span>Manuel J Fonseca and Joaquim A Jorge. Indexing high-dimensional
    data for content-based retrieval in large databases. In *Eighth International
    Conference on Database Systems for Advanced Applications, 2003.(DASFAA 2003).
    Proceedings.*, pp. 267–274. IEEE, 2003.

    - <span id="page-19-14"></span>Peter Foster, Siddharth Sigtia, Sacha Krstulovic,
    Jon Barker, and Mark D Plumbley. Chime-home: A dataset for sound source recognition
    in a domestic environment. In *2015 IEEE Workshop on Applications of Signal Processing
    to Audio and Acoustics (WASPAA)*, pp. 1–5. IEEE, 2015.

    - <span id="page-19-13"></span>Andreas Geiger, Philip Lenz, and Raquel Urtasun.
    Are we ready for autonomous driving? the kitti vision benchmark suite. In *2012
    IEEE conference on computer vision and pattern recognition*, pp. 3354–3361. IEEE,
    2012.

    - <span id="page-19-2"></span>Maksim Godovykh, Carissa Baker, and Alan Fyall.
    Vr in tourism: A new call for virtual tourism experience amid and after the covid-19
    pandemic. *Tourism and Hospitality*, 3(1):265–275, 2022.

    - <span id="page-19-3"></span>Philipp M. Grulich and Faisal Nawab. Collaborative
    edge and cloud neural networks for real-time video processing. *Proc. VLDB Endow.*,
    11(12):2046–2049, aug 2018. ISSN 2150-8097. doi: 10.14778/3229863.3236256. URL
    <https://doi.org/10.14778/3229863.3236256>.

    - <span id="page-19-8"></span>Brandon Haynes, Maureen Daum, Dong He, Amrita Mazumdar,
    Magdalena Balazinska, Alvin Cheung, and Luis Ceze. Vss: A storage system for video
    analytics. In *Proceedings of the 2021 International Conference on Management
    of Data*, pp. 685–696, 2021.

    - <span id="page-19-6"></span>Qunying Huang and Chen Xu. A data-driven framework
    for archiving and exploring social media data. *Annals of GIS*, 20(4):265–277,
    2014.

    - <span id="page-19-1"></span>Zhiyu Huang, Haochen Liu, and Chen Lv. Gameformer:
    Game-theoretic modeling and learning of transformer-based interactive prediction
    and planning for autonomous driving, 2023. URL <https://arxiv.org/abs/2303.05760>.

    - <span id="page-19-15"></span>ITU-T. Advanced video coding for generic audiovisual
    services. International Telecommunication Union, June 2019a. URL <https://www.itu.int/rec/T-REC-H.264>.
    ITU-T Recommendation H.264.

    - <span id="page-19-16"></span>ITU-T. High efficiency video coding. International
    Telecommunication Union, June 2019b. URL <https://www.itu.int/rec/T-REC-H.265>.
    ITU-T Recommendation H.265.

    - <span id="page-19-10"></span>Myoungsoo Jung. Hello bytes, bye blocks: Pcie storage
    meets compute express link for memory expansion (cxl-ssd). In *Proceedings of
    the 14th ACM Workshop on Hot Topics in Storage and File Systems*, pp. 45–51, 2022.

    - <span id="page-19-11"></span>Hwajung Kim, Heon Y Yeom, and Hanul Sung. Understanding
    the performance characteristics of computational storage drives: A case study
    with smartssd. *Electronics*, 10(21):2617, 2021.

    - <span id="page-19-4"></span>Yoonsung Kim, Changhun Oh, Jinwoo Hwang, Wonung
    Kim, Seongryong Oh, Yubin Lee, Hardik Sharma, Amir Yazdanbakhsh, and Jongse Park.
    Dacapo: Accelerating continuous learning in autonomous systems for video analytics.
    *arXiv preprint arXiv:2403.14353*, 2024.

    - <span id="page-20-5"></span>Terhi Korkiakangas. Challenges in archiving and
    sharing video data: Considering moral, pragmatic and substantial arguments. *Journal
    of Research Practice*, 10(1), 2014.

    - <span id="page-20-16"></span>Dur E Shahwar Kundi, Song Bian, Ayesha Khalid,
    Chenghua Wang, Máire O''Neill, and Weiqiang Liu. Axmm: Area and power efficient
    approximate modular multiplier for r-lwe cryptosystem. In *2020 IEEE International
    Symposium on Circuits and Systems (ISCAS)*, pp. 1–5. IEEE, 2020.

    - <span id="page-20-13"></span>Los Alamos National Laboratory. Los alamos national
    laboratory and sk hynix to demonstrate first-of-a-kind ordered key-value store
    computational storage device. [https://discover.lanl.](https://discover.lanl.gov/news/0728-storage-device/)
    [gov/news/0728-storage-device/](https://discover.lanl.gov/news/0728-storage-device/).
    (Accessed on 11/13/2023).

    - <span id="page-20-11"></span>Shiju Li, Kevin Tang, Jin Lim, Chul-Ho Lee, and
    Jongryool Kim. Computational storage for an energy-efficient deep neural network
    training system. In *European Conference on Parallel Processing*, pp. 304–319.
    Springer, 2023.

    - <span id="page-20-15"></span>Weiqiang Liu, Sailong Fan, Ayesha Khalid, Ciara
    Rafferty, and Máire O''Neill. Optimized schoolbook polynomial multiplication for
    compact lattice-based cryptography on fpga. *IEEE Transactions on Very Large Scale
    Integration (VLSI) Systems*, 27(10):2459–2463, 2019.

    - <span id="page-20-14"></span>Corne Lukken and Animesh Trivedi. Past, present
    and future of computational storage: A survey. *arXiv preprint arXiv:2112.09691*,
    2021.

    - <span id="page-20-7"></span>Siwei Ma, Xinfeng Zhang, Chuanmin Jia, Zhenghui
    Zhao, Shiqi Wang, and Shanshe Wang. Image and video compression with neural networks:
    A review. *IEEE Transactions on Circuits and Systems for Video Technology*, 30(6):1683–1698,
    2019.

    - <span id="page-20-10"></span>Vikram Sharma Mailthody, Zaid Qureshi, Weixin Liang,
    Ziyan Feng, Simon Garcia De Gonzalo, Youjie Li, Hubertus Franke, Jinjun Xiong,
    Jian Huang, and Wen-mei Hwu. Deepstore: In-storage acceleration for intelligent
    queries. In *Proceedings of the 52nd Annual IEEE/ACM International Symposium on
    Microarchitecture*, pp. 224–238, 2019.

    - <span id="page-20-12"></span>Michael Mesnier. Intel labs showcases multi-vendor,
    computational storage platform. [https://community.intel.com/t5/Blogs/Tech-Innovation/Data-Center/](https://community.intel.com/t5/Blogs/Tech-Innovation/Data-Center/Intel-Labs-Showcases-Multi-Vendor-Computational-Storage-Platform/post/1404651)
    [Intel-Labs-Showcases-Multi-Vendor-Computational-Storage-Platform/post/](https://community.intel.com/t5/Blogs/Tech-Innovation/Data-Center/Intel-Labs-Showcases-Multi-Vendor-Computational-Storage-Platform/post/1404651)
    [1404651](https://community.intel.com/t5/Blogs/Tech-Innovation/Data-Center/Intel-Labs-Showcases-Multi-Vendor-Computational-Storage-Platform/post/1404651),
    August 2022. (Accessed on 11/13/2023).

    - <span id="page-20-3"></span>Daniele Micciancio and Oded Regev. Lattice-based
    cryptography. In *Post-quantum cryptography*, pp. 147–191. Springer, 2009.

    - <span id="page-20-6"></span>Evgeny Milanov. The rsa algorithm. *RSA laboratories*,
    pp. 1–11, 2009.

    - <span id="page-20-0"></span>Cyan Subhra Mishra, Jack Sampson, Mahmut Taylan
    Kandemir, Vijaykrishnan Narayanan, and Chita R Das. Usas: A sustainable continuous-learning´
    framework for edge servers. In *2024 IEEE International Symposium on High-Performance
    Computer Architecture (HPCA)*, pp. 891–907. IEEE, 2024.

    - <span id="page-20-1"></span>National Cybersecurity Center of Excellence (NCCoE).
    Post-quantum cryptography migration: Nist sp 1800-38b preliminary draft. Technical
    report, National Institute of Standards and Technology (NIST), 2023. URL [https://www.nccoe.nist.gov/sites/default/files/2023-12/](https://www.nccoe.nist.gov/sites/default/files/2023-12/pqc-migration-nist-sp-1800-38b-preliminary-draft.pdf)
    [pqc-migration-nist-sp-1800-38b-preliminary-draft.pdf](https://www.nccoe.nist.gov/sites/default/files/2023-12/pqc-migration-nist-sp-1800-38b-preliminary-draft.pdf).
    Accessed: 2024-08-01.

    - <span id="page-20-2"></span>National Institute of Standards and Technology (NIST).
    Post-quantum cryptography. National Institute of Standards and Technology, 2024.
    URL [https://csrc.nist.gov/projects/](https://csrc.nist.gov/projects/post-quantum-cryptography)
    [post-quantum-cryptography](https://csrc.nist.gov/projects/post-quantum-cryptography).
    Accessed: 2024-08-01.

    - <span id="page-20-9"></span>Samsung Newsroom. Samsung electronics develops second-generation
    smartssd computational storage drive with upgraded processing functionality. <https://bit.ly/3PXwecP>.
    (Accessed on 11/13/2023).

    - <span id="page-20-8"></span>NVIDIA Corporation. Nvidia rtx dlss. <https://developer.nvidia.com/rtx/dlss>,
    2024. Accessed: 2024-08-02.

    - <span id="page-20-4"></span>James Jie Pan, Jianguo Wang, and Guoliang Li. Vector
    database management techniques and systems. In *Companion of the 2024 International
    Conference on Management of Data*, pp. 597–604, 2024.

    - <span id="page-21-8"></span>Inhyuk Park, Qing Zheng, Dominic Manno, Soonyeal
    Yang, Jason Lee, David Bonnie, Bradley Settlemyer, Youngjae Kim, Woosuk Chung,
    and Gary Grider. Kv-csd: A hardware-accelerated key-value store for data-intensive
    applications. *dimensions*, 30:33.

    - <span id="page-21-0"></span>Lorenzo Pichierri, Guido Carnevale, Lorenzo Sforni,
    Andrea Testa, and Giuseppe Notarstefano. A distributed online optimization strategy
    for cooperative robotic surveillance. In *2023 IEEE International Conference on
    Robotics and Automation (ICRA)*, pp. 5537–5543, 2023. doi: 10. 1109/ICRA48891.2023.10160700.

    - <span id="page-21-2"></span>Roberto Pierdicca, Michele Sasso, Flavio Tonetto,
    Francesca Bonelli, Andrea Felicetti, and Marina Paolanti. Immersive insights:
    virtual tour analytics system for understanding visitor behavior. In *Augmented
    Reality, Virtual Reality, and Computer Graphics: 8th International Conference,
    AVR 2021, Virtual Event, September 7–10, 2021, Proceedings 8*, pp. 135–155. Springer,
    2021.

    - <span id="page-21-4"></span>"premioinc". How much data do autonomous vehicles
    generate? <https://premioinc.com>. (Accessed on 11/13/2023).

    - <span id="page-21-7"></span>Abhishek Rawat, Kartik Sehgal, Amartya Tiwari, Abhishek
    Sharma, and Ashish Joshi. A novel accelerated implementation of rsa using parallel
    processing. *Journal of Discrete Mathematical Sciences and Cryptography*, 22(2):309–322,
    2019.

    - <span id="page-21-3"></span>Sylvestre-Alvise Rebuffi, Alexander Kolesnikov,
    Georg Sperl, and Christoph H Lampert. icarl: Incremental classifier and representation
    learning. In *Proceedings of the IEEE conference on Computer Vision and Pattern
    Recognition*, pp. 2001–2010, 2017.

    - <span id="page-21-11"></span>Sahand Salamat, Armin Haj Aboutalebi, Behnam Khaleghi,
    Joo Hwan Lee, Yang Seok Ki, and Tajana Rosing. Nascent: Near-storage acceleration
    of database sort on smartssd. In *The 2021 ACM/SIGDA International Symposium on
    Field-Programmable Gate Arrays*, pp. 262–272, 2021.

    - <span id="page-21-12"></span>Sahand Salamat, Hui Zhang, Yang Seok Ki, and Tajana
    Rosing. Nascent2: Generic near-storage sort accelerator for data analytics on
    smartssd. *ACM Transactions on Reconfigurable Technology and Systems (TRETS)*,
    15(2):1–29, 2022.

    - <span id="page-21-9"></span>ScaleFlux. Csd 2000. <https://scaleflux.com/products/csd-2000/>,
    a. (Accessed on 11/13/2023).

    - <span id="page-21-10"></span>ScaleFlux. Csd 3000. <https://scaleflux.com/products/csd-3000/>,
    b. (Accessed on 11/13/2023).

    - <span id="page-21-13"></span>Debendra Das Sharma. Compute express link (cxl):
    Enabling heterogeneous data-centric computing with heterogeneous memory hierarchy.
    *IEEE Micro*, 43(2):99–109, 2022a.

    - <span id="page-21-14"></span>Debendra Das Sharma. Compute express link®: An
    open industry-standard interconnect enabling heterogeneous data-centric computing.
    In *2022 IEEE Symposium on High-Performance Interconnects (HOTI)*, pp. 5–12. IEEE,
    2022b.

    - <span id="page-21-5"></span>Heng Tao Shen, Beng Chin Ooi, and Xiaofang Zhou.
    Towards effective indexing for very large video sequence database. In *Proceedings
    of the 2005 ACM SIGMOD international conference on Management of data*, pp. 730–741,
    2005.

    - <span id="page-21-1"></span>Vibhaalakshmi Sivaraman, Pantea Karimi, Vedantha
    Venkatapathy, Mehrdad Khani, Sadjad Fouladi, Mohammad Alizadeh, Frédo Durand,
    and Vivienne Sze. Gemino: Practical and robust neural compression for video conferencing.
    In *21st USENIX Symposium on Networked Systems Design and Implementation (NSDI
    24)*, pp. 569–590, 2024.

    - <span id="page-21-15"></span>Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla,
    Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai,
    and Benjamin et al. Caine. Scalability in perception for autonomous driving: Waymo
    open dataset. In *Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition*, pp. 2446–2454, 2020.

    - <span id="page-21-6"></span>Yao Tian, Xi Zhao, and Xiaofang Zhou. Db-lsh 2.0:
    Locality-sensitive hashing with query-based dynamic bucketing. *IEEE Transactions
    on Knowledge and Data Engineering*, 2023.

    - <span id="page-22-13"></span>Mahdi Torabzadehkashi, Ali Heydarigorji, Siavash
    Rezaei, Hosein Bobarshad, Vladimir Alves, and Nader Bagherzadeh. Accelerating
    hpc applications using computational storage devices. In *2019 IEEE 21st International
    Conference on High Performance Computing and Communications; IEEE 17th International
    Conference on Smart City; IEEE 5th International Conference on Data Science and
    Systems (HPCC/SmartCity/DSS)*, pp. 1878–1885, 2019a. doi: 10.1109/HPCC/SmartCity/DSS.
    2019.00259.

    - <span id="page-22-12"></span>Mahdi Torabzadehkashi, Siavash Rezaei, Ali HeydariGorji,
    Hosein Bobarshad, Vladimir Alves, and Nader Bagherzadeh. Computational storage:
    an efficient and scalable platform for big data and hpc applications. *Journal
    of Big Data*, 6:1–29, 2019b.

    - <span id="page-22-10"></span>Min-Han Tsai, Nalini Venkatasubramanian, and Cheng-Hsin
    Hsu. Analytics-aware storage of surveillance videos: Implementation and optimization.
    In *2020 IEEE International Conference on Smart Computing (SMARTCOMP)*, pp. 25–32.
    IEEE, 2020.

    - <span id="page-22-3"></span>Urban Traffic Dataset. https://github.com/edge-video-services/ekya#urban-traffic-dataset.

    - <span id="page-22-0"></span>Shaohua Wan, Zonghua Gu, and Qiang Ni. Cognitive
    computing and wireless communications on the edge for healthcare service robots.
    *Computer Communications*, 149:99–106, 2020. ISSN 0140- 3664. doi: https://doi.org/10.1016/j.comcom.2019.10.012.
    URL [https://www.sciencedirect.](https://www.sciencedirect.com/science/article/pii/S0140366419307960)
    [com/science/article/pii/S0140366419307960](https://www.sciencedirect.com/science/article/pii/S0140366419307960).

    - <span id="page-22-2"></span>"Simon Wright". Autonomous cars generate more than
    300 tb of data per year. [https:](https://www.tuxera.com/blog/autonomous-cars-300-tb-of-data-per-year/)
    [//www.tuxera.com/blog/autonomous-cars-300-tb-of-data-per-year/](https://www.tuxera.com/blog/autonomous-cars-300-tb-of-data-per-year/).
    (Accessed on 11/13/2023).

    - <span id="page-22-1"></span>www.dlapiperdataprotection.com. Sweden data collection
    & processing. [https://www.](https://www.dlapiperdataprotection.com/index.html?t=collection-and-processing&c=SE)
    [dlapiperdataprotection.com/index.html?t=collection-and-processing&c=SE](https://www.dlapiperdataprotection.com/index.html?t=collection-and-processing&c=SE).
    (Accessed on 11/21/2022).

    - <span id="page-22-11"></span>Tiantu Xu, Luis Materon Botelho, and Felix Xiaozhu
    Lin. Vstore: A data store for analytics on large videos. In *Proceedings of the
    Fourteenth EuroSys Conference 2019*, pp. 1–17, 2019.

    - <span id="page-22-5"></span>Yang Yang, Zhi Guan, Huiping Sun, and Zhong Chen.
    Accelerating rsa with fine-grained parallelism using gpu. In *Information Security
    Practice and Experience: 11th International Conference, ISPEC 2015, Beijing, China,
    May 5-8, 2015, Proceedings*, pp. 454–468. Springer, 2015.

    - <span id="page-22-9"></span>Ziyu Ying, Shulin Zhao, Sandeepa Bhuyan, Cyan Subhra
    Mishra, Mahmut T. Kandemir, and Chita R. Das. Pushing point cloud compression
    to the edge. In *2022 55th IEEE/ACM International Symposium on Microarchitecture
    (MICRO)*, pp. 282–299, 2022a. doi: 10.1109/MICRO56248. 2022.00031.

    - <span id="page-22-6"></span>Ziyu Ying, Shulin Zhao, Haibo Zhang, Cyan Subhra
    Mishra, Sandeepa Bhuyan, Mahmut T. Kandemir, Anand Sivasubramaniam, and Chita
    R. Das. Exploiting frame similarity for efficient inference on edge devices. In
    *2022 IEEE 42nd International Conference on Distributed Computing Systems (ICDCS)*,
    pp. 1073–1084, 2022b. doi: 10.1109/ICDCS54860.2022.00107.

    - <span id="page-22-4"></span>Hang Yue, Laurence R Rilett, and Peter Z Revesz.
    Spatio-temporal traffic video data archiving and retrieval system. *GeoInformatica*,
    20:59–94, 2016.

    - <span id="page-22-7"></span>Haibo Zhang, Prasanna Venkatesh Rengasamy, Shulin
    Zhao, Nachiappan Chidambaram Nachiappan, Anand Sivasubramaniam, Mahmut T. Kandemir,
    Ravi Iyer, and Chita R. Das. Race-to-sleep + content caching + display caching:
    a recipe for energy-efficient video streaming on handhelds. In *Proceedings of
    the 50th Annual IEEE/ACM International Symposium on Microarchitecture*, MICRO-50
    ''17, pp. 517–531, New York, NY, USA, 2017. Association for Computing Machinery.
    ISBN 9781450349529. doi: 10.1145/3123939.3123948. URL [https://doi.org/10.1145/](https://doi.org/10.1145/3123939.3123948)
    [3123939.3123948](https://doi.org/10.1145/3123939.3123948).

    - <span id="page-22-8"></span>Shulin Zhao, Haibo Zhang, Sandeepa Bhuyan, Cyan
    Subhra Mishra, Ziyu Ying, Mahmut T. Kandemir, Anand Sivasubramaniam, and Chita
    R. Das. Déjà view: Spatio-temporal compute reuse for'' energyefficient 360° vr
    video streaming. In *2020 ACM/IEEE 47th Annual International Symposium on Computer
    Architecture (ISCA)*, pp. 241–253, 2020. doi: 10.1109/ISCA45697.2020.00030.

    - <span id="page-23-1"></span>Shulin Zhao, Haibo Zhang, Cyan Subhra Mishra, Sandeepa
    Bhuyan, Ziyu Ying, Mahmut Taylan Kandemir, Anand Sivasubramaniam, and Chita Das.
    Holoar: On-the-fly optimization of 3d holographic processing for augmented reality.
    In *MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture*,
    MICRO ''21, pp. 494–506, New York, NY, USA, 2021. Association for Computing Machinery.
    ISBN 9781450385572. doi: 10.1145/3466752.3480056. URL <https://doi.org/10.1145/3466752.3480056>.

    - <span id="page-23-0"></span>Jingjie Zhu, Mingming Cheng, and Ying Wang. Viewer
    in-consumption engagement in proenvironmental tourism videos: A video analytics
    approach. *Journal of Travel Research*, pp. 00472875231219634, 2024.'
- title: "OpenEarable ExG: Open-Source Hardware for Ear-Based Biopotential Sensing\n\
    \  Applications"
  abstract: 'While traditional earphones primarily offer private audio spaces, so-called

    "earables" emerged to offer a variety of sensing capabilities. Pioneering

    platforms like OpenEarable have introduced novel sensing platforms targeted at

    the ears, incorporating various sensors. The proximity of the ears to the eyes,

    brain, and facial muscles has also sparked investigation into sensing

    biopotentials. However, currently there is no platform available that is

    targeted at the ears to sense biopotentials. To address this gap, we introduce

    OpenEarable ExG - an open-source hardware platform designed to measure

    biopotentials in and around the ears. OpenEarable ExG can be freely configured

    and has up to 7 sensing channels. We initially validate OpenEarable ExG in a

    study with a left-right in-ear dual-electrode montage setup with 3

    participants. Our results demonstrate the successful detection of smooth

    pursuit eye movements via Electrooculography (EOG), alpha brain activity via

    Electroencephalography (EEG), and jaw clenching via Electromyography (EMG).

    OpenEarable ExG is part of the OpenEarable initiative and is fully open-source

    under MIT license.'
  url: http://arxiv.org/abs/2410.06533v1
  keywords: ''
  document: '# OpenEarable ExG: Open-Source Hardware for Ear-Based Biopotential Sensing
    Applications


    Philipp Lepold lepold@teco.edu Karlsruhe Institute of Technology Karlsruhe, Germany


    > Kai Kunze kai@kmd.keio.ac.jp Keio Media Design Keio, Japan


    Tobias Röddiger tobias.roeddiger@kit.edu Karlsruhe Institute of Technology Karlsruhe,
    Germany


    Christoph Maurer christoph.maurer@uniklinikfreiburg.de University of Freiburg
    Freiburg, Germany


    ## Tobias King tobias.king@kit.edu Karlsruhe Institute of Technology Karlsruhe,
    Germany


    Michael Beigl michael.beigl@kit.edu Karlsruhe Institute of Technology Karlsruhe,
    Germany


    Ear-Based Biopotential Sensing Applications. In Companion of the 2024 ACM International
    Joint Conference on Pervasive and Ubiquitous Computing (UbiComp Companion ''24),
    October 5–9, 2024, Melbourne, VIC, Australia. ACM, New York, NY, USA, [5](#page-4-0)
    pages. <https://doi.org/10.1145/3675094.3678480>


    ## ABSTRACT


    While traditional earphones primarily offer private audio spaces, so-called "earables"
    emerged to offer a variety of sensing capabilities. Pioneering platforms like
    OpenEarable have introduced novel sensing platforms targeted at the ears, incorporating
    various sensors. The proximity of the ears to the eyes, brain, and facial muscles
    has also sparked investigation into sensing biopotentials. However, currently
    there is no platform available that is targeted at the ears to sense biopotentials.
    To address this gap, we introduce OpenEarable ExG - an open-source hardware platform
    designed to measure biopotentials in and around the ears. OpenEarable ExG can
    be freely configured and has up to 7 sensing channels. We initially validate OpenEarable
    ExG in a study with a left-right in-ear dualelectrode montage setup with 3 participants.
    Our results demonstrate the successful detection of smooth pursuit eye movements
    via Electrooculography (EOG), alpha brain activity via Electroencephalography
    (EEG), and jaw clenching via Electromyography (EMG). OpenEarable ExG is part of
    the OpenEarable initiative and is fully open-source under MIT license.


    # CCS CONCEPTS


    • Hardware → Emerging technologies; • Human-centered computing → Ubiquitous and
    mobile computing systems and tools; Ubiquitous and mobile devices;


    ## KEYWORDS


    open-source; earables; hearables; bio-potential; Electrooculography; Electroencephalography;
    Electromyography; EOG; EEG; EMG


    #### ACM Reference Format:


    Philipp Lepold, Tobias Röddiger, Tobias King, Kai Kunze, Christoph Maurer, and
    Michael Beigl. 2024. OpenEarable ExG: Open-Source Hardware for


    UbiComp Companion ''24, October 5–9, 2024, Melbourne, VIC, Australia


    © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
    ACM ISBN 979-8-4007-1058-2/24/10


    <https://doi.org/10.1145/3675094.3678480>


    ### 1 INTRODUCTION AND RELATED WORK


    The head offers unique opportunities for integrating sensing and actuation [\[3\]](#page-4-1).
    As the primary location for many of our senses including sight, sound, touch,
    taste, and smell - the head is an intriguing focal point for innovative wearables,
    such as Earables. Earables are earphones that offer functionalities beyond audio
    in- and output [\[22\]](#page-4-2). Röddiger et al. [\[22\]](#page-4-2) have shown
    that in past earables research, various sensing principles have been investigated
    including motion, audio, optical, and environmental sensor. Another sensing principle
    that has been investigated broadly on the ears are biopotentials including Electronencephalography
    (EEG) [\[16\]](#page-4-3), Electrooculography (EOG) [\[13,](#page-4-4) [17\]](#page-4-5),
    Electrocardiography (ECG) [\[26\]](#page-4-6), and Electromyography (EMG) [\[6\]](#page-4-7).
    Phenomena that can be detected using biopotentials measured at the ears include
    EEG standard protocols such as SSVEP [\[2\]](#page-4-8) or AAR [\[7\]](#page-4-9).
    Concrete applications based on ear biopotentials include seizures detection [\[12\]](#page-4-10),
    jaw clenching monitoring [\[11\]](#page-4-11), and tracking eating [\[6\]](#page-4-7).


    While there are research platforms such as OpenEarable [\[23\]](#page-4-12), eSense
    [\[15\]](#page-4-13), and the OpenBCI-based Open ExG Headphones [? ] for research
    on different ear-based sensing applications, there is currently no open-source
    research platform targeted at the ears to measure biopotentials in-ear. Therefore,
    in this paper we introduce OpenEarable ExG, an open-source platform to measure
    biopotentials. Our hardware is based on OpenEarable v1.3 [\[23\]](#page-4-12)
    and OpenBCI [\[21\]](#page-4-14). We provide the schematics, PCB design files
    and CAD models for a device that sits on the neck able to record biopotentials.
    We support up to 7 sensing channels (incl. 1 reference) plus ground. In a study,
    we showcase our system for EEG, EMG and EOG applications. The device can also
    be extended freely via two connectors and 2×10 jumper pins. Overall, the contributions
    of our paper are:


    - (1) OpenEarable ExG: an open-source, extensible platform for sensing biopotentials
    in and around the ears

    - (2) an evaluation with 3 participants showing that OpenEarable ExG can measure
    EEG, EMG, and EOG phenomena


    Permission to make digital or hard copies of all or part of this work for personal
    or classroom use is granted without fee provided that copies are not made or distributed
    for profit or commercial advantage and that copies bear this notice and the full
    citation on the first page. Copyrights for components of this work owned by others
    than the author(s) must be honored. Abstracting with credit is permitted. To copy
    otherwise, or republish, to post on servers or to redistribute to lists, requires
    prior specific permission and/or a fee. Request permissions from permissions@acm.org.


    UbiComp Companion ''24, October 5–9, 2024, Melbourne, VIC, Australia Philipp Lepold
    et al.


    ![](_page_1_Figure_2.jpeg)


    Figure 1: An overview of the system architecture of OpenEarable ExG which builds
    upon OpenEarable v1.3 [\[23\]](#page-4-12).


    #### 2 DESIGN PRINCIPLES


    Our vision is to establish the OpenEarable ExG as a versatile opensource platform.
    To achieve this, we adhere to the best practices of the Open-Source Hardware Association
    (OSHWA) [\[20\]](#page-4-15) by (i) sharing all original hardware design source
    files and ready-to-view outputs (e.g. PDFs); (ii) using freely available tools,
    common production processes and widely available components for all designs; (iii)
    open-sourcing all firm- and software; (iv) providing photos and instructions how
    to make and use the hardware; (v) licensing all resources under MIT license[1](#page-1-0)
    . Components are selected based on proven parts from related hardware which are
    known to have good availability, primarily based on OpenEarable 1.3 [\[23\]](#page-4-12).


    #### 3 HARD- AND SOFTWARE


    The following sections describe the hard- and software, and the mechanical assembly
    of OpenEarable ExG as well as the extensibility options of the platform via different
    connectors.


    #### 3.1 OpenEarable Core


    At the core of OpenEarable ExG is the "OpenEarable 1.3" by Röddiger et al. [\[23\]](#page-4-12).
    It is based on the NINA-B306-00B module containing the Nordic nRF52840 [\[25\]](#page-4-16),
    a single core ARM Cortex-M4 microcontroller. It also integrates the Bosch BMX160
    9-axis IMU (accelerometer, gyroscope, magnetometer) [\[8\]](#page-4-17) able to
    filter out motion artifacts, a charging controller (Microchip Technology MCP73831T
    [\[18\]](#page-4-18)), a slide switch to turn the device on and off, an RGB status
    LED, and a charging indicator LED. Lastly, OpenEarable ExG also has a digital-to-analog
    converter (MAX98357AETE [\[5\]](#page-4-19)) to output audio signals and a microSD
    card slot to record data at high sampling rates locally.


    #### 3.2 ExG Hardware


    The following subsections introduce the ExG specific hardware of OpenEarable ExG.


    3.2.1 Analog-to-Digital Converter. The most critical component of OpenEarable
    ExG is the analog digital converter. We chose the Analog Devices AD7124-4, 4-channel
    (4 differential/7 pseudo differential inputs), 24-bit sigma-delta ADC [\[4\]](#page-4-20)
    mainly due to its high resolution (similar to state-of-the-art devices like the
    OpenBCI [\[21\]](#page-4-14)), its very high gain stage, and its built-in 50 Hz/60
    Hz rejection of up to 130 dB. This enables OpenEarable ExG to directly use the
    sampled data on device, without any latency due to digital filtering, for example
    for on-device classification of different ExG phenomena. The ADC needs to have
    sufficiently good resolution to sample the small voltage differences and, together
    with an in-amp gain of ~50, in a 3.3 V system at least 17 bit are necessary so
    that artifacts are not suppressed by quantization [\[24\]](#page-4-21). Among
    all alternatives, the AD7124-4 is (as of today) also consistently in stock and
    relatively cheap (approx. USD 12.73). The AD7124-4 features three power modes,
    including one low power mode which uses less than 1 mW, making it an excellent
    choice for a wearables. ADC AIN0 channel is connected to the virtual ground of
    1.65 V (VDD/2), allowing it to measure the difference between the reference pin
    of OpenEarable ExG and Electrode 1 & 2 (ADC channels AIN1 and AIN2) which both
    are amplified by instrumentation amplifiers before reaching the ADC. This leaves
    ADC channels AIN3 - AIN7 unoccupied and accessible via the ExG pin headers (see
    [Figure 2.](#page-2-0)A for the pinout).


    3.2.2 Instrumentation and Operational Amplifier. To achieve a good common mode
    rejection (120 dB, VCM < 5V) between the electrodes and even better 50 Hz/60 Hz
    rejection, two Cosine COSINA333 [\[9\]](#page-4-22) rail-to-rail instrumentation
    amplifiers are incorporated in Open-Earable ExG, between Electrode 1 & 2 and the
    Analog-to-Digital-Converter. These in-amps have been chosen due to their well
    suited performance and capability to operate with 3.3 V as well as their


    <span id="page-1-0"></span><sup>1</sup><https://github.com/OpenEarable/open-earable-ExG>


    OpenEarable ExG: Open-Source Hardware for Ear-Based Biopotential Sensing Applications
    UbiComp Companion ''24, October 5–9, 2024, Melbourne, VIC, Australia


    <span id="page-2-0"></span>![](_page_2_Figure_2.jpeg)


    Figure 2: (A) PCB and components of OpenEarable ExG; (B) 3D-printed in-ear plugs
    based on Dätwyler SoftPulse in-ear electrodes [\[10\]](#page-4-23) (C) assembled
    device with 3D-printed housing and custom in-ear electrodes; (D) user wearing
    OpenEarable ExG.


    ability to set the gain using only one resistor, minimising space on the PCB.
    The gain of ~50 is set through a 2.2 kΩ resistor. This gain alongside with the
    24-bit resolution in a 3.3 V powered device allows for a theoretical minimal detectable
    voltage difference as low as ~4 nV between the reference and measurement electrodes.


    To provide a stable virtual ground of 1.65 V (VDD/2), which enables measuring
    positive as well as negative differences between the reference and measurement
    electrode, a voltage follower circuit is incorporated, using the Willsemi WS72324Q
    operational amplifier. The WS72324Q has rail-to-rail capabilities, operates at
    3.3 V and is available in a tiny four-opamp-in-one QFN3x3-16L package, making
    it ideal for OpenEarable ExG. The Driven-Right-Leg active ground also needs two
    opamp circuits, leaving one circuit unused.


    3.2.3 Driven-Right-Leg Active Ground. To increase the 50 Hz/60 Hz rejection even
    further and provide better signal quality, a Driven-Right-Leg (DRL) circuit is
    integrated into OpenEarable ExG and available via the (active) GND pin, as seen
    in [Figure 2.](#page-2-0)A. This circuit senses the 50 Hz/60 Hz noise at the instrumentation
    amplifier stage and inverts this signal to feed it back to the body via the GND
    electrode reducing/canceling the 50 Hz/60 Hz noise [\[1\]](#page-4-24). This is,
    simplified, comparable to noise cancellation in modern headphones. The DRL functionality
    can be disabled via the DRL switch (refer to [Figure 2.](#page-2-0)A) so that
    the GND pin only outputs the virtual ground voltage of 1.65 V (VDD/2).


    3.2.4 Pin-Assignment. OpenEarable ExG features 10 header 2.54 mm pitch jumper
    pins on each side. The header pins provide easy access to connect electrodes.
    The pinout is detailed in [Figure 2.](#page-2-0)A.


    3.2.5 Earpiece and Mechanical Design. The earpieces of OpenEarable ExG are custom
    made. We designed a 3D-printed enclosure


    into which a male 3.5 mm banana plug with cable is glued into (see [Figure 2.](#page-2-0)C).
    The eartips are Dätwyler SoftPulse in-ear electrodes [\[10\]](#page-4-23) which
    are simply plugged onto the connector. This design is lightweight, comfortable
    to wear, looks like standard earphones and the electrodes can be easily swapped
    for different users.


    The enclosure of OpenEarable ExG (see [Figure 2.](#page-2-0)B) is 3D-printed.
    It leaves all necessary ports and IO available from the outside and encloses the
    LiPo battery. All parts are printed with a Formlabs 3+ SLA 3D printer and the
    CAD files are open-source.


    3.2.6 Extensibility. OpenEarable ExG features 10 symmetrical header pins on each
    side. These provide easy access via jumper wires to the two inputs that have an
    instrumentation amplifier stage and their reference, the five other inputs that
    are directly attached to the ADC as well as the virtual ground and a driven right
    leg output. Moreover, we ensure that the platform remains extensible by adding
    the 8-pin and 4-pin connector of OpenEarable 1.3 [\[23\]](#page-4-12) which supports
    PDM microphones, I2C devices, analog audio output, and generalpurpose IO. This
    will enable researcher to integrate OpenEarable ExG in diverse research settings,
    e.g. including audio applications or the integration of a variety of biosensing
    principles.


    #### 3.3 Software


    OpenEarable ExG extends the firmware of OpenEarable 1.3 which is based on Arduino
    [\[23\]](#page-4-12). For data recording, different Python scripts are available
    to record data via Bluetooth or wired via USB Serial. Over serial, sampling rates
    of up to 19,200 SPS are possible (limited by ADC hardware), over BLE sampling
    rates of up to 500 SPS are possible. The firmware and recording software are available
    in the GitHub repository of OpenEarable ExG<sup>1</sup> .


    UbiComp Companion ''24, October 5–9, 2024, Melbourne, VIC, Australia Philipp Lepold
    et al.


    <span id="page-3-0"></span>![](_page_3_Figure_2.jpeg)


    Figure 3: (1.A) - (1.F) display brain activity measured via ear EEG for three
    participants, with eyes open and closed, revealing increased alpha activity when
    eyes are closed. (2.A) - (2.F) illustrate the rise in signal amplitude measured
    by ear EMG during jaw muscle clenching for the same participants. (3.A) - (3.E)
    present smooth pursuit signals measured via ear EOG, showing signal deflection
    in opposite directions based on the participants'' gaze direction.


    #### 4 APPLICATIONS


    To demonstrate the versatility of OpenEarable ExG, we present its effectiveness
    in three applications: brain activity sensing via EEG, jaw clenching detection
    via EMG and eye tracking via EOG. We recruited three male participants through
    a sample of convenience (mean age 24.0 years, SD = 1.4 years). The in-ear electrodes
    were placed in the left and right ears, with the right ear serving as reference
    and the left ear as measurement electrode (using Electrode 1 with in-amp stage).
    No ground electrode was used. The results of this study can be seen in [Figure
    3.](#page-3-0)


    #### 4.1 EEG: Brain Activity Sensing


    Previous studies have established that alpha band power in brain activity increases
    when the eyes are closed [\[14\]](#page-4-25). Accordingly, we asked participants
    to fixate on a point for two minutes with their eyes open on a computer screen,
    and then close their eyes for two minutes. [Figure 3.](#page-3-0)1 illustrates
    a clear increase in alpha power for all three participants when comparing the
    eyes-open condition to the eyes-closed condition. This suggests OpenEarable ExG
    offers the foundation for the exploration of wearable EEG scenarios.


    #### 4.2 EMG: Jaw Clenching


    The ears are situated near several muscular structures of the head and face. Jaw
    clenching, which often occurs during sleep or high stress, involves excessive
    pressing of the teeth. In our study, participants were asked to clench their teeth
    for 10 seconds as hard as they comfortably can. [Figure 3.](#page-3-0)2 presents
    sample plots from each participant, highlighting the noise produced by the flexing
    of the masseter muscle, which is located close to the in-ear electrodes. The the
    differences compared to the resting state signal are obvious across all participants.


    #### 4.3 EOG: Eye Tracking


    Eye gaze and in particular EOG is an interesting parameter to diagnose imbalance
    disorders [\[19\]](#page-4-26). To understand gaze tracking capabilities of OpenEarable
    ExG, participants were seated with their heads positioned in a chin rest, 40 cm
    away from a 23-inch 1920 × 1080 px screen, which was centered at eye level. By
    following a point on the screen moving horizontally, participants performed a
    smooth pursuit with an angle of around 52.8° in 0.5 s both to the left and right
    respectively, repeating this 15 times per side. The resulting signal deflections
    are shown in [Figure 3.](#page-3-0)3. It can be seen that OpenEarable ExG is perfectly
    capable of measuring EOG signals across all study participants.


    #### 5 CONCLUSION


    In this paper, we introduced OpenEarable ExG, an open-source hardware platform
    for ear-based biopotential sensing applications. The key contributions of this
    work include: Our results show that OpenEarable ExG is capable of detecting standard
    EEG phenomena like alpha brain activity, jaw clenching via EMG, and smooth pursuit
    eye movements via EOG, all using a simple in-ear electrode configuration. This
    versatility highlights the power and ease of use of ear-based biopotential sensing
    using OpenEarable ExG for a wide range of applications, including health monitoring,
    humancomputer interaction, and cognitive state assessment. By making OpenEarable
    ExG fully open-source under the MIT license, we aim to lower the barrier to entry
    for researchers and developers interested in exploring ear-based biosensing. The
    extensible nature of the platform allows for easy customization and integration
    with other sensors or systems, opening up possibilities for novel multi-modal
    sensing applications.


    <span id="page-4-0"></span>OpenEarable ExG: Open-Source Hardware for Ear-Based
    Biopotential Sensing Applications UbiComp Companion ''24, October 5–9, 2024, Melbourne,
    VIC, Australia


    #### ACKNOWLEDGMENTS


    This work was partially funded by the Deutsche Forschungsgemeinschaft (DFG, German
    Research Foundation) – GRK2739/1 - Project Nr. 447089431 – Research Training Group:
    KD2School – Designing Adaptive Systems for Economic Decisions and by the Carl-Zeiss-Stiftung
    (Carl-Zeiss-Foundation) as part of the project "JuBot - Staying young with robots".


    #### REFERENCES


    - <span id="page-4-24"></span>[1] Venkatesh Acharya. 2011. Improving common-mode
    rejection using the right-leg drive amplifier. Texas Instruments (2011), 1–11.

    - <span id="page-4-8"></span>[2] JW Ahn, Y Ku, DY Kim, J Sohn, J-H Kim, and HC
    Kim. 2018. Wearable in-the-ear EEG system for SSVEP-based brain–computer interface.
    Electronics Letters 54, 7 (2018), 413–414.

    - <span id="page-4-1"></span>[3] Oliver Amft, Florian Wahl, Shoya Ishimaru, and
    Kai Kunze. 2015. Making Regular Eyeglasses Smart. IEEE Pervasive Computing 14,
    3 (2015), 32–43. [https://doi.org/](https://doi.org/10.1109/MPRV.2015.60) [10.1109/MPRV.2015.60](https://doi.org/10.1109/MPRV.2015.60)

    - <span id="page-4-20"></span>[4] Analog Devices. 2017. AD7124-4: 4-Channel, Low
    Noise, Low Power, 24-Bit, Sigma-Delta ADC with PGA. [https://www.analog.com/media/en/technical](https://www.analog.com/media/en/technical-documentation/data-sheets/ad7124-4.pdf)[documentation/data-sheets/ad7124-4.pdf](https://www.analog.com/media/en/technical-documentation/data-sheets/ad7124-4.pdf)
    Accessed: 2024-06-25.

    - <span id="page-4-19"></span>[5] Analog Devices, Inc. 2021. MAX98357A/MAX98357B.
    Analog Devices, Inc. [https://www.analog.com/media/en/technical-documentation/data-sheets/](https://www.analog.com/media/en/technical-documentation/data-sheets/max98357a-max98357b.pdf)
    [max98357a-max98357b.pdf](https://www.analog.com/media/en/technical-documentation/data-sheets/max98357a-max98357b.pdf)
    Document 19-6416; Rev 4.

    - <span id="page-4-7"></span>[6] Shengjie Bi, Tao Wang, Ellen Davenport, Ronald
    Peterson, Ryan Halter, Jacob Sorber, and David Kotz. 2017. Toward a wearable sensor
    for eating detection. In Proceedings of the 2017 workshop on wearable systems
    and applications. 17–22.

    - <span id="page-4-9"></span>[7] Martin G Bleichner and Stefan Debener. 2017.
    Concealed, unobtrusive earcentered EEG acquisition: cEEGrids for transparent EEG.
    Frontiers in human neuroscience 11 (2017), 163.

    - <span id="page-4-17"></span>[8] Bosch Sensortec. 2015. BMX160 Small, low power
    inertial measurement unit. Bosch Sensortec GmbH. [https://www.mouser.com/pdfdocs/BST-BMX160-DS000-](https://www.mouser.com/pdfdocs/BST-BMX160-DS000-11.pdf)
    [11.pdf](https://www.mouser.com/pdfdocs/BST-BMX160-DS000-11.pdf) Document Number:
    BST-BMX160-DS000-11.

    - <span id="page-4-22"></span>[9] COSINE. 2024. COSINA333MRB Datasheet. [https://wmsc.lcsc.com/wmsc/](https://wmsc.lcsc.com/wmsc/upload/file/pdf/v2/lcsc/2402190355_COSINE-COSINA333MRB_C17702539.pdf)
    [upload/file/pdf/v2/lcsc/2402190355\\_COSINE-COSINA333MRB\\_C17702539.pdf](https://wmsc.lcsc.com/wmsc/upload/file/pdf/v2/lcsc/2402190355_COSINE-COSINA333MRB_C17702539.pdf)
    Accessed: 2024-07-03.

    - <span id="page-4-23"></span>[10] Datwyler. 2024. SoftPulse Electrodes. [https://datwyler.com/company/](https://datwyler.com/company/innovation/softpulse/products)
    [innovation/softpulse/products](https://datwyler.com/company/innovation/softpulse/products)
    Accessed: 2024-07-03.

    - <span id="page-4-11"></span>[11] Hao Dong, Paul M Matthews, and Yike Guo. 2016.
    A new soft material based in-the-ear EEG recording technique. In 2016 38th Annual
    International Conference of the IEEE Engineering in Medicine and Biology Society
    (EMBC). IEEE, 5709–5712.

    - <span id="page-4-10"></span>[12] Ying Gu, Evy Cleeren, Jonathan Dan, Kasper
    Claes, Wim Van Paesschen, Sabine Van Huffel, and Borbála Hunyadi. 2017. Comparison
    between scalp EEG and


    behind-the-ear EEG for development of a wearable seizure detection system for
    patients with focal epilepsy. Sensors 18, 1 (2017), 29.


    - <span id="page-4-4"></span>[13] L''uboš Hládek, Bernd Porr, and W Owen Brimijoin.
    2018. Real-time estimation of horizontal gaze angle by saccade integration using
    in-ear electrooculography. Plos one 13, 1 (2018), e0190420.

    - <span id="page-4-25"></span>[14] Wiremu Hohaia, Blake W Saurels, Alan Johnston,
    Kielan Yarrow, and Derek H Arnold. 2022. Occipital alpha-band brain waves when
    the eyes are closed are shaped by ongoing visual processes. Scientific reports
    12, 1 (2022), 1194.

    - <span id="page-4-13"></span>[15] Fahim Kawsar, Chulhong Min, Akhil Mathur, and
    Alessandro Montanari. 2018. Earables for personal-scale behavior analytics. IEEE
    Pervasive Computing 17, 3 (2018), 83–89.

    - <span id="page-4-3"></span>[16] Preben Kidmose, David Looney, Michael Ungstrup,
    Mike Lind Rank, and Danilo P Mandic. 2013. A study of evoked potentials from ear-EEG.
    IEEE Transactions on Biomedical Engineering 60, 10 (2013), 2824–2830.

    - <span id="page-4-5"></span>[17] Hiroyuki Manabe, Masaaki Fukumoto, and Tohru
    Yagi. 2013. Conductive rubber electrodes for earphone-based eye gesture input
    interface. In Proceedings of the 2013 International Symposium on Wearable Computers.
    33–40.

    - <span id="page-4-18"></span>[18] Microchip Technology Inc. 2014. MCP73831/2.
    Microchip Technology Inc. [https://eu.mouser.com/datasheet/2/268/MCP73831\\_Family\\_Data\\_Sheet\\_](https://eu.mouser.com/datasheet/2/268/MCP73831_Family_Data_Sheet_DS20001984H-3441711.pdf)
    [DS20001984H-3441711.pdf](https://eu.mouser.com/datasheet/2/268/MCP73831_Family_Data_Sheet_DS20001984H-3441711.pdf)
    Document DS20001984H.

    - <span id="page-4-26"></span>[19] Jacob L Newman, John S Phillips, and Stephen
    J Cox. 2021. Detecting positional vertigo using an ensemble of 2D convolutional
    neural networks. Biomedical Signal Processing and Control 68 (2021), 102708.

    - <span id="page-4-15"></span>[20] Open Source Hardware Association. [n. d.].
    Open Source Hardware Association (OSHWA). [https://www.oshwa.org/.](https://www.oshwa.org/)
    Accessed: 2024-07-04.

    - <span id="page-4-14"></span>[21] OpenBCI. 2023. Cyton Biosensing Board. [https://shop.openbci.com/products/](https://shop.openbci.com/products/cyton-biosensing-board-8-channel)
    [cyton-biosensing-board-8-channel.](https://shop.openbci.com/products/cyton-biosensing-board-8-channel)
    Accessed: 2024-07-03.

    - <span id="page-4-2"></span>[22] Tobias Röddiger, Christopher Clarke, Paula Breitling,
    Tim Schneegans, Haibin Zhao, Hans Gellersen, and Michael Beigl. 2022. Sensing
    with earables: A systematic literature review and taxonomy of phenomena. Proceedings
    of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 6, 3 (2022),
    1–57.

    - <span id="page-4-12"></span>[23] Tobias Röddiger, Tobias King, Dylan Ray Roodt,
    Christopher Clarke, and Michael Beigl. 2022. Openearable: Open hardware earable
    sensing platform. In Adjunct Proceedings of the 2022 ACM International Joint Conference
    on Pervasive and Ubiquitous Computing and the 2022 ACM International Symposium
    on Wearable Computers. 246–251.

    - <span id="page-4-21"></span>[24] M Teplan. 2002. FUNDAMENTALS OF EEG MEASUREMENT.
    MEASUREMENT SCIENCE REVIEW 2 (2002).

    - <span id="page-4-16"></span>[25] u-blox. 2023. NINA-B3 series. u-blox AG. [https://content.u-blox.com/sites/](https://content.u-blox.com/sites/default/files/NINA-B3_DataSheet_UBX-17052099.pdf)
    [default/files/NINA-B3\\_DataSheet\\_UBX-17052099.pdf](https://content.u-blox.com/sites/default/files/NINA-B3_DataSheet_UBX-17052099.pdf)
    Document UBX-17052099.

    - <span id="page-4-6"></span>[26] Wilhelm Von Rosenberg, Theerasak Chanwimalueang,
    Valentin Goverdovsky, Nicholas S Peters, Christos Papavassiliou, and Danilo P
    Mandic. 2017. Hearables: Feasibility of recording cardiac rhythms from head and
    in-ear locations. Royal Society open science 4, 11 (2017), 171214.'
- title: "Evaluation of Run-Time Energy Efficiency using Controlled Approximation\n\
    \  in a RISC-V Core"
  abstract: 'The limited energy available in most embedded systems poses a significant

    challenge in enhancing the performance of embedded processors and

    microcontrollers. One promising approach to address this challenge is the use

    of approximate computing, which can be implemented in both hardware and

    software layers to balance the trade-off between performance and power

    consumption. In this study, the impact of dynamic hardware approximation

    methods on the run-time energy efficiency of a RISC-V embedded processor with

    specialized features for approximate computing is investigated. The results

    indicate that the platform achieves an average energy efficiency of 13.3

    pJ/instruction at a 500MHz clock frequency adhering approximation in 45nm CMOS

    technology. Compared to accurate circuits and computation, the approximate

    computing techniques in the processing core resulted in a significant

    improvement of 9.21% in overall energy efficiency, 60.83% in multiplication

    instructions, 14.64% in execution stage, and 9.23% in overall power

    consumption.'
  url: http://arxiv.org/abs/2410.07027v1
  keywords: Embedded systems, RISC-V, approximate computing, low-power design, energy
    efficient embedded systems, very large scale integration
  document: '# Evaluation of Run-Time Energy Efficiency using Controlled Approximation
    in a RISC-V Core


    A. Delavari F. Ghoreishy H. S. Shahhoseini S. Mirzakuchaki


    *School of Electrical Engineering*


    *Iran University of Science and Technology*


    Tehran, Iran


    {arvin delavari, faraz ghoreishy}@elec.iust.ac.ir, {shahhoseini, m kuchaki}@iust.ac.ir


    arXiv:2410.07027v1 [cs.AR] 9 Oct 2024


    *Abstract*—The limited energy available in most embedded systems poses a significant
    challenge in enhancing the performance of embedded processors and microcontrollers.
    One promising approach to address this challenge is the use of approximate computing,
    which can be implemented in both hardware and software layers to balance the trade-off
    between performance and power consumption. In this study, the impact of dynamic
    hardware approximation methods on the run-time energy efficiency of a RISC-V embedded
    processor with specialized features for approximate computing is investigated.
    The results indicate that the platform achieves an average energy efficiency of
    13.3 pJ/instruction at a 500MHz clock frequency adhering approximation in 45nm
    CMOS technology. Compared to accurate circuits and computation, the approximate
    computing techniques in the processing core resulted in a significant improvement
    of 9.21% in overall energy efficiency, 60.83% in multiplication instructions,
    14.64% in execution stage, and 9.23% in overall power consumption.


    *Index Terms*—Embedded systems, RISC-V, approximate computing, low-power design,
    energy efficient embedded systems, very large scale integration


    #### I. INTRODUCTION


    The increasing demand for energy-efficient and low-power embedded systems has
    driven research efforts towards enhancing the overall performance of embedded
    processors, especially microcontrollers. This is an important challenge, as the
    limited energy available in most embedded applications imposes constraints on
    the design and operation of these devices. A promising approach to address this
    challenge is the use of approximate computing, which can be implemented in both
    hardware and software layers to balance the trade-off between performance and
    power consumption. Approximation is allowed in error-resilient applications such
    as image processing [\[1\]](#page-5-0), neural networks [\[2\]](#page-5-1), Processing
    in Memory (PIM) [\[3\]](#page-5-2) and etc.


    The RISC-V instruction set architecture has emerged as a popular choice for embedded
    processor designers, offering a flexible platform for a wide range of applications
    [\[4\]](#page-5-3) [\[5\]](#page-5-4). The integration of RISC-V and approximate
    computing techniques presents an opportunity to further improve the energy efficiency
    and performance of embedded processors.


    This study investigates the impact of dynamic hardware approximation methods on
    the run-time energy efficiency of a RISC-V embedded processor with specialized
    features for approximate computing. The results demonstrate that the platform
    achieves a significant improvement in energy efficiency, area, and power consumption
    compared to accurate circuits and computation, highlighting the potential of this
    approach for enhancing the performance of embedded systems. Main contributions
    of this paper are as follows:


    - Utilizing a RISC-V embedded processor platform Hardware/Software interface method
    using standard RISC-V CSRs and special programming conventions for approximate
    computing.

    - Design of error-configurable adder/subtracter and multiplier circuit for the
    experimental setup.

    - Analysis and evaluation of approximation and faultinjection methods efficiency
    at core level, in an embedded processor.


    In the remainder of this paper, we will present: a review of related previous
    works in this subject (Section [II\)](#page-0-0), a description of the experimental
    setup used in this research (Section [III\)](#page-1-0), the results of the experiments
    and a comparison of the findings (Section [IV\)](#page-3-0), and an overall summary
    of the project along with concluding remarks (Section [V\)](#page-4-0).


    #### II. RELATED WORKS


    <span id="page-0-0"></span>Approximate computing has enhanced energy efficiency
    in many applications in recent years. There has been exploration into combining
    RISC-V with approximate computing methods. Software adaptations have proposed
    ISA extensions [\[6\]](#page-5-5) and multi-level accuracy control mechanisms
    to enable this. The AxPIKE ISA simulator [\[7\]](#page-5-6) enables incorporating
    hardware approximation at the instruction level to assess quality-energy trade-offs.
    Other studies have characterized RISC-V architecture extensions that orchestrate
    diverse circuit-level approximation techniques, especially approximate multipliers
    [\[8\]](#page-5-7). Customized RISC-V cores have also integrated approximate multipliers
    to demonstrate energy-efficient execution of neural networks [\[9\]](#page-5-8).


    There have been investigations on the advantages of multicore systems that combine
    exact and approximate compute cores [\[10\]](#page-5-9). This heterogeneous approach
    aims to leverage the benefits of approximation while maintaining the precision
    of exact computation. [\[11\]](#page-5-10) proposes an approach to improve the
    performance of multi-core systems by configurable approximate arithmetic logic
    units, with a machine learning based framework for online power regulation and
    quality monitoring to adjust the frequency and precision of the arithmetic units
    and maximize performance.


    The proposed design here involves a RISC-V core where each execution unit can
    dynamically switch between accurate and approximate circuits and also if applicable
    control the error level of integrated approximate execution units. This combines
    the efficiency gains of dynamic approximation with the precision of exact computation
    all within a single reconfigurable core.


    #### III. EXPERIMENTAL SETUP


    <span id="page-1-0"></span>The considered platform for these experiments is an
    embedded processor named phoeniX [\[12\]](#page-5-11) which supports RV32IEM standard
    instruction set of the open-source and license-free RISC-V ISA. The chosen core
    is a 3-stage pipelined scalar processor with specialized and novel features for
    approximate computing, in order to be a general platform for fault-tolerant applications
    and approximation-enhanced low-power design studies. In this study, "fault-tolerant"
    refers to applications that can accommodate errors (specifically approximations)
    resulting from their functions and data. Pipeline stages are considered as Instruction
    Fetch (IF) and Decode (ID), Execution (EXE) and Memory access with Write-back
    (MEM/WB). The high-level architecture of the processor''s execution stage within
    the pipeline is illustrated in Fig. [1.](#page-1-1)


    ![](_page_1_Figure_5.jpeg)


    <span id="page-1-1"></span>Fig. 1. Execution stage high-level block diagram of
    the processor including accurate and approximate arithmetic circuits


    Execution stage consists of Arithmetic Logic Unit, Multiplier Unit and Divider
    Unit due to the support of RV32IEM instruction set. These three computational
    units share a unique method for adhering and controlling approximation in circuitlevel.
    The approximation level is determined from a customized special-purpose control
    status registers addressed in *0x800*, *0x801* and *0x802* in standard RISC-V
    CSR addressing space [\[5\]](#page-5-4). Each execution unit can host four arithmetic
    circuit (accurate or approximate) supported by the standard signaling conventions
    of the processor''s execution engine. The execution unit''s active circuit and
    target result along with approximation control logic for each is determined by
    the value in the CSRs, which the decoding and framing is explained further in
    this paper.


    These CSRs are named alucsr, mulcsr and divcsr, designed for enabling and controlling
    approximation level of the dynamically configurable circuits. Each field of CSRs
    is responsible for a feature in approximation control and advancement. Bit 0 is
    for enabling approximation, in which approximate arithmetic is used when signal
    value is 1, and will perform accurate arithmetic with value of 0. Bits 2 to 1
    are for determination of selected circuit, for controlling result multiplexer
    and switching off unused circuits. Bits 3 to 7 are used for dynamic truncation
    control in arithmetic circuits which may support this technique. Bits 8 to 11
    and bits 12 to 15 are also custom fields of control status registers for designer-defined
    control logic and features. In the end, bits 16 to 31 are responsible for dynamic
    error control in case of integration of error controllable arithmetic circuits
    within the core.


    Each execution unit can embed up to 4 arithmetic circuit with platform''s special
    conventions for circuit integration. Circuits can be accurate, approximate with
    static configurations and error, or dynamic error-configurable such as selected
    circuits in this study. The platform can adhere and synchronize execution process
    with different circuit which may vary in timing and other hardware specifications.
    Block diagram of the execution units'' structure is shown in Fig. [2](#page-1-2)
    In this experiment an error-configurable multiplier, alongside an accurate multiplier,
    and fully accurate division circuit is considered, because of the importance of
    multiplication and accumulation in most computational workloads, including fault-tolerant
    applications. The accurate multiplier is positioned in the default slot (Circuit
    I), while the approximate multiplier is placed in the secondary slot (Circuit
    II). The remaining slots are unutilized and remained empty in this configuration.


    ![](_page_1_Figure_11.jpeg)


    <span id="page-1-2"></span>Fig. 2. Error control, circuit switching, and hardware
    approximation level determination based on custom CSR values in EXE stage of processor


    After determination of the operating circuit through the respected CSRs of each
    execution unit, the other circuits will be switched off in order to exclude redundant
    processing of data and unnecessary static and dynamic power consumption. It is
    important to note that all execution units including adders in ALU Arithmetic
    Logic Unit, Multiplier Unit and Divider unit are capable of approximation features
    explained, except for addition required in address generation and other control
    flow instructions which uses an accurate adder because approximation is not allowed
    in such operations.


    #### *A. Dynamic Accuracy-Controllable Carry Select Adder*


    The adder/subtracter integrated within the processor, is a fast and low power
    carry select adder, with improved performance by decreasing the critical path
    in comparison with conventional design. Also, the proposed adder, depicted in
    Fig. [3,](#page-2-0) is an error-controllable design which can be used in different
    applications due to controllable accuracy of the results. The adder inside arithmetic
    logic unit is responsible for addition and subtraction of I/E extension of RISC-V
    instruction set architecture. In the experiments, the adder was set to default
    mode which enables accurate addition. In fact, the proposed adder can perform
    both accurate and approximate addition in a single circuit, without any need of
    additional hardware for circuit switching in the execution.


    ![](_page_2_Figure_4.jpeg)


    <span id="page-2-0"></span>Fig. 3. Error controllable low-power and fast carry
    select adder


    The ECA shown in Fig. [3](#page-2-0) is a 4-bit error controllable ripple carry
    adder, in which the fundamental full-adders have an additional control signal
    in comparison with conventional full adder. This full adder as shown in Fig. [4,](#page-2-1)
    can perform accurately when the error control signal is high (logical value 1),
    but may produce inaccurate result by decreasing or increasing carry value when
    the signal is low. When using approximation, the error in the final result is
    applied unevenly, as in half the input scenarios, the approximate result is lower
    than the accurate result by one, and in the other half, the approximate result
    is higher than the accurate result by one.


    #### *B. Dynamic Accuracy-Controllable Array Multiplier*


    For this setup, an original low-power approximate 8x8 unsigned multiplier with
    reduced circuital path and area in comparison with a conventional Wallace tree
    multiplier is designed. The design also incorporates the full-adder presented
    in Fig. [4.](#page-2-1) The unbiased error of the adder (having both positive


    ![](_page_2_Figure_9.jpeg)


    <span id="page-2-1"></span>Fig. 4. Proposed full-adder with error signal used
    in 4-bit ripple carry adders of the CSA circuit


    and negative error in results, in error enabled modes) improves the multiplier
    with Error Rate (ER) and Mean Relative Error Distance (MRED) [\[13\]](#page-5-12)
    [\[14\]](#page-5-13) in term of accuracy. The fulladder illustrated in Fig. [4,](#page-2-1)
    is integrated in the final addition stage, after partial product generation and
    compression stages of Wallace multiplier which is shown in Fig. [5.](#page-2-2)


    The ripple carry adder''s error in the final stage can be controlled using the
    error control signals of the full-adders. 7-bits of the 12-bit addition is controllable,
    which gives 128 configurations and error levels in one circuit, which means 128
    configurations of dynamic power consumption. The mode with all error control signals
    equal to zero is the configuration with lowest power consumption, which also has
    the highest application-level quality in error (ER and MRED). .


    ![](_page_2_Figure_13.jpeg)


    <span id="page-2-2"></span>Fig. 5. 8-bit multiplier''s high level block diagram


    The proposed Multiplier has an area of 269µm² with a range of power between 70.2µW
    and 101.3µW in different configurations. Fig. [6](#page-3-1) shows ER and MRED
    for all configurations of the proposed design.


    Table [I](#page-3-2) shows an analysis in terms of hardware (power, delay and
    area) and accuracy efficiency. The proposed design showcases the best result in
    terms of critical path delay and area occupation. Additionally, in one of the
    configurations the design has a power consumption of 70.2µW which is the lowest
    in the comparison. The design in [\[10\]](#page-5-9) produces the best results
    in accuracy efficiency criteria.


    ![](_page_3_Figure_1.jpeg)


    <span id="page-3-1"></span>Fig. 6. Error Rate and Mean Relative Error Distance
    (MRED) analysis of the proposed multiplier in all dynamic configurations (128
    Approximation Factors)


    <span id="page-3-2"></span>TABLE I HARDWARE - ACCURACY EFFICIENCY COMPARISON OF
    PROPOSED MULTIPLIER AND SIMILAR APPROXIMATE MULTIPLIERS


    | Multipliers        | Power<br>(µW)   | Delay<br>(ns) | Area<br>(µm²) | ER<br>(%)        |
    MRED<br>(%)    |

    |--------------------|-----------------|---------------|---------------|------------------|----------------|

    | Proposed<br>Design | 70.2 -<br>101.3 | 0.64          | 269.6         | 36.16
    -<br>65.06 | 0.85 -<br>8.94 |

    | [15]               | 81.2            | 0.64          | 27.8.8        | 32.42            |
    0.18           |

    | [16]               | 156.7           | 0.80          | 323.9         | 48.35            |
    1.26           |

    | [16]               | 143.5           | 0.77          | 301.6         | 69.73            |
    2.83           |


    #### *C. Hierarchical 32×32 Approximate Multiplier Design*


    For final integration of multiplier within the core, a 32 bit multiplier architecture
    consisting of several 8-bit modules is designed. The method used for creation
    of the 32-bit architecture is a hierarchical configuration of 8-bit multipliers.
    The original 8-bit multiplier is used over multiple cycles to perform 16-bit multiplication.
    This hardware is replicated four times to perform a 32-bit multiplication. There
    is also an accurate multiplier integrated within the core in order to have a comprehensive
    comparison in term of power and energy efficiency gains, using dynamic circuit
    switching feature which is presented in the paper. The accurate multiplier circuit
    is an original 32-bit multiplier implemented with DesignWare software, offered
    by Synopsys Design Compiler.


    #### IV. RESULTS AND COMPARISON


    <span id="page-3-0"></span>The processor and it''s approximate and accurate execution
    units are implemented in 45nm CMOS technology using Design Compiler, enabling
    a clock frequency of 500MHz which is considered for this set of experiments. All
    power and energy consumption data reported in this paper are extracted with post-synthesis
    simulation based on switching activity files.


    Before delving into the main experiment and comparisons, Fig. [7](#page-3-3) is
    shown for an estimation of the power distribution in the processor, excluding
    memories and register files.


    It can be concluded from Fig. [7](#page-3-3) that the execution stage has the
    highest power consumption in the inner core microarchitecture. In the target processor,
    the execution stage, including the arithmetic logic unit, multiplier unit (with
    2 multipliers), and divider unit, have the largest share of the power distribution
    chart, accounting for 80%. The results provide motivation for using approximation
    techniques not just in software or memory approaches, but also at the circuit
    level. The execution stage, excluding memories and register files, consumes the
    most power within the inner modules. This is the reason for the focus of this
    study, the impact of user-controlled circuit level approximation on the overall
    performance of a processor.


    ![](_page_3_Figure_12.jpeg)


    Note: [%] in the figure is the share within the modules (not the processor). •
    Rest of the core (pipeline latches, internal core logic etc.) = 22.6 %


    <span id="page-3-3"></span>• Modules including Register Files = 77%


    ![](_page_3_Figure_15.jpeg)


    Fig. [8](#page-3-4) presents a detailed breakdown of instruction types following
    compilation. The average code size is comprised of 221992 instructions and utilization
    of multiplication is 242 instructions per code in average. Multiplication instructions
    are 6.47% of arithmetic instruction (addition/multiplication/ division) in average.
    Approximation is enabled exclusively for multiplication instructions, specifically
    the mul and mulh instructions. Both of these instructions are classified as Rtype,
    meaning they operate with registers without involving any immediate values. They
    utilize the same circuitry, which is determined by the mulcsr setting in the program.
    In contrast, addition instructions (add and addi) operate in accurate mode.


    ![](_page_3_Figure_17.jpeg)


    <span id="page-3-4"></span>Fig. 8. Number of arithmetic and memory accessing instructions
    after code compilation using GCC compiler [\[17\]](#page-5-16) with constant options


    There is a selected set of standard applications using a notable load of multiplication
    within the calculation process. Some of these applications are regarded as fault-tolerant
    applications due to their inherent resilience against error in accumulative workload.
    As an example, 2D convolutions are used in image processing applications and filters,
    which are perfect examples of applications allowed to adhere approximation. All
    of the programs are compiled using standard GCC-GNU [\[17\]](#page-5-16) compiler
    for RISC-V with similar compilation options. Duration for execution of each code
    and the instruction count is measured using RISC-V standard CSRs in order to calculate
    energy efficiency of the processor in each application. Energy efficiency in this
    study is regarded as the consumed power in a limited time period per instructions
    executed in the program.


    Fig. [9](#page-4-1) illustrates energy efficiency in each program in pJ/instructions
    using both accurate and approximate multipliers. Circuit switching was done utilizing
    the mulcsr circuit selection field. The approximate multiplier is set to a default
    accuracy with error control field of CSR equal to 0x7E. Fig. [10](#page-4-2) shows
    the impact of approximate multiplication on total power consumption of the embedded
    processor. In every application, the approximate multiplication has decreased
    overall power consumption. The conserved power in each application highlights
    the efficiency of approximation techniques in scenarios where a reasonable margin
    of calculation error is permissible. The selected error-level (0x7E) of the multiplier
    has an MRED of 0.85% and ER of 36.2%.


    ![](_page_4_Figure_3.jpeg)


    <span id="page-4-1"></span>Fig. 9. Energy efficiency comparison of applications
    executed with accurate and approximate multiplication


    Fig. [10](#page-4-2) shows the impact of approximate multiplication on total power
    consumption of the embedded processor. In every application, the approximate multiplication
    has decreased overall power consumption. The saved power in each application showcases
    the effectiveness of approximation in applications where error in calculations
    are allowed by a reasonable margin. The selected error-level of the multiplier
    has an MRED of 0.85% and ER of 36.2%.


    Overall improvements in term of power consumption are illustrated in Fig. [11.](#page-4-3)
    This analysis was carried out from 3 separate perspectives: Total improvement
    (processor''s power consumption), Execution stage of the processor, and the Multiplier
    Unit (multiplication operation solely). The percentage of improvement in each
    considered hardware is written next to


    ![](_page_4_Figure_7.jpeg)


    <span id="page-4-2"></span>Fig. 10. Overall power consumption comparison of the
    processor, executing applications using accurate and approximate multiplication


    ![](_page_4_Figure_9.jpeg)


    <span id="page-4-3"></span>Fig. 11. Power consumption improvements by approximation,
    in the processor''s overall power, execution stage and multiplication instructions


    each column. Power consumption in each execution unit and the execution stage
    of the core are presented in Table [II](#page-5-17) and Table [III](#page-5-18)
    in both accurate and approximate modes.


    From the results, it is indicated that there is an average improvement of 9.213%
    in energy-efficiency in comparison with accurate calculation. By selecting the
    desired circuit in each execution unit though CSRs, the other circuits will be
    turned off, decreasing the dynamic and static power consumption of unused units
    near to zero. The highest improvement is for*matMul int* with 10.813% percent
    with a power consumption of 5.279mW with approximate multiplier.


    The overall power consumption during the execution stage of the pipelined processor
    is reduced by 18.05% through the use of approximate multiplication, with the multiplication
    process alone achieving a significant improvement of 71.16% within this stage.
    The most significant improvement in these metrics is resulted from the*matMul
    int* and *2Dconv3x3*.


    ## V. CONCLUSION


    <span id="page-4-0"></span>This study examines the impact of dynamic hardware
    approximation on the energy efficiency of a RISC-V embedded processor with specialized
    capabilities for approximate computing. The findings demonstrate an average 9.21%
    improvement in the processor''s run-time energy efficiency, along


    <span id="page-5-17"></span>TABLE II POWER CONSUMPTION OF EXECUTION UNITS IN ACCURATE
    MULTIPLICATION MODE


    | Applications | Power Consumption [mW] |       |       |           |  |  |

    |--------------|------------------------|-------|-------|-----------|--|--|

    |              | MUL                    | ALU   | DIV   | EXE Stage |  |  |

    | 2DConv3x3    | 0.435                  | 0.626 | 0.601 | 1.662     |  |  |

    | 2DConv5x5    | 0.438                  | 0.627 | 0.605 | 1.670     |  |  |

    | fir int      | 0.448                  | 0.598 | 0.622 | 1.668     |  |  |

    | iir int      | 0.443                  | 0.592 | 0.634 | 1.669     |  |  |

    | matMul int   | 0.437                  | 0.646 | 0.507 | 1.590     |  |  |

    | nr solver    | 0.431                  | 0.589 | 0.632 | 1.652     |  |  |

    | factorial    | 0.440                  | 0.593 | 0.615 | 1.648     |  |  |


    <span id="page-5-18"></span>TABLE III POWER CONSUMPTION OF EXECUTION UNITS IN
    APPROXIMATE MULTIPLICATION MODE


    | Applications | Power Consumption [mW] |       |       |           |  |  |

    |--------------|------------------------|-------|-------|-----------|--|--|

    |              | MUL                    | ALU   | DIV   | EXE Stage |  |  |

    | 2DConv3x3    | 0.139                  | 0.638 | 0.615 | 1.392     |  |  |

    | 2DConv5x5    | 0.184                  | 0.636 | 0.619 | 1.439     |  |  |

    | fir int      | 0.187                  | 0.607 | 0.636 | 1.43      |  |  |

    | iir int      | 0.202                  | 0.605 | 0.649 | 1.456     |  |  |

    | matMul int   | 0.126                  | 0.655 | 0.522 | 1.303     |  |  |

    | nr solver    | 0.181                  | 0.6   | 0.655 | 1.436     |  |  |

    | factorial    | 0.185                  | 0.602 | 0.626 | 1.413     |  |  |


    with a 14.64% reduction in overall power consumption during the execute stage
    of the scalar core. These results confirm that approximate computing, particularly
    through hardwareapproximated arithmetic circuits, is a promising approach for
    dynamic energy management in embedded processors. The research highlights the
    potential of such techniques to optimize power usage without compromising computational
    performance, making them particularly valuable in resourceconstrained environments.
    This study further reinforces the growing relevance of hardware approximation
    in addressing energy efficiency challenges in modern embedded systems.


    The results highlight the potential of integrating RISC-V processors and approximate
    computing for enhancing the performance of energy-constrained embedded systems
    in execution process. The ability to dynamically adjust approximation offers a
    flexible approach to manage accuracy and efficiency. Future work could explore
    extending these techniques across the processor architecture and to a wider range
    of embedded applications. Further hardware optimization and advanced error management
    could lead to even greater efficiency gains. Automating the process of accuracy
    and energy control using real-time monitoring systems within the processor can
    be a promising solution to address the accuracy/efficiency trade-off in embedded
    cores. Overall, this study demonstrates the significant outlook for RISC-V embedded
    processors leveraging approximate computing.


    #### ACKNOWLEDGMENT


    The authors would like to express their appreciation to A. M. H. Monazzah for
    their valuable insights, which have significantly enhanced the quality of this
    paper.


    ### REFERENCES


    - <span id="page-5-0"></span>[1] G. Anusha and P. Deepa, "Design of approximate
    adders and multipliers for error tolerant image processing," *Microprocessors
    and Microsystems*, vol. 72, p. 102940, 2020.

    - <span id="page-5-1"></span>[2] S. Shakibhamedan, N. Amirafshar, A. S. Baroughi,
    H. S. Shahhoseini and N. Taherinejad, "ACE-CNN: Approximate carry disregard multipliers
    for energy-efficient CNN-Based image classification," *in IEEE Transactions on
    Circuits and Systems I: Regular Papers*, 2024.

    - <span id="page-5-2"></span>[3] S. Bavikadi, P. R. Sutradhar, M. Indovina, A.
    Ganguly and S. M. P. Dinakarrao, "ReApprox-PIM: Reconfigurable approximate Look-Up-Table
    (LUT)-Based Processing-in-Memory (PIM) machine learning accelerator," *in IEEE
    Transactions on Computer-Aided Design of Integrated Circuits and Systems*, 2024.

    - <span id="page-5-3"></span>[4] A. Waterman and K. Asanovic. (2021). "The RISC-V
    Instruction Set Manual—Volume II: Privileged Architecture," [Online]. Available:
    https://riscv.org/specifications/privileged-isa/

    - <span id="page-5-4"></span>[5] E. Cui, T. Li and Q. Wei, "RISC-V Instruction
    Set Architecture Extensions: A Survey," *in IEEE Access*, vol. 11, pp. 24696-24711,
    2023.

    - <span id="page-5-5"></span>[6] T. Trevisan J et al., "Approxrisc: An approximate
    computing infrastructure for RISC-V," *RISC-V Workshop in Barcelona*, May 2018,
    poster.

    - <span id="page-5-6"></span>[7] I. Felzmann, J. F. Filho and L. Wanner, "AxPIKE:
    Instruction-level Injection and Evaluation of Approximate Computing," *2021 Design,
    Automation & Test in Europe Conference & Exhibition (DATE)*, Grenoble, France,
    2021, pp. 491-494.

    - <span id="page-5-7"></span>[8] A. Verma, P. Sharma and B. P. Das, "RISC-V Core
    with Approximate Multiplier for Error-Tolerant Applications," *2022 25th Euromicro
    Conference on Digital System Design (DSD)*, Maspalomas, Spain, 2022, pp. 239-246.

    - <span id="page-5-8"></span>[9] F. Guella, E. Valpreda, M. Caon, G. Masera and
    M. Martina, "MARLIN: A Co-Design Methodology for Approximate ReconfigurabLe Inference
    of Neural Networks at the Edge," *in IEEE Transactions on Circuits and Systems
    I: Regular Papers*, doi: 10.1109/TCSI.2024.3365952.

    - <span id="page-5-9"></span>[10] A. S. Baroughi, S. Huemer, H. S. Shahhoseini
    and N. TaheriNejad, "AxE: An approximate-exact multi-processor system-on-chip
    platform," *2022 25th Euromicro Conference on Digital System Design (DSD)*, pp.
    60-66, 2022.

    - <span id="page-5-10"></span>[11] S. A. K. Gharavi and S. Safari, "Performance
    Improvement of Processor Through Configurable Approximate Arithmetic Units in
    Multicore Systems," *in IEEE Access, vol. 12*, pp. 43907-43917, 2024.

    - <span id="page-5-11"></span>[12] A. Delavari, F. Ghoreishy, H. S. Shahhoseini
    and S. Mirzakuchaki, "A Reconfigurable Approximate Computing RISC-V Platform for
    Fault-Tolerant Applications," *2024 27th Euromicro Conference on Digital System
    Design (DSD)*, Paris, France, 2024, pp. 81-89.

    - <span id="page-5-12"></span>[13] A. G. M. Strollo, E. Napoli, D. D. Caro, N.
    Petra, and G. D. Meo, "Comparison and extension of approximate 4–2 compressors
    for lowpower approximate multipliers," *IEEE Trans. Circuits Syst. I, Reg. Papers*,
    vol. 67, no. 9, pp. 3021–3034, Sep. 2020.

    - <span id="page-5-13"></span>[14] P. Yin, C. Wang, H. Waris, W. Liu, Y. Han,
    and F. Lombardi, "Design and analysis of energy-efficient dynamic range approximate
    logarithmic multipliers for machine learning," *IEEE Trans. Sustain. Comput.*,
    vol. 6, no. 4, pp. 612–625, Oct. 2021.

    - <span id="page-5-14"></span>[15] N. Amirafshar, A. S. Baroughi, H. S. Shahhoseini
    and N. TaheriNejad, "An Approximate Carry Disregard Multiplier with Improved Mean
    Relative Error Distance and Probability of Correctness," *2022 25th Euromicro
    Conference on Digital System Design (DSD)*, Maspalomas, Spain, 2022, pp. 46-52.

    - <span id="page-5-15"></span>[16] H. Waris, C. Wang, W. Liu, J. Han and F. Lombardi,
    "Hybrid Partial Product-Based High-Performance Approximate Recursive Multipliers,"
    *in IEEE Transactions on Emerging Topics in Computing*, vol. 10, no. 1, pp. 507-513,
    1 Jan.-March 2022.

    - <span id="page-5-16"></span>[17] RISC-V Collaboration, "GNU toolchain for RISC-V,
    including GCC." [Online]. Available: <https://github.com/riscv-collab/riscv-gnu-toolchain>'
- title: Optimized Spatial Architecture Mapping Flow for Transformer Accelerators
  abstract: 'Recent innovations in Transformer-based large language models have

    significantly advanced the field of general-purpose neural language

    understanding and generation. With billions of trainable parameters, deployment

    of these large models relies on high-performance hardware accelerators to

    efficiently deliver the required computation. Spatial architectures, such as

    TPUs, offer a promising solution to accelerating computation-intensive

    workloads. However, the design process for existing spatial architectures is

    predominantly manual, and it often involves time-consuming redesigns for new

    applications and new problem dimensions, which greatly limits the development

    of optimally designed accelerators for Transformer models. To address these

    challenges, we propose SAMT (Spatial Architecture Mapping for Transformers), a

    comprehensive framework designed to optimize the dataflow mapping of

    Transformer inference workloads onto spatial accelerators. We demonstrate the

    effectiveness of SAMT in improving the performance of spatial accelerators for

    Transformer models. We propose and leverage the dynamic operator fusion schemes

    for the Transformer models and co-search the optimal dataflow mapping

    strategies for spatial accelerators. SAMT significantly reduces inference

    latency by 12% to 91% and energy consumption by 3% to 23% for evaluated

    Transformer models compared to traditional spatial accelerator designs among

    edge, mobile and cloud settings.'
  url: http://arxiv.org/abs/2410.07407v1
  keywords: ''
  document: '# Optimized Spatial Architecture Mapping Flow for Transformer Accelerators


    Haocheng Xu, Faraz Tahmasebi, Ye Qiao, Hongzheng Tian, Hyoukjun Kwon, Sitao Huang


    *University of California, Irvine*


    {*haochx5, tahmasef, yeq6, hyoukjun.kown, sitaoh*}@uci.edu


    *Abstract*—Recent innovations in Transformer-based large language models have
    significantly advanced the field of generalpurpose neural language understanding
    and generation. With billions of trainable parameters, deployment of these large
    models relies on high-performance hardware accelerators to efficiently deliver
    the required computation. Spatial architectures, such as TPUs, offer a promising
    solution to accelerating computationintensive workloads. However, the design process
    for existing spatial architectures is predominantly manual, and it often involves
    time-consuming redesigns for new applications and new problem dimensions, which
    greatly limits the development of optimally designed accelerators for Transformer
    models. To address these challenges, we propose SAMT (Spatial Architecture Mapping
    for Transformers), a comprehensive framework designed to optimize the dataflow
    mapping of Transformer inference workloads onto spatial accelerators. We demonstrate
    the effectiveness of SAMT in improving the performance of spatial accelerators
    for Transformer models. We propose and leverage the dynamic operator fusion schemes
    for the Transformer models and co-search the optimal dataflow mapping strategies
    for spatial accelerators. SAMT significantly reduces inference latency by 12%
    to 91% and energy consumption by 3% to 23% for evaluated Transformer models compared
    to traditional spatial accelerator designs among edge, mobile and cloud settings.


    ### I. INTRODUCTION


    <span id="page-0-1"></span>Machine learning (ML) workloads are gradually shifting
    from simple multilayer perceptrons (MLPs), feed-forward models, and convolutional
    neural networks (CNNs) to large-scale Transformer-based models [\[32\]](#page-11-0).
    Transformer-based models have delivered superior model performance (e.g., accuracy)
    compared to previous models such as CNNs in various application domains including
    computer vision [\[14\]](#page-10-0) and natural language processing (NLP) using
    large language models (LLMs) such as GPT-4 [\[25\]](#page-10-1) and Llama2 [\[29\]](#page-10-2).
    Their success was achieved with billions of trainable parameters with a large
    and complex computation graph in attention layers, which is the backbone of Transformer
    [\[32\]](#page-11-0). Unlike CNNs which mainly consist of convolutions, self-attention
    blocks include multiple batched matrix multiplication (BMM) blocks and memorybound
    operators (e.g., softmax) [\[22\]](#page-10-3), which leads to overall lower arithmetic
    intensity compared to CNNs. In addition, Transformer layers involve complex operator
    dependencies as shown in Fig. [2.](#page-0-0) Such new characteristics of Transformer
    workload motivate specialized accelerator architectures for Transformers to exploit
    new optimization opportunities implied by the specific dependency patterns and
    low arithmetic intensity in Transformers.


    ![](_page_0_Figure_8.jpeg)


    Fig. 1: Lack of fusion exploration in spatial accelerators. Both traditional fixed
    dataflow accelerators (like ShiDianNao [\[15\]](#page-10-4), NVDLA [\[2\]](#page-10-5),
    Eyeriss [\[10\]](#page-10-6), and TPU [\[17\]](#page-10-7)) and flexible dataflow
    accelerators (like MAERI [\[24\]](#page-10-8) and GAMMA [\[18\]](#page-10-9))
    did not consider operator fusion opportunities. \*FLAT [\[20\]](#page-10-10) only
    considers one fusion while SAMT (ours) considers 64 fusion schemes. Details of
    accelerators without fusion can be found in Fig. [8.](#page-5-0)


    <span id="page-0-0"></span>![](_page_0_Figure_10.jpeg)


    Fig. 2: Major Computation Steps in a Transformer layer


    In addition, recent trend in the usage of large language models such as ChatGPT
    [\[25\]](#page-10-1) indicate that the input sequence length is sharply increasing
    for complex tasks such as document summarization [\[7\]](#page-10-11), presentation
    generation [\[34\]](#page-11-1), and video generation [\[26\]](#page-10-12). For
    example, the upper limit of the number of input tokens for GPT-35-Turbo is 4K
    (4096), GPT-4 is 8K and GPT-4-32K is 32K [\[3\]](#page-10-13). Such a trend is
    imposing a major challenge to the hardware since longer sequence length results
    in lower arithmetic intensity, as we show in Fig. [3.](#page-2-0) Although the
    arithmetic intensity of the Transformer model increases until the sequence length
    of 512, it starts decreasing sharply beyond 512 tokens, which introduces a severe
    memory bottleneck problem for recent use cases with 4K or longer input sequences.


    As a solution for such a low arithmetic intensity challenge, operator fusion,
    which merges sequential operations into a single composite operation and reduces
    unnecessary global memory and off-chip memory access between sequential operators,
    has been explored [\[6\]](#page-10-14), [\[8\]](#page-10-15), [\[16\]](#page-10-16),
    [\[28\]](#page-10-17). However, most previous works either focus on GPUs [\[11\]](#page-10-18),
    [\[12\]](#page-10-19) or CNNs [\[6\]](#page-10-14), [\[16\]](#page-10-16). Some
    works covered Transformer targeting accelerators, but the operator fusion search
    space is limited to batch dimension [\[8\]](#page-10-15) or a subset of operators
    in Transformer blocks [\[20\]](#page-10-10). Recent advances in machine learning
    algorithms enabled full integer arithmetic in Transformer blocks [\[13\]](#page-10-20),
    [\[21\]](#page-10-21), [\[33\]](#page-11-2) and revealed more unexplored operator
    fusion opportunities in Transformer blocks, which motivates a comprehensive study
    like this work.


    However, exploring the new operator fusion search space is not a trivial optimization
    problem. Not only does it add more dimensions of complexity to the dataflow design
    space, but it explicitly depends on the underlying hardware accelerator and resources.
    Spatial accelerators often consist of thousands of processing elements (*PEs*)
    [\[17\]](#page-10-7) with kilobytes of local (*S1*) and megabytes of shared (*S2*)
    scratchpad memories for data reuse and a customized network-on-chip (*NoC*) for
    interconnection. Due to the increasing scale of the hardware, the hardware design
    space is large, which is easily in a scale of O(1012) for a fully flexible accelerator
    based on MAERI [\[24\]](#page-10-8) architecture estimated for 1.72 mm<sup>2</sup>
    of chip area using 15 nm technology. Moreover, the number of available dataflow
    mappings on the hardware outnumbers the hardware design space due to the various
    loop ordering, tiling, and temporal scheduling options, particularly with the
    large tensor dimensions in the Transformer-based models. For example, mapping
    the projection layer of Llama2-13B [\[29\]](#page-10-2) where there are 40 attention
    heads, each with a dimensionality of 128, embedding size of 5120, and the input
    sequence length of 1024, onto a 16×16 PE array with 128 KB of on-chip scratchpad
    memory results in O(10<sup>11</sup>) potential dataflow mappings. The interplay
    between hardware design and dataflow mapping is crucial for optimal accelerator
    performance, yet co-optimizing these aspects remains a significant challenge.
    The problem is even more challenging when considering the operator fusion with
    the complex dependency in Transformer models together.


    Therefore, to address these challenges, we propose SAMT (Spatial Architecture
    Mapping for Transformers), as shown in Fig. [7,](#page-5-1) a comprehensive framework
    designed to optimize the dataflow mapping of Transformer inference workloads onto
    spatial accelerators, with operator fusion optimizations. Unlike previous works,
    SAMT considers the full spectrum of operator fusion possibilities, accommodating
    various hardware configurations, including the number of PEs, local (*S1*) and
    shared (*S2*) scratchpad memory sizes, and *NoC* bandwidth. Our proposed SAMT
    flow models all the possible coarse-grained operator fusion opportunities when
    mapping Transformer workloads to spatial accelerators, and explore the combined
    design space of operator fusion and dataflow mapping. To the best of our knowledge,
    SAMT is the first work that optimizes the spatial architecture mapping considering
    all possible operator fusion opportunities in a Transformer block.


    The major contributions of this work can be summarized as follows.


    - We develop a fused dataflow mapping optimization framework, *SAMT*, which maps
    Transformer inference workloads to spatial accelerators with a fixed or flexible
    dataflow.

    - We thoroughly explore all the possible operator fusion opportunities in Transformer
    blocks and show new optimization opportunities in the expanded fusion space, which
    on average lead to 21% and 15% lower latency and energy consumption, respectively,
    compared to stateof-the-art works with operator fusion.

    - We develop an operator fusion explorer (*OFE*), which automatically identifies
    the optimal fusion scheme for a given Transformer inference workload and a target
    spatial accelerator.

    - We extend an analytical cost model, MAESTRO, to support operator fusion (*MAESTRO
    FUSION*) that can evaluate the performance of fused and non-fused operators running
    on spatial accelerators.

    - We propose a mapping space explorer (*MSE*), a genetic algorithm based mapper
    that identifies Pareto-optimal mapping strategies that support both fixed and
    flexible dataflow accelerators.


    ## II. BACKGROUND


    <span id="page-1-0"></span>To better understand the need and challenges for accelerating
    Transformer models with spatial accelerators, we elaborate on the details from
    model, mapping, and hardware perspectives in this section.


    # *A. Spatial Accelerators and Dataflow Mapping*


    As shown in Fig. [4,](#page-2-1) a spatial accelerator architecture consists of
    a PE (processing element) array, local scratchpad memory S1 within each PE, a
    shared scratchpad memory S2, and NoCs (network-on-chips) that interconnect PE
    array and shared scratchpad memory S2. PEs serve as computation units within accelerators,
    containing ALUs (arithmetic logic units) or MAC (multiply-accumulate units) with
    a local scratchpad S1. Similar to cache memory, scratchpad memory is used to reduce
    the number of accesses for off-chip memory S3 and it provides the flexibility
    for the hardware designers to customize data layout and the process of reading/writing
    the data. The spatial accelerators have different inter-PE communication patterns
    as well. With the flexibility of spatial accelerators, we can further


    <span id="page-2-0"></span>![](_page_2_Figure_0.jpeg)


    Fig. 3: (a) Arithmetic intensity with different sequence length l for BERT-Base
    model (embedding size d = 768 and number of heads n<sup>h</sup> = 12) and GPT3-Medium
    model''s prefilling stage (d = 768 and number of heads n<sup>h</sup> = 12) (b)
    The percentage of the memory operations for the operations (for example, A and
    S in Fig. [2\)](#page-0-0) that scale with the sequence length. (c) Arithmetic
    intensity for different operators within the same transformer model.


    <span id="page-2-1"></span>![](_page_2_Figure_2.jpeg)


    Fig. 4: Abstraction of spatial accelerator architecture


    <span id="page-2-2"></span>![](_page_2_Figure_4.jpeg)


    Fig. 5: Example of mapping in MAESTRO FUSION


    explore data reuse opportunities. Most layers in the transformer involve tensor-tensor
    MAC operations and provide a huge space for data reuse opportunities along different
    dimensions of the tensors [\[30\]](#page-11-3). These data reuse opportunities
    could be temporal, spatial, or temporal-spatial. As we can tell from the Fig.
    [6,](#page-4-0) PEs in spatial accelerators can communicate directly using special
    NoC and provide three types of data reuse, temporal reuse, spatial reuse, and
    spatial−temporal reuse, via multicasting, broadcasting, forwarding, and reduction
    as part of the NoC.


    We define dataflow as the mechanism to leverage the maximum data reuse opportunity
    in domain-specific accelerators. Dataflow contains three typical mechanisms: parallelization
    strategy, loop order, and tiling strategy. The parallelization strategy specifies
    spatial and temporal mappings for each dimension. For example, setting a chunk
    size of one in a temporal dimension uniformly distributes data indices across
    all Processing Elements (PEs), facilitating spatial data reuse within the same
    timestep. The sequence of parallel and temporal loops in a loop nest directs data
    movement and impacts data mapping to PEs over time. Alterations in loop order
    can drastically change data''s stationary behavior. Tiling subdivides mapping
    dimensions into smaller segments, increasing mapping flexibility and overcoming
    the limitations of traditional loop nests. Unlike standard loops, which complete
    all inner loops before progressing, tiling allows for dynamic loop execution.
    This can reduce peak memory demands per PE and enable strategic exploitation of
    partial temporal reuse opportunities.


    In Fig. [5,](#page-2-2) we provide an example of how to map a GEMM with fixed
    dataflow using dataflow directives in MAESTR FUSION. Fig. [5](#page-2-2) (a) describes
    a matrix multiplication between matrix A (M × K) and matrix B (K × N) and produce
    a matrix C = A ⊗ B with dimension M = 3, N = 3, K = 3. Fig. [5](#page-2-2) (b)
    shows how we can use dataflow directives to describe the mapping of GEMM. TemporalMap
    indicates that data will change over time and remain static over PEs. SpatialMap
    implies data will change over space (across PEs). Cluster controls how many PEs
    will be grouped into a cluster. Size means mapping size, which tells us how much
    data is mapped onto each PE or how many PEs are mapped onto each cluster. Offset
    means Temporal Offset, which means the mapping update over time, or Spatial Offset,
    which means spatial mapping across the PEs with offset. Fig. [5](#page-2-2) (c)
    shows the dataflow directives for this GEMM in (a). We used three directives to
    describe the GEMM: Inter-Cluster Mapping, Cluster, and Intra-Cluster Mapping.


    Cluster: it will divide the entire PE array into different clusters, each cluster
    has an equal amount of PEs. For example, we can assume the current accelerator
    has 6 PEs in total, and Fig. [10](#page-7-0) (c) and (d) indicate that these 6
    PEs are divided into 2 groups and each group has 3 PEs. This enables exploration


    in a 2D PE array by spatially mapping two dimensions of the GEMM. Intra-Cluster
    Mapping: it describes the details of mapping within the cluster. in Fig. [10](#page-7-0)
    (c), the K dimension is spatially mapped (data are different across PEs) while
    the M and N dimensions are temporally mapped (data only change with time). With
    Size = 1 and Offset = 1, each PE will only receive one unique element from matrices
    A and B since M and N stay the same. Each cluster will compute one element of
    matrix C. Inter-Cluster Mapping: it describes a similar mapping process but across
    the clusters. In Fig. [10](#page-7-0) (c), the N dimension is spatially mapped,
    and M and K dimensions are temporally mapped. This implies the elements from the
    matrix B will be distributed across clusters while the elements from the matrix
    A remain the same. The Size and Offset for K is 3 means each cluster receives
    3 elements from each matrix. The computation order depends on the directives described
    in Fig. [5](#page-2-2) (c) and (d).


    ## *B. Computation in Transformer and the low arithmetic intensity challenge*


    Transformer models typically consist of two main blocks, which are usually referred
    to as the *attention* block and the *feedforward* module, each of which is followed
    by a LayerNorm and a residual connection [\[32\]](#page-11-0).


    As illustrated in Fig. [2,](#page-0-0) attention blocks and feed-forward module
    first project the sequence input X of dimension d × l (d and l represent the embedding
    size and sequence length, respectively) by different weight matrices WQ, WK, W<sup>V</sup>
    of dimension d×d to get the so-called *query* Q, *key* K, and *value* V matrices
    of dimension d × l. The Q matrix and transposed K matrix will yield an activation
    matrix A of size l × l. Then attention score S will be calculated based on the
    softmax value of the activation matrix A. To get the output of the attention block,
    it will multiply the value matrix V with the attention score S, resulting in a
    matrix O with a dimension of d × l. Finally, to get the output Y of the entire
    attention block, it projects the matrix O by one more weight matrix WO. After
    the attention block, the intermediate tensor will be passed into two consecutive
    linear projection layers MLP with a given hidden dimension dF F N plus two GELU
    (Gaussian Error Linear Unit) non-linear activation layers in between.


    To understand the reason behind the limits of tokens in the recent transformer
    models as we discussed in [I,](#page-0-1) we profiled the arithmetic intensity
    I, which is calculated by dividing FLOPs (the number of floating-point operations)
    by MOPs (the number of memory access counts) (assuming that each memory access
    will fetch one-byte data):


    $$I = \frac{FLOPs}{MOPs} \tag{l}$$


    As shown in Fig. [3\(](#page-2-0)a), the arithmetic intensity first increases
    then decreases after reaching sequence length l = 512 for both the BERT-Base and
    the GPT3-Medium models. The main reason for this drop in arithmetic intensity
    is that some operations'' (like A = Q⊗ K and S = Sof tmax(A)) memory access count
    will scale up quadratically with the sequence lengths l while keeping the number
    of floating point operations at low level. In Fig. [3\(](#page-2-0)b), we can
    see that those operations will quickly dominate in memory access count in the
    entire transformer model as sequence length l increases.


    As shown in Fig. [3\(](#page-2-0)c), we can find some operators have low arithmetic
    intensity. As sequence length increases, Operator QKV Y (projection) and MLP''s
    (feed-forward) arithmetic intensity increases accordingly while the other two
    type operators A/O and S''s arithmetic intensity stay steady or even decreases.
    This further proves that those memory operations will become a bottleneck when
    sequence length increases in the transformer model.


    ### *C. Lack of fusion exploration in flexible dataflow accelerator*


    There are several hardware-aware compiler frameworks [\[4\]](#page-10-22), [\[9\]](#page-10-23)
    for fusion optimization. However, they can not directly target flexible dataflow
    spatial accelerators. FLAT [\[20\]](#page-10-10) tries to target fixed dataflow
    accelerators and perform inter-layer fusion; however, it is limited to activation-to-activation
    fusion and does not fully explore all the fusion schemes. There are two main reasons
    why fusion exploration has been limited on spatial accelerators: 1) Most spatial
    accelerators suffered from model performance due to integer arithmetic; 2) Nonlinear
    operators in transformer models like softmax and layer norm do not naturally support
    tiling due to the nature that it requires global views of the data(for example,
    softmax needs the maximum of the row and summation of the row). That means if
    we fused the naive safety softmax with other adjacent operators, a very large
    local memory will be needed to store all the data for softmax calculation, which
    is not practical in the real world.


    Luckily, a recent study [\[33\]](#page-11-2) enables lossless 8-bit weight and
    activation quantization for LLMs which further expands the usage of spatial accelerators
    in transformer models. The difficulty of fusing non-linear operators has also
    been addressed by the recent GPU work FlashAttention [\[11\]](#page-10-18), [\[12\]](#page-10-19).


    To the best of our knowledge, there is no prior work that removes all limitations
    on the fusion exploration of dataflow accelerators. Despite increasing the design
    search space, fusion exploration brings great optimization opportunities. The
    goal of this work is to introduce such a framework that finds the best fusion
    scheme with arbitrary dataflow to be exploited by a flexible hardware accelerator,
    where it introduces a novel approach to computing attention in Transformer models,
    particularly emphasizing the importance of operation fusion to handle the intensive
    memory requirements and computational challenges of one-head attention mechanisms.


    ## III. SAMT SPATIAL ACCELERATOR MAPPING FOR TRANSFORMER


    Based on the motivation toward thorough exploration of fused mappings in Transformer
    layers enabled by recent integer arithmetic algorithms discussed in Section [II,](#page-1-0)
    we develop a fused mapping optimization framework, SAMT, as illustrated in Fig.
    [7.](#page-5-1) SAMT receives workload description, target accelerator hardware
    design parameters, and dataflow


    <span id="page-4-0"></span>![](_page_4_Figure_0.jpeg)


    Fig. 6: Understanding data reuse in spatial architecture: Temporal reuse happens
    when PEs access the same data across time via on-chip scratchpads or buffers.
    In Fig. [6\(](#page-4-0)a), PE0, PE1, and PE2 stored the same data a, b, and c
    for the timestamps 1, 2, 3, and so on. Spatial reuse occurs when multiple PEs
    access the same data at the same time via multicasting. In Fig. [6\(](#page-4-0)b),
    all the PEs receive the same data a at timestamp 1, data b at timestamp 2, and
    data c at timestamp 3. Spatial-temporal reuse forwards the data to the adjacent
    PEs in a skewed manner via neighbor-to-neighbor connections with forward buffers.
    In Fig. [6\(](#page-4-0)c), PE0 sends the data a to adjacent PE1 from timestamp
    1 to 2 and PE1 sends the data to PE2 from timestamp 2 to 3.


    configuration (fixed or flexible) as inputs. Using those inputs, SAMT reports
    optimized fused mapping and corresponding latency, energy, and memory access count.
    SAMT consists of OFE, MSE, and MAESTRO FUSION. We discuss each component in SAMT
    in detail.


    ### *A. OFE: Operator Fusion Explorer*


    As we discussed in [II,](#page-1-0) the main source of latency and energy benefits
    from operator fusion is based on the conversion of costly S2/DRAM accesses into
    efficient inter-PE communication. However, the amount of benefits are dependent
    on the model parameters (embedding size, sequence length, and hidden dimension),
    as listed in TABLE[.I.](#page-6-0) Also, S2 memory size is another major consideration
    because it constraints available fusions (one or combination of what is listed
    in TABLE[.I.](#page-6-0) Therefore, we develop OFE, Operator Fusion Explore, which
    systematically explores the complex design space of the fused dataflow and accelerator
    hardware parameters.


    The design space for finding the optimal dataflow mapping given a fused or non-fused
    transformer model is large. As mentioned in [I,](#page-0-1) mapping the projection
    layer of Llama2-13B [\[29\]](#page-10-2) can result in O(10<sup>11</sup>) different
    dataflow mappings. Inspired by previous works'' effectiveness of genetic algorithms
    for exploring such complex mapping space [\[18\]](#page-10-9), [\[19\]](#page-10-24),
    [\[27\]](#page-10-25), [\[31\]](#page-11-4), we also adopt a genetic algorithm
    (Algorithm [1\)](#page-6-1) with an extended search space with fusion.For that,
    we develop new gene encoding for each operator into a 4 by 2 dimension of the
    genome as illustrated in Fig. [5\(](#page-2-2)d). Here we define genome as a design
    choice of the entire mapping space for dataflow. As shown in Fig[.5\(](#page-2-2)d),
    there are two levels of mapping corresponding to the inter-cluster and intra-cluster
    mapping. In the intra-cluster mapping, the first column describes the dimension
    (K) to parallelize and the following three columns describe the computation order
    and the tiling size for each TemporalMap or SpatialMap. In the inter-cluster mapping,
    the same terminology applies here. The first column in the intercluster mapping
    describes the dimension (N) to map spatially and the following three determine
    the order of computation and the corresponding tiling size.


    We define the fused operator in TABLE [I](#page-6-0) as our smallest primitives
    and encode the entire fusion scheme into a binary code. For example, as shown
    in Fig[.9,](#page-6-2) fusion scheme 110110 means we are using two newly fused
    operators, Op<sup>12</sup> and Op45. Op<sup>12</sup> means fusing the original
    two fused operators Op1 and Op2. Op<sup>12</sup> needs to read matrix WQ, WK,
    X from the off-chip memory S3 and will compute S = Sof tmax(W<sup>Q</sup> ⊗ W<sup>K</sup>
    ⊗ X), following by writing S back to the off-chip memory S3. Compared to the non-fused
    version, it will reduce the entire memory footprint by 5dl + 2l <sup>2</sup> with
    larger S2 requirements. Similarly, Op<sup>45</sup> will reduce the entire memory
    footprint by 3dl. OFE will generate all the fusion schemes to be further evaluated.


    We can fuse some adjacent operators to reduce the intermediate tensor''s memory
    footprint and increase the data reuse. The new fused operators are defined in
    TABLE [I](#page-6-0) and can be visualized in Fig. [2.](#page-0-0) As illustrated
    in Fig. [2,](#page-0-0) the original computation to get the intermediate results
    A is: 1) loading input X twice and weight matrices W<sup>Q</sup> and W<sup>K</sup>
    with the input tensor memory footprint of 2dl + 2d 2 ; 2) multiplying the X with
    W<sup>Q</sup> and W<sup>K</sup> separately, and storing the intermediate results
    Q and K with the memory footprint of 2dl; 3)loading the intermediate results Q
    and K with the same footprint; 4) multiplying Q and K to get the intermediate
    results A and storing it with the memory footprint of l 2 . As we can see from
    step 1), the input tensor has been loaded twice for two different projection operators;
    and, from step 3), the intermediate tensors Q and K have been stored and loaded
    without any further calculation, which is a waste of data movement. However, if
    we fused those operators together, which aligned with our definition of fused
    Op 1, we will see the result will be the same but it will only cost one input
    tensor X load and skip storing and loading the intermediate tensors Q and K. This


    <span id="page-5-1"></span>![](_page_5_Figure_0.jpeg)


    Fig. 7: SAMT: Spatial Transformer Architecture Mapping Framework


    fusion will reduce the global memory S3 read and write by 5dl with a bit of shared
    scratchpad S2 pressure. We will explore the trade-off between the fusion scheme
    and the on-chip buffer size in the later.


    ## *B. MSE: Mapping Space Explorer*


    As described in Algorithm [1,](#page-6-1) for each fusion scheme, we can get the
    M apping Candidates through OFE and MAE-STRO FUSION with local scratchpad size
    S1, shared scratchpad size S2, NoCs bandwidth B, number of PEs P, transformer
    model matrix dimensions M<sup>i</sup> , N<sup>i</sup> , K<sup>i</sup> and F usedOp
    (comes from OFE). Then for each M apping Candidates, we perform a genetic search
    algorithm. As illustrated in Fig. [7,](#page-5-1) after initializing the population
    of the mapping candidates, we perform Crossover, Mutation, and Reorder. Crossover:
    it will randomly select two genomes (mappings) from the parent pool and interchange
    the tile size for those two genomes; Mutation: it will randomly generate a new
    dimension to parallelize if accelerator types allow and a new tile size for one
    dimension of the mappings; Reorder: it will swap the tile size of two dimensions
    from two mappings randomly. After evolution, MAESTRO FUSION will provide a performance
    analysis (latency, energy, PE utilization, etc.) and a fitness function will be
    applied based on the target optimization (latency, energy, S1 size, S2 size, etc.).
    Here we picked latency as our first optimization goal and energy as the second.
    The selection process will keep track of the current best mapping strategy. After
    selection, we update the parents pool and elites pool before entering the next
    iteration.


    We enable two types of outputs here: fixed-dataflow type and flexible dataflow
    type. Fixed dataflow means the same dataflow mapping except the tiling sizes will
    be applied to each operator in the entire model; flexible dataflow means different
    dataflow mappings will be enabled for different operators in the transformer model.


    <span id="page-5-0"></span>


    | Dataflow                   | Parallelizing<br>Dimension |                  |
    Computation Order | Cluster          |                           |  |

    |----------------------------|----------------------------|------------------|-------------------|------------------|---------------------------|--|

    | Mapping<br>Style           | Inter<br>Cluster           | Intra<br>Cluster |
    Inter<br>Cluster  | Intra<br>Cluster | Size                      |  |

    | TTS-NMK<br>NVDLA-like      | N                          | K                |
    N→K→M             | N→M→K            | 64                        |  |

    | TTS-MNK<br>Eyeriss-like    | M                          | K                |
    M→N→K             | M→N→K            | 12                        |  |

    | TTS-NMK<br>TPU-like        | N                          | K                |
    N→M→K             | N→M→K            | 256                       |  |

    | TST-MNK<br>ShiDianNao-like | M                          | N                |
    M→N→K             | M→N→K            | 8                         |  |

    | Flexible                   | M or N<br>or K             | M or N<br>or K   |
    any               | any              | tile size of<br>last dim. |  |


    Fig. 8: Dataflow mapping styles of different types of spatial accelerators


    # *C. MAESTRO FUSION: Cost Model*


    After enabling the dynamic fusion scheme for the transformer model, we need to
    evaluate the performance of the spatial accelerators to find the optimal dataflow
    mapping. We tried to leverage the detailed analytical modeling framework, MAESTRO


    TABLE I: Definition of Fused Operators and Corresponding Memory Footprints


    <span id="page-6-0"></span>


    | ID | Fused Op                        | Orignal Ops                                        |
    Memory<br>Fused       | Input Tensor<br>Fused | Output<br>Tensor<br>Fused | Memory<br>Orginal      |
    Input Tensor<br>Original | Output Tensor<br>Original | Memory<br>Reduced |

    |----|---------------------------------|----------------------------------------------------|-----------------------|-----------------------|---------------------------|------------------------|--------------------------|---------------------------|-------------------|

    | 1  | A = WQ ⊗ X ⊗ WK ⊗ X             | Q = WQ ⊗ X,<br>K = WK ⊗ X,<br>A = Q ⊗
    K            | 2 + l<br>2 + dl<br>2d | 2 + dl<br>2d          | l2                        |
    2 + 6dl<br>2 + l<br>2d | 2 + 4dl<br>2d            | l2 + 2dl                  |
    5dl               |

    | 2  | S = Sof tmax(Q ⊗ K)             | A = Q ⊗ K,<br>S = Sof tmax(A)                      |
    2 + 2dl<br>l          | 2dl                   | l2                        | 2
    + 2dl<br>3l          | l2 + 2dl                 | 2<br>2l                   |
    2<br>2l           |

    | 3  | O = V ⊗ Sof tmax(A)             | S = Sof tmax(A),<br>O = V ⊗ S                      |
    2 + 2dl<br>l          | l2 + dl               | dl                        | 2
    + 2dl<br>3l          | 2 + dl<br>2l             | l2 + dl                   |
    2<br>2l           |

    | 4  | O = (WV ⊗ X) ⊗ S                | V = WV ⊗ X,<br>O = V ⊗ S                           |
    2 + 2dl<br>2 + l<br>d | d2 + l<br>2 + dl      | dl                        | d2
    + l<br>2 + 4dl      | d2 + 2dl + l<br>2        | 2dl                       | 2dl               |

    | 5  | Y = (W0 ⊗ (V ⊗ S))              | O = V ⊗ S,<br>Y = W0 ⊗ O                           |
    2 + 2dl<br>2 + l<br>d | d2 + l<br>2 + dl      | dl                        | d2
    + l<br>2 + 4dl      | d2 + 2dl + l<br>2        | 2dl                       | 2dl               |

    | 6  | F = a2 ⊗ GELU(a1 ⊗ Y + b1) + b2 | L1 = GELU(a1 ⊗ Y + b1),<br>L2 = GELU(a2
    ⊗ L1 + b2) | 2((dF F N + l)d)      | 2dF F N d + dl        | dl                        |
    2(dF F N (l + d) + dl) | dF F N d + dl            | dF F N l                  |
    2dF F N l         |


    \*l : Sequence Length, \*d : Embedding Size, \*dF F N : Hidden Dimension of Linear
    Projection Layer


    ## <span id="page-6-1"></span>Algorithm 1 SAMT Kernel Fusion Algorithm


    - 1: Input: Matrix dimensions for each layer of the transformer model: Mi, Ni,
    Ki; The local scratchpad size: S1; The shared scratchpad size S2; The bandwidth
    for NoC: B; The total number of PEs: P; The type of accelerator: Arch, Generation
    for search: α; T arget1 and T arget2 the optimization goal (chosen from latency,
    energy, S1 size, S2 size, P).

    - 2: Output: Datacentric Mapping descriptions for the optimal mapping strategy

    - 3: Dataf low Candidates = Get Dataflow( Arch )

    - 4: for F usion id = 1 → Num F unsion Scheme do

    - 5: F usedOp = Get Fused Operators(F usion id)

    - 6: Mapping Candidates = MAESTRO FUSION ( S1, S2, B, P, Mi, Ni, Ki, F usedOp
    )

    - 7: for i = 1 → Dataf low Candidates do

    - 8: while Current Iteration <α do

    - 9: ( P.D., C.O., T.S ) = Crossover( Mapping Candidates )

    - 10: ( P.D., T.S. ) = Mutation( Mapping Candidates )

    - 11: ( C.O ) = Reorder( Mapping Candidates )

    - 12: Update Childs()

    - 13: if Childs F itness(T arget1, T arget2) >T hreshold then 14: Update Elites()
    15: Update Parents()

    - 16: end if

    - 17: end while

    - <span id="page-6-2"></span>18: end for 19: end for

    - Op 1 2 3 4 5 6 Code 1 1 0 1 1 0 45: ⊗ ⊗ 12: ( ⊗ ⊗ )


    Fig. 9: Example of new fused operators in OFE


    [\[23\]](#page-10-26), to describe regular transformer operators and our proposed
    fused operators in OFE. However, the original MAESTRO does not support fused operators
    from OFE. Thus, we expanded the native MAESTRO to MAESTRO FUSION to enable dataflow
    directives given a fused operator. MAESTR FUSION extended fused operator support
    by converting the S2/DRAM access to more efficient inter-PE communication and
    can report the latency of the given model, energy usage of the hardware accelerator,
    and memory access count on S1 scratchpad and S2 scratchpad given the hardware
    configurations. The efficiency of MAESTR FUSION lies in its swift analysis of
    diverse forms of data reuse within the accelerator. This comprehensive set of
    metrics offers valuable insights into the performance characteristics of the spatial
    accelerator. Inputs to MAESTR FUSION are made of a transformer model description
    and hardware resource details, provided in the form of lists, while the mapping
    is articulated through a data-centric representation. This representation encompasses
    three directives, facilitating a succinct and compiler-friendly depiction of mappings.


    # *D. How different mapping affects data reuse*


    For each mapping, which refers to the specific dataflow strategy, we can describe
    it using computation order, parallelism, and tiling strategy. Each mapping determines
    how much data will be used at each PE at the time of processing. These data need
    to be moved from the off-chip memory to the shared scratchpad S2, and from the
    shared scratchpad S2 to the local scratchpad S1, as shown in Fig. [4.](#page-2-1)
    Data reuse happens when some portion of data remains in the local scratchpad S1
    or shared scratchpad S2. Operator fusion further increases the chance to keep
    some of the data in on-chip memory rather than sending it back to the off-chip
    memory.


    For a mapping, if the dimension of the matrix is larger than the physical dimension
    of the PE array, then the matrix must be tiled. As shown in Fig. [10,](#page-7-0)
    it shows the impact of different tile sizes for mapping a GEMM (a 3 × 3 matrix
    A multiplied with a 3×3 matrix B) on a spatial architecture with 6 PEs. We use
    TM, T<sup>N</sup> and S<sup>K</sup> to represent the temporal mapping for M and
    N dimension and spatial mapping for K dimension. As computation order and parallelizing
    dimension are fixed in both inter-cluster and intra-cluster in this example, different
    cluster sizes and tiling sizes will affect the data reuse opportunity. In Fig.
    [10](#page-7-0) (a), a non-cluster mapping has been applied here. <span id="page-7-0"></span>(a)
    Mapping without cluster


    ![](_page_7_Figure_1.jpeg)


    Fig. 10: Different Mapping Strategies


    SpatialMap (1, 1) K


    Only 3 PEs (number 1 to 3) are spatially mapped onto the K dimension, which is
    the column of matrix A and the row of the matrix B. With this mapping, the entire
    accelerator is computing one entry of the matrix C and leaving 3 PEs (from number
    3 to 6) idle. In Fig. [10](#page-7-0) (b), we fully map 3 PEs from each cluster
    onto the K dimension. All 6 PEs are utilized to compute two entries of matrix
    C. However, this non-tiled mapping does not provide the optimal performance. In
    Fig. [10](#page-7-0) (c), we split 6 PEs into 3 clusters. 2 PEs from each cluster
    are spatially mapped onto K dimension and yet fully cover the K dimension, which
    is called the tiled-mapping. However, this time we temporally map the M and N
    dimensions with size 2, which means each PE will compute two different data across
    the time. This will yield an underutilization of cluster 3 because there is not
    enough data in the N dimension to allocate for cluster 3. Finally, Fig. [10](#page-7-0)
    (d) shows an optimized tiled mapping strategy with a cluster size of 2. All 2
    PEs in each cluster will be fully utilized and this enables maximum data reuse.


    ### *E. Hardware design space and dataflow mapping space*


    To enable flexible spatial accelerator design, our framework defines four key
    configurations in Algorithm [1](#page-6-1) for the hardware specification: number
    of PEs P, shared scratchpad size S2, local scratchpad size S1, and network-on-chip
    (NoC) bandwidth B. Even though it is impossible to do an apple-toapple comparison
    with previous accelerator works, we try to normalize the hardware configurations
    across different works for a fair comparison. We evaluated all the different mapping
    strategies previous works can provide, and assign the same hardware configurations
    to all the accelerators under comparison. For example, in GEMM, the K dimension
    is reduced


    <span id="page-7-1"></span>TABLE II: Example of different Hardware Configurations


    | Config     | # of PEs | S1<br>Size | S2<br>Size | NoC<br>Bandwidth | Off-Chip<br>Mem.
    Bandwidth |

    |------------|----------|------------|------------|------------------|----------------------------|

    | Edge [1]   | 256      | 256B       | 20 MB      | 16 GB/s          | 80 GB/s
    (DRAM)             |

    | Mobile [5] | 4098     | 512B       | 40 MB      | 40 GB/s          | 80 GB/s
    (DRAM)             |

    | Cloud [17] | 65536    | 2048B      | 100 MB     | 800 GB/s         | 1000 GB/s
    (HBM)            |


    <span id="page-7-2"></span>TABLE III: Latency and Energy Reduction under Various
    S2 Scratchpad Sizes


    | S2 Size<br>(MB) | Fusion Code | Latency Reduced<br>(Million Cycles) | Energy
    Reduced<br>(µJ) |

    |-----------------|-------------|-------------------------------------|------------------------|

    | 12              | 110101      | 24.64                               | 52.74                  |

    | 15              | 110101      | 27.79                               | 59.47                  |

    | 17              | 111101      | 31.98                               | 68.45                  |

    | 20              | 111111      | 35.13                               | 75.18                  |


    when calculating the output matrix, thus we spatially map the K dimension within
    the cluster of PEs for the accelerators that support spatial reduction (TPU-like,
    Eyeriss-like, NVDLA-like). These accelerators support spatial reduction through
    forwardand-store or reduction trees. However, the ShiDianNao-style accelerators,
    do not support spatial reduction, thus we spatially map the N dimension within
    the cluster instead of the K dimension. Listed in Fig. [8,](#page-5-0) we summarized
    the important mapping information of different types of accelerators. For example,
    T T S − NMK represents that intracluster mapping style is temporally map N and
    K dimensions and spatially map K dimension, which aligns with fixed dataflow NVDLAlike
    accelerators. The parallelizing dimension, computation order, and cluster size
    for those accelerators are fixed except for the flexible one. Thus, given the
    type of accelerator Arch (e.g., NVDLA-like, TPU-like, Fleixble), we can determine
    the data-centric directives for the mapping space of a transformer model.


    ## IV. EVALUATION


    In this section, we show that SAMT framework can find Pareto-front solutions given
    different Transformer models and spatial accelerators.


    We evaluated five types of accelerators as shown in Fig. [8.](#page-5-0) For consistent
    comparison, each accelerator was configured with identical hardware settings (refer
    to the Edge setting in Table [II\)](#page-7-1), including 256 Processing Elements
    (PEs), 256 Bytes for S1, 20 MB for S2, and a Network on Chip (NoC) bandwidth of
    16 GB/s. We used the GPT-2 transformer model in this analysis, which has an embedding
    size of 768 and a sequence input length of 1024). Fig. [11](#page-8-0) (a)-(d)
    demonstrates how different accelerator designs and the integration of operator
    fusion techniques influence latency improvements. Specifically, Fig. [11](#page-8-0)
    (a) highlights that the TTS-NMK accelerators (as per the setup in Fig. [8\)](#page-5-0)
    experience a 14% latency reduction with a flexible dataflow approach without fusion,
    and between 15% and 22% with basic fusion primitives. Further, Fig. [11](#page-8-0)
    (b) to (d) indicate latency improvements ranging from 12% to 26% using flexible
    dataflow without fusion, and from 13% to 34%


    <span id="page-8-0"></span>![](_page_8_Figure_0.jpeg)


    Fig. 11: Evaluation Results of Latency and Energy Consumption under Various Settings


    <span id="page-8-1"></span>![](_page_8_Figure_2.jpeg)


    Fig. 12: The Pareto-Front Solutions Found by SAMT


    with basic fusion strategies. Finally, Fig. [11\(](#page-8-0)e) and (f) show that
    under the same hardware configurations, flexible dataflow combined with optimal
    fusion can lead to up to 91% latency reduction and up to 23% energy savings compared
    to fixed dataflow without fusion.


    TABLE [III](#page-7-2) shows the trade-off between the latency and energy reduction
    because of operator fusion and S2 scratchpad size. As S2 size increases, we can
    incorporate a more aggressive fusion scheme, which means more operators will be
    fused and that requires a larger S2 capacity. In short, larger S2 further reduces
    the runtime latency and energy consumption.


    Fig. [12](#page-8-1) shows the Pareto-front solution sets found by SAMT, where
    each point represents a fusion scheme supported and enabled by OFE. This figure
    illustrates the strong correlation between latency and energy within out hardware
    setting. Given the hardware constraints, especially the size of S2, we observe
    various fusion schemes could lead to optimal latency reduction,


    <span id="page-8-2"></span>![](_page_8_Figure_7.jpeg)


    Fig. 13: Latency vs. Energy of different dataflow accelerators across different
    platforms


    which aligns well with the findings from TABLE [III.](#page-7-2) We notice that,
    within S2 limits, for example, S2 less than 25 MB, the different fusion schemes
    along the dashed line demonstrate the potential to reduce latency without increasing
    energy consumption.


    Fig. [13](#page-8-2) shows the optimal fixed dataflow and flexible dataflow accelerators
    with fusion reported by SAMT in the edge, mobile, and cloud settings as detailed
    in Table[.II.](#page-7-1) In each of those settings, SAMT consistently identifies
    solutions that achieve the lowest latency while maintaining relatively low energy
    consumption, which aligns with our prioritization of latency first and energy
    second. The figure highlights the advantages of employing flexible dataflow accelerators
    compared to fixed dataflow accelerators (NVDLA-like and TPU-like) when SAMT can
    successfully determine the optimal dataflow mapping for them with the fusion.


    To further show the potential of flexible dataflow accelerators in SAMT for transformer
    model, we consider the different computation patterns for the prefilling and decoding
    stage of GPT-3. The prefill stage of the decoder-only model (GPT-3) has the same
    computation pattern as the encoder-only model (BERT-Base). However, the decoding
    stage of the decoder-only model has an autoregressive computation pattern. From
    the SAMT report (assuming GPT-3 Medium model with a prompt length of 128, and
    sequence length of 1024), the latency and energy for each iteration using the
    same dataflow as the prefill stage are 2.5 × 10<sup>10</sup> cycles and 5.1 ×
    10<sup>8</sup> nJ, different from using optimized flexible dataflow, whose latency
    and energy are 1.8 × 10<sup>8</sup> cycles and 6.6 × 10<sup>7</sup> nJ. It shows
    the potential of flexible dataflow accelerators in decoder-only transformer models.


    ### V. RELATED WORK


    Previous research has addressed deep learning accelerator loop ordering and mapping
    strategies, primarily through compiler optimizations. These efforts typically
    target multiple platforms and favor architectures with broader applicability.
    While some of these works incorporate dataflow mappings as one aspect of optimization,
    they often do not explicitly explore the design of dedicated specialized accelerators,
    as we do in our current study.


    Additionally, there are works that specifically focus on optimizing dataflow mapping
    and exploring the design space for accelerators. However, many of these efforts
    lack considerations at the operator fusion level. Moreover, the majority of them
    are tailored for Convolutional Neural Networks (CNNs), whereas our research explicitly
    targets transformer networks, incorporating detailed analysis of the heavily leveraged
    operator fusion scheme.


    ### *A. Compiler Optimization*


    TVM [\[9\]](#page-10-23) is an open-source machine learning compiler framework
    that automates the process of optimizing neural network models for a variety of
    hardware targets, including CPUs, GPUs, and specialized accelerators. It uses
    a deep learning-based approach to determine efficient operator fusion, optimizing
    the performance of the computational graph by considering the cache size and memory
    hierarchy of the target hardware. Although TVM provides extensive support for
    multiple hardware platforms, its optimization strategies focus mainly on traditional
    computing units and do not intrinsically consider the unique needs of emerging
    spatial architectures for Transformer models. Unlike SAMT, TVM does not explicitly
    adjust its strategy based on the different data flows and hardware configurations
    required by space accelerators (such as the number of PEs, NoC architecture, and
    memory bandwidth).


    XLA [\[4\]](#page-10-22) is another compiler that can convert high-level representations
    of neural networks into optimized machine code for CPUs, GPUs, and other ML accelerators.
    XLA''s strength lies in its ability to tightly integrate with computation graph,
    enabling significant performance boosts. However, XLA''s optimizations are generally
    confined to CPU and GPU architectures with


    existing LLVM backend support. For deployment on non-CPUlike hardware, XLA requires
    extensive backend development, which can be a resource-intensive process. This
    limitation is particularly relevant when considering spatial architectures for
    which XLA does not naturally provide support, contrasting sharply with SAMT''s
    direct focus on optimizing dataflow mappings specific to such accelerators.


    Ansor [\[35\]](#page-11-5) uses a cost model to guide its search for the optimal
    kernel implementation across various hardware platforms. However, Ansor still
    primarily targets traditional accelerator designs and does not address the complexities
    associated with spatial architectures that are central to SAMT. Ansor''s general
    approach is to optimize operator-level performance without specific consideration
    for the architectural variations and dataflow strategies that are essential for
    the next-generation hardware accelerators used in processing Transformer models.


    ## *B. Accelerator Mappings*


    Kao et al. [\[18\]](#page-10-9) introduced the GAMMA framework, designed to automate
    the hardware mapping of Deep Neural Network (DNN) models onto accelerators. By
    employing Genetic Algorithms, they sought to optimize the mapping process, thereby
    enhancing the efficiency and performance of DNN accelerators. The utilization
    of genetic algorithms involves a heuristic search strategy inspired by natural
    selection processes, leading to iterative refinement of hardware mappings. It
    is noteworthy, however, that GAMMA primarily focuses on mapping strategies for
    Convolutional Neural Networks (CNNs), and its applicability to mainstream transformer
    models is limited due to distinct parallelization and data reuse schemes inherent
    to these models.


    MAERI [\[24\]](#page-10-8) focuses on the evaluation of spacial accelerator architectures
    that employ a tiled general matrix-matrix multiplication (GEMM) kernel. The study
    introduces a framework dedicated to identifying optimized mappings, including
    dataflow and tile sizes, for a tiled GEMM tailored to a given spatial accelerator
    and workload combination. The evaluations, conducted across five spatial accelerators,
    demonstrate that the systematically generated tiled GEMM mappings outperform various
    GEMM workloads on diverse accelerators, emphasizing the potential for high performance
    achieved through thoughtful optimization.


    The FLAT framework, as introduced by Kao et al. [\[20\]](#page-10-10), presents
    a dataflow optimization strategy addressing attention bottlenecks in neural network
    models. Attention mechanisms, integral for capturing intricate data relationships,
    often introduce operational intensity and other bottlenecks on existing hardware.
    FLAT employs a tiling technique and limited operator fusion to augment data reuse
    opportunities, thereby reducing memory requirements and increasing arithmetic
    intensity.


    On the other hand, our SAMT framework further refines the operator fusion scheme.
    It facilitates the co-optimization between various transformer models and hardware
    configurations, ultimately determining the optimal fixed or flexible (if hardware-supported)
    dataflow mapping.


    ### VI. CONCLUSION


    In this work, we shed a light to the unexplored design space in the intersection
    of operator fusion and flexible dataflow for addressing low arithmetic intensity
    challenges in Transformer models. We explored all the possible operator fusion
    in Transformer blocks enabled by recent integer arithmetic algorithms on non-trivial
    operators such as softmax and presented benefits from the new operator fusion
    space.


    We codified our methodologies into a framework SAMT that consists of operator
    fusion explorer *OFE*, an extended MAESTRO cost-model supporting operator fusion
    *MAE-STRO FUSION*, and genetic algorithm-based mapping space explorer *MSE*. Our
    framework not only leverages the dynamic operator fusion schemes for the transformer
    models but also co-search the optimal dataflow mapping strategies for spatial
    accelerators. Our evaluation shows that such a comprehensive approach leads to
    significant latency and energy benefits: reduce inference latency by 12% to 91%
    and energy consumption by 3% to 23%. Such promising results motivate future works
    to consider the extended fusion and dataflow mapping optimization for Transformer
    accelerators, which can be built on top of our SAMT framework.


    ### REFERENCES


    - <span id="page-10-27"></span>[1] "Advanced neural network processing for low-power
    devices." [Online]. Available: <https://coral.ai/technology>

    - <span id="page-10-5"></span>[2] "http://nvdla.org/."

    - <span id="page-10-13"></span>[3] "https://learn.microsoft.com/en-us/azure/ai-services/openai/howto/chatgpt?pivots=programming-language-chat-completions."

    - <span id="page-10-22"></span>[4] "https://openxla.org/xla."

    - <span id="page-10-28"></span>[5] "Unlocking on-device generative ai with an
    npu and heterogeneous computing." [Online]. Available: [https://www.qualcomm.com/content/dam/qcomm-martech/dm](https://www.qualcomm.com/content/dam/qcomm-martech/dm-assets/documents/Unlocking-on-device-generative-AI-with-an-NPU-and-heterogeneous-computing.pdf)[assets/documents/Unlocking-on-device-generative-AI-with-an-NPU](https://www.qualcomm.com/content/dam/qcomm-martech/dm-assets/documents/Unlocking-on-device-generative-AI-with-an-NPU-and-heterogeneous-computing.pdf)[and-heterogeneous-computing.pdf](https://www.qualcomm.com/content/dam/qcomm-martech/dm-assets/documents/Unlocking-on-device-generative-AI-with-an-NPU-and-heterogeneous-computing.pdf)

    - <span id="page-10-14"></span>[6] M. Alwani, H. Chen, M. Ferdman, and P. Milder,
    "Fused-layer cnn accelerators," in *2016 49th Annual IEEE/ACM International Symposium
    on Microarchitecture (MICRO)*. IEEE, 2016, pp. 1–12.

    - <span id="page-10-11"></span>[7] A. Bertsch, U. Alon, G. Neubig, and M. Gormley,
    "Unlimiformer: Long-range transformers with unlimited length input," in *Advances
    in Neural Information Processing Systems*, A. Oh, T. Neumann, A. Globerson, K.
    Saenko, M. Hardt, and S. Levine, Eds., vol. 36. Curran Associates, Inc., 2023,
    pp. 35 522–35 543. [Online]. Available: [https://proceedings.neurips.cc/paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/6f9806a5adc72b5b834b27e4c7c0df9b-Paper-Conference.pdf)
    files/paper/ [2023/file/6f9806a5adc72b5b834b27e4c7c0df9b-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2023/file/6f9806a5adc72b5b834b27e4c7c0df9b-Paper-Conference.pdf)

    - <span id="page-10-15"></span>[8] J. Cai, Y. Wei, Z. Wu, S. Peng, and K. Ma,
    "Inter-layer scheduling space definition and exploration for tiled accelerators,"
    in *Proceedings of the 50th Annual International Symposium on Computer Architecture*,
    2023, pp. 1–17.

    - <span id="page-10-23"></span>[9] T. Chen, T. Moreau, Z. Jiang, L. Zheng, E.
    Yan, H. Shen, M. Cowan, L. Wang, Y. Hu, L. Ceze *et al.*, "{TVM}: An automated
    {End-to-End} optimizing compiler for deep learning," in *13th USENIX Symposium
    on Operating Systems Design and Implementation (OSDI 18)*, 2018, pp. 578–594.

    - <span id="page-10-6"></span>[10] Y.-H. Chen, T. Krishna, J. S. Emer, and V.
    Sze, "Eyeriss: An energyefficient reconfigurable accelerator for deep convolutional
    neural networks," *IEEE Journal of Solid-State Circuits*, vol. 52, no. 1, pp.
    127–138, 2017.

    - <span id="page-10-18"></span>[11] T. Dao, "Flashattention-2: Faster attention
    with better parallelism and work partitioning," 2023.

    - <span id="page-10-19"></span>[12] T. Dao, D. Y. Fu, S. Ermon, A. Rudra, and
    C. Re, "Flashattention: Fast ´ and memory-efficient exact attention with io-awareness,"
    2022.

    - <span id="page-10-20"></span>[13] T. Dettmers, M. Lewis, Y. Belkada, and L.
    Zettlemoyer, "Llm.int8(): 8-bit matrix multiplication for transformers at scale,"
    2022.

    - <span id="page-10-0"></span>[14] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D.
    Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S.
    Gelly *et al.*, "An image is worth 16x16 words: Transformers for image recognition
    at scale," in *The Ninth International Conference on Learning Representations
    (ICLR)*, 2021.

    - <span id="page-10-4"></span>[15] Z. Du, R. Fasthuber, T. Chen, P. Ienne, L.
    Li, T. Luo, X. Feng, Y. Chen, and O. Temam, "Shidiannao: Shifting vision processing
    closer to the sensor," in *2015 ACM/IEEE 42nd Annual International Symposium on
    Computer Architecture (ISCA)*, 2015, pp. 92–104.

    - <span id="page-10-16"></span>[16] M. Gao, X. Yang, J. Pu, M. Horowitz, and C.
    Kozyrakis, "Tangram: Optimized coarse-grained dataflow for scalable nn accelerators,"
    in *Proceedings of the Twenty-Fourth International Conference on Architectural
    Support for Programming Languages and Operating Systems*, 2019, pp. 807–820.

    - <span id="page-10-7"></span>[17] N. P. Jouppi, G. Kurian, S. Li, P. Ma, R. Nagarajan,
    L. Nai, N. Patil, S. Subramanian, A. Swing, B. Towles, C. Young, X. Zhou, Z. Zhou,
    and D. Patterson, "Tpu v4: An optically reconfigurable supercomputer for machine
    learning with hardware support for embeddings," 2023.

    - <span id="page-10-9"></span>[18] S.-C. Kao and T. Krishna, "Gamma: Automating
    the hw mapping of dnn models on accelerators via genetic algorithm," in *2020
    IEEE/ACM International Conference On Computer Aided Design (ICCAD)*, 2020, pp.
    1–9.

    - <span id="page-10-24"></span>[19] S.-C. Kao and T. Krishna, "Magma: An optimization
    framework for mapping multiple dnns on multiple accelerator cores," in *2022 IEEE
    International Symposium on High-Performance Computer Architecture (HPCA)*. IEEE,
    2022, pp. 814–830.

    - <span id="page-10-10"></span>[20] S.-C. Kao, S. Subramanian, G. Agrawal, A.
    Yazdanbakhsh, and T. Krishna, "Flat: An optimized dataflow for mitigating attention
    bottlenecks," in *Proceedings of the 28th ACM International Conference on Architectural
    Support for Programming Languages and Operating Systems, Volume 2*, 2023, pp.
    295–310.

    - <span id="page-10-21"></span>[21] S. Kim, A. Gholami, Z. Yao, M. W. Mahoney,
    and K. Keutzer, "I-bert: Integer-only bert quantization," 2021.

    - <span id="page-10-3"></span>[22] S. Kim, C. Hooper, T. Wattanawong, M. Kang,
    R. Yan, H. Genc, G. Dinh, Q. Huang, K. Keutzer, M. W. Mahoney *et al.*, "Full
    stack optimization of transformer inference: a survey," *arXiv preprint arXiv:2302.14017*,
    2023.

    - <span id="page-10-26"></span>[23] H. Kwon, P. Chatarasi, V. Sarkar, T. Krishna,
    M. Pellauer, and A. Parashar, "Maestro: A data-centric approach to understand
    reuse, performance, and hardware cost of dnn mappings," *IEEE Micro*, vol. 40,
    no. 3, pp. 20–29, 2020.

    - <span id="page-10-8"></span>[24] H. Kwon, A. Samajdar, and T. Krishna, "Maeri:
    Enabling flexible dataflow mapping over dnn accelerators via reconfigurable interconnects,"
    in *Proceedings of the Twenty-Third International Conference on Architectural
    Support for Programming Languages and Operating Systems*, ser. ASPLOS ''18. New
    York, NY, USA: Association for Computing Machinery, 2018, p. 461–475. [Online].
    Available: <https://doi.org/10.1145/3173162.3173176>

    - <span id="page-10-1"></span>[25] OpenAI, "Gpt-4 technical report," *arXiv preprint
    arXiv:2303.08774*, 2023.

    - <span id="page-10-12"></span>[26] OpenAI, "Video generation models as world
    simulators," [https://openai.](https://openai.com/research/video-generation-models-as-world-simulators)
    [com/research/video-generation-models-as-world-simulators,](https://openai.com/research/video-generation-models-as-world-simulators)
    2024, [Online; accessed 04-18-2024].

    - <span id="page-10-25"></span>[27] N. Suda, V. Chandra, G. Dasika, A. Mohanty,
    Y. Ma, S. Vrudhula, J.-s. Seo, and Y. Cao, "Throughput-optimized opencl-based
    fpga accelerator for large-scale convolutional neural networks," in *Proceedings
    of the 2016 ACM/SIGDA international symposium on field-programmable gate arrays*,
    2016, pp. 16–25.

    - <span id="page-10-17"></span>[28] A. Symons, L. Mei, S. Colleman, P. Houshmand,
    S. Karl, and M. Verhelst, "Stream: A modeling framework for fine-grained layer
    fusion on multicore dnn accelerators," in *2023 IEEE International Symposium on
    Performance Analysis of Systems and Software (ISPASS)*. IEEE, 2023, pp. 355–357.

    - <span id="page-10-2"></span>[29] H. Touvron, L. Martin, K. Stone, P. Albert,
    A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel,
    L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu,
    W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R.
    Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S.
    Koura, M.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet,
    T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta,
    K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang,
    R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan,
    M. Kambadur,


    S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom, "Llama 2: Open
    foundation and fine-tuned chat models," 2023.


    - <span id="page-11-3"></span>[30] K. Tushar, H. Kwon, P. Angshuman, P. Michael,
    and S. Ananda, *Data Orchestration in Deep Learning Accelerators*. Springer Cham,
    2020.

    - <span id="page-11-4"></span>[31] N. Vasilache, O. Zinenko, T. Theodoridis, P.
    Goyal, Z. DeVito, W. S. Moses, S. Verdoolaege, A. Adams, and A. Cohen, "Tensor
    comprehensions: Framework-agnostic high-performance machine learning abstractions,"
    *arXiv preprint arXiv:1802.04730*, 2018.

    - <span id="page-11-0"></span>[32] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,
    L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, "Attention is all you need,"
    2023.

    - <span id="page-11-2"></span>[33] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth,
    and S. Han, "Smoothquant: Accurate and efficient post-training quantization for
    large language models," 2024.

    - <span id="page-11-1"></span>[34] Z. Zhang, Y. Zhang, S. Ge, G. Weng, M. Narang,
    X. Song, and S. Tiwary, "Genserp: Large language models for whole page presentation,"
    2024.

    - <span id="page-11-5"></span>[35] L. Zheng, C. Jia, M. Sun, Z. Wu, C. H. Yu,
    A. Haj-Ali, Y. Wang, J. Yang, D. Zhuo, K. Sen, J. E. Gonzalez, and I. Stoica,
    "Ansor: generating high-performance tensor programs for deep learning," in *Proceedings
    of the 14th USENIX Conference on Operating Systems Design and Implementation*,
    ser. OSDI''20. USA: USENIX Association, 2020.'
- title: "The BRAM is the Limit: Shattering Myths, Shaping Standards, and Building\n\
    \  Scalable PIM Accelerators"
  abstract: "Many recent FPGA-based Processor-in-Memory (PIM) architectures have appeared\n\
    with promises of impressive levels of parallelism but with performance that\n\
    falls short of expectations due to reduced maximum clock frequencies, an\ninability\
    \ to scale processing elements up to the maximum BRAM capacity, and\nminimal hardware\
    \ support for large reduction operations. In this paper, we\nfirst establish what\
    \ we believe should be a \"Gold Standard\" set of design\nobjectives for PIM-based\
    \ FPGA designs. This Gold Standard was established to\nserve as an absolute metric\
    \ for comparing PIMs developed on different\ntechnology nodes and vendor families\
    \ as well as an aspirational goal for\ndesigners.\n  We then present IMAGine,\
    \ an In-Memory Accelerated GEMV engine used as a case\nstudy to show the Gold\
    \ Standard can be realized in practice. IMAGine serves as\nan existence proof\
    \ that dispels several myths surrounding what is normally\naccepted as clocking\
    \ and scaling FPGA performance limitations. Specifically,\nIMAGine clocks at the\
    \ maximum frequency of the BRAM and scales to 100% of the\navailable BRAMs. Comparative\
    \ analyses are presented showing execution speeds\nover existing PIM-based GEMV\
    \ engines on FPGAs and achieving a 2.65x - 3.2x\nfaster clock. An AMD Alveo U55\
    \ implementation is presented that achieves a\nsystem clock speed of 737 MHz,\
    \ providing 64K bit-serial multiply-accumulate\n(MAC) units for GEMV operation.\
    \ This establishes IMAGine as the fastest\nPIM-based GEMV overlay, outperforming\
    \ even the custom PIM-based FPGA\naccelerators reported to date. Additionally,\
    \ it surpasses TPU v1-v2 and Alibaba\nHanguang 800 in clock speed while offering\
    \ an equal or greater number of MAC\nunits."
  url: http://arxiv.org/abs/2410.07546v1
  keywords: Processing-in-Memory, Gold Standard, System Design, Block RAM, GEMV engine,
    Processor Array.
  document: '© 2024 IEEE. Personal use of this material is permitted. Permission from
    IEEE must be obtained for all other uses, in any current or future media, including
    reprinting/republishing this material for advertising or promotional purposes,
    creating new collective works, for resale or redistribution to servers or lists,
    or reuse of any copyrighted component of this work in other works.


    This work is an extended version of a poster, presented at the 2024 32nd IEEE
    International Symposium on Field-Programmable Custom Computing Machines (FCCM)
    and will appear in the proceedings and on the IEEE website soon.


    # The BRAM is the Limit: Shattering Myths, Shaping Standards, and Building Scalable
    PIM Accelerators


    MD Arafat Kabir<sup>∗</sup> , Tendayi Kamucheka<sup>∗</sup> , Nathaniel Fredricks<sup>∗</sup>
    ,


    Joel Mandebi† , Jason Bakos‡ , Miaoqing Huang<sup>∗</sup> , and David Andrews<sup>∗</sup>


    <sup>∗</sup>Department of Electrical Engineering and Computer Science, University
    of Arkansas,


    ‡Department of Computer Science and Engineering, University of South Carolina,


    †Advanced Micro Devices, Inc. (AMD)


    {makabir, tfkamuch, njfredri, mqhuang, dandrews}@uark.edu, jmandebi@amd.com, jbakos@cse.sc.edu,


    *Abstract*—Many recent FPGA based Processor-in-Memory (PIM) architectures have
    appeared with promises of impressive levels of parallelism but with performance
    that falls short of expectations due to reduced maximum clock frequencies, an
    inability to scale processing elements up to the maximum BRAM capacity, and minimal
    hardware support for large reduction operations. In this paper, we first establish
    what we believe should be a "Gold Standard" set of design objectives for PIM-based
    FPGA designs. This Gold Standard was established to serve as an absolute metric
    for comparing PIMs developed on different technology nodes and vendor families
    as well as an aspirational goal for designers.


    We then present IMAGine, an In-Memory Accelerated GEMV engine used as a case study
    to show the Gold Standard can be realized in practice. IMAGine serves as an existence
    proof that dispels several myths surrounding what is normally accepted as clocking
    and scaling FPGA performance limitations. Specifically, IMAGine clocks at the
    maximum frequency of the BRAM and scales to 100% of the available BRAMs. Comparative
    analyses are presented showing execution speeds over existing PIM-based GEMV engines
    on FPGAs and achieving a 2.65× – 3.2× faster clock. An AMD Alveo U55 implementation
    is presented that achieves a system clock speed of 737 MHz, providing 64K bitserial
    multiply-accumulate (MAC) units for GEMV operation. This establishes IMAGine as
    the fastest PIM-based GEMV overlay, outperforming even the custom PIM-based FPGA
    accelerators reported to date. Additionally, it surpasses TPU v1-v2 and Alibaba
    Hanguang 800 in clock speed while offering an equal or greater number of MAC units.


    *Index Terms*—Processing-in-Memory, Gold Standard, System Design, Block RAM, GEMV
    engine, Processor Array.


    #### I. INTRODUCTION


    The exponential growth of network-connected devices (IoT) and social media applications
    has significantly changed the landscape of computing workloads. Modern workloads,
    such as scientific computation, graph processing, and machine learning, generate
    and process datasets that are expanding at a rate that outpaces Moore''s Law [\[1\]](#page-11-0).
    However, today''s processors remain constrained by the "Memory Wall" of the von
    Neumann architecture, which limits the ability to exploit the parallelism within
    these memory-intensive tasks. Processingin-memory (PIM) or Compute-in-Memory (CIM)
    architectures are being pursued [\[2\]](#page-11-1)–[\[13\]](#page-11-2) to mitigate
    the memory wall and enable processing performance to scale with memory capacity.


    This material is based upon work supported by the National Science Foundation
    under Grant No. 1955820.


    Modern Field Programmable Gate Arrays (FPGAs) with 100s of Mbits of SRAM distributed
    throughout the device in the form of disaggregated memory resources can provide
    several TB/s of internal bandwidth. This is an ideal programmable substrate for
    creating customized Processor In/Near Memory accelerators. Several PIM array-based
    accelerator designs [\[6\]](#page-11-3)– [\[11\]](#page-11-4) have been proposed
    to harness this massive internal bandwidth. However, results reported to date
    reinforce a longheld community belief that competitive overlays and not toy examples
    cannot clock at the maximum frequency of a BRAM or reach a compute density sufficient
    to compete with their custom Application Specific Integrated Circuit (ASIC) counterparts.
    These limitations have served as motivation for researchers to propose redesigned
    Block-RAM (BRAMs)-LUT integrated PIM tiles to increase compute densities of FPGAs.


    While compute density is increased the maximum BRAM clock frequency is reduced
    similar to overlays. Additional scalability issues are also introduced at the
    system level: a faster and larger device does not imply a faster system speed
    or a linear increase in the compute units with increased BRAM density. While each
    proposal offers some relative improvement over prior designs, there does not yet
    exist an absolute metric or yardstick that could be used to evaluate the efficiency
    of these designs. This has made it difficult to perform quantitative comparisons
    between PIM arrays implemented in different logic families from one vendor or
    between different vendors.


    This paper first lays out a set of design objectives that form a "Gold Standard"
    or theoretical upper limit for BRAM-LUT-based PIM array architectures. This standard
    can serve as an absolute metric for comparing the efficiency of existing designs
    while also serving as an aspirational set of design objectives for future designs.
    The key implementation challenges for each objective are discussed and the approaches
    we employed to create IMAGine, a PIM design that approaches the theoretical performance
    of the Gold Standard. Run time results show that IMAGine shatters some of the
    myths concerning performance limitations of FPGA overlays. Our contributions can
    be summarized as follows,


    - A set of Gold Standard design goals for PIM array-based accelerators. We argue
    these goals need to be met to claim a "Scalable High-Performance PIM design" on
    FPGAs.

    - We develop a case study that explores the design chal-


    lenges and techniques that must be mastered to achieve those goals.


    - We present IMAGine, an In-Memory Accelerated GEMV engine, as the fastest FPGA
    PIM-based GEMV accelerator that clocks faster than Google''s TPU v1-v2.

    - We study IMAGine and existing PIM-based FPGA accelerators to demonstrate how
    the proposed standard can be used as a guide to make near-optimal design choices.


    Our designs will be published as open-source implementations and freely available
    for study, use, modification, and distribution without restriction.


    ## II. RELATED WORK


    ## <span id="page-2-0"></span>*A. Custom-BRAM PIMs*


    Wang et al [\[6\]](#page-11-3) proposed the Compute-Capable BRAM (CCB) as a PIM
    tile (or block) based on Neural Cache [\[14\]](#page-11-5). CCB exposes compute
    parallelism within a BRAM by converting each BRAM bitline into a bit-serial Processing
    Element (PE). This comes at the cost of increased implementation complexity, as
    activating multiple wordlines requiring extra voltage supply and reduced voltage
    levels for robustness. CCB was used to build RIMA [\[6\]](#page-11-3) to accelerate
    recurrent neural networks (RNNs). RIMA achieved 1.25× and 3× higher performance
    compared to the Brainwave DL soft processor [\[15\]](#page-11-6) for 8-bit integer
    and block floating-point precisions, respectively. On a CCB-enhanced Stratix 10
    FPGA, RIMA achieved an order-ofmagnitude higher performance over a comparable
    GPU.


    Arora et al [\[8\]](#page-11-7), [\[9\]](#page-11-8) proposed CoMeFa. Similar
    to CCB, CoMeFa employs bit-serial PEs per SRAM bitline but exploits the dual-port
    nature of a BRAM to read two operands without requiring voltage supply modifications.
    Still, adjustments to Sense Amplifiers (SAs), additional latches for time multiplexing,
    and SA cycling are required. To evaluate the performance and energy benefits of
    CoMeFa RAMs, various microbenchmarks, including General Matrix-Vector Multiplication
    (GEMV) and General Matrix-Matrix Multiplication (GEMM) were studied in [\[9\]](#page-11-8).
    Augmenting an Intel Arria 10 like FPGA with CoMeFa RAMs delivered a geomean speedup
    of 2.55× across diverse applications.


    CCB and CoMeFa use a transposed layout [\[6\]](#page-11-3), [\[9\]](#page-11-8)
    to store data along the bitlines in a column-major format, introducing additional
    latency due to conversion from the original data''s row-major format. Chen et
    al addressed this issue in BRAMAC [\[10\]](#page-11-9) and M4BRAM [\[11\]](#page-11-4).
    They avoid computing on the slow and power-intensive primary BRAM array by copying
    operands to a smaller "dummy array" for MAC operations. BRAMAC requires 2-/4-/8-bit
    predefined weights and activations, limiting its use to quantized uniform-precision
    deep neural nets. M4BRAM overcomes some of these limitations by enabling variable
    activation precision between 2 and 8 bits with linearly scaled MAC latency. BRAMAC
    was integrated into Intel''s Deep Learning Accelerator (DLA) [\[16\]](#page-11-10)
    targetting Alexnet and ResNet-34 convolution models. Combining BRAMAC-2SA/BRAMAC-1DA
    with Intel''s DLA resulted in an average speedup of 2.05×/1.7× for AlexNet and
    1.33×/1.52× for ResNet-34. Similarly evaluated using Intel''s DLA, M4BRAM showed
    an average speedup of 2.16× with less than 0.5% accuracy loss using mixed-precisions
    compared to a full-precision baseline. M4BRAM surpassed BRAMAC by an average of
    1.43× across diverse benchmarks.


    # *B. BRAM-Overlay PIMs*


    The proposed custom BRAM-based PIM architectures show promise for the future but
    are currently unavailable in mainstream FPGAs. To leverage the benefits of PIM
    architectures in contemporary FPGAs, PIM overlay architectures have been proposed.
    Panahi et al [\[7\]](#page-11-11), [\[17\]](#page-11-12), [\[18\]](#page-11-13)
    proposed a PIM overlay as part of the SPAR-2 accelerator, connecting bit-serial
    PEs from the programmable fabric with BRAMs. SPAR-2 PIM blocks contain a 4×4 PE
    array with North-East-West-South (NEWS) network support, forming a 2D array of
    SIMD bitserial processors at the system level.


    SPAR-2 was implemented on Virtex-7 and Virtex UltraScale FPGAs with 10K PEs to
    accelerate MLP, LSTM, GRU, and CNN [\[18\]](#page-11-13) deep learning applications.
    It achieved up to 34.2× and 3.5× speedups compared to other custom HLS-based and
    RTL-based accelerators, respectively.


    Building upon the PIM overlay of SPAR-2 (SPAR2-PIM), Kabir et al proposed PiCaSO
    [\[13\]](#page-11-2) with configurable pipeline stages along the datapath. PiCaSO
    introduced an intermediate muxing module to enable zero-copy in-block reduction
    and a "binary-hopping" pipelined NEWS network for arraylevel reduction. PiCaSO
    provided competitive performance and memory utilization efficiency compared to
    both CCB and CoMeFa custom-BRAM architectures.


    # *C. PIM Emulators*


    Mosanu et al [\[19\]](#page-11-14) proposed a synthesizable SystemVerilog model
    for prototyping PIM architectures on their emulation platform PiMulator. Their
    model is compatible with other FPGA-SoC frameworks, providing a high degree of
    usability in the FPGA and RISC-V ecosystems.


    Dervay et al [\[20\]](#page-11-15) proposed a synthesizable RTL model of CIM (PIM)
    for their open-source emulation framework CIMulator. They defined an instruction
    set for the model to integrate it with a RISC processor. A fully functional processor/CIM
    system was demonstrated featuring cycle-accurate simulation/emulation, instruction-level
    energy estimation, and end-to-end program execution with guaranteed correctness.


    ## III. DEFINING A GOLD STANDARD


    <span id="page-2-1"></span>Table [I](#page-3-0) summarizes the maximum achieved
    frequencies of the PIM designs discussed in section [II.](#page-2-0) From the
    relative frequency columns (Rel.), it is observable that the clock frequency fP
    IM of all the PIM Tiles (Blocks), both overlays and custom BRAMs, are significantly
    slower compared to the maximum frequency for the device BRAMs (fBRAM), with the
    exception of PiCaSO. Their system frequencies (fSys) are 2.1× – 3.7× slower than
    the BRAM maximum frequencies (fBRAM). This decrease in system frequency was attributed
    to the limitations of the soft logic and the routing resources


    <span id="page-3-0"></span>TABLE I MAXIMUM FREQUENCY (MHZ) OF EXISTING FPGA-PIM
    DESIGNS


    | PIM Design        | Type   | Device              | fBRAM fP IM |     | Rel.
    | fSys Rel. |         |

    |-------------------|--------|---------------------|-------------|-----|------|-----------|---------|

    | CCB               | Custom | Stratix 10          | 1000        | 624 | 62%  |           |
    455 46% |

    | CoMeFa-A          | Custom | Arria 10            | 730         | 294 | 40%  |           |
    288 39% |

    | CoMeFa-D          | Custom | Arria 10            | 730         | 588 | 81%  |           |
    292 40% |

    | BRAMAC-2SA Custom |        | Arria 10            | 730         | 586 | 80%  |
    -         | -       |

    | BRAMAC-1DA Custom |        | Arria 10            | 730         | 500 | 68%  |
    -         | -       |

    | M4BRAM            | Custom | Arria 10            | 730         | 553 | 76%  |
    -         | -       |

    | SPAR-2            |        | Overlay UltraScale+ | 737         | 445 | 60%  |           |
    200 27% |

    | PiMulator         |        | Overlay UltraScale+ | 737         | -   | -    |           |
    333 45% |

    | PiCaSO            |        | Overlay UltraScale+ | 737         | 737 | 100%
    | -         | -       |


    of the FPGAs. It was also reported as unlikely that an FPGA accelerator at the
    system level would operate at a frequency surpassing the degraded frequency (fP
    IM) of these PIM designs, even in a more advanced node than the evaluation platforms
    [\[8\]](#page-11-7)–[\[11\]](#page-11-4).


    Further observation yielded that most of these systems could not utilize all available
    BRAMs as PIMs. This lower utilization combined with a lower clock frequency results
    in less efficient use of the available internal BRAM bandwidth of the devices
    and a lower system-level compute density. A final observation shows a troubling
    common pattern: as the utilization of BRAMs increases the achievable system-level
    clock frequency decreases [\[6\]](#page-11-3), [\[9\]](#page-11-8).


    These observations motivated our interest in understanding if these results were
    a new reality of BRAM PIM arrays or symptomatic of specific design decisions and
    implementation choices. This led us to ask two questions. What is the fastest
    frequency a PIM-based design should/could be able to achieve on FPGAs, and was
    it possible to scale compute density up to the maximum BRAM capacity without degrading
    the clock frequency?


    We posited that compute efficiency should be measured relative to an FPGA''s full
    internal bandwidth and only limited by the density of the devices BRAMs. This
    would require a PIM architecture to run at the BRAM maximum speed with compute
    density scaling linearly up to the full density of BRAMs on a device. We termed
    such an ideal PIM architecture as our Gold Standard.


    # <span id="page-3-2"></span>*A. Ideal Clocking*


    In FPGAs, BRAMs are the single component with the longest latency [\[21\]](#page-11-16)–[\[23\]](#page-11-17),
    thus representing the timing bottleneck for setting clock speed. Though in a typical
    FPGA design, the logic and routing delays can dominate the overall path delays,
    at the unit level, FPGA resources like LUTs, FF, and routing blocks are much faster
    than the BRAM. Thus, we define the maximum frequency (Fmax) of the BRAM as the
    Gold Standard clock frequency for the PIM accelerator. This requires processing
    elements along bit lines to be designed such that they do not degrade this frequency.


    To assess the practicality of this standard, we closely examined two AMD FPGA
    families: Virtex-7 and UltraScale+. While Virtex-7 CLB resource''s delay numbers
    are available in the datasheet [\[21\]](#page-11-16), those numbers are not publicly
    available for UltraScale+ devices. Thus, we created a design where


    <span id="page-3-1"></span>TABLE II DELAY (NS) BREAKDOWN OF 1-LEVEL LOGIC PATH
    IN AMD DEVICES


    |    |       |      | FF-C2Q1 LUT2 FF-Setup | Total3 |       | BRAM4 Net Budget
    | Min5  |

    |----|-------|------|-----------------------|--------|-------|------------------|-------|

    | V7 | 0.290 | 0.34 | 0.255                 | 0.885  | 1.839 | 0.954            |
    0.272 |


    US+ 0.087 0.15 0.098 0.335 1.356 1.021 0.102


    <sup>1</sup> Clock-to-Q delay of flip-flops <sup>2</sup> Average of delay through
    LUTs


    <sup>3</sup> Total cell delay


    <sup>4</sup> BRAM pulse-width requirement, clock period for Fmax


    <sup>5</sup> Minimum net delay through a switchbox


    all timing paths are one logic level deep and averaged all paths to obtain Table
    [II.](#page-3-1) The Total column sums the cell delays in the columns to its left.
    The BRAM column lists the clock period for BRAM Fmax. The Min column displays
    the minimum delay of a net passing through a switchbox. Net Budget is derived
    by subtracting the Total column from the BRAM column. Comparing the net budget
    with the minimum net delay shows the possibility of designing at least two LUTs
    deep logic paths that can run at the BRAM Fmax on these device families.


    In certain FPGA families, achieving this constraint may be challenging due to
    the presence of multiple dies and fixedfunction blocks supporting various functionalities,
    such as PLLs, high-performance IO blocks, PCIe blocks [\[24\]](#page-11-18), and
    application processors [\[25\]](#page-11-19). These blocks impact placement and
    routing, eventually affecting the maximum achievable clock frequency. In such
    devices, this Gold Standard may not be achievable. However, FPGA vendors can employ
    this clocking standard to ensure the achievability of this constraint in device
    families targeting PIM designs.


    *1) Design Challenges:* Achieving the Gold Standard for clock speed involves addressing
    challenges at the architectural level. System logic residing in the same clock
    domain as the BRAM should match or exceed the BRAM Fmax. This may require adding
    optional pipelines in the design, which can be enabled at a later stage of implementation
    if the logic depth is limiting the clock. PIM array-based designs typically share
    control logic across multiple PIM blocks [\[6\]](#page-11-3), [\[7\]](#page-11-11),
    [\[18\]](#page-11-13), resulting in high fanout nets which can impact the system
    clock. To address this issue, the control logic can be replicated, thereby reducing
    the fanout. If high fanout is unavoidable, pipelined fanout trees should be synthesized
    for these signals.


    Despite the aforementioned design considerations, timing failures may still occur
    during placement and routing due to long routes causing excessive net delays.
    To mitigate this, the system architecture should employ a tile-based approach
    at the system level [\[6\]](#page-11-3)–[\[9\]](#page-11-8) to localize logic
    and routing. The RTL design should be implemented with an awareness of potential
    placement and routing issues. If feasible, alternative implementations should
    be incorporated to be selected during the implementation stage to address placement
    and routing issues.


    # *B. Ideal Scaling of Peak-Performance*


    The key advantage of a PIM architecture is the massive parallelism it can offer:
    all concurrent bitlines of the memory


    ![](_page_4_Figure_0.jpeg)


    <span id="page-4-0"></span>Fig. 1. Ideal scaling vs. actual TOPS of RIMA on Stratix
    10 GX2800


    array can be designed into concurrent processing elements [\[6\]](#page-11-3),
    [\[8\]](#page-11-7), [\[9\]](#page-11-8), [\[14\]](#page-11-5). As a Gold Standard,
    we posited that the peakperformance of a PIM design needs to scale linearly with
    the on-chip BRAM count. Existing PIM designs do not adhere to this standard.


    The compute capacity in custom-BRAM-based PIM designs [\[6\]](#page-11-3), [\[8\]](#page-11-7)–[\[11\]](#page-11-4)
    scales linearly with BRAM count if all BRAM tiles are used in PIM mode. However,
    a significant sacrifice is imposed in the clock frequency that ends up limiting
    the achievable peak-performance on the device. Table [I](#page-3-0) fP IM column
    indicates that the custom-BRAM PIM designs run up to 2.5× slower than the BRAM
    Fmax. In addition, their system frequency decreases with an increase in BRAM usage.
    Fig. [1](#page-4-0) plots RIMA''s peak-performance from Table-II of [\[6\]](#page-11-3),
    computed using reported BRAM utilization and M-DPE clock frequency. The irregular
    trend is attributed to RIMA''s systemlevel architecture. If RIMA adhered to the
    ideal scaling Gold Standard, even at the degraded CCB frequency of 624 MHz, its
    peak-performance would align with the CCB Ideal TOPS line. The gap between these
    plots represents wasted compute capacity and memory bandwidth provided by CCB
    BRAMs.


    *1) Design Challenges:* In PIM accelerators, PIM blocks handle computation, while
    the rest of the system primarily manages data pipelines and control logic [\[6\]](#page-11-3)–[\[9\]](#page-11-8).
    The main obstacle to ideal scaling in existing PIM accelerators is the high logic
    utilization in the rest of the system. The same design principle as in ideal clocking
    must be applied here as well: the BRAM should be the utilization bottleneck, not
    the control logic. To prevent the control logic from becoming the utilization
    bottleneck, controllers should be shared between multiple PIM blocks. Control
    signals should be designed to minimize the number of unique control sets in the
    system. A control set is the group of control signals (set/reset, clock enable,
    and clock) for a register or latch [\[26\]](#page-11-20). Excessive unique control
    sets can degrade placement, impacting system scalability and clock frequency.
    Overall, the rest of the system should complement the PIM array without limiting
    its scalability and performance.


    #### *C. Ideal Reduction Latency*


    Reduction is a critical operation in applications like GEMM and deep learning
    as it requires data movement throughout the distributed BRAM memories and/or processing
    elements of the PIM array. Accumulation, a common reduction operation, is often
    implemented using a pipelined adder tree [\[6\]](#page-11-3), [\[9\]](#page-11-8)
    as shown in Fig. [2.](#page-4-1) Adder trees are resource-hungry, especially on
    routing, requiring more adders and routing resources as


    ![](_page_4_Figure_7.jpeg)


    <span id="page-4-1"></span>Fig. 2. Reduction latency breakdown of a pipelined
    binary adder tree maintaining ideal clocking constraint.


    TABLE III PARAMETERS OF GOLD STANDARD FOR REDUCTION LATENCY


    <span id="page-4-2"></span>


    | Parameter | Ideal Range | Related to                             |

    |-----------|-------------|----------------------------------------|

    | a         | 1/N ≤ a ≤ 2 | Latency of reduction steps (addition)  |

    | b         | 0 ≤ b ≤ 1   | Latency of data movement               |

    | c         | 0 ≤ c       | Cycles spent outside reduction network |


    the PIM row size increases. Bit-parallel implementations are even harder to manage
    compared to bit-serial: INT8 requires 8× more logic and routing resources than
    bit-serial implementations. Such high utilization can subsequently affect system
    frequency and scalability.


    To prevent the reduction network from becoming the utilization and routing bottleneck,
    sharing logic and routing resources with the rest of the system may be necessary.
    Existing high-performance reduction architectures for FPGA implementation [\[27\]](#page-11-21)–[\[32\]](#page-11-22)
    highlight the unavoidable trade-off between reduction latency and resource utilization.
    To guide this trade-off, we propose the Gold Standard for reduction latency as
    the following empirical model,


    $$\text{Array-Level Reduction}\_{\text{gold}} = aN \log P + bP + c \quad (\text{l})$$


    <span id="page-4-4"></span><span id="page-4-3"></span>

    $$\text{In-Block Reduction}\_{\text{gold}} = aN \log k \tag{2}$$


    In this context, a PIM "block" is a single BRAM tile, like CCB, BRAMAC, PiCaSO,
    etc., along with its related logic. In-Block reduction generates partial sums
    accumulating the PEs in a PIM block, while array-level reduction accumulates these
    partial sums. Here, P = no. of partial sums obtained from all PIM columns involved
    in the reduction process, N = operand width (precision), k = no. of PE columns
    in a PIM block; a, b, c are implementation-specific parameters with their ideal
    range specified in Table [III.](#page-4-2)


    The intuition behind [\(1\)](#page-4-3) and the parameter ranges is explained
    using Fig. [2.](#page-4-1) The total reduction (accumulation) latency can be broken
    down into two parts: reduction operation (add) and data movement. The term aN
    log P represents the latency of reduction operations (add) only, requiring at
    least log P reduction steps; the base of the log represents the number of operands
    reduced per step, typically 2. The PE architecture (bit-serial, bit-sliced, or
    bit-parallel) determines the value of a. The lower bound of a is 1/N because at
    least one cycle is needed per reduction step (aN ≥ 1). We set the upper bound
    for a to 2 because bit-serial PEs, especially in overlays, commonly require 2
    cycles to process each bit (aN = 2N) of the operand [\[7\]](#page-11-11), [\[13\]](#page-11-2),
    [\[18\]](#page-11-13).


    Under the ideal clocking constraint, data-movement latency depends linearly on
    the number of PIM columns in a row,


    <span id="page-5-0"></span>TABLE IV REDUCTION/ACCUMULATION LATENCY OF EXISTING
    PIM DESIGNS


    |                     | Block Level            | PIM Array Level    | Utilization1
    |

    |---------------------|------------------------|--------------------|--------------|

    | Linear-Add2<br>[18] | 3N (k-1)               | 3N (P-1)           | Low          |

    | Binary-Add2<br>[18] | 2Nlog(k) + N(k-1)      | 2Nlog(P) + N(P-1)  | Low          |

    | CCB/CoMeFa [6]      | 2Nlog(k) + log2<br>(k) | log(P) + 2         | High         |

    | Binary-hopping [13] | (N+4) log(k)           | (N+4) log(P) + P-1 | Medium       |

    | Proposed Standard   | aNlog(k)               | aNlog(P) + bP + c  | Balanced3    |


    <sup>1</sup> Qualitative logic and routing resource utilization


    <sup>2</sup> NEWS network with linear shift then add


    <sup>3</sup> Offers latency vs. resource utilization trade-off


    represented by bP. Assume each PIM block (p) in Fig. [2](#page-4-1) occupies a
    large enough area such that a net does extend beyond two consecutive PIM blocks
    without violating the ideal clocking constraint. Then constructing the adder tree
    without sacrificing the clock speed requires one pipeline stage to be placed along
    each PIM column as in Fig. [2.](#page-4-1) This requires P/2 cycles (b = 1/2)
    for accumulation towards the middle in this example. In general, the value of
    b depends on the speed of FPGA''s routing resource relative to BRAM Fmax. Typically
    b < 1 in modern FPGAs with fast routing resources allowing nets to span multiple
    PIM columns at BRAM Fmax. The lower bound of b is 0, corresponding to a bit-parallel
    pipelined reduction tree that perfectly overlaps data movement with computation.


    In [\(1\)](#page-4-3), c represents delays outside the reduction network. The
    In-Block partial-sum generation latency [\(2\)](#page-4-4) is a simpler version
    of [\(1\)](#page-4-3), where k denotes the number of PE columns accumulated within
    the PIM block. A term for pipelined data movement is absent as signals can move
    within a block without violating the ideal timing constraint. In-Block reduction
    latency [\(2\)](#page-4-4) can be absorbed into c, making [\(1\)](#page-4-3) as
    the overall reduction latency standard. The lower bound of c is 0, representing
    no additional cycles outside of the reduction network. Having no upper bound allows
    the architecture to vary without violating the Gold Standard; some preprocessing
    steps may be employed such as the pop-count-based adder in RIMA [\[6\]](#page-11-3),
    before entering the reduction network that requires tens or hundreds of cycles.


    Though the proposed standard [\(1\)](#page-4-3) closely resembles a pipelined
    adder tree, it can provide insights and help identify design inefficiencies in
    various other architectures. Importantly it can steer designers towards optimal
    or near-optimal reduction network designs. We explore this in Section [V](#page-6-0)
    through a quantitative study of existing designs.


    *1) Design Approaches:* Table [IV](#page-5-0) shows the latency equations for
    bit-serial PIM architectures [\[6\]](#page-11-3), [\[7\]](#page-11-11), [\[9\]](#page-11-8),
    [\[13\]](#page-11-2) and their relative utilization of resources. The bit-parallel
    architectures [\[10\]](#page-11-9), [\[11\]](#page-11-4) are not listed because
    the block-level reduction is built into their MAC units and they do not have a
    proposed implementation for array-level reduction.


    SPAR-2 [\[18\]](#page-11-13) implements two reduction approaches: linear shift
    through a NEWS network of PE columns followed by either linear-add or binary-add.
    The binary-add approach was an optimization over the linear-add to reduce the
    number of


    ![](_page_5_Figure_10.jpeg)


    <span id="page-5-1"></span>Fig. 3. System architecture of IMAGine illustrating
    the data and instruction flow (a) through the GEMV engine and (b) within GEMV
    tiles.


    add operations. Block-level accumulation in [\[6\]](#page-11-3), [\[9\]](#page-11-8)
    involves across-bitline copies, bit-serial add, and a pop-count-based adder. Its
    log<sup>2</sup> (k) term is due to the increase in the precision by 1-bit after
    each iteration. Based on the limited discussion in [\[6\]](#page-11-3), we assume
    a perfect "global reduction tree" with 2 cycles pipeline overhead, and has the
    array-level latency presented in CCB/CoMeFa row in Table [IV.](#page-5-0) The
    binaryhopping reduction network in [\[13\]](#page-11-2) utilizes a NEWS network
    with register stages between PIM blocks, enabling direct copying between distant
    blocks without intermediate writes; a pipelined muxing module eliminates the overhead
    of acrossbitline copies in block-level reduction.


    The NEWS network is the slowest but simplest, requiring minimal resources, while
    the global reduction tree is the fastest but demands the highest resources. Binary-hopping
    NEWS achieves faster speed than vanilla NEWS at the expense of higher utilization.
    These implementations inherently trade off latency and resource utilization, aligning
    with the proposed standard [\(1\)](#page-4-3). In the analysis section, we will
    quantify these latencies and show how the standard can help identify design inefficiencies
    and guide toward optimal design.


    ## IV. GOLD STANDARD CASE-STUDY: IMAGINE


    While the Gold Standard is an ultimate objective to strive for, it can also serve
    as a framework for crafting an efficient architecture of a PIM accelerator on
    FPGAs. In this section, we discuss the architecture of IMAGine, a PIM array-based
    GEMV accelerator, that will be implemented and analyzed in Section [V](#page-6-0)
    on an off-the-shelf FPGA. IMAGine serves a dual purpose: it provides a case study
    demonstrating the use of the Gold Standard to make better design choices and evaluate
    how achievable those standards are in a practical design.


    ## *A. System-Level Architecture*


    The top-level system is illustrated in Fig. [3\(](#page-5-1)a). It consists of
    (1) a 2D array of GEMV tiles, (2) a set of input registers, (3) a fanout tree
    connecting the input registers to the tile array, and (4) a column of shift-registers
    to read out the final result. The input registers are used by the front-end processor
    to send instructions to the GEMV tile controllers. The fanout tree is parameterized
    to be adjusted during implementation. The 2D tile array is implemented as a parameterized
    module that instantiates and connects GEMV tiles to build the tile array.


    ![](_page_6_Figure_0.jpeg)


    <span id="page-6-1"></span>Fig. 4. Architectures of (a) GEMV controller and (b)
    PiCaSO-IM, the adapted version of PiCaSO-F [\[13\]](#page-11-2).


    The bits written to the register file of the leftmost PE in the array are shifted
    into the column shift registers. At the end of the GEMV operation, the output
    vector is stored in the column shift registers, which can be shifted up and read
    through the FIFO-out port, one element per cycle.


    #### <span id="page-6-3"></span>*B. IMAGine Tile Architecture*


    Illustrated in Fig. [3\(](#page-5-1)b), the GEMV tile is the heart of IMAGine.
    It consists of (1) an FSM-based controller, (2) a 2D array of PIM blocks, and
    (3) a fanout tree between them. The controller receives the instruction written
    to the input registers at the top level, decodes it, and generates the sequence
    of control signals needed to execute the instruction. The fanout tree connects
    the control signals to all PEs in the PIM array and is parameterized for adjustment
    during implementation. The PIM array interfaces allow cascading with arrays in
    neighboring tiles on each side. During accumulation, partial results move from
    east to west through PIM arrays, ultimately accumulating in the left-most PE column
    of the left-most GEMV tile in a row.


    #### *C. Tile Controller*


    Fig. [4\(](#page-6-1)a) shows the architecture of the tile controller. As discussed
    in Section [III-A,](#page-3-2) logic paths need to be short enough to achieve
    the ideal clock rate. However, estimating precise logic depth during RTL design
    is challenging and the requirement varies across devices. Thus, we grouped the
    combinatorial logic into meaningful steps and added optional pipeline stages illustrated
    by the dashed lines A, B, and C in Fig. [4\(](#page-6-1)a). Running synthesis,
    we ensured that each step could be implemented in one or two logic levels.


    The controller takes a 30-bit instruction, which is executed by either the single-cycle
    or the multicycle driver. The 2-state driver-selection FSM enables any one of
    them based on the opcode. The single-cycle driver can execute one instruction
    every cycle, while the multicycle driver takes several cycles to execute instructions
    like ADD, SUB, MULT, etc. including an additional cycle to load its parameters
    from the Op-Params module. All inputs and outputs are registered to localize timing
    paths within the controller.


    #### <span id="page-6-4"></span>*D. PIM Module*


    We based the design of the IMAGine PIM module called PiCaSO-IM on PiCaSO [\[13\]](#page-11-2)
    for the following three reasons:


    <span id="page-6-2"></span>TABLE V UTILIZATION AND CLOCK FREQUENCY OF MODIFIED
    PICASO 4×4 TILE


    | LUT   | FF          | Slice        | DSP        | BRAM   | Max Freq. |

    |-------|-------------|--------------|------------|--------|-----------|

    |       |             |              |            |        | 737 MHz   |

    | 49    | 113         | 15           | 0          | 0.5    | 737 MHz   |

    |       |             |              |            |        | 737 MHz   |

    | 85    | 125         | 18           | 0          | 0.5    | 737 MHz   |

    | 74.7% | 10.6%       | 8.6%         | -          | 0.0%   | 0%        |

    |       | 774<br>1352 | 1799<br>1989 | 243<br>264 | 0<br>0 | 8<br>8    |


    (1) it is publicly available and open-source [\[33\]](#page-11-23), (2) it is
    a modifiable overlay that can be ported and studied on existing AMD devices, and
    (3) PiCaSO-F, a pipelined configuration of PiCaSO, already clocks at the BRAM
    Fmax.


    Modifications to PiCaSO-F were needed to enhance its control capabilities to implement
    the GEMV tile. These modifications are highlighted in red in Fig. [4\(](#page-6-1)b).
    The additional logic supporting the original NEWS network was removed, keeping
    only the parts needed for east-to-west data movement. PiCaSO-F lacked control
    signals for selectively enabling/disabling a block required in IMAGine. Block-ID-based
    selection logic was included in PiCaSO-IM. Our accumulation algorithm needed 3
    addresses to maximize the overlap of data movement and computation. As PiCaSO-F
    supported only 2 simultaneous addresses, we added a pointer register for the third
    address.


    If PiCaSO is realized as a custom-BRAM tile as proposed in [\[13\]](#page-11-2),
    these changes can be implemented in programmable logic fabric, keeping registerfile,
    OpMux, and ALU modules within the BRAM tile. We name such a custom-BRAM implementation
    of PiCaSO-IM as PiCaSO-CB.


    #### V. IMAGINE IMPLEMENTATION AND ANALYSIS


    <span id="page-6-0"></span>In this section, we discuss the bottom-up implementation
    of IMAGine, setting the design goal to be the Gold Standard discussed in Section
    [III.](#page-2-1) In [\[13\]](#page-11-2), PiCaSO was studied on AMD Alveo U55C
    (xcu55c, -2 speed grade). We use the same device as our implementation platform
    to keep the results predictable. The BRAM Fmax on this device is 737 MHz [\[22\]](#page-11-24),
    which sets the target clock period to be 1.356 ns. All of the following studies
    were carried out using Vivado 2022.2.


    #### *A. PiCaSO-IM Block*


    We first verified that the additional logic added to the original PiCaSO-F did
    not degrade the BRAM Fmax within the PIM block or create a logic utilization bottleneck.
    A 4×4 array of the new PiCaSO-IM was tested and compared against the numbers reported
    in [\[13\]](#page-11-2). This comparison is shown in Table [V.](#page-6-2) The
    Change% row shows the utilization and clock speed change compared to the original
    implementation. BRAM utilization is 0.5 because a PiCaSO block uses one RAMB18
    tile, which is reported by Vivado as 0.5 number of RAMB36 tile of AMD devices.
    As observed, the modifications did not affect the clock frequency and the utilizations
    of BRAM and DSP. The flip-flop utilization increased by only 10.6%. Though there
    is a significant increase (74.7%) in the LUT utilization, the overall Slice utilization
    only increased by 8.6%. This means that the additional logic has a high packing


    <span id="page-7-0"></span>TABLE VI UTILIZATION AND FREQUENCY OF 12×2 GEMV TILE
    COMPONENTS


    |             | Controller Rel. |      | Fanout | Rel.  | PIM Array | Rel.   |
    Tile |

    |-------------|-----------------|------|--------|-------|-----------|--------|------|

    | LUT         | 167             | 5.8% | 0      | 0.0%  | 2736      | 94.2%  |
    2903 |

    | FF          | 155             | 4.0% | 615    | 15.9% | 3096      | 80.1%  |
    3866 |

    | DSP         | 0               | -    | 0      | -     | 0         | -      |
    0    |

    | BRAM        | 0               | 0.0% | 0      | 0.0%  | 12.0      | 100.0% |
    12   |

    | Freq. (MHz) | 890             | 1.2× | 890    | 1.2×  | 737       | 1×     |
    737  |


    factor in the logic slices. As a result, the additional logic would not be expected
    to introduce a utilization bottleneck.


    ## *B. IMAGine GEMV Tile*


    Before we implemented the GEMV tile discussed in Section [IV-B,](#page-6-3) its
    components were studied individually to verify if they met the design requirements.
    The GEMV tile contains a 12×2 PIM array and 2 stages of pipeline in the fanout
    tree This configuration best fits the physical layout of the Alveo U55 FPGA as
    discussed later in this section. Table [VI](#page-7-0) shows the utilization and
    performance of these components and their relative values compared to the entire
    GEMV tile.


    The controller together with the fanout network passed the timing constraints
    at a clock rate up to 890 MHz. Because the PIM array contains the BRAM, it cannot
    run faster than the BRAM Fmax. It passed the timing at 737 MHz, which is the ideal
    clock for Alveo U55 according to the Gold Standard. As observed in Table [VI,](#page-7-0)
    the logic utilization of the controller is around 5% of the entire tile and requires
    no DSPs, while around 90% of the logic resources are consumed by the PIM array.
    Thus, the controller and the fanout tree are not expected to bottleneck system
    frequency or utilization. The GEMV tile''s speed and scalability are fundamentally
    dependant on the PIM array, which is the desired outcome.


    ## *C. Scalability Study*


    To evaluate the scalability of our architecture on different device families,
    we followed the approach in [\[13\]](#page-11-2). Along with Alveo U55, four representatives
    were selected from AMD''s Virtex-7 and UltraScale+ devices based on two criteria:
    BRAM capacity and LUT-to-BRAM ratio. Table [VII](#page-7-1) lists these devices
    with their BRAM capacity, LUT-to-BRAM ratio, and a short ID used in Fig. [5.](#page-7-2)
    These are the same devices on which the scalability of PiCaSO-F was studied. The
    target clock frequency of the system was set to 100 MHz on all devices to avoid
    timing issues and only focus on the logic utilization of the system at this point.


    Fig. [5](#page-7-2) shows a bar graph of post-implementation utilization numbers
    of IMAGine on the representative devices. As observed, IMAGine can utilize 100%
    of the available BRAMs as PIM overlays providing 64K PEs in U55, with only 25%
    logic and 6% control set utilization. This leaves sufficient logic resources to
    implement the fanout trees and pipeline stages if they are needed to achieve the
    target clock speed. In fact, IMAGine scaled up to 100% of available BRAM in all
    the representative devices for Virtex-7 and UltraScale+ families.


    In the Virtex-7 family, the device V7-a has the smallest number of BRAMs and the
    smallest LUT-to-BRAM ratio.


    <span id="page-7-1"></span>TABLE VII REPRESENTATIVES OF VIRTEX-7 AND ULTRASCALE+
    FAMILIES [\[13\]](#page-11-2)


    | Device          | Tech | BRAM# | Ratio1 | Max PE#2 | ID   |

    |-----------------|------|-------|--------|----------|------|

    | xcu55c-fsvh-2   | US+  | 2016  | 646    | 64K      | U55  |

    | xc7vx330tffg-2  | V7   | 750   | 272    | 24K      | V7-a |

    | xc7vx485tffg-2  | V7   | 1030  | 295    | 32K      | V7-b |

    | xc7v2000tfhg-2  | V7   | 1292  | 946    | 41K      | V7-c |

    | xc7vx1140tflg-2 | V7   | 1880  | 379    | 60K      | V7-d |

    | xcvu3p-ffvc-3   | US+  | 720   | 547    | 23K      | US-a |

    | xcvu23p-vsva-3  | US+  | 2112  | 488    | 67K      | US-b |

    | xcvu19p-fsvb-2  | US+  | 2160  | 1892   | 69K      | US-c |

    | xcvu29p-figd-3  | US+  | 2688  | 643    | 86K      | US-d |

    |                 |      |       |        |          |      |


    <sup>1</sup> LUT-to-BRAM ratio


    <sup>2</sup> Number of PEs utilizing all BRAMs as PIMs


    ![](_page_7_Figure_14.jpeg)


    <span id="page-7-2"></span>Fig. 5. Resource usage of IMAGine on representatives
    of Virtex-7 and Ultrascale+ families utilizing 100% BRAMs as PIM overlays.


    IMAGine used around 60% logic resources to provide 24K PEs in V7-a. In the UltraScale+
    family, US-a and US-b have the smallest number of BRAMs and the smallest LUT-to-BRAM
    ratio, respectively. In these devices IMAGine provide 23K and 67K PEs, respectively,
    using roughly 30% logic resources. For devices with more BRAMs and a higher LUTto-BRAM
    ratio the logic utilization is very small: the logic utilization in US-c is less
    than 10% providing 69K PEs.


    If we can keep this scaling and run the PIM blocks at BRAM Fmax, the Gold Standard
    of linear scaling of peak performance would be met. Compared to the scalability
    study presented in [\[13\]](#page-11-2), the logic utilization of IMAGine is roughly
    11% more than just the PiCaSO array on average across all the devices. This increase
    is due to the controller logic, data pipelines, and the additional logic implemented
    in PiCaSO-IM.


    ## *D. System-Level Timing Optimizations*


    For the final implementation, the target clock was set to the Gold Standard for
    Alveo U55 with a period of 1.356 ns to match the BRAM Fmax. The goal of the study
    was to find out how close we can get to the Gold Standard, and what are the practical
    challenges that limit us from achieving the Gold Standard. Achieving the best
    performance on a device always requires several iterations of device-specific
    optimizations. In the first iteration, we started with GEMV tiles having a 4×4
    PIM array, without the fanout tree between the controller and the array. After
    going through the implementation flow with the default settings of Vivado and
    a few optimization iterations, we achieved a setup slack of -0.52 ns. The critical
    paths were within the controller with a logic depth of 4, going through the pipeline
    stage A of the controller as shown in Fig. [4\(](#page-6-1)a). So, we enabled
    the pipeline stage A in the controller and moved forward with the second iteration.


    At the end of the second iteration of implementation, we achieved a setup slack
    of -0.38 ns. The critical nets were


    ![](_page_8_Figure_0.jpeg)


    <span id="page-8-0"></span>Fig. 6. Avoiding unnecessary hard-block (CMAC) crossing
    (a) placement and net connections before floorplanning, (b) floorplanning to localize
    logic and routing, (b) placement and net connections in the final design.


    the control signals between the controller and the PIM array. These nets were
    failing the timing requirement because of high fanout and long routes of the control
    signals between the controller and the PIM array. Thus, we synthesize a fanout
    tree between the controller and the PIM array empirically choosing 2 levels and
    a fanout of 4 for the next iteration.


    The design achieved a setup slack of -0.27 ns in the third iteration. This time,
    we had to take a closer look at the design to reveal the main reason for the timing
    failures. Alveo U55 contains several hardened blocks including high-performance
    Ethernet port (CMAC) [\[34\]](#page-11-25). Most of the failing paths were due
    to the long routes crossing such hard blocks. The white lines in Fig. [6\(](#page-8-0)a)
    highlight some of those critical nets crossing a CMAC block. To avoid placement
    results creating such paths, we created floorplanning blocks for each tile to
    localize the placement of logic and routing within a region dedicated to the tile.
    The floorplanning blocks were placed avoiding those hard blocks as shown in Fig.
    [6\(](#page-8-0)b). This required defining a tile with the PIM array dimension
    of 12×2 for Alveo U55.


    Fig. [6\(](#page-8-0)c) shows the placement and net connections in the final iteration.
    The logic and routing of each tile were localized on either side of the hard block;
    the white lines representing the high fanout control signals do not cross the
    CMAC block. Only the logically essential nets, the inter-tile connections for
    east-to-west accumulation, cross the CMAC block requiring minimal routing resources.
    The yellow lines in Fig. [6\(](#page-8-0)c) highlight some of those inter-tile
    nets.


    All paths in the final design met the timing requirement at 737 MHz clock, which
    demonstrates that the ideal clocking standard is practically achievable. Utilizing
    100% available BRAMs as PIMs, this design also achieved the ideal linear scaling
    of peak-performance. Surprisingly, this clock rate is faster than custom GEMM
    accelerator ASICs TPU v1-v2 [\[1\]](#page-11-0) and Alibaba Hanguang 800 [\[35\]](#page-11-26),
    that run at 700 MHz. Both Alveo U55 and TPU v2 are manufactured at 16 nm and Hanguang
    800 at 12nm technology nodes. So, this clock improvement is not due to a technology
    node difference. On Alveo U55, IMAGine has an equal number of PEs compared to
    TPU v1 (64K), and 4× of TPU v2 (16K). However, IMAGine can only deliver up to
    0.33 TOPS which is significantly smaller compared to TPU v1 (92 TOPS) and v2 (46
    TOPS)


    <span id="page-8-1"></span>TABLE VIII UTILIZATION AND FREQUENCY OF PIM-BASED GEMV/GEMM
    ENGINES


    |               | LUT   | FF    | DSP   | BRAM   |     | fSys1 Rel. Freq |

    |---------------|-------|-------|-------|--------|-----|-----------------|

    | RIMA-Fast     |       | 60%   | 50%   | 55%    | 455 | 45.5%           |

    | RIMA-Large    |       | 89%   | 50%   | 93%    | 278 | 27.8%           |

    | CCB GEMV      |       | 27.9% | 90.1% | 91.8%  | 231 | 31.6%           |

    | CoMeFa-A GEMV |       | 27.9% | 90.1% | 91.8%  | 242 | 33.2%           |

    | CoMeFa-D GEMM |       | 25.5% | 92.4% | 86.7%  | 267 | 36.6%           |

    | SPAR-2 (US+)  | 11.3% | 2.4%  | 0.0%  | 14.5%  | 200 | 27.1%           |

    | SPAR-2 (V7)   | 28.5% | 7.0%  | 0.0%  | 30.4%  | 130 | 23.9%           |

    | IMAGine       | 35.6% | 24.8% | 0.0%  | 100.0% | 737 | 100.0%          |

    | IMAGine-CB2   | 10.1% | 7.2%  | 0.0%  | 100.0% | 737 | 100.0%          |

    |               |       |       |       |        |     |                 |


    <sup>1</sup> System frequency in MHz


    2 IMAGine with custom-BRAM PiCaSO-F (PiCaSO-CB)


    due its bit-serial architecture. This makes a compelling case: even though an
    FPGA design will probably never outperform custom ASICs in terms of peak-performance
    or performanceper-watt, the right set of design goals and guiding principles can
    bring it very close in terms of clock speed and compute density. Our proposed
    Gold Standard can serve that purpose for PIM array-based FPGA designs.


    #### *E. Comparison With Other PIM Accelerators*


    Table [VIII](#page-8-1) shows the utilization and system frequencies of existing
    GEMV engines and equivalent PIM-based systems. System-level utilizations and frequencies
    for BRAMAC and M4BRAM-based systems were not reported in [\[10\]](#page-11-9),
    [\[11\]](#page-11-4). Though RIMA is specialized for accelerating RNNs, a major
    part of the system implements GEMV operation using Dot Product Engines (M-DPEs)
    [\[6\]](#page-11-3). The RIMA numbers are taken from Table II of [\[6\]](#page-11-3)
    for comparison, which was evaluated on a Stratix 10 GX2800 FPGA with a BRAM Fmax
    of 1 GHz [\[23\]](#page-11-17). Its fastest reported configuration (RIMA-Fast)
    runs at 455 MHz, which is 2.2× slower than the BRAM Fmax. The largest reported
    configuration (RIMA-Large) utilizes 93% of BRAMs and runs at 278 MHz, 4× slower
    compared to BRAM Fmax. The GEMV/GEMM systems based on CCB and CoMeFa were evaluated
    on an Arria 10 GX900 with a BRAM Fmax of 730 MHz [\[9\]](#page-11-8). Though CoMeFa-based
    designs run slightly faster than the CCB-GEMV engine, they are still roughly 3×
    slower than the BRAM Fmax of the device. Thus, neither of the CCB and CoMeFa-based
    GEMV/GEMM engines scaled well at the system level.


    SPAR-2 [\[7\]](#page-11-11) utilized only 30% of the BRAMs while running 4× slower
    than BRAM Fmax on both tested platforms. Thus, its performance and scalability
    are even worse than CCB and CoMeFa-based systems. On the other hand, IMAGine has
    a system clock running at the BRAM Fmax while utilizing 100% device BRAM as PIMs.
    Thus IMAGine takes advantage of the full internal bandwidth offered by the BRAMs
    in the device. As a PIM-based GEMV engine, IMAGine not only outperformed all existing
    designs but also verified the Gold Standard in terms of BRAM Fmax clock frequency
    and peakperformance scalability up to the full internal bandwidth is achievable.
    This is an important proof of concept design that dispels earlier beliefs that
    overlays cannot achieve BRAM Fmax clock frequencies at the system level. It is
    the fastest


    ![](_page_9_Figure_0.jpeg)


    <span id="page-9-0"></span>Fig. 7. Cycle latency and execution time of GEMV operation
    on different PIM array-based FPGA accelerators


    PIM-based GEMV engine implemented on any FPGA, running at a clock rate 2.65× –
    3.2× faster than any existing design.


    As observed in Table [VIII,](#page-8-1) RIMA and CCB/CoMeFa-based GEMV engines
    exhaust either the logic resources or the DSPs of the device even though their
    PIM blocks are implemented by customizing the BRAM tile itself. Even after being
    an overlay, IMAGine is achieving faster clock and better scalability using 0 DSPs
    and only one-third of the device logic resources due to its near-optimal architectural
    choices guided by the Gold Standard. Like SPAR-2, IMAGine does not use DSPs to
    implement the bit-serial PEs. With a custom-BRAM PIM module like PiCaSO-CB discussed
    in Section [IV-D,](#page-6-4) IMAGine would consume about 10% of device resources
    while being fully scalable and implementable even in resource-limited FPGAs.


    #### *F. GEMV Execution Latency*


    Fig. [7\(](#page-9-0)a) displays GEMV cycle latency for the PIM designs showing
    matrix dimensions (matrices are square) on the x-axis and cycle latency in log
    scale on the y-axis. The execution times shown in Fig. [7\(](#page-9-0)b) are
    computed by multiplying cycle latencies with the corresponding clock periods of
    CCB GEMV, CoMeFa-D GEMM, SPAR-2 (US+), and IMAGine from Table [VIII](#page-8-1)
    system frequencies. We adopted the approach in [\[10\]](#page-11-9) to model the
    block-level cycle latencies of CCB, CoMeFa, BRAMAC, and SPAR-2 using their analytical
    models. IMAGine''s latency model was developed and validated through cycle-accurate
    simulations of the system. The global reduction tree of RIMA was modeled as an
    adder tree, perfectly pipelined with the last iteration of in-block accumulation.
    For CoMeFa and BRAMAC GEMV latency, we assumed the same system as RIMA but with
    CoMeFa or BRAMAC replacing the CCB blocks. For SPAR-2, only the latency for the
    binary-add version is shown because the linear-add version is too slow to plot
    together with the other systems.


    As observed in Fig. [7\(](#page-9-0)a), BRAMAC has the shortest cycle latency,
    due to their hybrid bit-serial & bit-parallel MAC2 algorithm. MAC latency in BRAMAC
    grows linearly with operand bit-width, while it grows quadratically in bit-serial
    architectures like CCB, CoMeFa, SPAR-2, and PiCaSO. This result is not surprising;
    domain-specific architectures can deliver the best performance at a very low cost.
    BRAMAC supports only 2, 4, and 8-bit precisions particularly targetting low-precision
    applications like quantized neural network acceleration. However, BRAMAC is less
    suitable for general computing tasks like GEMV for full-precision scientific computing
    or even neural networks requiring wider precisions. BRAMAC [\[10\]](#page-11-9)
    did not report their system-level frequency which is why we could not plot its
    execution time.


    In all precisions, SPAR-2 has the longest cycle latency and execution time due
    to its slow NEWS accumulation network. Its accumulation latency increases almost
    linearly with the matrix dimension. CCB and CoMeFa-based GEMV engines have the
    shortest cycle latency among bit-serial architectures across all precisions. This
    is due to their fast reduction algorithm based on a pop-count adder and pipelined
    adder tree. The cycle latency of IMAGine is significantly shorter compared to
    SPAR-2 but longer than CCB/CoMeFa-based implementations. However, IMAGine has
    the fastest clock, which is at a minimum 2× faster than any of the other GEMV
    engines. When accounting for the clock frequency, IMAGine outperforms all other
    GEMV engines in terms of overall execution time. This highlights the importance
    of the system clock speed over the cycle latency. Although the reduction tree-based
    approach in the CCB/CoMeFa GEMV engines has the shortest cycle latency, the slower
    clock ultimately degrades


    <span id="page-10-0"></span>TABLE IX CURVE-FITTED PARAMETERS OF EQN. [\(1\)](#page-4-3)
    FOR 32-BIT ACCUMULATION


    |                   | Fitted Value |      |       | Speed Interpretation |              |  |

    |-------------------|--------------|------|-------|----------------------|--------------|--|

    |                   | a            | b    | c     | Addition (a)         | Movement
    (b) |  |

    | SPAR-2 Linear-Add | 0            | 96   | 0     | Very Slow            | Very
    Slow    |  |

    | SPAR-2 Binary-Add | 2            | 32   | 0     | Standard             | Very
    Slow    |  |

    | CCB/CoMeFa        | 0.03         | 0.02 | 203.1 | Fast                 | Fast         |  |

    | IMAGine           | 1.2          | 0.9  | 143   | Standard             | Standard     |  |


    the end-to-end latency times below that of IMAGine.


    ## *G. Further Improvements*


    While IMAGine outperforms the existing designs, this does not guarantee it is
    the optimal implementation. There could still be some room for improvement, particularly
    in the reduction network. When examining all of the architectures, we can intuitively
    understand which designs are better suited for the reduction process. However,
    the proposed Gold Standard can quantify these insights even without the architectural
    knowledge of the implementations. Since the studied systems employ different reduction
    approaches, we define the reduction latency for the GEMV operation as any cycle
    spent outside the multiplication stage, encompassing the In-Block latency [\(2\)](#page-4-4).
    We curve-fitted the proposed model [\(1\)](#page-4-3) to the reduction cycle latencies
    of the designs in Fig. [7\(](#page-9-0)a). Table [IX](#page-10-0) shows the fitted
    parameters for 32-bit operands and their interpretations.


    SPAR-2 linear-add parameters significantly deviate from their ideal ranges outlined
    in Table [III,](#page-4-2) revealing a suboptimal reduction network design. SPAR-2
    binary-add has a in the standard range, which means the reduction operation is
    at least near optimal. This was achieved through an optimization reported in [\[18\]](#page-11-13)
    to reduce the number of add operations. A high value of b in both approaches indicates
    that accumulation latency is dominated by the data movement part, which is notably
    slower than the ideal. The value of c being 0 indicates the reduction process
    does not involve extra cycles outside the reduction network, which is true by
    design.


    For CCB/CoMeFa, both parameters are near the smallest possible values: here 0.3
    ≊ 1/N, N=32. This implies it has the shortest possible cycle latency. The value
    of c approximates the In-Block accumulation and pipeline setup latency of 202
    cycles spent generating the 16 partial sums per block [\[6\]](#page-11-3) before
    they enter the reduction network.


    IMAGine''s parameters fall within the standard range, implying at least a near-optimal
    implementation. In this case, as well, the value of c approximates the In-Block
    accumulation latency of 144 cycles. Since b is near its upper bound, it indicates
    room for improvement in the data movement part. Because IMAGine is utilizing only
    30% of the logic resources in U55, we can modify the network to implement 2-bit
    or 4-bit bit-sliced accumulation, possibly without affecting the system clock
    speed. This has the potential to further improve the cycle latency of the GEMV
    operation. Additionally, the PEs can be modified to implement Booth''s Radix-4
    instead of the default Radix-2 algorithm adopted from PiCaSO.


    The IMAGine-slice4 curves in Fig. [7](#page-9-0) shows the GEMV latency of a variant
    of IMAGine with a 4-bit sliced accumulation network and a PE implementing Booth''s
    radix-4 multiplication. This latency is estimated by adjusting the analytical
    model of IMAGine assuming no effect on the clock rate. In terms of cycle latency,
    it can run almost as fast as CCB/CoMeFa-based GEMV implementations. Because of
    the higher system frequency, it will then significantly outperform all other state-of-the-art
    PIM-based GEMV accelerators. This example demonstrates how the proposed Gold Standard
    can aid in identifying design inefficiencies and making optimal design choices
    for PIM-based accelerators in FPGAs.


    ## VI. CONCLUSIONS AND FUTURE WORK


    Processor In/Close to Memory PIM/CIM architectures have become popular frameworks
    replacing classic von Neumann architectures within domain-specific machine learning
    accelerators. PIM overlays as well as new reconfigurable fabrics for next-generation
    FPGAs are being explored with results showing projected clock frequencies much
    lower than the maximum for a BRAM and continuing to degrade as the number of PIM
    blocks scales. Furthermore, most current designs do not provide efficient hardware
    support between PIM blocks for the important reduction steps within GEMV matrix-vector
    operations.


    This paper presented a study that established a theoretical upper limit for PIM
    array-based architectures in BRAM-LUTbased FPGAs as an aspirational Gold Standard.
    It was then shown how the Gold Standard can be used for comparing and evaluating
    the efficiency of existing designs, both overlays and next-generation PIM reconfigurable
    fabrics. Important design challenges and implementation techniques were explored
    that can affect a specific design''s ability to reach the performance of the Gold
    Standard. Results were presented verifying the Gold Standard is achievable in
    FPGA-based overlays as well as redesigned BRAM-LUT PIM tiles. The IMAGine case
    study showed the full bandwidth of the BRAMs could be exploited and operated at
    BRAM''s maximum frequency.


    Scalability studies were presented, demonstrating that processing capacity scales
    linearly with increasing BRAM density, even for devices with low LUT-to-BRAM ratios.
    IMAGine, a PIM array-base GEMV accelerator, with 64K PEs was implemented and run
    on Alveo U55, which achieved clock speed faster than the Tensor Processing Unit
    (TPU v1-v2) and Alibaba Hanguang 800. This breaks the myth that FPGA overlays
    and fabrics must clock slower than ASIC designs.


    Although IMAGine achieved better performance compared to state-of-the-art FPGA-based
    PIM architectures and clocks faster than some ASIC accelerators, there are additional
    improvements that require additional exploration. Further improvement in end-to-end
    GEMV latency can be achieved by overlapping data broadcasting and multiply-accumulate
    operations, which will benefit deep learning applications. We are currently working
    on a new MLIR-based compiler framework for hardware/software codesign and application-specific
    customization of IMAGine-like PIM array-based accelerators.


    #### REFERENCES


    - <span id="page-11-0"></span>[1] N. P. Jouppi, D. Hyun Yoon, M. Ashcraft, M.
    Gottscho, T. B. Jablin, G. Kurian *et al.*, "Ten lessons from three generations
    shaped google''s TPUv4i : Industrial product," in *2021 ACM/IEEE 48th Annual International
    Symposium on Computer Architecture (ISCA)*. IEEE, Jun. 2021, pp. 1–14.

    - <span id="page-11-1"></span>[2] J.-H. Kim, J. Lee, J. Lee, H.-J. Yoo, and J.-Y.
    Kim, "Z-PIM: An Energy-Efficient Sparsity Aware Processing-In-Memory Architecture
    with Fully-Variable Weight Precision," in *2020 IEEE Symposium on VLSI Circuits*.
    IEEE, Jun. 2020, pp. 1–2.

    - [3] B. Zhang, S. Yin, M. Kim, J. Saikia, S. Kwon, S. Myung, H. Kim, S. J. Kim,
    J.-S. Seo, and M. Seok, "PIMCA: A Programmable In-Memory Computing Accelerator
    for Energy-Efficient DNN Inference," *IEEE Journal of Solid-State Circuits*, vol.
    58, no. 5, pp. 1436–1449, May 2023.

    - [4] C.-F. Lee, C.-H. Lu, C.-E. Lee, H. Mori, H. Fujiwara, Y.-C. Shih, T.-L.
    Chou, Y.-D. Chih, and T.-Y. J. Chang, "A 12nm 121-TOPS/W 41.6-TOPS/mm2 All Digital
    Full Precision SRAM-based Computein-Memory with Configurable Bit-width For AI
    Edge Applications," in *2022 IEEE Symposium on VLSI Technology and Circuits (VLSI
    Technology and Circuits)*. IEEE, Jun. 2022, pp. 24–25.

    - [5] Y. Kwon, G. Kim, N. Kim, W. Shin, J. Won, H. Joo *et al.*, "Memory-Centric
    Computing with SK Hynix''s Domain-Specific Memory," in *2023 IEEE Hot Chips 35
    Symposium (HCS)*, 2023, pp. 1–26.

    - <span id="page-11-3"></span>[6] X. Wang, V. Goyal, J. Yu, V. Bertacco, A. Boutros,
    E. Nurvitadhi, C. Augustine, R. R. Iyer, and R. Das, "Compute-Capable Block RAMs
    for Efficient Deep Learning Acceleration on FPGAs," *2021 IEEE 29th Annual International
    Symposium on Field-Programmable Custom Computing Machines (FCCM)*, pp. 88–96,
    2021.

    - <span id="page-11-11"></span>[7] A. Panahi, S. Balsalama, A.-T. Ishimwe, J.
    M. Mbongue, and D. Andrews, "A Customizable Domain-Specific Memory-Centric FPGA
    Overlay for Machine Learning Applications," in *2021 31st International Conference
    on Field-Programmable Logic and Applications (FPL)*, Aug. 2021, pp. 24–27.

    - <span id="page-11-7"></span>[8] A. Arora, T. Anand, A. Borda, R. Sehgal, B.
    Hanindhito, J. Kulkarni, and L. K. John, "CoMeFa: Compute-in-Memory Blocks for
    FPGAs," in *2022 IEEE 30th Annual International Symposium on Field-Programmable
    Custom Computing Machines (FCCM)*, May 2022, pp. 1–9.

    - <span id="page-11-8"></span>[9] A. Arora, A. Bhamburkar, A. Borda, T. Anand,
    R. Sehgal, B. Hanindhito, P.-E. Gaillardon, J. Kulkarni, and L. K. John, "CoMeFa:
    Deploying Compute-in-Memory on FPGAs for Deep Learning Acceleration," *ACM Transactions
    on Reconfigurable Technology and Systems*, vol. 16, no. 3, pp. 1–34, Sep. 2023.

    - <span id="page-11-9"></span>[10] Y. Chen and M. S. Abdelfattah, "BRAMAC: Compute-in-BRAM
    Architectures for Multiply-Accumulate on FPGAs," in *2023 IEEE 31st Annual International
    Symposium on Field-Programmable Custom Computing Machines (FCCM)*. Marina Del
    Rey, CA, USA: IEEE, May 2023, pp. 52–62.

    - <span id="page-11-4"></span>[11] Y. Chen, J. Dotzel, and M. S. Abdelfattah,
    "M4BRAM: Mixed-Precision Matrix-Matrix Multiplication in FPGA Block RAMs," Nov.
    2023, arXiv:2311.02758 [cs]. [Online]. Available: [http://arxiv.org/abs/2311.](http://arxiv.org/abs/2311.02758)
    [02758](http://arxiv.org/abs/2311.02758)

    - [12] M. A. Kabir, J. Hollis, A. Panahi, J. Bakos, M. Huang, and D. Andrews,
    "Making BRAMs Compute: Creating Scalable Computational Memory Fabric Overlays,"
    in *2023 IEEE 31st Annual International Symposium on Field-Programmable Custom
    Computing Machines (FCCM)*. IEEE, May 2023, pp. 224–224.

    - <span id="page-11-2"></span>[13] M. A. Kabir, E. Kabir, J. Hollis, E. Levy-Mackay,
    A. Panahi, J. Bakos, M. Huang, and D. Andrews, "FPGA Processor In Memory Architectures
    (PIMs): Overlay or Overhaul ?" in *2023 33rd International Conference on Field-Programmable
    Logic and Applications (FPL)*. Gothenburg, Sweden: IEEE, Sep. 2023, pp. 109–115.

    - <span id="page-11-5"></span>[14] C. Eckert, X. Wang, J. Wang, A. Subramaniyan,
    R. Iyer, D. Sylvester, D. Blaauw, and R. Das, "Neural Cache: Bit-Serial in-Cache
    Acceleration of Deep Neural Networks," in *2018 ACM/IEEE 45Th annual international
    symposium on computer architecture (ISCA)*, 2018, pp. 383–396.

    - <span id="page-11-6"></span>[15] J. Fowers, K. Ovtcharov, M. Papamichael, T.
    Massengill, M. Liu, D. Lo *et al.*, "A Configurable Cloud-Scale DNN Processor
    for Real-Time AI,"


    in *2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture
    (ISCA)*, 2018, pp. 1–14.


    - <span id="page-11-10"></span>[16] U. Aydonat, S. O''Connell, D. Capalija, A.
    C. Ling, and G. R. Chiu, "An OpenCL™ Deep Learning Accelerator on Arria 10," in
    *Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable
    Gate Arrays*. Association for Computing Machinery, 2017, p. 55–64.

    - <span id="page-11-12"></span>[17] S. Basalama, A. Panahi, A.-T. Ishimwe, and
    D. Andrews, "SPAR-2: A SIMD Processor Array for Machine Learning in IoT Devices,"
    in *2020 3rd International Conference on Data Intelligence and Security (ICDIS)*.
    IEEE, 2020, pp. 141–147.

    - <span id="page-11-13"></span>[18] A. Panahi, "A memory-centric customizable
    domain-specific FPGA overlay for accelerating machine learning applications,"
    Ph.D dissertation, University of Arkansas, 2022.

    - <span id="page-11-14"></span>[19] S. Mosanu, M. N. Sakib, T. Tracy, E. Cukurtas,
    A. Ahmed, P. Ivanov, S. Khan, K. Skadron, and M. Stan, "PiMulator: a fast and
    flexible processing-in-memory emulation platform," in *2022 Design, Automation
    & Test in Europe Conference & Exhibition (DATE)*. IEEE, Mar. 2022, pp. 1473–1478.

    - <span id="page-11-15"></span>[20] A. Dervay and W. Zhao, "CIMulator: A computing
    in memory emulator framework," *IEEE Transactions on Circuits and Systems II:
    Express Briefs*, vol. 69, no. 10, pp. 4183–4187, 2022.

    - <span id="page-11-16"></span>[21] *Virtex-7 T and XT FPGAs Data Sheet: DC and
    AC Switching Characteristics*, AMD, 2021. [Online]. Available: [https://docs.xilinx.](https://docs.xilinx.com/v/u/en-US/ds183_Virtex_7_Data_Sheet)
    [com/v/u/en-US/ds183](https://docs.xilinx.com/v/u/en-US/ds183_Virtex_7_Data_Sheet)
    Virtex 7 Data Sheet

    - <span id="page-11-24"></span>[22] *Virtex UltraScale+ FPGA Data Sheet: DC and
    AC Switching Characteristics*, AMD, 2021. [Online]. Available: [https://docs.xilinx.](https://docs.xilinx.com/v/u/en-US/ds923-virtex-ultrascale-plus)
    [com/v/u/en-US/ds923-virtex-ultrascale-plus](https://docs.xilinx.com/v/u/en-US/ds923-virtex-ultrascale-plus)

    - <span id="page-11-17"></span>[23] *Intel® Stratix® 10 Device Datasheet*, Intel.
    [Online]. Available: [https://www.intel.com/content/www/us/en/docs/programmable/](https://www.intel.com/content/www/us/en/docs/programmable/683181/current/memory-block-specifications.html)
    [683181/current/memory-block-specifications.html](https://www.intel.com/content/www/us/en/docs/programmable/683181/current/memory-block-specifications.html)

    - <span id="page-11-18"></span>[24] *UltraScale Architecture Libraries Guide*,
    AMD, 2021. [Online]. Available: [https://docs.xilinx.com/v/u/2018.1-English/](https://docs.xilinx.com/v/u/2018.1-English/ug974-vivado-ultrascale-libraries)
    [ug974-vivado-ultrascale-libraries](https://docs.xilinx.com/v/u/2018.1-English/ug974-vivado-ultrascale-libraries)

    - <span id="page-11-19"></span>[25] *Zynq UltraScale+ MPSoC Data Sheet: Overview
    (DS891)*, AMD. [Online]. Available: [https://docs.xilinx.com/v/u/en-US/](https://docs.xilinx.com/v/u/en-US/ds891-zynq-ultrascale-plus-overview)
    [ds891-zynq-ultrascale-plus-overview](https://docs.xilinx.com/v/u/en-US/ds891-zynq-ultrascale-plus-overview)

    - <span id="page-11-20"></span>[26] *UltraFast Design Methodology Guide for FPGAs
    and SoCs (UG949)*, AMD. [Online]. Available: [https://docs.xilinx.com/r/en-US/](https://docs.xilinx.com/r/en-US/ug949-vivado-design-methodology/Control-Signals-and-Control-Sets)
    [ug949-vivado-design-methodology/Control-Signals-and-Control-Sets](https://docs.xilinx.com/r/en-US/ug949-vivado-design-methodology/Control-Signals-and-Control-Sets)

    - <span id="page-11-21"></span>[27] Ling Zhuo, G. Morris, and V. Prasanna, "Designing
    Scalable FPGA-Based Reduction Circuits Using Pipelined Floating-Point Cores,"
    in *19th IEEE International Parallel and Distributed Processing Symposium*, 2005,
    pp. 147a–147a.

    - [28] L. Zhuo, G. R. Morris, and V. K. Prasanna, "High-Performance Reduction
    Circuits Using Deeply Pipelined Operators on FPGAs," *IEEE Transactions on Parallel
    and Distributed Systems*, vol. 18, no. 10, pp. 1377–1392, 2007.

    - [29] Y.-G. Tai, C.-T. D. Lo, and K. Psarris, "Accelerating Matrix Operations
    with Improved Deeply Pipelined Vector Reduction," *IEEE Transactions on Parallel
    and Distributed Systems*, vol. 23, no. 2, pp. 202–210, 2012.

    - [30] M. Huang and D. Andrews, "Modular Design of Fully Pipelined Reduction Circuits
    on FPGAs," *IEEE Transactions on Parallel and Distributed Systems*, vol. 24, no.
    9, pp. 1818–1826, 2013.

    - [31] L. Tang, G. Cai, Y. Zheng, and J. Chen, "A Resource and Performance Optimization
    Reduction Circuit on FPGAs," *IEEE Transactions on Parallel and Distributed Systems*,
    vol. 32, no. 2, pp. 355–366, 2021.

    - <span id="page-11-22"></span>[32] S. Ullah, S. Rehman, M. Shafique, and A. Kumar,
    "High-Performance Accurate and Approximate Multipliers for FPGA-Based Hardware
    Accelerators," *IEEE Transactions on Computer-Aided Design of Integrated Circuits
    and Systems*, vol. 41, no. 2, pp. 211–224, 2022.

    - <span id="page-11-23"></span>[33] M. A. Kabir, E. Kabir, J. Hollis, E. Levy-Mackay,
    A. Panahi, J. Bakos, M. Huang, and D. Andrews, "PiCaSO: A Scalable and Fast PIM
    Overlay." [Online]. Available:<https://github.com/Arafat-Kabir/PiCaSO>

    - <span id="page-11-25"></span>[34] *Alveo U55C Data Center Accelerator Card User
    Guide*, AMD. [Online]. Available:<https://docs.xilinx.com/r/en-US/ug1469-alveo-u55c>

    - <span id="page-11-26"></span>[35] Y. Jiao, L. Han, R. Jin, Y.-J. Su, C. Ho,
    L. Yin *et al.*, "7.2 a 12nm programmable convolution-efficient neural-processing-unit
    chip achieving 825tops," in *2020 IEEE International Solid- State Circuits Conference
    - (ISSCC)*. IEEE, Feb. 2020, pp. 136–140.'
- title: "vCLIC: Towards Fast Interrupt Handling in Virtualized RISC-V\n  Mixed-criticality\
    \ Systems"
  abstract: 'The widespread diffusion of compute-intensive edge-AI workloads and the

    stringent demands of modern autonomous systems require advanced heterogeneous

    embedded architectures. Such architectures must support high-performance and

    reliable execution of parallel tasks with different levels of criticality.

    Hardware-assisted virtualization is crucial for isolating applications

    concurrently executing these tasks under real-time constraints, but interrupt

    virtualization poses challenges in ensuring transparency to virtual guests

    while maintaining real-time system features, such as interrupt vectoring,

    nesting, and tail-chaining. Despite its rapid advancement to address

    virtualization needs for mixed-criticality systems, the RISC-V ecosystem still

    lacks interrupt controllers with integrated virtualization and real-time

    features, currently relying on non-deterministic, bus-mediated message-signaled

    interrupts (MSIs) for virtualization. To overcome this limitation, we present

    the design, implementation, and in-system assessment of vCLIC, a virtualization

    extension to the RISC-V CLIC fast interrupt controller. Our approach achieves

    20x interrupt latency speed-up over the software emulation required for

    handling non-virtualization-aware systems, reduces response latency by 15%

    compared to existing MSI-based approaches, and is free from interference from

    the system bus, at an area cost of just 8kGE when synthesized in an advanced

    16nm FinFet technology.'
  url: http://arxiv.org/abs/2410.07798v1
  keywords: Real-time, Automotive, Interrupt, Virtualization, RISC-V, Predictability,
    Mixed-Criticality System
  document: '# vCLIC: Towards Fast Interrupt Handling in Virtualized RISC-V Mixed-criticality
    Systems


    Enrico Zelioli<sup>∗</sup> , Alessandro Ottaviano<sup>∗</sup> , Robert Balas<sup>∗</sup>
    , Nils Wistoff<sup>∗</sup> , Angelo Garofalo∗†, Luca Benini∗†


    ∗ IIS, ETH Zurich, Switzerland; †DEI, University of Bologna, Italy


    {ezelioli, aottaviano, balasr, nwistoff, agarofalo, lbenini}@iis.ee.ethz.ch


    *Abstract*—The widespread diffusion of compute-intensive edge-AI workloads and
    the stringent demands of modern autonomous systems require advanced heterogeneous
    embedded architectures. Such architectures must support high-performance and reliable
    execution of parallel tasks with different levels of criticality. Hardware-assisted
    virtualization is crucial for isolating applications concurrently executing these
    tasks under real-time constraints, but interrupt virtualization poses challenges
    in ensuring transparency to virtual guests while maintaining real-time system
    features, such as interrupt vectoring, nesting, and tail-chaining. Despite its
    rapid advancement to address virtualization needs for mixed-criticality systems,
    the RISC-V ecosystem still lacks interrupt controllers with integrated virtualization
    and real-time features, currently relying on non-deterministic, bus-mediated message-signaled
    interrupts (MSIs) for virtualization.


    To overcome this limitation, we present the design, implementation, and in-system
    assessment of vCLIC, a virtualization extension to the RISC-V CLIC fast interrupt
    controller. Our approach achieves 20× interrupt latency speed-up over the software
    emulation required for handling non-virtualization-aware systems, reduces response
    latency by 15% compared to existing MSI-based approaches, and is free from interference
    from the system bus, at an area cost of just 8kGE when synthesized in an advanced
    16nm FinFet technology.


    *Index Terms*—Real-time, Automotive, Interrupt, Virtualization, RISC-V, Predictability,
    Mixed-Criticality System


    ## I. INTRODUCTION


    Applications like autonomous driving, navigation, and robotics require safety,
    security, and energy efficiency for edge-AI tasks [\[1\]](#page-3-0), driving
    the need for heterogeneous mixed criticality systems (MCSs). As software (SW)
    complexity grows, so does the integration of general-purpose operating systems
    (GPOSs) in embedded systems. However, they can only partially support the specialized
    real-time features of realtime operating systems (RTOSs), which prioritize worst-case
    execution time. Virtualization has emerged as a key technology to enable the simultaneous
    operation of GPOSs and RTOSs on the same hardware (HW) platform [\[2\]](#page-3-1).
    In safety-critical systems, real-time guarantees are essential, especially regarding
    low and deterministic interrupt latency. For such systems, HWassisted virtualization
    of resources — e.g., interrupt controllers (ICs) and executing cores — are vital
    to minimize hypervisor intervention, reducing response latency and jitter.


    Commercial off-the-shelf general-purpose embedded systems often feature diverse
    IC architectures to allocate and sequence interrupts based on specific application
    needs. For instance, Arm''s generic IC (GIC) is versatile and can manage wired
    and message-signaled interrupts (MSIs) at different design scales. Albeit GICv3
    and GICv4 support partial hardware-assisted virtualization — for example, direct
    injection of interrupts to virtual machines (VMs) is not supported —, they are
    not designed to handle interrupt nesting, and their real-time capabilities are
    limited to a particular channel for fast and critical interrupts. In contrast,
    the Nested Vectored Interrupt Controller (NVIC) is specifically designed for real-time
    operations in a non-virtualized setting on small embedded devices, with interrupt
    latencies and context switching times as low as 12 and 96 clock cycles, respectively
    [\[3\]](#page-3-2). Similarly, RISC-V introduced the Core-Local Interrupt Controller
    (CLIC), optimized for real-time applications, showcasing features like interrupt
    nesting and tailchaining. However, there is no support for virtualization of the
    CLIC in the most recent specifications [\[4\]](#page-3-3). On the other end, the
    RISC-V advanced interrupt architecture (AIA) caters to larger, high-performance
    virtualized systems, but without support for time-critical interrupt handling
    [\[5\]](#page-3-4). Notably, efforts to extend the AIA with real-time and predictability
    features have been made, mitigating the bus interference effects typical of MSIbased
    systems [\[6\]](#page-3-5). Although improving predictability of virtual MSIs,
    AIA-based solutions still lack support for real-time interrupt handling features.
    Sa et al. enhanced the RISC-V Platform-Level Interrupt Controller (PLIC) to support
    virtualization [\[7\]](#page-3-6). While significantly improving the best-case
    interrupt latency over alternative software emulation approaches, the PLIC does
    not support any of the aforementioned real-time features.


    Therefore, existing IC solutions do not entirely meet the diverse requirements
    of real-time, compact, virtualized embedded MCSs at once. Focusing on the growing,
    open RISC-V ecosystem as an ideal domain for design space exploration and HW/SW
    co-design, we fill this gap by presenting vCLIC, a novel, lightweight virtualization
    extension for the CLIC. We evaluate its impact on system size and demonstrate
    its benefits on a virtualized, embedded RISC-V platform built around the CVA6
    application-class core [\[8\]](#page-3-7). The vCLIC synthesizable HW description
    is freely accessible and open-source.[1](#page-0-0)


    *Contribution:* The contributions of our work include:


    - 1) vCLIC extension We design and implement an interrupt virtualization extension
    of the RISC-V CLIC and the application-class CVA6 core (Section [II\)](#page-1-0).
    vCLIC enables direct injection of interrupts to guest VMs and prioritization of
    real-time, safety-critical VMs.

    - 2) In-system implementation assessment We integrate vCLIC in Cheshire, a Linux-capable
    RISC-V system-on-


    <span id="page-0-0"></span><sup>1</sup><https://github.com/pulp-platform/clic>


    This work was supported in part by the TRISTAN project (101095947) that received
    funding from the HORIZON CHIPS-JU programme.


    chip (SoC) [\[9\]](#page-3-8), and characterize it in a 16 nm FinFet technology
    node. The virtualization extension in its minimal configuration incurs an area
    overhead of just 8 kGE at iso-frequency with the vanilla CLIC design and a total
    overhead of just 1 % at the SoC level (Section [III-D\)](#page-3-9).


    3) In-system functional assessment We demonstrate the beneficial effects of low
    interrupt latency and jitter of vCLIC. Our approach achieves 20x speed-up over
    the software emulation method required by non-virtualized systems (Section [III-B\)](#page-2-0).
    Besides reducing response latency by 15% compared to existing MSI-based virtualization
    strategies (RISC-V AIA), vCLIC is free from interference with the system bus.


    ## II. ARCHITECTURE


    <span id="page-1-0"></span>In virtualized RISC-V systems, the vCLIC extension
    allows a hypervisor to delegate individual interrupts to VMs. The delegation reduces
    interrupt latency by eliminating the hypervisormediated SW emulation of the IC.
    The vCLIC is integrated into the Cheshire platform [\[9\]](#page-3-8), briefly
    described in Section [II-A.](#page-1-1) The vCLIC design is split into two modular
    extensions: (i) VSCLIC, which enables the delegation of interrupt lines to VMs,
    and (ii) VSPRIO, which allows the hypervisor to prioritize interrupts depending
    on the VM they are assigned to, as discussed in Section [II-B.](#page-1-2) Furthermore,
    the enhancements required in the core (in this work, CVA6) connected to the vCLIC
    are described in Section [II-C.](#page-1-3) Figure [1b](#page-1-4) details the
    vCLIC-CVA6 interface, highlighting the novel HW features compared to vanilla CLIC.


    # <span id="page-1-1"></span>*A. Cheshire Platform*


    Cheshire [\[9\]](#page-3-8) is a minimal system built around the 64-bit, application-class
    CVA6 RISC-V processor [\[8\]](#page-3-7). The platform is highly configurable,
    supporting seamless integration of domainspecific accelerators or other general-purpose
    engines with diverse criticality levels via an AXI4 fully connected crossbar,
    as shown in Figure [1.](#page-1-5) In a multi-core setup, each vCLIC instance
    is tightly coupled to each core, ensuring independence from the system bus and
    thereby eliminating interference at the interconnect level.


    ## <span id="page-1-2"></span>*B. vCLIC extension*


    *VSCLIC:* The VSCLIC extension allows to delegate interrupt lines to virtual supervisor
    (VS) mode. It introduces a new set of memory-mapped configuration registers, namely
    clicintv, that are under the control of the hypervisor (Figure [1b\)](#page-1-4).
    For each interrupt line *i*, clicintv[i] is a 1 B register comprising (i) a virtualization
    bit (*v* field), which adds a new orthogonal privilege mode to the privilege mode
    interrupt attribute, and (ii) a virtual supervisor ID (*vsid* field), which holds
    the virtual supervisor ID (VSID) of the VM to which the interrupt is delegated.


    vCLIC multiplexes the virtual configuration registers of each target on a single
    set of physical registers, minimizing the area overhead on the register file.
    The privilege of software accessing vCLIC configuration registers is determined
    by the address range used to access them by partitioning the vCLIC configuration
    address space. To guarantee isolation, each VM must be granted access to only
    one of these address regions


    <span id="page-1-5"></span>![](_page_1_Figure_9.jpeg)


    (a) Cheshire platform diagram. Per-core vCLIC is highlighted.


    <span id="page-1-4"></span>![](_page_1_Figure_11.jpeg)


    ![](_page_1_Figure_12.jpeg)


    (i.e., the one associated with its VSID). Such isolation can be enforced by privileged
    software by means of physical memory protection (PMP) units or — on systems supporting
    two-stage address translation — by virtual memory.


    <span id="page-1-6"></span>*VSPRIO:* The vCLIC supports up to 64 simultaneous
    active guests, identified by a VSID stored in the *VGEIN* field of the *hstatus*
    control and status register (CSR), as per the RISC-V H extension. This allows
    to statically delegate interrupt lines of pass-through peripherals to VMs, eliminating
    the need of saving and restoring the IC configuration upon VM context switch,
    thereby improving the worst-case interrupt latency.


    Nevertheless, to ensure that the real-time properties of interrupts delegated
    to safety-critical guests are not affected, the VSPRIO extension allows the hypervisor
    to prioritize virtual interrupts based on the associated VMs. The extension enlarges
    the interrupt priority space by adding a set of privileged configuration registers
    that hold a per-VM priority value. This enables dynamic prioritization of real-time
    and safety-critical VMs, allowing the deployment of systems with more tightly
    bound worst-case interrupt latencies.


    ## <span id="page-1-3"></span>*C. CVA6 extension*


    We extend CVA6''s interrupt handling logic to behave as follows when a VM is running:
    (i) if the incoming interrupt belongs to a higher privilege level, the core traps
    to that privilege level; (ii) if the incoming interrupt is directed to the currently
    running VM (i.e., the interrupt''s and running VM''s VSID match), the interrupt
    is serviced by the running VM; (iii) if the incoming interrupt is directed to
    a different VM, the core traps to hypervisor mode if and only if the target VM
    has higher priority. Remarkably, item (iii) is crucial for the hypervisor to regain
    control of the core in case of high-priority events that must be handled by the
    dedicated VM with minimal latency.


    Furthermore, the virtualization extension requires the following CSRs to be added:
    (i) VSINTTHRESH, the interrupt


    <span id="page-2-1"></span>![](_page_2_Figure_0.jpeg)


    Figure 2: Interrupt latency comparison for *bare-metal* (left), and *virtualized*
    with FreeRTOS on top of Bao (right) scenarios.


    *level* threshold (managing interrupt nesting in CLIC-based systems), (ii) VSTVT,
    the guest physical address of VS trap vector table (in selective hardware vectoring
    (SHV) mode), and (iii) VSNXTI for interrupt tail-chaining. In CLIC mode, the vsie,
    vsip, and vsideleg appear hardwired to zero, as their functionality is embedded
    in the IC itself. Finally, vCLIC supports the original CLIC SHV extension, allowing
    fine-grained management of virtual interrupts vectoring.


    ## III. EVALUATION


    # <span id="page-2-2"></span>*A. Framework*


    To carry out fast and cycle-accurate latency measurements, we integrate vCLIC
    into the Cheshire SoC shown in Figure [1](#page-1-5) and emulate the system on
    a Digilent Genesys 2 FPGA board. On FPGA, Cheshire is configured with one CVA6
    core and vCLIC pair, 64 input interrupt lines, 32 KiB of data and instruction
    cache, 128 KiB of last-level cache, and is constrained to run at 50 MHz. As a
    SW stack, we use two Hypervisors: the Bao static partitioning hypervisor (SPH)
    [\[10\]](#page-3-10) and the Xvisor dynamic partitioning hypervisor (DPH) [\[11\]](#page-3-11),
    both open-source. Bao allows pinning a virtual guest to a physical core, improving
    the overall system determinism. We use it to assess the interrupt latency and
    jitter of vCLIC in a built-in, predictable SW setting. Xvisor, in contrast, allows
    multiple VM guests to be assigned to a single physical core. We use Xvisor to
    assess interrupt prioritization and preemption across VM guests.


    ## <span id="page-2-0"></span>*B. Functional*


    *Interrupt latency:* Interrupt latency includes both HW and SW contributions.
    We include SW contributions affected by the core and IC design in the interrupt
    latency measurement to ensure a fair comparison. These are, for instance, the
    time to save the interrupt context and to identify the interrupt source.


    Following this definition, we evaluate the interrupt latency of external interrupt
    sources with various interrupt subsystems: (i) vanilla RISC-V PLIC implementation
    [\[12\]](#page-3-12); (ii) vanilla RISC-V CLIC implementation [\[4\]](#page-3-3);
    (iii) vanilla AIA implementation, i.e. one A-PLIC and one Incoming MSI Controller
    (IMSIC) [\[13\]](#page-3-13); and (iv) vCLIC proposed in this work.


    On the SW side, CVA6 executes in two settings: nonvirtualized, or bare-metal,
    and virtualized, using FreeRTOS VM on top of the Bao monitor. We collect the interrupt
    latencies by running each experiment 100 times, assuming a warm microarchitectural
    state (i.e., we do not flush the caches and TLBs between iterations). This setup
    represents a lower bound on the expected interrupt latency, which can be affected
    by several factors, as discussed in the following sections.


    Figure [2](#page-2-1) reports the best (minimum) and worst (maximum) interrupt
    latency of a single external interrupt source (e.g., a programmable timer) for
    each scenario. We observe a significant increase of about 20× in the interrupt
    latency of configurations (i) and (ii) without HW-assisted interrupt virtualization.
    The increase is due to the overhead of the IC SW emulation that the hypervisor
    must perform in the absence of HW support. The PLIC emulation results in a larger
    overhead due to the extra memory access required by the PLIC to read the interrupt
    ID that must be emulated by the hypervisor. Virtualization does not affect interrupt
    latency for both the AIA and the vCLIC configurations. The vCLIC achieves a lower
    interrupt latency than AIA due to the use of wired interrupts instead of interconnect-dependent
    MSIs.


    *Interrupt jitter:* We identify two main contributions to the interrupt jitter:
    (i) instruction and data caches and (ii) TLBs. We assume two states for each component,
    i.e., *warm* (recently accessed) and *cold* (not recently accessed). We assess
    the impact of such components by measuring the interrupt latency of a custom bare-metal
    guest with virtual memory support under varying initial states of each element.
    On average, we observe that the interrupt latency — independently from the IC
    — is about 8× higher when caches are cold. As expected, cache misses only impact
    the SW contributions to the interrupt latency, such as the context save and restore
    and the interrupt decoding operations. Conversely, TLB misses caused by cold TLBs
    result in a moderate 5 % average slowdown. This happens because, for simple interrupt
    handlers, the entire interrupt service routine (ISR) fits within a single page.


    # *C. System resources contention*


    To assess the worst-case interrupt latency, we expand our experiments to consider
    HW resource contention. We identify three main reasons for increased interrupt
    latency due to resource contention: (i) interrupt arbitration, (ii) bus contention,
    and (iii) VM interference.


    *Interrupt arbitration:* The vCLIC prioritizes interrupts based on the assigned
    privilege mode, level, and priority, as well as based on a configurable VM priority
    (VSPRIO extension). However, this only affects the ordering of interrupts before
    they are serviced. Once an interrupt request is taken, higher-priority incoming
    interrupts may be delayed depending on the IC and SW implementation. vCLIC minimizes
    the impact of interrupt contention through interrupt nesting [\[4\]](#page-3-3),
    [\[14\]](#page-3-14), and extends this feature to virtual interrupts, allowing
    efficient implementations of preemptible ISRs.


    *Bus contention:* Existing virtualized systems are commonly based on MSIs. MSIs
    are mostly used in server-class computing systems, where predictability and real-time
    features are not critical requirements. As shown by Marques et al. [\[6\]](#page-3-5),
    some system interconnect topologies can increase the average interrupt latency
    by up to 7× due to bus contention for AIAbased systems, along with a significant
    rise (about 750 ns at


    <span id="page-3-15"></span>![](_page_3_Figure_0.jpeg)


    <span id="page-3-16"></span>Figure 3: VSCLIC area breakdown compared to vanilla
    CLIC.


    ![](_page_3_Figure_2.jpeg)


    Figure 4: VSPRIO area impact on vCLIC with the number of priority bits. Case 0
    indicates VSCLIC-only implementation.


    50 MHz) in the response jitter over time. Unlike MSI-based systems, we highlight
    that our proposed vCLIC is entirely free from bus interference, regardless of
    the system interconnect topology, as it relies on core-local wired interrupt lines.


    *Inter-VM interference:* In systems relying on DPHs, interrupt latencies can vary
    substantially over time, depending on the resource allocation status at the time
    of the interrupt. To evaluate such scenarios, we run XVisor with Linux and FreeRTOS
    as guests. We evaluate the worst-case scenario – a critical interrupt directed
    to FreeRTOS arrives while Linux is running – by measuring the time required by
    the system to preempt the Linux execution in favor of FreeRTOS. On Cheshire, XVisor
    takes approximately 35000 clock cycles to perform a VM context switch. As discussed
    in section [II-B,](#page-1-6) if the VSPRIO extension is disabled, the hypervisor
    must also save the configuration of interrupts delegated to the VM being switched
    out and set the configuration for the next VM. The performance degradation due
    to the additional interrupt context associated with each VM is proportional to
    the amount of interrupts delegated to the VM. The CLIC context switch requires
    up to 10000 clock cycles for VMs with 64 virtual interrupts, increasing the overall
    context switch time by almost 30%. Conversely, the context switch overhead is
    more tolerable if few interrupts are delegated to a VM, down to about 1250 clock
    cycles in case of a single virtual interrupt per VM.


    # <span id="page-3-9"></span>*D. Implementation*


    We synthesize vCLIC as part of the Cheshire platform, as described in Section
    [III-A.](#page-2-2) We use Synopsys Design Compiler 2022.03, targeting Intel 16
    FinFet technology at 1.1 GHz, SS corner, and 125 °C. One gate equivalent (GE)
    for this technology equals 0.233 µm<sup>2</sup> . Our extension does not affect
    the critical path of the design.


    Figure [3](#page-3-15) shows the area overhead introduced by the VSCLIC extension
    compared to vanilla CLIC, totaling about 8 kGE. Most of the additional resources
    are due to the increased memory-mapped registers. CLIC''s binary tree size to
    propagate the highest level and priority interrupt is equally affected with roughly
    27 % increase. We evaluate the impact of the VSPRIO extension based on the number
    of priority levels, as shown in Figure [4.](#page-3-16) The arbitration tree size
    grows almost linearly with the number of implemented priority bits.


    Finally, we complete the in-system assessment by measuring the area of Cheshire
    and the relative impact of vCLIC. Accounting for the memory macros footprint,
    vCLIC occupies merely 1 % of the system. Compared to platform-wide centralized
    approaches, this result justifies a per-core, distributed solution for interrupt
    management in a virtualized scenario.


    ## IV. CONCLUSION


    To meet the real-time requirements of virtualized embedded MCSs and manage increasing
    software complexity, interrupt virtualization is crucial. This work introduces
    vCLIC, a virtualization extension to the RISC-V CLIC fast interrupt controller.
    vCLIC reduces interrupt latency by 20X compared to nonvirtualized systems, offers
    a 15 % faster response than MSIbased approaches, and avoids system bus interference.
    Synthesized in a 16nm FinFet technology, vCLIC adds only 8kGE of logic without
    affecting the target frequency, providing a lowoverhead solution for real-time
    embedded MCSs.


    ## REFERENCES


    - <span id="page-3-0"></span>[1] F. Rehm *et al.*, "The Road towards Predictable
    Automotive High - Performance Platforms," vol. 2021-February. Institute of Electrical
    and Electronics Engineers Inc., 2 2021.

    - <span id="page-3-1"></span>[2] G. Heiser, "The role of virtualization in embedded
    systems," in *Proceedings of the 1st Workshop on Isolation and Integration in
    Embedded Systems*, ser. IIES ''08. New York, NY, USA: Association for Computing
    Machinery, 2008, p. 11–16.

    - <span id="page-3-2"></span>[3] J. Yiu, *The Definitive Guide to ARM Cortex-M3
    and Cortex-M4 Processors, Third Edition*, 3rd ed. USA: Newnes, 2013.

    - <span id="page-3-3"></span>[4] RISC-V, ""Smclic" Core-Local Interrupt Controller
    (CLIC) RISC-V Privileged Architecture Extension."

    - <span id="page-3-4"></span>[5] RISC-V, "RISC-V Advanced Interrupt Architecture
    (AIA)."

    - <span id="page-3-5"></span>[6] F. Marques *et al.*, ""Interrupting" the Status
    Quo: A First Glance at the RISC-V Advanced Interrupt Architecture (AIA)," *IEEE
    Access*, vol. 12, pp. 9822–9833, 2024.

    - <span id="page-3-6"></span>[7] B. Sa´ *et al.*, "A first look at risc-v virtualization
    from an embedded systems perspective," *IEEE Transactions on Computers*, vol.
    71, no. 9, pp. 2177–2190, 2022.

    - <span id="page-3-7"></span>[8] F. Zaruba *et al.*, "The Cost of Application-Class
    Processing: Energy and Performance Analysis of a Linux-Ready 1.7-GHz 64-Bit RISC-V
    Core in 22-nm FDSOI Technology," *IEEE Transactions on Very Large Scale Integration
    (VLSI) Systems*, vol. 27, no. 11, pp. 2629–2640, 2019.

    - <span id="page-3-8"></span>[9] A. Ottaviano *et al.*, "Cheshire: A lightweight,
    linux-capable risc-v host platform for domain-specific accelerator plug-in," *IEEE
    TCAS II: Express Briefs*, pp. 1–1, 2023.

    - <span id="page-3-10"></span>[10] J. Martins *et al.*, "Bao: A lightweight static
    partitioning hypervisor for modern multi-core embedded systems," in *NG-RES@HiPEAC*,
    2020.

    - <span id="page-3-11"></span>[11] A. Patel *et al.*, "Embedded hypervisor xvisor:
    A comparative analysis," in *Proceedings of the 2015 23rd Euromicro International
    Conference on Parallel, Distributed, and Network-Based Processing*, ser. PDP ''15.
    USA: IEEE Computer Society, 2015, p. 682–691.

    - <span id="page-3-12"></span>[12] RISC-V, "RISC-V Platform-Level Interrupt Controller
    Specification."

    - <span id="page-3-13"></span>[13] F. Costa *et al.*, "Open Source RISC-V Advanced
    Interrupt Architecture (AIA) IP."

    - <span id="page-3-14"></span>[14] R. Balas *et al.*, "Cv32rt: Enabling fast interrupt
    and context switching for risc-v microcontrollers," *IEEE Transactions on Very
    Large Scale Integration (VLSI) Systems*, pp. 1–13, 2024.'
- title: RISC-V V Vector Extension (RVV) with reduced number of vector registers
  abstract: 'To reduce the area of RISC-V Vector extension (RVV) in small processors,
    the

    authors are considering one simple modification: reduce the number of registers

    in the vector register file. The standard ''V'' extension requires 32 vector

    registers that we propose to reduce to 16 or 8 registers. Other features of RVV

    are still supported. Reducing the number of vector registers does not generate

    a completely new programming model: although the resulting core does not have

    binary code compatibility with standard RVV, compiling for it just requires

    parameterization of the vector register file size in the compiler. The reduced

    vector register file allows for still high utilization of vector RVV processor

    core. Many useful signal processing kernels require few registers, and become

    efficient at 1:4 chaining ratio.'
  url: http://arxiv.org/abs/2410.08396v1
  keywords: ''
  document: '# RISC-V V Vector Extension (RVV) with reduced number of vector registers


    Eino Jacobs <sup>∗</sup><sup>1</sup> , Dmitry Utyansky †<sup>1</sup> , Muhammad
    Hassan ‡<sup>2</sup> , and Thomas Roecker §<sup>2</sup>


    > <sup>1</sup> Synopsys, Inc, Sunnyvale, USA 2 Infineon Technologies AG, Munich,
    Germany


    > > October 14, 2024


    #### Abstract


    To reduce the area of the RISC-V Vector extension (RVV) in small processors, the
    authors are considering one simple modification: reduce the number of registers
    in the vector register file. The standard "V" extension requires 32 vector registers
    that we propose to reduce to 16 or 8 registers. Other features of RVV are still
    supported.


    Reducing the number of vector registers does not generate a completely new programming
    model: although the resulting core does not have binary code compatibility with
    the standard RVV, compiling for it just requires parameterization of the vector
    register file size in the compiler.


    The reduced vector register file still allows for high utilization of the RVV
    vector processor core. Many useful signal processing kernels require few registers.


    ## 1 Introduction


    The RISC-V Vector Extension (RVV), ratified in version 1.0 [\[4\]](#page-8-0),
    provides an Instruction Set Architecture (ISA) that has scalability and flexibility
    with respect to the underlying processor implementation. This allows optimization
    of processor designs for different area/power/performance targets that all share
    a consistent programming model. One of the advantages of this approach is cost
    reduction through reuse of hardware, compilers, libraries and software.


    <sup>∗</sup> eino.jacobs@synopsys.com


    <sup>†</sup>dmitry.utyansky@synopsys.com


    <sup>‡</sup>Muhammad.Hassan@infineon.com


    <sup>§</sup>Thomas.Roecker@infineon.com


    However, for low-end processors the mandatory vector register file with 32 vector
    registers contributes visibly to the overall core size. In this paper we assume
    that the smallest practical vector length (V LEN) is 64 bits and therefore the
    smallest practical vector register file consists of 32 registers of 64 bits for
    a total of 2048 bits. We also assume that the number of physical vector registers
    equals the number of architectural vector registers. The RVV standard allows for
    a VLEN that is lower than 64, VLEN of 32 bits, but we do not consider this practical,
    because ecosystem support is currently weak for this small size and 64-bit data
    types are not supported. We argue that an option to reduce the vector register
    file down to 16 or even 8 registers is a good choice for some designs. A vector
    register file with 8 registers of 64 bits has in total 512 register bits. This
    is the same number of bits as in a scalar register file with 16 registers of 32
    bits that are in an RV32E version of RISC-V.


    ## 2 Number of Vector Registers, their Sizes and Typical Application Requirements


    The number of vector registers and the length of vector registers must be considered
    together for performance analysis. Depending on the processor data path length
    (DLEN), available functional units and other details, the dimensions of the vector
    register file that are required to obtain good cycle performance and good hardware
    utilization will vary. This also depends on compute kernels: complexity of computations,
    our ability to reuse data loaded into registers, etc., and, therefore, depends
    on the application domain. In this paper we focus on embedded applications and
    processor configurations minimizing processor size and power consumption. At this
    area/performance point the processor has a gate count of a few dozen kgates, datapath
    length of 32 bits (DLEN = 32), and either in-order single-issue or limited fusion/multi-issue
    capabilities.


    One attractive feature of RVV is potential "quasi-multi-issue" through chaining.
    If the vector length is longer than DLEN, a vector instruction issued in one cycle
    is executed for multiple cycles (V LEN/DLEN > 1 cycle). The extra cycles can be
    used to issue other instructions. These can be executed in parallel with the first
    vector instruction (in the same convoy using vector processors terminology), as
    long as they use other execution resources. One can think of chaining ratio as
    (EMUL · V LEN)/DLEN, where EMUL, or effective LMUL (vector length multiplier),
    is defined by RVV 1.0 [\[4\]](#page-8-0) as the number of registers required to
    hold vector operand elements. So, with e.g. chaining ratio of 1:4 one can see
    the code as packages (convoys) of 4 multicycle instructions using different execution
    resources.


    A few considerations need to be taken into account to dimension the vector register
    file.


    - Cycle performance and the program''s ability to fully load the processor''s
    functional units. There is a sweet spot for the chaining ratio, beyond which the
    execution resources are fully saturated, therefore for a given DLEN there is no
    benefit in increasing VLEN beyond that.

    - Processor size. For small processors, the vector register file is a substantial
    contributor to the area, so obviously the smaller the better.

    - Applications. There is a limit to vector length that the application can use.
    For typical


    embedded applications this is naturally not too big.


    We observe that in small processor designs the full RVV 1.0 vector register file
    (32 64-bit vector registers) can take about 30% of the overall logic area. In
    this case, a reduction of the vector register file by half (16 64-bit vector registers)
    or to a quarter size (8 64-bit vector registers) can save 15% to 23% of the processor
    area, respectively.


    For application examples, consider the automotive industry. One such application
    is the filtering of ADC data, necessitating the use of algorithms such as Finite
    Impulse Response (FIR) or averaging filters. Another application is the enhancement
    of sensor data accuracy through virtual sensors or filtering of sensor data. This
    typically involves system state modelling with Kalman Filters or Embedded AI Neural
    Network (NN) topologies like Multilayer Perceptrons (MLP), autoencoders and Recurrent
    Neural Networks (RNNs). Additionally, audio sensor data processing, such as key
    word recognition and environment sensing using audio, is another area for which
    embedded processors are utilized. These algorithms typically rely heavily on matrix
    and vector computations. Efficient execution of these algorithms on embedded processors
    is crucial for ensuring the reliability and accuracy of the automotive systems
    that rely on them.


    Compute kernels used in linear algebra and Digital Signal Processing (DSP) usually
    do not require huge amounts of data in flight. E.g., a simple "load-load-multiply-accumulate"
    pattern can be implemented with just the bare minimum of 2 vector registers for
    input, 1 vector register for output.


    RVV-like vector architectures allow to avoid explicit software pipelining and
    loop unrolling, with multiple loop iterations in flight, which would otherwise
    require multiple copies of data in registers, requiring more registers. Essentially
    hardware takes care of the required pipelining, enabling parallelism of e.g. loads
    and ALU operations.


    Many well-known vector processors used fairly few vector registers with good results.
    E.g., all Cray computers had 8 vector registers ( [\[2\]](#page-8-1) [\[3\]](#page-8-2)),
    though of longer length each. Closer to modern days, ARM M-profile Vector Extensions
    (MVE) use 8 128-bit vector registers [\[1\]](#page-8-3). RVV''s 32 vector registers
    together with the ability to combine vectors into groups using LMUL factor allows
    to configure the processor for fewer longer vector registers: with LMUL=8 one
    effectively has 4 vector registers of 8x longer size.


    ## 3 Processor Configuration for Embedded DSP


    In the subsequent sections we focus on fixed point kernels for small embedded
    DSP use-cases. We assume a small processor without floating point; inclusion of
    a relatively large floating point unit would make the area savings of a reduction
    of the number of vector registers seem less important.


    Other features we assume:


    - V LEN = 64 bits, floating point deconfigured (i.e., Zve64x extension). 64 bits
    as minimum also makes sense because 64-bit integers are useful as accumulators.

    - DLEN = 32 bits in the smallest configuration. This can be scaled up for bigger
    proces-


    sors, with the matching vector length increase.


    - Processor implementation supporting chaining of operations (at least between
    vector load and ALU and Multiply-accumulate unit).

    - No other multi-issue capabilities. Chaining is the only way to organize "quasi-multiissue".

    - Load/store bandwidth of 32 bits. A higher bandwidth of 64 bits (DLEN · 2) would
    relax the load bottleneck for some kernels, This is a topic to explore further.

    - Reasonably short latency of load, MAC and ALU operations. Typically such designs
    use relatively low clock frequency, hence shallow processor pipeline. For the
    examples considered we use 1-cycle latency for loads, 5 cycles for multiply-accumulate,
    3 cycles for other ALU operations. For the kernels considered in this paper a
    small variation of the latency does not change the results.


    ## 4 DSP Kernel Analysis with Reduced Vector Registers


    In this chapter we analyze several digital signal processing kernels, useful for
    many applications. To quantify the quality of mapping of the kernel to the processor
    we use utilization of the critical resource for each kernel. Typically this is
    either ALU, multiplier, or load/store bandwidth.


    One important consideration is "run-time V LEN agnosticism". Depending on the
    use case, this might be more or less important. For deeply embedded applications
    it often does not matter: the program is compiled for the specific processor,
    and knowledge about its parameters, like hardware vector length, can be used by
    the compiler to better optimize the code. Specifically that allows to drop "vset\*"
    instructions from tight loops, minimizing number of instructions. Also, special
    tail handling often can be avoided by choosing convenient sizes or explicit tail
    handling outside of the main loop. In the subsequent analysis we assume that a
    program is compiled for the specific configuration, so vset\* instructions can
    be dropped from the inner loops.


    #### 4.1 Matrix Multiplication


    Each table in this and the subsequent sections shows the inner kernel of the processing
    loop in sustained operation, with columns showing the instructions issued at each
    cycle, the functional unit used by the instruction, and the results written to
    the vector register file by the selected functional units at that cycle. A colon-separated
    number after vector register name is a sequential number of DLEN-sized group of
    bits output at that cycle. For (V LEN = 64, DLEN = 32)-processor each vector register
    is treated as two such groups, :0 and :1. So e.g. writes of v0:0,v0:1 as a result
    of vwmacc v0 operation appear +5 cycles later, while the preceding writes by the
    MAC correspond to the previous loop iteration. Vmacc is a widening multiply-accumulate
    operation, so it generates 2x wider result, two DLEN-sized chunks.


    For the reference processor configuration, starting with the tile size of 2 ×
    (V LEN · 2) (e.g. 2 × 16 samples for 8-bit samples and V LEN = 64) we can get
    to 8/9 = 89% utilization of the multiply-accumulate unit. For this we need 1:4
    chaining ratio, achievable with LMUL=2, as shown in Table [1.](#page-4-0) Total
    of 9 cycles in the loop (assuming single issue, one instruction per cycle, no
    stalls), of which MAC unit is active for 8 cycles, producing double-wide result
    in each cycle.


    For some cases the utilization can be 100%. If one of the scalar loads can use
    immediate offset, both loads can reuse the same base address and so only one address
    increment is required, reducing number of instructions to 8, as shown in Table
    [2.](#page-4-1)


    If 2×16 tiles are too wide for the application, inevitably we have fewer data
    processed with the same code and same cycles, so e.g. for 2 × 8 tiles we will
    have LMUL=1 and utilization of 4/9=44%, as shown in Table [3.](#page-5-0) Or 50%
    if just one address update addi is needed, i.e., when overall matrix row size
    fits into a 12-bit immediate offset allowed in the lb instruction.


    For comparison, increasing the tile width to fit LMUL=4 allows us to obtain 100%
    utilization, with enough "issue cycles" to spare that can be potentially used
    for some additional computations while maintaining the same MAC utilization, as
    shown in Table [4](#page-5-1)


    | Cycle | Instruction issued     | Unit   | VMAC:M      | VLOAD:L |

    |-------|------------------------|--------|-------------|---------|

    | 0     | vle8.v v8,(x28)        | VLOAD  | M:v1:0,v1:1 | L:v8:0  |

    | 1     | c.addi x28,0x10        | Scalar | M:v2:0,v2:1 | L:v8:1  |

    | 2     | lb x7,(x13)            | Scalar | M:v3:0,v3:1 | L:v9:0  |

    | 3     | vwmacc.vx v0,x7,v8     | VMAC   | M:v4:0,v4:1 | L:v9:1  |

    | 4     | c.addi x13,0x1         | Scalar | M:v5:0,v5:1 |         |

    | 5     | lb x8,(x5)             | Scalar | M:v6:0,v6:1 |         |

    | 6     | c.addi x5,0x1          | Scalar | M:v7:0,v7:1 |         |

    | 7     | vwmacc.vx v4,x8,v8     | VMAC   |             |         |

    | 8     | bne x13,x15,0xffffffe6 | Scalar | M:v0:0,v0:1 |         |


    Table 1: Matrix multiplication kernel, 2 × V LEN · 2 tile, 8 ∗ 8 → 16 bits 10
    vector registers used


    <span id="page-4-0"></span>


    | Cycle | Instruction issued     | Unit   | VMAC:M      | VLOAD:L |

    |-------|------------------------|--------|-------------|---------|

    | 0     | vle8.v v8,(x28)        | VLOAD  | M:v1:0,v1:1 | L:v8:0  |

    | 1     | c.addi x28,0x10        | Scalar | M:v2:0,v2:1 | L:v8:1  |

    | 2     | lb x7,N(x5)            | Scalar | M:v3:0,v3:1 | L:v9:0  |

    | 3     | vwmacc.vx v0,x7,v8     | VMAC   | M:v4:0,v4:1 | L:v9:1  |

    | 4     | lb x8,(x5)             | Scalar | M:v5:0,v5:1 |         |

    | 5     | c.addi x5,0x1          | Scalar | M:v6:0,v6:1 |         |

    | 6     | vwmacc.vx v4,x8,v8     | VMAC   | M:v7:0,v7:1 |         |

    | 7     | bne x13,x15,0xffffffe6 | Scalar | M:v0:0,v0:1 |         |


    <span id="page-4-1"></span>Table 2: Matrix multiplication kernel, 2 × V LEN ·
    2 tile, 8 ∗ 8 → 16 bits Single x-pointer is used to access samples from column,
    separated by N bytes 10 vector registers used


    | Cycle | Instruction issued     | Unit   | VMAC:M      | VLOAD:L |

    |-------|------------------------|--------|-------------|---------|

    | 0     | vle8.v v8,(x28)        | VLOAD  | M:v1:0,v1:1 | L:v8:0  |

    | 1     | c.addi x28,0x10        | Scalar | M:v2:0,v2:1 | L:v8:1  |

    | 2     | lb x7,(x13)            | Scalar | M:v3:0,v3:1 |         |

    | 3     | vwmacc.vx v0,x7,v8     | VMAC   |             |         |

    | 4     | c.addi x13,0x1         | Scalar |             |         |

    | 5     | lb x8,(x5)             | Scalar |             |         |

    | 6     | c.addi x5,0x1          | Scalar |             |         |

    | 7     | vwmacc.vx v4,x8,v8     | VMAC   |             |         |

    | 8     | bne x13,x15,0xffffffe6 | Scalar | M:v0:0,v0:1 |         |


    <span id="page-5-0"></span>Table 3: Matrix multiplication kernel, 2 × V LEN tile,
    8 ∗ 8 → 16 bits 4 vector registers used


    | Cycle | Instruction issued     | Unit   | VMAC:M        | VLOAD:L |

    |-------|------------------------|--------|---------------|---------|

    | 0     | vle8.v v16,(x28)       | VLOAD  | M:v5:0,v5:1   | L:v16:0 |

    | 1     | addi x28,x28,0x20      | Scalar | M:v6:0,v6:1   | L:v16:1 |

    | 2     | lh x7,(x13)            | Scalar | M:v7:0,v7:1   | L:v17:0 |

    | 3     | - -                    | -      | M:v8:0,v8:1   | L:v17:1 |

    | 4     | - -                    | -      | M:v9:0,v9:1   | L:v18:0 |

    | 5     | - -                    | -      | M:v10:0,v10:1 | L:v18:1 |

    | 6     | vwmacc.vx v0,x7,v16    | VMAC   | M:v11:0,v11:1 | L:v19:0 |

    | 7     | c.addi x13,0x2         | Scalar | M:v12:0,v12:1 | L:v19:1 |

    | 8     | lh x8,(x5)             | Scalar | M:v13:0,v13:1 |         |

    | 9     | c.addi x5,0x2          | Scalar | M:v14:0,v14:1 |         |

    | 10    | - -                    | -      | M:v15:0,v15:1 |         |

    | 11    | - -                    | -      | M:v0:0,v0:1   |         |

    | 12    | - -                    | -      | M:v1:0,v1:1   |         |

    | 13    | - -                    | -      | M:v2:0,v2:1   |         |

    | 14    | vwmacc.vx v8,x8,v16    | VMAC   | M:v3:0,v3:1   |         |

    | 15    | bne x13,x15,0xffffffe4 | Scalar | M:v4:0,v4:1   |         |


    <span id="page-5-1"></span>Table 4: Matrix multiplication kernel, 2 × V LEN ·
    4 tile, 8 ∗ 8 → 16 bits 20 vector registers used


    Summarizing, while RVV 1.0 32 vector registers provide ample margin, 10 vector
    registers is almost as good for this kernel. Also note that 2×V LEN · 4 tile size
    might be impractical for some applications: in small embedded applications one
    is often dealing with smaller matrices. And for e.g. 8-bit data 2 × V LEN · 4
    (2x32 elements) might be too wide a tile.


    #### 4.2 Vector Accumulation


    Accumulation appears in many applications, for instance if one needs to compute
    the sum or average of an array of data samples. The standard approach is to accumulate
    data in a vector and then do a reduction sum of the vector. For sizeable vectors
    most cycles are spent in the accumulation loop, so we focus on it.


    | Cycle | Instruction issued | Unit   | ALU:A       | VLOAD:L |

    |-------|--------------------|--------|-------------|---------|

    | 0     | vl4re16.v v0,(a0)  | VLOAD  | A:v2:0,v2:1 | L:v0:0  |

    | 1     | vwadd.wv v2,v2,v0  | ALU    | A:v3:0,v3:1 | L:v0:1  |

    | 2     | addi a0,a0,16      | Scalar | A:v4:0,v4:1 | L:v1:0  |

    | 3     | bne a5,a0,pc - 16  | Scalar | A:v5:0,v5:1 | L:v1:1  |


    Assuming the processor is capable of writing double-wide results to a vector register
    each cycle, we have the kernel shown in Table [5,](#page-6-0) with 100% ALU and
    load bandwidth utilization.


    <span id="page-6-0"></span>Table 5: Vector accumulation, V LEN · 2 tile (LMUL=2),
    16 ∗ 16 → 32 bits 6 vector registers used


    #### 4.3 Dot Product


    Dot product is a building block of many linear algebra and DSP kernels. In the
    vector\*vector form it is load-bound, requiring two vector loads per one vector
    MAC, so if vector loads are only DLEN-wide we can at best have 50% MAC utilization.
    LMUL=2 is enough to get to full utilization of the load unit, as shown in Table
    [6](#page-6-1)


    | Cycle | Instruction issued     | Unit   | MAC:M       | VLOAD:L |

    |-------|------------------------|--------|-------------|---------|

    | 0     | vle16.v v0,(x10)       | VLOAD  |             | L:v0:0  |

    | 1     | c.addi x10,0x10        | Scalar |             | L:v0:1  |

    | 2     | - -                    | -      |             | L:v1:0  |

    | 3     | - -                    | -      | M:v4:0,v4:1 | L:v1:1  |

    | 4     | vle16.v v2,(x11)       | VLOAD  | M:v5:0,v5:1 | L:v2:0  |

    | 5     | c.addi x11,0x10        | Scalar | M:v6:0,v6:1 | L:v2:1  |

    | 6     | vwmacc.vv v4,v0,v2     | VMAC   | M:v7:0,v7:1 | L:v3:0  |

    | 7     | bne x15,x10,0xfffffff0 | Scalar |             | L:v3:1  |


    <span id="page-6-1"></span>Table 6: Dot product, V LEN · 2 tile (LMUL=2), 16 ∗
    16 → 32 bits 8 vector registers used


    #### 4.4 Matrix by Vector Multiplication


    Matrix by vector multiplication is a common kernel, used for AI/inference (e.g.,
    dense layers, perceptron-like) and DSP (e.g., Finite Impulse Response filter).
    For the processor considered this is both a load- and MAC-bound kernel, and best
    utilization is with LMUL=4, with 4 vector registers used for input and 8 vector
    registers used for output, as shown in Table [7:](#page-7-0) 9 cycles, of which
    MAC is active in 8 cycles, so 8/9 = 89%


    | Cycle | Instruction issued | Unit   | MAC:M         | LOAD:L |

    |-------|--------------------|--------|---------------|--------|

    | 0     | lbu a2,0(a5)       | Scalar | M:v10:0,v10:1 | a2     |

    | 1     | vle8.v v0,(a4)     | VLOAD  | M:v9:0,v9:1   | L:v0:0 |

    | 2     | c.addi a5,1        | Scalar | M:v11:0,v11:1 | L:v0:1 |

    | 3     | addi a4,a4,128     | Scalar | M:v4:0,v4:1   | L:v1:0 |

    | 4     | - -                | -      | M:v5:0,v5:1   | L:v1:1 |

    | 5     | - -                | -      | M:v6:0,v6:1   | L:v2:0 |

    | 6     | vwmacc.vx v4,v0,a2 | VMAC   | M:v7:0,v7:1   | L:v2:1 |

    | 7     | bne s0,a5,pc - 18  | Scalar | M:v8:0,v8:1   | L:v3:0 |

    | 8     | - -                |        |               | L:v3:1 |


    <span id="page-7-0"></span>Table 7: Matrix by vector multiplication, V LEN · 4
    tile (LMUL=4), 8 ∗ 8 → 16bits 12 vector registers used


    #### 4.5 Results Summary


    Table [8](#page-7-1) summarizes kernels for a few tiling sizes, LMUL factor used,
    required vector register number and resulting MAC utilization.


    An increase of the number of vector registers beyond the indicated numbers does
    not improve MAC utilization further. All considered microkernels are either MAC-bound
    or load/store bandwidth bound.


    | Kernel        | Data Size                | LMUL | Vector         | MAC            |
    Comment                       |

    |---------------|--------------------------|------|----------------|----------------|-------------------------------|

    |               |                          |      | registers used | Utilization,
    % |                               |

    | Matrix*Matrix | 2 ×<br>·<br>(V LEN<br>2) | 2    | 10             | 89 to 100      |
    Depends on size (+addi)       |

    | Matrix*Matrix | 2 ×<br>V LEN             | 1    | 4              | 44 to 50       |
    tile too small for efficiency |

    | Accumulation  | V LEN<br>·<br>2          | 2    | 6              | 100            |                               |

    | Dot product   | ·<br>V LEN<br>2          | 2    | 8              | 50             |
    Load-limited                  |

    | Matrix*Vector | ·<br>V LEN<br>4          | 4    | 12             | 89             |                               |


    <span id="page-7-1"></span>Table 8: Results summary for kernels, number of vector
    registers used and MAC utilization


    ## 5 Conclusions


    Our analysis shows that important kernels can be efficiently implemented with
    fewer than 32 vector registers, the number required by the RVV 1.0 standard. This
    applies to vector lengths as small as 64 bits. All kernels fit into 16 vector
    registers. Some fit into 8 vector registers and all fit into 8 registers with
    lower utilization.


    Configurations with a reduced number of vector registers enable smaller processor
    designs. This approach does not require a completely new programming model. Although
    the resulting processor does not have binary code compatibility with standard
    RVV, compiling for it only requires parameterization of the vector register file
    size in the compiler. Overall optimization approaches are still the same, allowing
    to have a unified source code base. This appears to be


    a good choice for deeply embedded designs, for which unconstrained binary code
    compatibility is not a hard requirement.


    Our result suggests an ISA extension to reduce the number of vector registers
    to 16 or 8 as part of the RVV standard.


    ## References


    - <span id="page-8-3"></span>[1] Arm Limited. Arm® Cortex®-M55 Processor Technical
    Reference Manual, Revision: r1p1. <https://developer.arm.com/documentation/101051/0101/?lang=en>,
    2024. [Online; accessed 6-Aug-2024].

    - <span id="page-8-1"></span>[2] Cray Research, Inc. Cray-1 Computer System Hardware
    Reference Manual 2240004, Rev C. [http://bitsavers.trailing-edge.com/pdf/cray/CRAY-1/2240004C\\_CRAY-1\\_Hardware\\_Reference\\_Nov77.pdf](http://bitsavers.trailing-edge.com/pdf/cray/CRAY-1/2240004C_CRAY-1_Hardware_Reference_Nov77.pdf),
    1977. [Online; accessed 6-Aug-2024].

    - <span id="page-8-2"></span>[3] Cray Research, Inc. CRAY-Y MP EL Functional Description,
    HR-04027. [https://cray-history.net/wp-content/uploads/2021/08/HR-04027-CRAY-Y-MP-EL-Functional-Description](https://cray-history.net/wp-content/uploads/2021/08/HR-04027-CRAY-Y-MP-EL-Functional-Description.pdf)
    1992. [Online; accessed 30-July-2024].

    - <span id="page-8-0"></span>[4] RISC-V International. RISC-V "V" Vector Extension
    Version 1.0. <https://github.com/riscv/riscv-v-spec/releases/tag/v1.0>, 2021.
    [Online; accessed 6-Aug-2024].'
- title: "MENAGE: Mixed-Signal Event-Driven Neuromorphic Accelerator for Edge\n  Applications"
  abstract: 'This paper presents a mixed-signal neuromorphic accelerator architecture

    designed for accelerating inference with event-based neural network models.

    This fully CMOS-compatible accelerator utilizes analog computing to emulate

    synapse and neuron operations. A C2C ladder structure implements synapses,

    while operational amplifiers (op-amps) are used to realize neuron functions. To

    enhance hardware resource utilization and power efficiency, we introduce the

    concept of a virtual neuron, where a single neuron engine emulates a set of

    model neurons, leveraging the sparsity inherent in event-based neuromorphic

    systems. Additionally, we propose a memory-based control technique to manage

    events in each layer, which improves performance while maintaining the

    flexibility to support various layer types. We also introduce an integer linear

    programming (ILP)-based mapping approach for efficiently allocating the model

    onto the proposed accelerator. The accelerator is a general-purpose

    neuromorphic platform capable of executing linear and convolutional neural

    models. The effectiveness of the proposed architecture is evaluated using two

    specially designed neuromorphic accelerators and two event-based datasets. The

    results show that the proposed architecture achieves 12.1 TOPS/W energy

    efficiency when accelerating a model trained on CIFAR10-DVS.'
  url: http://arxiv.org/abs/2410.08403v1
  keywords: Neuromorphic Accelerator, Mixed-signal design, Fully CMOS compatible,
    Event-Driven, Energy Efficiency.
  document: '# MENAGE: Mixed-Signal Event-Driven Neuromorphic Accelerator for Edge
    Applications


    Armin Abdollahi, Mehdi Kamal, and Massoud Pedram


    Department of Electrical and Computer Engineering, University of Southern California,
    Los Angeles, CA, USA {arminabd, mehdi.kamal, pedram}@usc.edu


    *Abstract*—This paper presents a mixed-signal neuromorphic accelerator architecture
    designed for accelerating inference with event-based neural network models. This
    fully CMOS-compatible accelerator utilizes analog computing to emulate synapse
    and neuron operations. A C2C ladder structure implements synapses, while operational
    amplifiers (op-amps) are used to realize neuron functions. To enhance hardware
    resource utilization and power efficiency, we introduce the concept of a virtual
    neuron, where a single neuron engine emulates a set of model neurons, leveraging
    the sparsity inherent in event-based neuromorphic systems. Additionally, we propose
    a memory-based control technique to manage events in each layer, which improves
    performance while maintaining the flexibility to support various layer types.
    We also introduce an integer linear programming (ILP)-based mapping approach for
    efficiently allocating the model onto the proposed accelerator. The accelerator
    is a general-purpose neuromorphic platform capable of executing linear and convolutional
    neural models. The effectiveness of the proposed architecture is evaluated using
    two specially designed neuromorphic accelerators and two event-based datasets.
    The results show that the proposed architecture achieves 12.1 TOPS/W energy efficiency
    when accelerating a model trained on CIFAR10-DVS.


    *Index Terms*—Neuromorphic Accelerator, Mixed-signal design, Fully CMOS compatible,
    Event-Driven, Energy Efficiency.


    #### I. INTRODUCTION


    Neuromorphic computing, inspired by the architecture and functionality of the
    human brain, offers a highly efficient alternative to traditional computing paradigms
    [\[1\]](#page-6-0). In the brain, neurons communicate through electrical impulses
    or spikes, which allow for parallel processing and event-driven computation [\[2\]](#page-6-1).
    Spiking Neural Networks (SNNs) are a key component of neuromorphic computing,
    utilizing discrete spikes for communication between artificial neurons [\[3\]](#page-6-2).
    This spike-based approach contrasts with traditional artificial neural networks
    (ANNs) that rely on continuous values, leading to significant reductions in power
    consumption and enabling the natural processing of temporal information [\[4\]](#page-6-3).
    Moreover, SNNs have shown potential in various applications, including sensory
    processing, autonomous systems, and robotics, where low latency and real-time
    responses are critical. SNN also could be used on medical diagnostics [\[5\]](#page-6-4),
    and hardware optimization [\[6\]](#page-6-5), [\[7\]](#page-6-6) to enhance the
    efficiency. Additionally, The use of spiking neural networks (SNNs) could complement
    traditional deep learning models in mortality prediction [\[8\]](#page-6-7), [\[9\]](#page-6-8)
    by leveraging the event-driven processing of temporal health data to improve efficiency.
    By leveraging the sparse nature of spike-based communication, SNNs excel in environments
    where data is dynamic and computational efficiency is essential, making them ideal
    for edge computing and other low-power scenarios [\[10\]](#page-6-9).


    The Leaky Integrate-and-Fire (LIF) neuron model, commonly used in SNNs, emulates
    the behavior of biological neurons by accumulating input signals and firing spikes
    when a threshold is exceeded, making SNNs suitable for tasks involving time-dependent
    data [\[11\]](#page-6-10). This model not only provides a biologically plausible
    representation of neural activity but also introduces inherent noise robustness
    and adaptability to varying input patterns, enhancing the system''s ability to
    handle complex, time-varying stimuli. The LIF model''s reliance on simple arithmetic
    operations further contributes to the computational efficiency of SNNs, making
    them a promising alternative for neuromorphic processors.


    Despite these advantages, scaling neuromorphic systems remains a significant challenge
    due to hardware constraints and energy inefficiencies. One critical limitation
    of existing neuromorphic chips is their high energy consumption and hardware complexity
    when scaling to large datasets and networks [\[12\]](#page-6-11). Traditional
    SNN implementations are often computationally expensive due to dense synaptic
    connections and the need for high precision in neural computations. One promising
    solution for improving energy efficiency is analog and mixed-signal computing.
    They can significantly enhance energy efficiency by directly processing in analog
    form, which reduces the need for power-intensive digital conversions, making them
    particularly suitable for handling the sparse, event-driven nature of neural models
    in neuromorphic applications.


    Memristor-based neuromorphic systems, which have gained attention for their potential
    to enable efficient in-memory computing, also face significant challenges such
    as device variability, limited endurance, and resistance drift, which can severely
    impact the stability and accuracy of neural computations [\[13\]](#page-6-12),
    [\[14\]](#page-6-13). These issues make it difficult to maintain consistent performance,
    particularly in large-scale deployments where variability can lead to unpredictable
    behavior. As neuromorphic systems become increasingly deployed in real-time and
    resource-constrained environments, reducing power consumption while maintaining
    accuracy is paramount.


    To address the aforesaid challenges, this paper describes a fully CMOS-compatible
    mixed-signal neuromorphic accelerator architecture, which leverages analog computing
    to emulate the synaptic connections and neuron behavior. The C2C ladder structure,
    which can act as a high-precision analog multiplier, is used to scale the spikes
    based on the model parameters, and opamp-based neurons have been used to emulate
    the LIF behavior. To improve the energy efficiency and because of the sparsity
    in the events, we suggest modeling more than one neuron in each physically designed
    neuron engine. Moreover, to fully utilize the accelerator components, we suggest
    an integer linear programming (ILP) formulation to mathematically formulate the
    mapping problem and solve it efficiently. The effectiveness of the designed accelerator
    is assessed on two well-known neuromorphic datasets.


    The remainder of this paper is organized as follows. Section II reviews the prior
    work. The details of the proposed accelerator architecture and the ILP formulation
    for the mapping are discussed in Section III. Section IV presents the evaluation
    results, and finally, the paper is concluded in Section V.


    #### II. RELATED WORK


    Neuromorphic hardware, such as IBM''s TrueNorth and Intel''s Loihi, is designed
    to support the unique requirements of SNNs, with architectures optimized for sparse,
    event-driven computation [\[12\]](#page-6-11), [\[15\]](#page-6-14). These chips
    integrate multiple cores to simulate vast networks of neurons and synapses efficiently,
    enabling real-time processing while maintaining low power consumption. [\[16\]](#page-6-15)
    demonstrates the use of memristor-based neural networks for efficient in-situ
    learning with adaptive capabilities to hardware imperfections, achieving competitive
    classification accuracy on standard machine learning datasets. In [\[17\]](#page-6-16),
    fully memristive neural networks are implemented using memristors for both neurons
    and synapses, facilitating pattern classification via unsupervised synaptic weight
    updates. [\[18\]](#page-6-17) explores the cooperative development of memristor-based
    spiking neural networks (SNNs), emphasizing the integration of neural network
    architectures and memristor technology for efficient energy-saving systems. [\[19\]](#page-6-18)
    discusses the BrainScaleS-2 accelerated analog neuromorphic computing architecture,
    which integrates both analog and digital components for efficient spike-based
    processing. In [\[20\]](#page-6-19), the authors present advancements in CMOS-integrated
    memristive arrays, emphasizing their use in high-performance computing systems
    based on brain-inspired architectures. The paper discusses how memristors, combined
    with crossbar architectures, enable efficient in-memory computing for vector-matrix
    multiplication. In [\[21\]](#page-6-20), large-scale memristor crossbar arrays
    are implemented for neuromorphic computing, enabling efficient parallel inmemory
    processing for neural networks. [\[22\]](#page-6-21) introduces a sparsity-aware
    neuromorphic computing unit that combines spiking and artificial neural networks
    using leaky integrated neurons. [\[23\]](#page-6-22) proposes a stochastic-bits
    enabled binary spiking neural network (sBSNN) that uses probabilistic computing
    for energy-efficient neuromorphic systems. The network leverages binary synapses
    and stochastic neuron. In [\[24\]](#page-6-23), the authors present NeuRRAM, a
    compute-in-memory chip utilizing resistive memory for energy-efficient edge AI
    applications. [\[25\]](#page-6-24) presents a 4-Mbyte compute-in-memory (CIM)
    macro based on resistive random-access memory (ReRAM) for AI edge devices. [\[26\]](#page-6-25)
    present a 4M-synapse integrated neural network processor based on analog ReRAM,
    with cell current-controlled writing to achieve high power efficiency. [\[27\]](#page-6-26)
    introduces a fully integrated analog ReRAM-based compute-in-memory chip for efficient
    neural network processing with parallel MAC operations. [\[28\]](#page-6-27) presents
    a mixed-signal spiking neural network processor with 8-bit synaptic weights and
    on-chip learning that operates in the continuous-time domain. [\[29\]](#page-6-28)
    propose a clock-free spiking neural network for AIoT applications, using a multilevel
    eventdriven architecture to reduce power consumption and inference latency. [\[30\]](#page-6-29)
    shows a fully integrated spiking neural network combining analog neurons and resistive
    RAM (RRAM) synapses, tailored for the N-MNIST classification task.


    #### III. MENAGE ARCHITECTURE


    The general structure of the proposed neuromorphic accelerator architecture (called
    MENAGE) is depicted in [Figure 1.](#page-2-0) The architecture contains a chain
    of mixed-signal Neuromorphic (MX-NEURACORE) engines, each used for executing one
    layer of the given neural model. Each MX-NEURACORE consists of a memory-based
    controller to manage the received events to the layer as well as scheduling and
    sending them to the analog synapse (A-SYN) engines. The output of A-SYN is connected
    to the Analog Neuron (A-NEURON) engine. The generated pulse by an A-NEURON of
    a MX-NEURACORE is passed to the next MX-NEURACORE. In the proposed architecture,
    the weights are mapped to the SRAM memories of A-SYNs, while control signals generated
    by a distiller are stored on the memories of the proposed memory-based controller.
    The control signals are generated based on the proposed ILP-based mapping approach,
    which aims to improve the utilization of the A-SYN and A-NEURON engines inside
    each MX-NEURACORE. The following subsections will discuss the details of the engines
    and units of the MX-NEURACORE.


    The proposed accelerator supports rate-based spike encoding where spikes are pulses
    passed between the MX-NEURACOREs. The weights are in the 8-bit digital format.
    Hence, the parameters of the given model should be quantized before mapping them
    to the accelerator''s memory. In addition, since our proposed accelerator supports
    pruned neural models, we suggest pruning the network before mapping the model
    onto the accelerator. The general flow for mapping the given model on the proposed
    accelerator is illustrated in [Algorithm 1.](#page-2-1)


    A system clock is used to synchronize the digital components. When the clock''s
    rising edge occurs, any new event received by a MX-NEURACORE is stored in the
    Event Memory (MEME). Each received event contains the index of the source neuron.
    The MX-NEURACORE includes a controller, which utilizes a polling-based approach
    to check the MEM<sup>E</sup> in each clock cycle. In the case of an event, the
    controller extracts the initial address and the number of the rows inside the
    Synapse and Neuron Assignment memory (MEMS&N), which contains the synapse connections.
    This information is obtained via an Event-to-Address memory (MEME2A). It may take
    more than one clock cycle to dispatch the received event from a neuron to the
    destination neurons. In this case, the controller does not fetch any new event
    from the MEME.


    In MEME2A, the initial bits specify the number of connections or rows that need
    to be read from MEMS&N for a specific element in MEME2A. In contrast, the subsequent
    bits


    ![](_page_2_Figure_0.jpeg)


    <span id="page-2-0"></span>Fig. 1. The proposed MENAGE architecture


    provide the starting address of these connections in MEMS&N. This sequential storage
    scheme facilitates efficient access and processing of synaptic connections. The
    MEMS&N utilizes an assignment derived from the Integer Linear Programming (ILP)
    solution, which allocates each connection to the appropriate MX-NEURACORE hardware
    component, optimizing resource utilization and ensuring adherence to hardware
    constraints.


    #### *A.* A-NEURON *Structure*


    The A-NEURON emulates the leaky integrate-and-fire (LIF) neuron, whose structure
    is illustrated in [Figure 2.](#page-3-0) In the LIF neuron, the membrane potential
    V (t) of the neuron evolves according to the differential equation as shown in
    [\(1\).](#page-2-2)


    <span id="page-2-2"></span>

    $$

    \tau\_m \frac{dV(t)}{dt} = -V(t) + R\_m I(t) \tag{1}

    $$


    In the above equation, τ<sup>m</sup> denotes the membrane time constant, V (t)
    is the membrane potential at time t, R<sup>m</sup> represents the membrane resistance,
    I(t) is the input current to the neuron. A neuron fires a spike when V (t) exceeds
    a threshold Vth, and the membrane potential is reset to Vreset. The implemented
    neuron emulates the LIF neuron model using discrete time steps (clock edges),
    updating the membrane potential at each step based on incoming spikes. However,
    using operational amplifiers (op-amps) results in significant area and power overheads,
    especially given the large number of neurons in neural models. Additionally, high
    sparsity is a characteristic of event-based neural models. To address these challenges,
    we propose utilizing a pair of storage cells (i.e., capacitance) within each neuron
    engine (A-NEURON), each referred to as a virtual neuron. This approach allows
    each A-NEURON to model multiple neurons using a single circuit for integration
    and firing, leading to substantial reductions in energy consumption and area requirements.
    It is important to note that neurons from a given model should be mapped onto
    an A-NEURON with lower overhead. We employ an ILP formulation to facilitate this
    mapping process, taking into account the overlap between Algorithm 1 Training,
    Pruning, Quantization, and Mapping Process


    - <span id="page-2-1"></span>1: Input: Dataset, Neural model, and Accelerator
    specifications

    - 2: Step 1: Train Network

    - 3: Train the network on a dataset.

    - 4: Step 2: Pruning and Quantization

    - 5: Apply pruning to reduce the number of synaptic connections in the network.

    - 6: Apply quantization to reduce the precision of synaptic weights.

    - 7: Ensure the network fits within the constraints of the hardware architecture.

    - 8: Step 3: Extract Weights and Spikes

    - 9: Extract the pruned and quantized weights from the trained network.

    - 10: Step 4: Generate the ILP Formulation

    - 11: Formulate the Integer Linear Programming (ILP) problem based on the extracted
    weights for each layer in different time steps.

    - 12: Define the objective function and constraints to mimic the Python-level
    spiking neural network behavior.

    - 13: Step 5: Mapping and Executing on the Hardware

    - 14: Store the ILP solutions as the config bits on the internal memories.

    - 15: Store the quantized parameters on the weight memory of A-SYNs.

    - 16: Using LIF Neurons to generate the output spikes for different layers.

    - 17: Determining the output class based on the output spikes.

    - 18: End


    neuron operations based on profiles generated through simulations conducted with
    SNNTorch [\[31\]](#page-6-30).


    In this structure, for each set of synaptic connections, the previously determined
    voltage of a neuron (which is stored in a dedicated capacitor within the A-NEURON)
    is restored and applied to the output of the op-amp. Following the integration
    phase, the resulting output voltage is temporarily stored in the capacitor when
    the input is accumulated into a voltage. This approach ensures that the stored
    voltage is preserved for subsequent processing cycles. To emulate the leaky behavior
    of the neuron, a portion of the stored voltage in the capacitors is discharged
    at each time step. The controller generates the command for this discharge process.


    ## *B.* A-SYN *Engine*


    C2C ladders can act as an analog multiplier, where one of its inputs is analog
    voltage (Vref ) while the other one is a multibit digital input (W) [\[32\]](#page-6-31).
    The output of this multiplier is an analog signal (Vout) which its value is obtained
    by


    $$V\_{out} = V\_{ref} \times \left(\sum\_{i=0}^{n-1} \left(W\_i \times 2^{i-n}\right)\right)
    \tag{2}$$


    The digital input of the C2C can be sourced directly from the memory array, allowing
    it to function as a computing engine


    ![](_page_3_Figure_0.jpeg)


    Fig. 2. A-NEURON architecture.


    located close to the memory array. This arrangement is advantageous for implementing
    charge-based, CMOS-compatible inmemory computing. In the proposed A-SYN engine,
    depicted in [Figure 3,](#page-3-1) a C2C ladder is utilized to model the synaptic
    connections, with the weights of these connections stored in SRAM memory. The
    bitlines of the SRAM are connected to the digital input of the C2C ladder. Additionally,
    Metal-Oxide-Metal (MOM) capacitors can implement the C2C ladder, significantly
    reducing the area overhead typically associated with C2C capacitance [\[33\]](#page-6-32).


    ![](_page_3_Figure_3.jpeg)


    <span id="page-3-1"></span>Fig. 3. The proposed A-SYN structure


    #### *C. Memory Construction*


    The memory structures for distributing the pulses of the received events to the
    proper A-SYN and A-NEURON engines in the MX-NEURACORE are shown in [Figure 4.](#page-4-0)
    In MEME, N<sup>i</sup> represents the index of a source neuron in the previous
    layer. This index is also the address for memory access to MEME2A. Note that the
    number of rows of MEME2A is higher than the number of neurons in the previous
    layer. Each row of MEME2A has two columns, i.e., B<sup>i</sup> and A<sup>i</sup>
    . B<sup>i</sup> indicates the number of rows in MEMS&N that are linked to N<sup>i</sup>
    , starting from the address pointed by A<sup>i</sup> . MEMS&N contains information
    about the synaptic connections and the destination neurons of the N<sup>i</sup>
    . This memory has three column groups (which are appropriately color-coded). The
    first column group contains M columns where M denotes the number of the A-NEURON
    engines. Each of these columns is a binary value (NIi) that indicates to which
    A-NEURON the received spike should be sent. The second column group contains the
    virtual neuron indices and identifies the virtual neuron in each A-NEURON to which
    the destination neuron is mapped. Thus the width of each column of this column
    type is log(N). Each column of the third and final column group shows the address
    of the synaptic weight in the weight memory of the A-SYN. Since a source neuron
    may be connected to more than M available A-NEURONs, its connections may be defined
    in a couple of rows, which, as mentioned before, is indicated by the B⋆.


    #### *D. Proposed ILP-based Mapping*


    <span id="page-3-0"></span>The objective of the ILP formulation is to allocate
    each neuron in the destination layer to a designated capacitor in a A-NEURON for
    each time step and layer. Once all the connections for a given neuron in the destination
    layer are processed, the capacitor tied to that neuron must be reassigned to another.
    This problem is NP-complete and can be modeled as an ILP problem. In the following
    paragraphs, we provide details of the ILP formulation for assigning synaptic connections
    and neurons to MX-NEURACORE and A-NEURON in a spiking neural network, ensuring
    compliance with several imposed constraints. Note that this ILP must be solved
    for each layer individually, requiring multiple ILPs to be solved at each time
    step. Below, we present the ILP formulation for a single layer.


    ILP is particularly valuable in this context because it allows us to define the
    objective function and constraints clearly and comprehensively. By employing the
    ILP formulation, we can minimize the number of unassigned neurons in the spiking
    network while ensuring efficient hardware utilization. The constraints guarantee
    that the capacity of each A-NEURON, as defined by the number of capacitors assigned
    to it, is not exceeded. Additionally, assigning neurons in the spiking network
    to capacitors within the NE is done in a manner that adheres to fan-out limitations
    and minimizes the communication overhead.


    The binary decision variable xi,j,k, as defined in [\(3\),](#page-3-2) is set
    to 1 if neuron i in the destination layer is assigned to the k th capacitor of
    j th A-NEURON and 0 otherwise. This variable is crucial to the optimization problem,
    as it reflects the assignment of neurons to capacitors in A-NEURONs.


    <span id="page-3-2"></span>

    $$x\_{i,j,k} = \begin{cases} 1, & \text{if neuron } i \text{ is assigned to the
    } k^{th} \\ \text{capactor of the } j^{th} \text{ A-NEURON} \\ 0, & \text{otherwise}
    \end{cases} \tag{3}$$


    The objective of the ILP is to minimize the [\(4\),](#page-3-3) which is the total
    number of unassigned neurons. This is formulated by minimizing the sum of (1 −
    xi,j,k) over all possible combinations. By doing so, the ILP ensures that as many
    neurons as possible are assigned to each A-NEURON, reducing the number of unassigned
    neurons. Here N<sup>1</sup> shows the number of neurons in the destination layer,
    M denotes the number of A-NEURON inside MX-NEURACORE, and N shows the number of
    capacitors inside each A-NEURON.


    <span id="page-3-3"></span>

    $$\text{Minimize}\quad\sum\_{i=1}^{N\_1}\sum\_{j=1}^{M}\sum\_{k=1}^{N}(1-x\_{i,j,k})\tag{4}$$


    ![](_page_4_Figure_0.jpeg)


    Fig. 4. The memory generated by the ILP problem to input to the system verilog


    The first constraint stated in [\(5\),](#page-4-1) called the Engine Capacity
    Constraint, ensures that the total number of neurons assigned to any A-NEURON
    does not exceed the number of capacitors in that A-NEURON, denoted by N. For each
    A-NEURON, the sum of xi,j,k over all neurons i and capacitors k must be less than
    or equal to N. This constraint guarantees that the capacity limitations of each
    A-NEURON are respected, preventing overloading. This is shown in [\(5\).](#page-4-1)


    <span id="page-4-1"></span>

    $$\sum\_{i=1}^{N\_1} \sum\_{k=1}^{N} x\_{i,j,k} \le N, \quad \forall j \in \{1,
    2, \dots, M\} \tag{5}$$


    The second constraint given in [\(6\),](#page-4-2) i.e., the Unique Engine Assignment
    Constraint, requires that each neuron in the destination layer is assigned to
    exactly one A-NEURON. This is achieved by ensuring that the sum of xi,j,k over
    all A-NEURONs and capacitors equals 1 for each neuron. This constraint ensures
    that each neuron has a unique assignment, preventing conflicts and ensuring that
    data routing is straightforward and efficient.


    <span id="page-4-2"></span>

    $$\sum\_{j=1}^{M} \sum\_{k=1}^{N} x\_{i,j,k} = 1, \quad \forall i \in \{1, 2,
    \dots, N\_1\} \tag{6}$$


    The third constraint provided in [\(7\),](#page-4-3) i.e., the Connection Constraint
    in [\(7\),](#page-4-3) applies to neurons in the source layer. It states that
    the sum of connections from each neuron in the source layer to neurons in the
    destination layer, across all A-NEURONs in a MX-NEURACORE, must not exceed the
    fan-out limit for that neuron. For each neuron m in the source layer, the sum
    of xi,j,k for all connected neurons i and all A-NEURONs and capacitors must be
    less than or equal to the predefined fan-out limit for that neuron. This constraint
    helps maintain the structural integrity of the network, ensuring that the connections
    do not exceed the maximum allowable fan-out. In this equation, N<sup>2</sup> denotes
    the number of neurons in the source layer, and S<sup>m</sup> is the set of connections
    starting from the source layer to i th neuron in the destination layer.


    <span id="page-4-3"></span>

    $$\sum\_{i \in S\_m} \sum\_{j=1}^M \sum\_{k=1}^N x\_{i,j,k} \le \text{fanout}\_m,
    \quad \forall m \in \{1, 2, \dots, N\_2\} \quad (7)$$


    Together, these constraints and the objective function create a robust framework
    for optimizing the assignment of neurons to A-NEURONs and capacitors in them in
    a way that is both efficient and adheres to hardware limitations.


    #### <span id="page-4-0"></span>IV. RESULTS AND DISCUSSION


    #### *A. Experimental Setup*


    In this study, we employed the SNNTorch framework, a Python-based library for
    building and training spiking neural networks, to execute our models on two neuromorphic
    datasets, N-MNIST [\[34\]](#page-6-33) and CIFAR10-DVS [\[35\]](#page-6-34). These
    datasets represent spike-based versions of the MNIST and CIFAR10 datasets, respectively,
    and are frequently used in neuromorphic computing research due to their suitability
    for testing SNN models.


    In this work, we have considered a multi-layer perception (MLP) neural model for
    both datasets. The network architectures were 200/100/40/10 and 1000/500/200/100/10,
    in the case of the N-MNIST and CIFAR10-DVS, respectively. Also, we designed two
    proposed accelerator models with 4 (Accel1) and 5 (Accel2) MX-NEURACOREs for executing
    the N-MNIST and CIFAR10-DVS. The total weight memory in each MX-NEURACORE of the
    Accel<sup>1</sup> (Accel2) was 400 KB (20 MB), and we have considered 10 (20)
    A-NEURON inside each MX-NEURACORE. Also, each A-NEURON consisted of 16 (32) virtual
    neuron.


    For both datasets, pruning and post-training quantization techniques were applied
    to optimize the networks while preserving accuracy. For the N-MNIST (CIFAR10-DVS)
    dataset, the network achieved an accuracy of 94.75% (65.38%) before pruning. After
    applying the unstructured L1 pruning and 8-bit quantization, the accuracy slightly
    dropped to 94.1% (65.03%). [Table I](#page-4-4) summarizes the details of the
    models and their training parameters.


    <span id="page-4-4"></span>TABLE I DETAILS OF THE MODELS AND THEIR TRAINING PARAMETERS


    | Attribute               | N-MNIST                             | CIFAR10-DVS                         |  |

    |-------------------------|-------------------------------------|-------------------------------------|--|

    | Number of Parameters    | 0.49 M                              | 33.4 M                              |  |

    | Number of Hidden Layers | 3 (200/100/40)                      | 4 (1000/500/200/100)                |  |

    | Output Neurons          | 10                                  | 10                                  |  |

    | Learning Rate           | 1e-3                                | 1e-3                                |  |

    | Epochs                  | 50                                  | 100                                 |  |

    | Pruning Technique       | L1 pruning                          | L1 pruning                          |  |

    | Quantization            | 8-bit post-training<br>quantization | 8-bit post-training<br>quantization
    |  |


    For the mapping process, the ILP was implemented in Python, and the PuLP package
    was utilized to solve the


    TABLE II COMPARISON OF OUR PROPOSED ACCELERATOR WITH PRIOR WORK BASED ON VARIOUS
    FEATURES


    <span id="page-5-3"></span>


    | Author                    | Neural Operations | TOPS/Watt | Bit Width | Technology
    | Evaluated Dataset                      | # Neurons |

    |---------------------------|-------------------|-----------|-----------|------------|----------------------------------------|-----------|

    | MENAGE (Accel1)           | Analog LIF        | 3.4       | 8         | 90nm       |
    N-MNIST                                | 40        |

    | MENAGE (Accel2)           | Analog LIF        | 12.1      | 8         | 90nm       |
    CIFAR10-DVS                            | 100       |

    | Liu et al. 2023 [29]      | Mixed Signal LIF  | 1.88      | 4         | 180nm      |
    MIT-BIH Arrhythmia                     | 102       |

    | Qi et al. 2024 [36]       | Mixed Signal LIF  | 0.67–5.4  | 8         | 55nm       |
    N/A                                    | 128-256   |

    | Zhang et al. 2024<br>[37] | Digital LIF       | 0.66      | 8-10      | 28nm       |
    N-MNIST, DVS-Gesture, N-TIDIGIT, SeNic | 522       |

    | Liu et al. 2024 [38]      | Digital LIF       | 0.26      | N/A       | 22nm       |
    N-MNIST, DVS-Gesture                   | N/A       |


    ILP problem. Also, the digital part of the accelerator was developed by SystemVerilog
    HDL, and its design parameters were extracted using the Synopsys Design Compiler
    tool. On the other hand, its analog parts have been defined by Spice scripts and
    simulated by Synopsys HSpice tool.


    #### *B. Results*


    The functionality of the A-NEURON is shown in [Figure 5.](#page-5-0) The output
    spike of the A-NEURON, which is the output of the comparator (second op-amp),
    and the output of the first op-amp in the neuron circuit, along with the input
    signal, is shown in this figure. The power consumption and delay for each A-NEURON
    are 97nW and 6.72 ns, respectively. Also, the whole MX-NEURACORE simulation shows
    that the operating frequency of the system is 103.2MHz, and Accel<sup>1</sup>
    (Accel2) could provide 3.4 (12.1) TOPS/W energy efficiency.


    ![](_page_5_Figure_5.jpeg)


    Fig. 5. Spice simulation of the designed A-NEURON circuit with input, output,
    and integration voltage


    [Figure 6](#page-5-1) and [Figure 7](#page-5-2) illustrates the average memory
    usage of MEMS&N, the highest memory-consuming component in this study, on N-MNIST
    and CIFAR10-DVS datasets using the proposed Accel<sup>1</sup> and Accel2, respectively,
    at various time steps during the processing of a single input image. The figures
    highlight that, due to the high sparsity of spikes in these datasets, average
    memory usage remains relatively low most of the time. However, there are instances
    where memory usage rises at specific time steps or in certain layers, particularly
    when a large number of spikes occur simultaneously, which the architecture efficiently
    manages. Notably, CIFAR10-DVS exhibits higher spike activity, leading to increased
    memory usage compared to N-MNIST. Finally, [Table II](#page-5-3) compares the
    features of our proposed neuromorphic accelerator with those of some fully programmable
    prior work. This table shows that our architecture is more power efficient compared
    to the previous work. Additionally, our approach employs substantially fewer neurons
    than other methods, even when handling a more complex dataset. This reduction
    in neuron count contributes to a significant decrease in the overall area, enhancing
    the efficiency of the design.


    ![](_page_5_Figure_9.jpeg)


    Fig. 6. The memory utilization of running N-MNIST on Accel<sup>1</sup> in different
    time steps.


    <span id="page-5-1"></span>![](_page_5_Figure_11.jpeg)


    <span id="page-5-0"></span>Fig. 7. The memory utilization of running CIFAR10-DVS
    on Accel<sup>2</sup> in different time steps.


    ### <span id="page-5-2"></span>V. CONCLUSION


    This work introduced a mixed-signal neuromorphic accelerator architecture that
    enhances the efficiency of event-based neural models using analog computing with
    C2C ladders and LIF for synapse and neuron emulation, respectively. By leveraging
    virtual neurons, the design improves resource utilization and power efficiency.
    The spike management inside the proposed MX-NEURACORE has been done through soft
    codes, which have been extracted during the model compilation. For efficient mapping
    of the neural model on the proposed accelerator, we have suggested an ILP formulation.
    The energy efficiency of the designed accelerator for executing the CIFAR10-DVS
    was 12.1 TOPS/W.


    #### REFERENCES


    - <span id="page-6-0"></span>[1] Carver Mead. Neuromorphic electronic systems.
    *Proceedings of the IEEE*, 78(10):1629–1636, 1990.

    - <span id="page-6-1"></span>[2] Wulfram Gerstner and Werner M Kistler. *Spiking
    neuron models: Single neurons, populations, plasticity*. Cambridge university
    press, 2002.

    - <span id="page-6-2"></span>[3] Wolfgang Maass. Networks of spiking neurons:
    the third generation of neural network models. *Neural networks*, 10(9):1659–1671,
    1997.

    - <span id="page-6-3"></span>[4] Michael Pfeiffer and Thomas Pfeil. Deep learning
    with spiking neurons: opportunities and challenges. *Frontiers in neuroscience*,
    12:409662, 2018.

    - <span id="page-6-4"></span>[5] Parsa Razmara, Tina Khezresmaeilzadeh, and B
    Keith Jenkins. Fever detection with infrared thermography: Enhancing accuracy
    through machine learning techniques. *arXiv preprint arXiv:2407.15302*, 2024.

    - <span id="page-6-5"></span>[6] Mohammad Erfan Sadeghi, Arash Fayyazi, Seyedarmin
    Azizi, and Massoud Pedram. Peano-vit: Power-efficient approximations of non-linearities
    in vision transformers. In *Proceedings of the 29th ACM/IEEE International Symposium
    on Low Power Electronics and Design*, pages 1–6, 2024.

    - <span id="page-6-6"></span>[7] Arya Fayyazi, Mehdi Kamal, and Massoud Pedram.
    Arco: Adaptive multi-agent reinforcement learning-based hardware/software cooptimization
    compiler for improved performance in dnn accelerator design. *arXiv preprint arXiv:2407.08192*,
    2024.

    - <span id="page-6-7"></span>[8] Negin Ashrafi, Armin Abdollahi, Greg Placencia,
    and Maryam Pishgar. Effect of a process mining based pre-processing step in prediction
    of the critical health outcomes. *arXiv preprint arXiv:2407.02821*, 2024.

    - <span id="page-6-8"></span>[9] Negin Ashrafi, Yiming Liu, Xin Xu, Yingqi Wang,
    Zhiyuan Zhao, and Maryam Pishgar. Deep learning model utilization for mortality
    prediction in mechanically ventilated icu patients. *Informatics in Medicine Unlocked*,
    49:101562, 2024.

    - <span id="page-6-9"></span>[10] Kaushik Roy, Akhilesh Jaiswal, and Priyadarshini
    Panda. Towards spike-based machine intelligence with neuromorphic computing. *Nature*,
    575(7784):607–617, 2019.

    - <span id="page-6-10"></span>[11] Anthony N Burkitt. A review of the integrate-and-fire
    neuron model: I. homogeneous synaptic input. *Biological cybernetics*, 95:1–19,
    2006.

    - <span id="page-6-11"></span>[12] Mike Davies, Narayan Srinivasa, Tsung-Han Lin,
    Gautham Chinya, Yongqiang Cao, Sri Harsha Choday, Georgios Dimou, Prasad Joshi,
    Nabil Imam, Shweta Jain, et al. Loihi: A neuromorphic manycore processor with
    on-chip learning. *Ieee Micro*, 38(1):82–99, 2018.

    - <span id="page-6-12"></span>[13] Giulia Pacchioni. Improving memristors'' reliability.
    *Nature Reviews Materials*, 7:594, 2022.

    - <span id="page-6-13"></span>[14] Huajun Liu, Badri Narayanan, Hua Zhou, and
    Dillon Fong. Kinetic monte carlo simulation analysis of the conductance drift
    in multilevel hfo2-based rram devices. *Nanoscale*, 2023.

    - <span id="page-6-14"></span>[15] Filipp Akopyan, Jun Sawada, Andrew Cassidy,
    Rodrigo Alvarez-Icaza, John Arthur, Paul Merolla, Nabil Imam, Yutaka Nakamura,
    Pallab Datta, Gi-Joon Nam, et al. Truenorth: Design and tool flow of a 65 mw 1
    million neuron programmable neurosynaptic chip. *IEEE transactions on computer-aided
    design of integrated circuits and systems*, 34(10):1537– 1557, 2015.

    - <span id="page-6-15"></span>[16] Can Li, Daniel Belkin, Yunning Li, Peng Yan,
    Miao Hu, Ning Ge, Hao Jiang, Eric Montgomery, Peng Lin, Zhongrui Wang, et al.
    Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.
    *Nature communications*, 9(1):2385, 2018.

    - <span id="page-6-16"></span>[17] Zhongrui Wang, Saumil Joshi, Sergey Savel''Ev,
    Wenhao Song, Rivu Midya, Yunning Li, Mingyi Rao, Peng Yan, Shiva Asapu, Ye Zhuo,
    et al. Fully memristive neural networks for pattern classification with unsupervised
    learning. *Nature Electronics*, 1(2):137–145, 2018.

    - <span id="page-6-17"></span>[18] Huihui Peng, Lin Gan, Xin Guo, HH Peng, L Gan,
    and X Guo. Memristor based spiking neural networks: Cooperative development of
    neural network architecture/algorithms and memristors. *Chip*, page 100093, 2024.

    - <span id="page-6-18"></span>[19] Johannes Schemmel, Sebastian Billaudelle, Philipp
    Dauer, and Johannes Weis. Accelerated analog neuromorphic computing. In *Analog
    Circuits for Machine Learning, Current/Voltage/Temperature Sensors, and Highspeed
    Communication: Advances in Analog Circuit Design 2021*, pages 83–102. Springer,
    2021.

    - <span id="page-6-19"></span>[20] Alexey N Mikhaylov, Evgeny G Gryaznov, Maria
    N Koryazhkina, Ilya A Bordanov, Sergey A Shchanikov, Oleg A Telminov, and Victor
    B Kazantsev. Neuromorphic computing based on cmos-integrated memristive arrays:
    current state and perspectives. *Supercomputing Frontiers and Innovations*, 10(2):77–103,
    2023.

    - <span id="page-6-20"></span>[21] Yesheng Li and Kah-Wee Ang. Hardware implementation
    of neuromorphic computing using large-scale memristor crossbar arrays. *Advanced
    Intelligent Systems*, 3(1):2000137, 2021.

    - <span id="page-6-21"></span>[22] Ying Liu, Zhiyuan Chen, Wentao Zhao, Tianhao
    Zhao, Tianyu Jia, Zhixuan Wang, Ru Huang, Le Ye, and Yufei Ma. Sparsity-aware
    inmemory neuromorphic computing unit with configurable topology of hybrid spiking
    and artificial neural network. *IEEE Transactions on Circuits and Systems I: Regular
    Papers*, 2024.

    - <span id="page-6-22"></span>[23] Minsuk Koo, Gopalakrishnan Srinivasan, Yong
    Shim, and Kaushik Roy. Sbsnn: Stochastic-bits enabled binary spiking neural network
    with on-chip learning for energy efficient neuromorphic computing at the edge.
    *IEEE Transactions on Circuits and Systems I: Regular Papers*, 67(8):2546– 2555,
    2020.

    - <span id="page-6-23"></span>[24] Weier Wan, Rajkumar Kubendran, Clemens Schaefer,
    Sukru Burc Eryilmaz, Wenqiang Zhang, Dabin Wu, Stephen Deiss, Priyanka Raina,
    He Qian, Bin Gao, et al. A compute-in-memory chip based on resistive random-access
    memory. *Nature*, 608(7923):504–512, 2022.

    - <span id="page-6-24"></span>[25] Je-Min Hung, Cheng-Xin Xue, Hui-Yao Kao, Yen-Hsiang
    Huang, Fu-Chun Chang, Sheng-Po Huang, Ta-Wei Liu, Chuan-Jia Jhang, Chin-I Su,
    Win-San Khwa, et al. A four-megabit compute-in-memory macro with eight-bit precision
    based on cmos and resistive random-access memory for ai edge devices. *Nature
    Electronics*, 4(12):921–930, 2021.

    - <span id="page-6-25"></span>[26] Reiji Mochida, Kazuyuki Kouno, Yuriko Hayata,
    Masayoshi Nakayama, Takashi Ono, Hitoshi Suwa, Ryutaro Yasuhara, Koji Katayama,
    Takumi Mikawa, and Yasushi Gohou. A 4m synapses integrated analog reram based
    66.5 tops/w neural-network processor with cell current controlled writing and
    flexible network architecture. In *2018 IEEE Symposium on VLSI Technology*, pages
    175–176. IEEE, 2018.

    - <span id="page-6-26"></span>[27] Qi Liu, Bin Gao, Peng Yao, Dong Wu, Junren
    Chen, Yachuan Pang, Wenqiang Zhang, Yan Liao, Cheng-Xin Xue, Wei-Hao Chen, et
    al. 33.2 a fully integrated analog reram based 78.4 tops/w compute-in-memory chip
    with fully parallel mac computing. In *2020 IEEE International Solid-State Circuits
    Conference-(ISSCC)*, pages 500–502. IEEE, 2020.

    - <span id="page-6-27"></span>[28] Seiji Uenohara and Kazuyuki Aihara. A 18.7
    tops/w mixed-signal spiking neural network processor with 8-bit synaptic weight
    on-chip learning that operates in the continuous-time domain. *IEEE Access*, 10:48338–48348,
    2022.

    - <span id="page-6-28"></span>[29] Ying Liu, Yufei Ma, Wei He, Zhixuan Wang, Linxiao
    Shen, Jiayoon Ru, Ru Huang, and Le Ye. An 82-nw 0.53-pj/sop clock-free spiking
    neural network with 40-µs latency for aiot wake-up functions using a multilevel-event-driven
    bionic architecture and computing-in-memory technique. *IEEE Transactions on Circuits
    and Systems I: Regular Papers*, 70(8):3075–3088, 2023.

    - <span id="page-6-29"></span>[30] A Valentian, F Rummens, E Vianello, T Mesquida,
    C Lecat-Mathieu de Boissac, O Bichler, and C Reita. Fully integrated spiking neural
    network with analog neurons and rram synapses. In *2019 IEEE International Electron
    Devices Meeting (IEDM)*, pages 14–3. IEEE, 2019.

    - <span id="page-6-30"></span>[31] Jason K Eshraghian, Benjamin R Ward, Emre O
    Neftci, Leo Wang, Yaocheng Chua, Changhao Wen, Xunzhao Xiang, Daniel Abbott, Benjamin
    Benz, Raymond Quek, and Tara J Hamilton. Training spiking neural networks with
    surrogate gradients: Theory, methods, and applications. *IEEE Transactions on
    Neural Networks and Learning Systems*, 33(9):3059–3071, 2021.

    - <span id="page-6-31"></span>[32] Behzad Razavi. The r-2r and c-2c ladders [a
    circuit for all seasons]. *IEEE Solid-State Circuits Magazine*, 11(3):10–15, 2019.

    - <span id="page-6-32"></span>[33] Hechen Wang, Renzhi Liu, Richard Dorrance,
    Deepak Dasalukunte, Dan Lake, and Brent Carlton. A charge domain sram compute-in-memory
    macro with c-2c ladder-based 8-bit mac unit in 22-nm finfet process for edge inference.
    *IEEE Journal of Solid-State Circuits*, pages 1–14, 2023.

    - <span id="page-6-33"></span>[34] Garrick Orchard, Ajinkya Jayawant, Gregory
    K Cohen, and Nitish Thakor. Converting static image datasets to spiking neuromorphic
    datasets using saccades. *Frontiers in neuroscience*, 9:437, 2015.

    - <span id="page-6-34"></span>[35] Hongmin Li, Hanchao Liu, Xiangyang Ji, Guoqi
    Li, and Luping Shi. Cifar10-dvs: an event-stream dataset for object classification.
    *Frontiers in neuroscience*, 11:309, 2017.

    - <span id="page-6-35"></span>[36] Xiang''ao Qi, Xiangting Li, Yuqing Lou, Yongfu
    Li, Guoxing Wang, Kea-Tiong Tang, and Jian Zhao. A 0.67-to-5.4 tsops/w spiking
    neural network accelerator with 128/256 reconfigurable neurons and asynchronous
    fully connected synapses. *IEEE Journal of Solid-State Circuits*, 2024.

    - <span id="page-7-0"></span>[37] Jilin Zhang, Dexuan Huo, Jian Zhang, Chunqi
    Qian, Qi Liu, Liyang Pan, Zhihua Wang, Ning Qiao, Kea-Tiong Tang, and Hong Chen.
    Anpi: A 28-nm 1.5-pj/sop asynchronous spiking neural network processor enabling
    sub-0.1-µj/sample on-chip learning for edge-ai applications. *IEEE Journal of
    Solid-State Circuits*, 2024.

    - <span id="page-7-1"></span>[38] Ying Liu, Yufei Ma, Ninghui Shang, Tianhao Zhao,
    Peiyu Chen, Meng Wu, Jiayoon Ru, Tianyu Jia, Le Ye, Zhixuan Wang, et al. 30.2
    a 22nm 0.26 nw/synapse spike-driven spiking neural network processing unit using
    time-step-first dataflow and sparsity-adaptive in-memory computing. In *2024 IEEE
    International Solid-State Circuits Conference (ISSCC)*, volume 67, pages 484–486.
    IEEE, 2024.'
- title: "GUST: Graph Edge-Coloring Utilization for Accelerating Sparse Matrix\n \
    \ Vector Multiplication"
  abstract: 'Sparse matrix-vector multiplication (SpMV) plays a vital role in various

    scientific and engineering fields, from scientific computing to machine

    learning. Traditional general-purpose processors often fall short of their peak

    performance with sparse data, leading to the development of domain-specific

    architectures to enhance SpMV. Yet, these specialized approaches, whether

    tailored explicitly for SpMV or adapted from matrix-matrix multiplication

    accelerators, still face challenges in fully utilizing hardware resources as a

    result of sparsity. To tackle this problem, we introduce GUST, a

    hardware/software co-design, the key insight of which lies in separating

    multipliers and adders in the hardware, thereby enabling resource sharing

    across multiple rows and columns, leading to efficient hardware utilization and

    ameliorating negative performance impacts from sparsity. Resource sharing,

    however, can lead to collisions, a problem we address through a specially

    devised edge-coloring scheduling algorithm. Our comparisons with various prior

    domain specific architectures using real-world datasets shows the effectiveness

    of GUST, with an average hardware utilization of $33.67\%$.'
  url: http://arxiv.org/abs/2410.09106v1
  keywords: ''
  document: "# GUST: Graph Edge-Coloring Utilization for Accelerating Sparse Matrix\
    \ Vector Multiplication\n\nArmin Gerami agerami@umd.edu Perceptual Interfaces\
    \ and Reality Laboratory University of Maryland, College Park College Park, MD,\
    \ USA\n\n# Abstract\n\nSparse matrix-vector multiplication (SpMV) plays a vital\
    \ role in various scientifc and engineering felds, from scientifc computing to\
    \ machine learning. Traditional general-purpose processors often fall short of\
    \ their peak performance with sparse data, leading to the development of domain-specifc\
    \ architectures to enhance SpMV. Yet, these specialized approaches, whether tailored\
    \ explicitly for SpMV or adapted from matrix-matrix multiplication accelerators,\
    \ still face challenges in fully utilizing hardware resources as a result of sparsity.\
    \ To tackle this problem, we introduce GUST, a hardware/software co-design, the\
    \ key insight of which lies in separating multipliers and adders in the hardware,\
    \ thereby enabling resource sharing across multiple rows and columns, leading\
    \ to efcient hardware utilization and ameliorating negative performance impacts\
    \ from sparsity. Resource sharing, however, can lead to collisions, a problem\
    \ we address through a specially devised edge-coloring scheduling algorithm. Our\
    \ comparisons with various prior domain specifc architectures using real-world\
    \ datasets shows the efectiveness of GUST, with an average hardware utilization\
    \ of 33.67%. We further evaluate GUST by comparing SpMV execution time and energy\
    \ consumption of length-256 and -87 GUST with length-256 1-dimensional systolic\
    \ array (1D), achieving an average speedup of 411× and 108×, and energy efciency\
    \ improvement of 137× and 148×, respectively. To asses the implementation aspect,\
    \ we compare resource consumption of GUST with 1D as a baseline through FPGA synthesis.\
    \ Length-256 GUST uses the same number of arithmetic units as length-256 1D, while\
    \ length-87 GUST uses considerably less. We also compare GUST with Serpens, a\
    \ state-of-the-art FPGA-based SpMV accelerator, with GUST achieving lower\n\n\
    Bahar Asgari bahar@umd.edu University of Maryland, College Park College Park,\
    \ MD, USA\n\nexecution time on seven out of nine matrices and lower energy consumption\
    \ on four.\n\n# 1 Introduction\n\nData in the form of sparse matrices is ubiquitous\
    \ in many scientific and engineering applications, such as electronic structure\
    \ simulation, graph analysis, computational fluid dynamics, recommendation models,\
    \ and machine learning. Most of the problems in these fields, from solving the\
    \ Schrödinger equation to calculating the kernel function of a support vector\
    \ machine, can be solved through sparse matrix-vector multiplication (SpMV). However,\
    \ without employing techniques such as sparsity-aware hardware or preprocessing\
    \ sparse data to make it hardware-compatible, zero elements would consume the\
    \ same amount of processing time as nonzero (NZ) elements, resulting in poor hardware\
    \ utilization and high execution time. To clarify, we define hardware utilization\
    \ as the ratio of average number of arithmetic units performing NZ operations\
    \ in each cycle to total number of arithmetic units.\n\nPrior work have followed\
    \ approaches such as taking advantage of the high data-reuse rate of 1-dimensional\
    \ (1D) [22] and 2-dimensional systolic arrays (2D) [10, 11, 33], efficient handling\
    \ of sparse data structures of balanced adder trees (AT) [1, 3, 9, 25, 27], mapping\
    \ the hardware to a custom calculational kernel [8, 23, 31], or employing a hardware/software\
    \ co-design that involves software-based techniques such as preprocessing, prefetching,\
    \ and novel storage formats working in tandem with custom hardware design [2,\
    \ 8, 12, 16, 21, 29–32, 36] to accelerate sparse matrix multiplication. In simple\
    \ terms, all the mentioned work deploy various mechanism to reduce the number\
    \ of zero elements in the input stream. Although they successfully do so and improve\
    \ their desired performance metric, their hardware utilization is still far from\
    \ ideal leaving room for improvement.\n\nIn this paper, we introduce GUST, a hardware/software\
    \ co-design SpMV accelerator developed with the purpose of maximizing hardware\
    \ utilization. Specifcally, g iven a number of arithmetic units, we aim to minimize\
    \ execution time. To achieve this goal, we design GUST on the principle that sharing\
    \ hardware resources among multiple rows and columns would lead to reducing the\
    \ stalls introduced by irregular sparsity patterns, increasing hardware utilization.\
    \ The hardware of GUST consists of a number of multipliers connected to a number\
    \ of adders through a crossbar connector, which enables resource sharing through\
    \ separating partial product and partial sum calculations. Resource sharing, however,\
    \ makes SpMV prone to collisions. To prevent collisions, we propose a scheduling\
    \ based on bipartitegraph Edge-Coloring, an example of using combinatorics to\
    \ solve computer architecture problems. For demonstration purposes, we adapt and\
    \ evaluate GUST with an FPGA as the target hardware, and provide comparison with\
    \ other FPGA-based designs. Similar to prior FPGA-based SpMV accelerators [1,\
    \ 10–12, 29–31] we perform preprocessing on the matrix, which is our scheduling\
    \ step.\n\nTo justify the rationale behind our design, we frst introduce well-known\
    \ architectures for matrix multiplication, systolic arrays and adder trees, and\
    \ recent work repurposing them for SpMV in Section 2, and comprehensively explains\
    \ GUST in Section 3. Then, based on the experimental setup summarized in Section\
    \ 4, we compare GUST's performance with the mentioned work with a focus on hardware\
    \ utilization in Section 5. We further evaluate GUST by comparing the end-to-end\
    \ SpMV wall-clock time and energy consumption against Serpens[29], a state-of-the-art\
    \ FPGA-based SpMV accelerator. For our evaluation setup, we use a combination\
    \ of synthetic and real-world matrices, covering a wide range of matrix densities,\
    \ dimensions and structures, and use Alveo U280 FPGA for synthesis, which we explain\
    \ in more detail in Section 4. Section 6 briefly mentions some of the relevant\
    \ work, and the paper concludes with Section 7. In summary, we achieve the following\
    \ results:\n\n- On real-world sparse matrices, length-256 GUST achieves average\
    \ hardware utilization of 33.67%.\n- Taking length-256 1D systolic arrays as baseline,\
    \ length-256 GUST achieves a speedup and energy efciency gain of 411× and 137×,\
    \ respectively. Moreover, length-87 GUST has a considerably less hardware and\
    \ energy consumption than length-256 1D, while achieving average speedup of 108×\
    \ and energy efciency gain of 148×.\n- Compared to Serpens [29], among the nine\
    \ real-world matrices we evaluated, GUST achieved lower execution time for seven,\
    \ and lower energy for four of the matrices.\n\n# 2 Previous Works & Challenges\n\
    \nThis section discusses four approaches for performing SpMV that fall under the\
    \ category of systolic arrays and adder trees: Flex-TPU [10], baseline 1D [17],\
    \ baseline AT [4], and Fafnir [1]. We analyze their shortcomings to uncover some\
    \ of the challenges governing efcient SpMV. Table 1 shows the qualities of these\
    \ designs as well as GUST.\n\n## 2.1 Systolic Arrays\n\nFirst introduced in 1978\
    \ [17], systolic arrays consist of a grid of processing elements (PEs) that execute\
    \ operations on data in a highly parallelized and pipelined fashion. The PEs are\
    \ linked to one another, creating a flow of data as it moves from one PE to the\
    \ next. The purpose of systolic arrays is to leverage spatial locality in matrix\
    \ multiplications. Spatial locality refers to the tendency for utilized data to\
    \ be located in close proximity in memory. As a result, data is forwarded between\
    \ PEs without accessing memory multiple times.\n\n2D Systolic Arrays– Given 2D\
    \ systolic arrays are available for the purpose of matrix-matrix multiplication\
    \ [14, 15], recent studies, Sparse TPU [11] and Flex-TPU [10], suggest repurposing\
    \ a 2D TPU to accelerate SpMV while reducing memory access. In the case of Flex-TPU\
    \ [10], the most recent 2D used for SpMV, each PE is either a Normal PE or a Separator.\
    \ Unlike baseline 2D, only the NZ matrix values are mapped to PEs, and they are\
    \ stored in the Normal PEs. Since we only store NZ elements, matrix elements from\
    \ diferent rows may be mapped in the same row of the systolic array; therefore,\
    \ we use Separator PEs to keep track of the matrix rows. Calculating SpMV, as\
    \ shown in Figure 1(a), involves a left-to-right loading of matrix elements and\
    \ separator signals during the reconfguration process. These separator signals\
    \ identify Processing Elements (PEs) that function as separators. Then, the input\
    \ vector elements are streamed top-to-bottom and each Normal PE checks the index\
    \ of the vector element against the column index of its stored matrix element.\
    \ If there is a match, it performs multiplication and forwards the result to its\
    \ right neighbor. The Separator PEs accumulate the partial results received from\
    \ their left-side neighbor and store the result (calculation process). Finally,\
    \ the dump signal enters through the top-left PE and flows through the PEs. The\
    \ Separator PEs dump their stored value once they receive the dump signal (dump\
    \ process). If the matrix size, × is bigger than those of Flex-TPU, × , the SpMV\
    \ is calculated through partitioning, where each partition requires approximately\
    \ 3 × cycles ( for reconfguration, for calculation, and for dump) to complete.\
    \ Given the overhead incurred by PEs that are idle for non-matching indices, and\
    \ those of reconfguration and dump processes, Flex-TPU is of low hardware utilization.\n\
    \n1D Systolic Array– As shown in Figure 1(b), to calculate matrix-vector multiplication,\
    \ the elements of the input matrix and the input vector enter the PEs top-to-bottom\
    \ and left-to-right, respectively. At each cycle, each PE multiplies its entries,\
    \ accumulates the result to its stored value, stores the result of the sum, forwards\
    \ its left-side entry to its rightside PE, and dumps its stored value upon receiving\
    \ the dump\n\n![](_page_2_Figure_0.jpeg)\n\nFigure 1. Mechanism of SpMV in Prior\
    \ Work– (a) Flex-TPU: At each iteration, frst the matrix values and Separators\
    \ () are loaded, then the vector values flow through the array, and then the dump\
    \ signal (); (b) 1D; (c) AT; (d) Fafnir: Each column of matrix enters through\
    \ one leaf.\n\n|  | Table 1. Qualities of related work and GUST for calculating\
    \ an SpMV for an � × � matrix. Each design has a length of �. |  |\n|--|-------------------------------------------------------------------------------------------------------------------------|--|\n\
    |--|-------------------------------------------------------------------------------------------------------------------------|--|\n\
    \n| Design<br>Criteria          | Flex-TPU [10]                            | 1D\
    \ Systolic Array [17] | Balanced<br>Adder Tree [4]                           \
    \                   | Fafnir [1]                                             \
    \                 | GUST (our work)                                          \
    \                           |\n|-----------------------------|------------------------------------------|------------------------|-------------------------------------------------------------------------|-------------------------------------------------------------------------|-------------------------------------------------------------------------------------|\n\
    | Hardware                    | grid of � × � PEs<br>(2D systolic array) | strip\
    \ of � PEs         | binary tree with<br>� multiplier<br>leaves and � − 1<br>reduction\
    \ nodes | binary tree with<br>� multiplier leaves<br>and � − 1 reduction<br>nodes\
    \ | � multipliers and<br>� adders connected<br>by a reconfgurable<br>crossbar\
    \ connector |\n| Execution Time<br>(#cycles) | #NZ elem.<br>∼3<br>�          \
    \           | � × �<br>+ � + 1<br>�  | � × �<br>+ ���(�) + 1<br>�            \
    \                                  | at least<br>���(�)<br>#NZ elem.×<br>4   \
    \                                | #NZ elem.<br>3 ×<br>*<br>�                \
    \                                          |\n| Hardware Utilization        |\
    \ 1.45%*                                   | 0.08%*                 | 0.08%* \
    \                                                                 | 4.67%*   \
    \                                                               | 33.67%*    \
    \                                                                         |\n\n\
    \\*The values reported are based on the geometric average of empirical results\
    \ for real-world matrices.\n\nsignal. Given that each matrix element requires\
    \ one cycle of processing on a PE and that the majority of the input stream for\
    \ SpMV are zeros, 1D has poor hardware utilization.\n\n## 2.2 Adder Trees\n\n\
    Balanced Adder Tree– An AT of length- has multipliers in the frst layer, /2 adders\
    \ in the second, /4 in the third, and so on (a total of − 1 adders) [4]. A pair\
    \ of multipliers or adders in each layer are connected to an adder in the layer\
    \ below. As illustrated in Figure 1(c) each multiplier receives two inputs, multiplies\
    \ them and forwards the result to the adder it is connected to. Similarly, each\
    \ adder accumulates the two inputs and forwards the result to the layer below.\
    \ To calculate matrix-vector multiplication, at each iteration, we map a row of\
    \ the input matrix and the input vector to the inputs of the multipliers and calculate\
    \ the dot product. Since each element occupies a multiplier and adder for one\
    \ cycle, and most input elements are zeros, AT demonstrates poor hardware utilization\
    \ in SpMV.\n\nFafnir– Fafnir [1] is a tree-like structure of PEs that uses LIL\
    \ format and can be used to accelerate sparse gathering and SpMV. A Fafnir with\
    \ length- would be a binary tree with leaf nodes, and a depth of (), totaling\
    \ in 2 −1 PEs. To calculate SpMV, as shown in Figure 1(d), each leaf node receives\
    \ an input matrix element, the corresponding row index, and the input vector element\
    \ it should be multiplied with. The leaf node multiplies matrix and vector elements,\
    \ forwarding the partial product and row index to its parent. Each nonleaf node\
    \ receives partial products and indices potentially\n\nfrom diferent rows. It\
    \ checks for index matches, accumulating and reducing values accordingly - only\
    \ forwarding the accumulated result instead of all matching values. The node then\
    \ forwards the reduced results, unreduced inputs, and corresponding indices. In\
    \ the worst case, the root node performs /2 accumulations per cycle. To prevent\
    \ stalls, it has/2 adders. Similarly, the (() −1)-th layer nodes each have /4\
    \ adders, (() − 2)-th layer /8, and so on. This makes the maximum attainable hardware\
    \ utilization of Fafnir to be 4/(), but in practice, it is signifcantly lower.\n\
    \n## 2.3 Challenges\n\nWe explained why each design achieves low hardware utilization,\
    \ and we confrm this empirically in Section 5. A key factor contributing to these\
    \ limitations lies in the conventional static resource allocation. For example,\
    \ at each iteration in 1D, each PE is allocated to a matrix row, and cannot be\
    \ utilized by another row, even if it is idle. Similarly in Flex-TPU, each PE\
    \ is allocated to an element. In AT/Fafnir, each multiplier is allocated to a\
    \ single row/column at each iteration, and each layer 2 adder allocated to 2 rows/columns,\
    \ layer 3 to 4, and so on. Having a predefned resource allocation limits our ability\
    \ to utilize the hardware based on the sparsity of each row/column.\n\n# 3 GUST\n\
    \nGUST is a hardware/software co-design with the main goal of achieving a high\
    \ hardware utilization for SpMV. The hardware of GUST facilitates this by enabling\
    \ resource sharing,\n\n![](_page_3_Figure_0.jpeg)\n\nFigure 2. Overview of GUST\
    \ Architecture– The Bufer Filler retrieves the matrix in the scheduled format\
    \ from ofchip memory, and flls four sets of bufers: the matrix and vector elements,\
    \ the row indices, and the dump signals.\n\nwhile the software schedules, load\
    \ balances, and maps the SpMV inputs to the hardware inputs.\n\n## 3.1 Key Insight\n\
    \nHigh hardware utilization is associated with a dense input stream; the denser\
    \ the input stream the less stalls/zero operations. Therefore, our goal is to\
    \ reshape the input matrix into a dense stream. Doing so will result in changing\
    \ the order of the matrix elements. As a result, we need to make sure that each\
    \ matrix element is multiplied by the correct vector element, and that the resulting\
    \ partial product is accumulated with the correct partial sum. One way to do so\
    \ is to separate the multipliers and adders. We then shape the multipliers' input\
    \ stream in a way so that correct values get multiplied together, and use a crossbar\
    \ connector between the multipliers and adders to control the adders' input stream,\
    \ so that correct partial products get accumulated with correct partial sums.\
    \ Thus the hardware utilization is increased through having a dense input stream,\
    \ and arithmetic units are shared among multiple rows and columns. As explained\
    \ with detail in Section 3. 3, resource sharing is prone to collisions. In order\
    \ to eliminate collisions, we propose our bipartite-graph Edge-Coloring scheduler.\
    \ We further improve the performance of GUST with a simple sort-based load balancing.\n\
    \n## 3.2 Hardware & Core Mechanisms\n\nHardware– As illustrated in Figure 2, the\
    \ hardware of GUST consists of three layers. A length GUST has multipliers, adders,\
    \ and a crossbar connector:\n\n-Multipliers: Each multiplier has two inputs; one\
    \ dedicated to the input matrix elements, and one to the input vector. In each\
    \ cycle, the multipliers multiply their inputs and forward the partial products\
    \ to the connector. The frst input of the -th multiplier is an element from the\
    \ -th column of the input matrix, the second input the -th element of the input\
    \ vector, and the output the -th input of the connector.\n\n-Crossbar Connector:\
    \ The connector has two sets of inputs, and one set of outputs. The frst inputs\
    \ are the partial products that are propagated from the multipliers. The outputs\
    \ of the connector are also partial products, connected to adders. The second\
    \ inputs are indices that range from 1 to , and regulate which partial product\
    \ entry is forwarded to which adder. Specifcally, the -th entry (a partial product)\
    \ is forwarded to the value of the ( + )-th input (an index).\n\n-Adders: Each\
    \ adder has two inputs; one forwarded from connector (a partial product), and\
    \ one dedicated to the dump signal. In each cycle, the adders accumulate the input\
    \ from the connector with their stored value, and store the result. The input\
    \ bufers are populated with the help of the Bufer Filler element, which utilizes\
    \ an on-chip memory. The process works as follows: First, the vector is forwarded\
    \ from of-chip memory to the Bufer Filler. Then, the bufers are flled in a two-step\
    \ pipelined fashion. Step 1: A partition of the scheduled matrix is forwarded\
    \ from of-chip memory to the Bufer Filler's on-chip memory. Step 2: The Bufer\
    \ Filler flls the input bufers using the matrix and vector elements stored in\
    \ its on-chip memory. The Bufer Filler's existence is necessary in order to facilitate\
    \ this pipelined process.\n\nData Flow– To calculate an element of the output\
    \ vector, = Σ , each partial product, , is calculated as follows. and enter the\
    \ -th multiplier. The multiplier calculates and forwards it to the -th input of\
    \ the connector. The row index value, , enters the connector as the ( +)-th input.\
    \ Based on the index value, , the connector forwards to the -th adder. The adder\
    \ accumulates the partial product to its stored value. The dumped value by the\
    \ -th adder will be = Σ, ≠0 . Each of the four inputs (matrix and vector elements,\
    \ row indices, and dump signal) are connected through an individual FIFO bufer.\n\
    \nIf the dimension of the input matrix, × , is bigger than the length of GUST,\
    \ , SpMV is done through windowing, meaning at each iteration, a set of rows enter\
    \ the multipliers, and once all of the NZ elements in the rows have gone through\
    \ the accelerator, the adders dump their values, and the next rows enter the multipliers\
    \ (i.e., matrix enters set-ofrows by set-of-rows). In addition, since , number\
    \ of columns, is bigger than , matrix elements enter the multipliers with respect\
    \ to their column index , where is the modulo operator. That is, the matrix elements\
    \ in the -th, ( + )-th, ( <sup>+</sup> <sup>2</sup>)-th, ..., ( + ( − 1))-th columns\
    \ enter through the -th multiplier, where 1 ≤ ≤ (we refer to segments of the -th,\
    \ ( + )-th, ... columns within each window as column segments). Through windowing,\
    \ resources are shared across both multiple rows and multiple columns. Figure\
    \ 3 provides a step-by-step example for a length-4 GUST.\n\n![](_page_4_Figure_0.jpeg)\n\
    \nFigure 3. Hardware Mechanism of GUST– Time step 1 shows how the matrix and vector\
    \ elements from the example in Figure 1 enter the hardware. The dashed-lines in\
    \ time steps 2 and 3 display which input of connector is forwarded to which output.\
    \ By receiving dump signal in timestep 4, adders dump their stored value.\n\n\
    ### 3.3 Scheduling and Load Balancing\n\nThe example in Figure 3 was handpicked\
    \ in the sense that in no cycle were there any two matrix elements from the same\
    \ row entering the multipliers, or equivalently, no two connector inputs were\
    \ destined for the same connector output. In general, this is not the case and\
    \ multiple matrix elements from the same row may enter in the same cycle and cause\
    \ a collision in the outputs of the connector, losing the partial products. The\
    \ naive method of preventing collisions is to simply not forward the values from\
    \ the bufers whenever there is a collision and add a stall. However, this method\
    \ introduces a high number of stalls, resulting in signifcant inefciency. In fact,\
    \ empirical results demonstrate that for 16384 × 16384 matrices with uniform distribution,\
    \ GUST using naive scheduling has a performance worse than 1D for densities exceeding\
    \ 0.008. As a remedy, we propose our scheduling algorithm, which is based on the\
    \ notion of Edge-Coloring a bipartite-graph.\n\nEdge-Coloring a bipartite-graph–\
    \ First, let us showcase a simple problem to see how Edge-Coloring could be of\
    \ any avail. As illustrated in Figure 4, assume we have an × table, and its cells\
    \ can either be empty or take a value. We want to fll the table with the set of\
    \ values = 1, 2, ..., , where = ,, ,+1, ..., , , and 0 ≤ ≤ ≤ . The values in each\
    \ are unique within but not necessarily within . We want to fll the -th column\
    \ of the table with in a way that each value of a cell would be unique in its\
    \ row. To do so, assume we have a bipartite-graph where the left-side vertices\
    \ represent the columns of the table, the right-side vertices the values, and\
    \ the edge colors the rows. Therefore, if an edge connects the -th left-side vertex\
    \ to the -th rightside vertex and with the color of , the cell in the -th row\
    \ and -th column has the value of . By coloring the edges in a way that no two\
    \ adjacent edges would have the same color, we fll the table and satisfy the mentioned\
    \ conditions. According to Vizing's theorem [34], the minimum number of colors\
    \ needed is the maximum degree of the graph, which\n\n![](_page_4_Figure_5.jpeg)\n\
    \nFigure 4. Edge Coloring– Filling the table such that flls the -th column and\
    \ no repeated values exist in any row. Since each vertex has at most one color\
    \ connected to it, there are no repeating values in any given column or row.\n\
    \nin our case would be { {| |}, }, where is the the maximum occurrences of a value\
    \ in .\n\nApplying Edge-Coloring a Bipartite-Graph to GUST– The collision issue\
    \ we are facing is analogous to the showcased problem, and we apply a similar\
    \ approach to address it. Here, matrix elements enter multipliers based on their\
    \ column index, and are directed towards adders based on their row index. Since\
    \ the column and row indices of the matrix elements are predetermined and out\
    \ of our control, we represent them as the right- and left-side vertices of a\
    \ bipartite-graph. An edge between the -th left-side vertex and the -th right-side\
    \ vertex represents the matrix element from row and column . Since the color of\
    \ edges can be deliberately assigned, we use colors as our scheduling metric.\
    \ Only one matrix element can enter a multiplier at each cycle, therefore, the\
    \ scheduling must be in a manner that no two elements from the same column end\
    \ up with the same time slot, meaning no two edges connected to a right-side vertex\
    \ can have the same color. Similarly, no two elements from the same row can enter\
    \ the adder at the same time slot, meaning no two edges connected to a left-side\
    \ vertex can have the same color. Our problem then becomes, given a bipartite-graph,\
    \ where the left-side vertices represent the matrix rows, the right-side vertices\
    \ the matrix columns, and the edge colors the scheduled position in the input\
    \ bufer,\n\n![](_page_5_Figure_0.jpeg)\n\nFigure 5. Edge Coloring in GUST– (a)\
    \ A sparse matrix operand for an SpMV. (b) The bipartite-graph representation\
    \ of the sparse matrix, in which left/right nodes correspond to rows/columns (adders/multipliers).\
    \ The top graph (red background) represents the frst three rows of the matrix,\
    \ and the bottom one (yellow background) the last three. Some of the edges have\
    \ been colored to illustrate their corresponding matrix element for clarifcation.\
    \ (c) The Edge-Color-scheduled graphs where the edge color corresponds to the\
    \ position of the matrix element in the FIFO bufer. No two edges with the same\
    \ color are connected to the same vertex. (d) The scheduled matrix elements entering\
    \ the multipliers of GUST.\n\nfnd a coloring such that no vertex has two edges\
    \ with the same color connected to it. This ensures that no two elements from\
    \ the same row would occupy the same position in the input bufers, eliminating\
    \ all collisions. In terms of relevance to the hardware, since each adder is responsible\
    \ for accumulating the partial products from a single row, and that each column\
    \ enters through a single column, each left-side vertex corresponds to an adder,\
    \ and right-side vertex to a multiplier. For example, the matrix element will\
    \ be represented by an edge connecting the -th left-side vertex to the -th right-side\
    \ vertex of the bipartite-graph, the edge color will be its position in the input\
    \ bufer of the -th multiplier, and it will be directed to the -th adder.\n\nIf\
    \ the dimension of the input matrix, × , is bigger than the length of GUST, ,\
    \ the scheduling is done through windowing; meaning frst, the frst rows are scheduled,\
    \ then the second rows and so on. In addition, since is bigger than , the scheduling\
    \ will be done with respect to the column of the matrix elements; meaning multiple\
    \ matrix columns will be represented by the same right-side vertex. To be specifc,\
    \ the -th, ( <sup>+</sup> )-th, ( <sup>+</sup> <sup>2</sup>)-th, ..., ( + ( −\
    \ 1))-th columns are represented by the -th right-side vertex, where 1 ≤ ≤ . This\
    \ also means that it is possible to have multiple edges connecting the same pair\
    \ of vertices.\n\nFigure 5 provides an example of scheduling for an SpMV with\
    \ a 6×9 matrix using a length-3 GUST. The top right-side vertex in Figure 5(b)\
    \ and (c) represent the frst, fourth, and seventh columns (A, D, G), the middle\
    \ vertex the second, ffth, and eighth columns (B, E, H), and the bottom vertex\
    \ the third, sixth, and ninth columns (C, F, I). As a result, we observe that\
    \ elements from the frst, fourth, and seventh columns are directed to the frst\
    \ multiplier, the second, ffth, and eighth columns to the second multiplier, and\
    \ the third, sixth, and ninth columns to the third multiplier.\n\nGUST Scheduling\
    \ Algorithm– The scheduling for a length- GUST generates three outputs: the matrix\
    \ in scheduled format ℎ, and the ℎ andℎ matrices. ℎ is\n\nan by C matrix, where\
    \ C is the total number of colors needed. ℎ contains elements of the original\
    \ matrix in a rearranged and compressed manner. ℎ and ℎ are also by C matrices.\
    \ They contain the original row and column indices of the elements in ℎ, and are\
    \ used to control the data-flow in the FPGA during SpMV. These matrices can be\
    \ viewed as a compressed storage format similar to the Coordinate format.\n\n\
    |    | 1 //l = Gust len , E = edge set , a 2- d variable |\n|----|---------------------------------------------------|\n\
    |    | 2 clr = 0;                                        |\n|    | 3 While(E not\
    \ Empty)                              |\n| 4  | matching = []                \
    \                     |\n| 5  | //iterate<br>through the left -side<br>vertices\
    \   |\n| 6  | for (i = 1 to l)                                  |\n| 7  | //iterate<br>through\
    \ the edges<br>connecetd       |\n| 8  | for (k = 1 to E[i]. size ())        \
    \              |\n| 9  | if(E[i][k] mod l not in matching)                 |\n\
    | 10 | color[i][E[i][k]] = clr                           |\n| 11 | matching. insert\
    \ (E[i][k] mod l)                  |\n| 12 | delete E[i][k]                  \
    \                  |\n| 13 | break                                           \
    \  |\n| 14 | clr += 1                                          |\n\nListing 1.\
    \ Edge-Coloring Algorithm\n\nTo fll these three matrices, we frst need to fnd\
    \ the scheduling. For an × matrix and a GUST of length , the matrix is represented\
    \ by / sets of edges, and we schedule each set independently. Each set connects\
    \ left-side vertices to right-side vertices, with the possibility of multiple\
    \ edges connecting the same two vertices. To do so, we start with = 0. At each\
    \ iteration, we fnd a matching (a matching is a subset of edges which share no\
    \ vertex), assign the color value of the matching as the current value, remove\
    \ the matching from the set, and update color value as = + 1. We repeat these\
    \ steps until all of the edges have a color assigned to them. Listing 1 is the\
    \ pseudo-code of our Edge-Coloring algorithm for a set of rows, where E represents\
    \ a set of edges from rows of the input matrix. To be specifc, E[i][k] = j indicates\
    \ an edge connecting the -th left-side vertex to\n\nthe -th right-side vertex.\
    \ This representation allows for a faster Edge-Coloring since when we choose an\
    \ edge from a left-side vertex to be in the matching, we do not need to consider\
    \ any edges connected to that vertex (the break statement at line 13). Moreover,\
    \ color[i][j] represents the color of the matrix elements in the -th row and -th\
    \ column.\n\nWith the schedule (color of edges) determined, we can now proceed\
    \ to fll ℎ, ℎ and ℎ. ℎ [] [] contains the matrix element that should enter the\
    \ -th multiplier at the -th timestep. ℎ [] [] and ℎ [] [] contain the original\
    \ column index and row index of this element. Using them we keep track of which\
    \ vector element ℎ [] [] should be multiplied with, and to which adder the result\
    \ of this multiplication should be forwarded; for instance, ℎ [] [] will be multiplied\
    \ with theℎ [] []-th element of the vector, and the crossbar will connect the\
    \ -th multiplier to the ℎ [] []-th adder. Listing 2 shows how to fll these matrices\
    \ with respect to color, where M\\_sch, row\\_sch and col\\_sch refer to ℎ, ℎ\
    \ and ℎ. To elaborate, if the color of a matrix element in the -th row and -th\
    \ column is, it will be flled in M\\_sch[c][j mod l], since it should enter the\
    \ -th multiplier at the -th time step. Moreover, row\\_sch[c][j mod l] and col\\\
    _sch[c][j mod l] will be flled with and . Note that the scheduling for each matrix\
    \ only needs to be computed once. We can then reuse this schedule to fll the bufers\
    \ for any SpMV with that matrix, even if the input vector changes. Furthermore,\
    \ if the matrix changes but the location of NZs remain the same (as it is the\
    \ case with Jacobian and Hessain matrices), the scheduling (Listing 1) does not\
    \ need to be repeated, rather ℎ (Listing 2) needs to be updated.\n\n //l = Gust\
    \ len , m = matrix height // M = input matrix for (i = 1 to m) for (k = 1 to colors[i].\
    \ size ()) M\\_sch[color[i][j]][j mod l] = M[i][j] row\\_sch[color[i][j]][j mod\
    \ l] = i mod l col\\_sch[ color[i][j]][j mod l] = j\n\nListing 2. Filling the\
    \ Output Matrices\n\nStreaming the Inputs– Given ℎ, ℎ and ℎ matrices, we will\
    \ now go over how to fll the input bufers. As a reminder, there are four types\
    \ of input bufers: matrix, vector, row indices and dump signal bufers, as shown\
    \ in Figure 2. We begin with forwarding the input vector to the Bufer Filler,\
    \ which utilizes on-chip memory. Then, we fll the bufers in a two-step pipelined\
    \ fashion. Step 1: A partition of the ℎ, ℎ and ℎ matrices are forwarded from of-chip\
    \ memory to the Bufer Filler's on-chip memory. Step 2: The Bufer Filler flls the\
    \ input bufers as follows: The matrix and row indices bufers are flled directly\
    \ with ℎ and ℎ respectively. The vector bufers are flled using the vector stored\
    \ in the on-chip memory, and with respect to ℎ, ensuring that each matrix element\
    \ is multiplied with\n\n![](_page_6_Figure_5.jpeg)\n\nFigure 6. Load Balancing–\
    \ (a) Initial matrix. (b) The matrix after applying step 1 (sorting the rows,\
    \ resulting in switching the second and third row), 2 (sorting the column segments,\
    \ resulting in switching the second and fourth column segments in the second row\
    \ set), and 3 (rearranging partial columns, resulting in switching the third and\
    \ forth column segments) of the load balancing algorithm.\n\nthe correct vector\
    \ element. Specifcally, the vector bufer of the -th multiplier at timestep is\
    \ flled with theℎ [] [] th element of the vector. For a length- GUST and matrix\
    \ with width , the ℎ elements are 32 bit floating point values, the ℎ are () bit\
    \ values (since they should index 1 to ), and the ℎ are () bit values (since they\
    \ should index 1 to ). Assuming the ℎ elements are 32 bits, and that the operating\
    \ frequency is Hz, the required BW by a length- GUST becomes (64 + () + 1) bits/s.\n\
    \n## 3.4 Statistical Bound\n\nIn this section, we fnd a statistical bound on the\
    \ hardware utilization to assess the efectiveness of our scheduling approach.\
    \ The input bufer length required to process a set of rows is equal to the number\
    \ of colors required C, and according to Vizing's theorem [34], this value is\n\
    \n$$C = \\max\\{\\max\\_{i}\\{\\#\\text{NZ in the } i\\text{-th row}\\},$$\n\n\
    $$\\max\\_{\\lambda \\ge 0} \\{\\sum\\_{\\mathbf{x} = \\mathbf{0}}^{n/l - 1} \\\
    #\\text{NZ in the } (j + \\mathbf{x})\\text{-th column}\\}. \\quad \\text{(1)}$$\n\
    \nIn other words, the number of colors needed is the maximum number of edges connected\
    \ to a vertex in the bipartite graph. The execution time in terms of number of\
    \ cycles is the sum of the number of colors for all of the edge sets (each edge\
    \ set is a set of rows) plus 2 (GUST has 3 levels). For example, in Figure 5(c)\
    \ the frst three rows require 5 colors and the last three 4, making the total\
    \ number of cycles 11.\n\nUsing Eq. 1, we will derive a statistical bound on the\
    \ execution time for sparse matrices with a uniform distribution. This bound will\
    \ then be used to analyze hardware utilization as a function of the probability\
    \ of each element being NZ. For a length GUST and an × matrix, each row has NZ\
    \ elements with probability\n\n$$Pr\\_{row}(k) = \\binom{N}{k} p^k (1-p)^{(N-k)}.\\\
    tag{2}$$\n\nAssuming > 9(1 − )/ (i.e., an average of at least 10 NZs in each row)\
    \ we can use the Central Limit Theorem and approximate\n\n$$Pr\\_{row}(k) \\sim\
    \ \\mathcal{N}(Np, Np(1-p)),\\tag{3}$$\n\nwhere N denotes the normal distribution.\
    \ Since is small, we can also assume that the number of NZ in each column segment\
    \ is independent from rows, making the probability of NZ elements in a column\
    \ segment\n\n$$Pr\\_{col}(k) \\sim \\mathcal{N}(Np, Np(1-p))\\tag{4}$$\n\nas well.\
    \ We have rows and column segments, and since C is the maximum number of NZs in\
    \ rows and column segments, the value of C is governed by the maximum of 2 independent\
    \ Gaussian distributions. i.e.\n\n$$\\mathcal{C} = \\max\\_{i, 1 \\le i \\le 2l}\
    \ X\\_i, \\quad X\\_i \\sim \\mathcal{N}(N\\rho, Np(1-\\rho)). \\tag{5}$$\n\n\
    To calculate the expectation of C, let us frst calculate the expectation of Y\
    \ = ,1≤≤2 , where ∼ N(0, <sup>2</sup> ). Using Jensen's inequality, we have\n\n\
    $$\\forall\\_t \\forall \\quad \\quad e^{tE\\left[\\mathcal{N}\\right]} \\le E\\\
    left\\{e^{t\\mathcal{N}}\\right\\} = E\\left\\{\\max\\_i \\{e^{tY\\_i}\\}\\right\\\
    }.\\tag{6}$$\n\nUsing Union bound, we bound the right-hand side of Eq. 6 with\n\
    \n$$\\mathbb{E}\\left[\\max\\_{i}\\{e^{tY\\_{i}}\\}\\right] \\le \\Sigma\\_{i=1}^{2l}\
    \ \\mathbb{E}\\{e^{tY\\_{i}}\\} = 2l e^{t^{2}\\sigma^{2}/2}.\\tag{7}$$\n\nSetting\
    \ = 2(2)/ we get\n\n$$E[Y] \\le \\sigma \\sqrt{2\\log(2l)}.\\tag{8}$$\n\nFor ∼\
    \ N(, <sup>2</sup> ), the right-hand side of Eq. 8 will be + 2(2) since we are\
    \ shifting the values with . Replacing Y with C we get\n\n$$E[\\mathcal{C}] \\\
    le Np + \\sqrt{2Np(1-p)\\log(2l)}.\\tag{9}$$\n\nThe expectation of execution time\
    \ then becomes\n\n$$E[\\text{exe.}] = \\frac{N}{l}(Np + \\sqrt{2Np(1-p)\\log(2l)})\
    \ + 2\\tag{10}$$\n\nsince execution time is the sum of all the colors for the\
    \ / edges sets. Having the expectation of the execution time, we can fnd the expectation\
    \ of hardware utilization as\n\n$$\\begin{aligned} E[\\text{hardware utilization}]\
    \ &= \\begin{array}{c} \\#NZ/l \\\\ E[\\text{exe.}] \\end{array} = \\frac{N^2\
    \ p/l}{E[\\text{exe.}]} \\\\ &= \\frac{1}{1 + \\sqrt{2(1 - p)\\log(2l)/Np}} .\
    \ \\end{aligned} \\tag{11}$$\n\n## 3.5 Load Balancing\n\nValue of C in Eq. 1 depends\
    \ on the maximum of NZs in column segments and rows, and not the total number\
    \ of NZs. As a result, performance of GUST can be negatively afected by inconsistencies\
    \ in the number of NZs (#NZ) between column segments or rows within row sets.\
    \ Consider the example in Figure 6. According to Eq. 1, case (a) requires 7 cycles\
    \ (4 cycles for the frst partition and 3 for the second) to process\n\nwhile case\
    \ (b) requires 5 cycles (4 cycles for the frst partition and 1 for the second).\
    \ In general, the smaller the standard deviation of #NZ in rows and column segments\
    \ within row sets, the smaller the execution time. Our load balancing is a simple\
    \ yet efective three step sorting procedure: Step 1: Sort the matrix rows based\
    \ on #NZ in each row. Step 2: Sort the column segments of each partition based\
    \ on #NZ in each column segment. Step 3: For even column segments, reverse the\
    \ order of columns (e.g. for a length-2 GUST, if the order of sorted column segments\
    \ are 1, 2, 3, 4, 5, 6, 7, 8, rearrange them to 1, 2, 4, 3, 5, 6, 8, 7). The frst\
    \ step helps with reducing the standard deviation of #NZ in rows and the second\
    \ and third step in column segments.\n\n# 4 Experimental Setup\n\nDataset. We\
    \ evaluate GUST based on synthetic and realworld matrices. For the synthetic data,\
    \ we include matrices with uniform, power-law and k-regular distribution and a\
    \ dimension of 16, 384 over a density range of 1 −4 to 5 −2 . The power-law and\
    \ k-regular synthetic data were acquired from SNAP [18] matrix generator. For\
    \ real-world data, we used a collection of matrices from the SuiteSparse [7] and\
    \ SNAP [18] Matrix Collections, with densities ranging from 1 −5 to 1 −1 .\n\n\
    Simulation. To compare the hardware efciency of GUST with the designs introduced\
    \ in Section 2, we assumed all the designs have 256 adders and 256 multipliers,\
    \ except for Fafnir, which has 448 adders and 128 multipliers. The hardware efciency\
    \ of the designs were calculated based on the dataflow of each specifc matrix.\n\
    \nWe use an Alveo U280 FPGA to synthesis length-256 GUST, which is HBM2 enabled\
    \ with a maximum memory bandwidth of 460 GB/s. We use float-32 arithmetic precision\
    \ and target clock frequency of 96 MHz, which is bounded by GUST's longest logic\
    \ route connecting the two ends of the crossbar. The minimum BW needed is 224\
    \ GB/s, as explained in Section 3. 3. Streaming the Inputs. Length-256 1D and\
    \ Serpens are synthesized using the same FPGA.\n\nThe energy efciency gain is\
    \ calculated by computing the energy consumption as a results of dynamic power,\
    \ NZ data movements, reads, writes, and arithmetic operations using the following\
    \ energy numbers for 32 bits in pJ [5, 6]: 64 and 11.84 for reading from of-chip\
    \ and on-chip memory read, 64 and 16 for of-chip and on-chip memory write, 10\
    \ for floating point accumulation and multiplication, 160 and 0.95 for moving\
    \ data for 1mm for of-chip and on-chip, with the distance being 5 mm, 1 mm, and\
    \ 129 mm, between of-chip memory and on-chip elements, on-chip elements in 1D,\
    \ and average distance between on-chip elements in GUST (we use average since\
    \ GUST has a crossbar connection). The dynamic power consumption values used for\
    \ length-256 1D, length-256 GUST and length-87 GUST are 35.3, 56.9 and 16.8 Watts\
    \ respectively, which were measured from the FPGA\n\nsynthesis. To account for\
    \ the energy consumption of the frst step of GUST's SpMV calculation (forwarding\
    \ the vector to Bufer Filler), we add the power consumption of GUST times the\
    \ duration it takes to forward the values.\n\nAlveo U280 has 32 physical channels\
    \ whereas we are interested in implementing a length-256 GUST, which has 18, 433\
    \ logical inputs (256 × 32 for matrix values, 256 × 32 for vector values, 256\
    \ × 8 for index values, and 1 for the dump signal. This means that the of-chip\
    \ memory cannot be directly connected to the multipliers. As a workaround, we\
    \ have introduced the Bufer Filler and a pipelined process for streaming the inputs.\
    \ To elaborate, at each timestep, the HBM2 streams the inputs to the on-chip memory\
    \ of the Bufer Filler, and the Bufer Filler appropriately forwards the values\
    \ to the multiplier, crossbar, and adder input bufers to perform the calculations\
    \ (double bufering). The required on-chip memory to enable this process is equal\
    \ to the twice the size of the input values in a timestep, which is 36, 866 bits\
    \ (4.5 KB) in here. Given that Alveo U280 ofers 41 MB of on-chip memory, this\
    \ leaves ample storage for a vector of dimension up to 10<sup>7</sup> , which\
    \ is 9 times larger than the biggest matrix encountered in our experiments (as\
    \ a reminder GUST stores the whole input vector as the frst step of the SpMV calculation).\
    \ We should clarify that that GUST's hardware implementation is not limited to\
    \ FPGAs. We adapted an FPGA implementation for demonstration purposes, but GUST\
    \ can be implemented on any hardware platform that can provide a set of multipliers\
    \ and adders, and a crossbar connector.\n\n# 5 Results\n\nThis section provides\
    \ a comprehensive evaluation of GUST. We frst compare the hardware utilization\
    \ and execution time of GUST with the designs introduced in Section 2. Next, we\
    \ asses GUST from an implementation perspective. We compare length-256 and length-87\
    \ GUST with length-256 1D in terms of speedup, energy efciency gain, and resource\
    \ consumption. Then, we compare end-to-end SpMV wall-clock time and energy consumption\
    \ with Serpens [29]. Then, we evaluate how the density and structure of the input\
    \ matrix afects the performance of GUST. Finally, we talk about scalability of\
    \ GUST in terms of its length.\n\n### 5.1 GUST vs Other Designs\n\nFigure 7(a)\
    \ shows the hardware utilization of GUST with naive scheduling (Naive), GUST with\
    \ Edge-Coloring (EC), GUST with Edge-Coloring and load balancing (EC/LB), and\
    \ the designs introduced in Section 2 for SpMV over a range of real-world sparse\
    \ matrices [7]. The results reveal a signifcant performance gap between GUST with\
    \ EC/LB and other designs, evident in both hardware utilization and execution\
    \ time. Specifcally, GUST with EC/LB demonstrates an average hardware utilization\
    \ of 33.67%, which is notably high when dealing with sparse matrices. Moreover,\
    \ comparing GUST with Naive and GUST with EC/LB reveals a substantial performance\
    \ gap as well, emphasizing the critical role of Edge-Coloring in GUST's efectiveness.\
    \ This gap widens with increasing density as collision-induced stalls add up,\
    \ to the point that GUST with Naive will have a worst performance than 1D and\
    \ AT. This underscores the importance\n\n![](_page_8_Figure_6.jpeg)\n\nFigure\
    \ 7. GUST VS Other Designs– (a) Hardware utilization and (b) execution time of\
    \ various designs for SpMV over real-world matrices with a wide density range\
    \ of 1 −5 to 1 −1 . GUST with Edge-Coloring and load balancing (EC/LB).\n\n![](_page_9_Figure_0.jpeg)\n\
    \nFigure 8. Performance Gain of GUST Over 1D– Speedup of length 256 GUST with\
    \ Naive, Edge-Coloring and Edge-Coloring/Load Balancing, as well as energy efciency\
    \ gain of length-256 GUST with Edge-Coloring/Load Balancing over length-256 baseline\
    \ 1D systolic array on (a) real, and synthetic data with (b) uniform, (c) power-law\
    \ and (d) k-mean distribution.\n\nof our proposed scheduling algorithm in increasing\
    \ hardware utilization. Without proper scheduling, the resource sharing enabled\
    \ by the hardware will not result into a noteworthy performance improvement. Figure\
    \ 7(b) shows the comparison for execution time. The results exhibit a consistent\
    \ pattern, demonstrating that high hardware utilization translates into reduced\
    \ execution time.\n\n### 5.2 Performance and Overhead of GUST Over 1D\n\nFigure\
    \ 8 shows the speedup and energy efciency gain of length-256 GUST with Naive,\
    \ GUST with EC, and GUST with EC/LB as well as length-87 GUST with EC/LB over\
    \ length-256 1D for real-world and synthetic matrices. Table 2 and shows the resource\
    \ consumption of these designs based on FPGA synthesis. Length-256 GUST with EC/LB\
    \ uses the same number of arithmetic units as length-256 1D while ofering speedup\
    \ and energy efciency gain in order of magnitudes. However, it introduces a considerable\
    \ overhead due to the crossbar connector. One might argue that the main reason\
    \ for GUST's performance gain over 1D is the mentioned overhead. For this purpose,\
    \ we also compare length-87 GUST, which uses signifcantly less hardware resources\
    \ than length-256\n\n1D while achieving substantial performance gains. Furthermore,\
    \ looking at the speedup ofered by GUST with Naive GUST with and EC/LB we notice\
    \ a wide gap, once again showing the importance of our scheduling, and that the\
    \ performance gain does not come from the hardware structure alone. For real-world\
    \ matrices, length-256 GUST with EC/LB achieves an average speedup of 88× over\
    \ length-256 GUST with Naive, and 1.8× over length-256 EC. Compared to length-256\
    \ 1D, length-256 and length-87 GUST with EC/LB achieve speedup of 411× and 108×,\
    \ and energy efciency gain of 137× and 148× respectively.\n\nFigure 9 shows the\
    \ average bandwidth utilization of length-256 and -87 GUST with EC/LB as well\
    \ as length-256 1D for SpMV over real-world matrices based on the target clock\
    \ frequency of 96 MHz synthesis. The noticeable diference in bandwidth utilization\
    \ between GUST and 1D stems from the substantial underutilization of bandwidth\
    \ by 1D due to sparsity. In fact, the high bandwidth of GUST indicates that we\
    \ were successful with our goal: to increase hardware utilization through creating\
    \ a dense input stream.\n\n![](_page_10_Figure_0.jpeg)\n\nFigure 9. BW Utilization–\
    \ Average BW utilized by length-256 1D and length-256 and -87 GUST with EC/LB.\
    \ The \"Maximum BW\" represents maximum possible bandwidth utilization, achievable\
    \ when all inputs are nonzero.\n\n### 5.3 GUST vs Serpens\n\nWe compare GUST with\
    \ Serpens on nine SuiteSparse [7] and SNAP [18] matrices, and Table 3 shows the\
    \ details of these matrices. Both GUST and Serpens are synthesized on an Alveo\
    \ U280 FPGA with operating frequency of 96 MHz and 223 MHz respectively, and dynamic\
    \ power of 56.9 and 46.2 Watts. Moreover, both take advantage of preprocessing\
    \ to reformat the sparse matrix into accelerator-efcient storage. Table 4 shows\
    \ the execution time, energy consumption and throughput of GUST during the preprocessing\
    \ and SpMV calculation. As we can see, GUST requires considerably less time and\
    \ energy to perform its preprocessing. As for the SpMV calculation, even though\
    \ GUST has a lower\n\nTable 2. Per-Element Resource Consumption of GUST and 1D–\
    \ Power and hardware consumption of length-256 1D, length-8, -87 and -256 GUST\
    \ based on FPGA synthesis.\n\n| Design<br>Power<br>(Watt) | length-256<br>1D |\
    \ length-8<br>GUST | length-87<br>GUST | length-256<br>GUST |\n|---------------------------|------------------|------------------|-------------------|--------------------|\n\
    | Static                    | 3.2              | 2.5              | 3.2      \
    \         | 3.8                |\n| Logic                     | 3.4          \
    \    | 0.1              | 1.8               | 14.3               |\n| Signals\
    \                   | 2.6              | 0.3              | 3.0              \
    \ | 8.1                |\n| DSP                       | 0.3              | 0.01\
    \             | 0.1               | 0.3                |\n| I/O              \
    \         | 25.7             | 0.5              | 8.6               | 30.3   \
    \            |\n| Total                     | 35.3             | 3.4         \
    \     | 16.8              | 56.9               |\n| Design                   \
    \ | length-256       | length-8         | length-87         | length-256     \
    \    |\n| Units                     | 1D               | GUST             | GUST\
    \              | GUST               |\n| Register                  | 8.2K    \
    \         | 512              | 5.6K              | 16.4K              |\n| Input\
    \ Bufers              | 8.2K             | 546              | 6.2K           \
    \   | 18K                |\n| LUT                       | 132K             | 5K\
    \               | 5.6K              | 888K               |\n| DSP            \
    \           | 256              | 16               | 174               | 256  \
    \              |\n| I/O Buss                  | 16K              | 802       \
    \       | 8.9K              | 2.7K               |\n| Maximum BW             \
    \   | 150 GB/s         | 5.8 GB/s         | 76GB/s            | 224GB/s      \
    \      |\n\nTable 3. Set of nine real-world matrices with density range of −5\
    \ to −3 .\n\n| ID  | Matrix               | Dimension | #NZ   | Density    |\n\
    |-----|----------------------|-----------|-------|------------|\n| (1) | crankseg_2\
    \ [7]       | 63.8K     | 14.1M | −3<br>3.4� |\n| (2) | Si41Ge41H72 [7]      |\
    \ 186K      | 15.0M | −4<br>4.3� |\n| (3) | TSOPF_RS_b2383 [7]   | 39.1K     |\
    \ 16.2M | −2<br>1.0� |\n| (4) | ML_Laplace [7]       | 377K      | 27.6M | −4<br>1.9�\
    \ |\n| (5) | mouse_gene [7]       | 45.1K     | 29M   | −3<br>1.4� |\n| (6) |\
    \ coPapersCiteseer [7] | 434K      | 21.1M | −4<br>1.1� |\n| (7) | PFlow_742 [7]\
    \        | 743K      | 37.1M | −5<br>6.7� |\n| (8) | googleplus [18]      | 108K\
    \      | 13.7M | −3<br>1.2� |\n| (9) | soc_pokec [18]       | 1.63M     | 30.6M\
    \ | −5<br>1.2� |\n\noperating frequency, it achieves lower execution time for\
    \ seven out of nine matrices. This is because the scheduling results in a denser\
    \ matrix format, and since the scheduling is collision-free, it leads to higher\
    \ FLOPs per cycle. Consequently, the total cycle count and overall wall-clock\
    \ time is reduced. Furthermore, GUST achieves lower energy consumption for four\
    \ of the matrices. Despite GUST's higher power consumption, the reduced execution\
    \ time results in overall lower energy consumption.\n\nWhile the preprocessing\
    \ times for both GUST and Serpens are orders of magnitude higher than the actual\
    \ SpMV calculation, this does not pose a signifcant issue. The reason is that\
    \ the preprocessing step is a one-time operation for a given sparse matrix, whereas\
    \ the SpMV calculation needs to be performed repeatedly within iterative solvers.\
    \ To be specifc, these SpMVs usually arise in linear algebra solvers of the form\
    \ = , where is the sparse matrix, and the goal is to solve for . Two common approaches\
    \ exist: reducing the rank of the linear system using techniques like QR decomposition,\
    \ which requires ( 3 ) operations ( being the matrix size), or employing gradient/conjugate\
    \ gradient descent methods. The latter approach minimizes the cost function ||\
    \ − || through iterative updates to . While gradient descent requires (1/accuracy)\
    \ iterations, sparse matrices often have high condition numbers, making the optimization\
    \ sensitive and forcing a very small learning rate.\n\nLet's consider performing\
    \ a regular matrix-vector multiplication on the FPGA, using the \"crankseg\\_2\"\
    \ matrix from Table 3 as an example. Assuming full utilization of the High Bandwidth\
    \ Memory (HBM), each multiplication would take approximately 0.7s. This time is\
    \ derived from the total number of float-32 elements (63800<sup>2</sup> × 2) that\
    \ need to enter the FPGA, and the FPGA bandwidth of 460GB/s. As for GUST, the\
    \ preprocessing step takes 4.32s, and each subsequent SpMV (even if the vector\
    \ changes) is completed in only 0.6ms.\n\n## 5.4 Efect of Matrix Density and Structure\
    \ on GUST\n\nFigures 8(b)-(d) show that the speedup of GUST over 1D follows a\
    \ (1/) trend for each of the matrix structures. This suggests that hardware efciency\
    \ remains relatively\n\nTable 4. GUST vs Serpens– Execution time (in terms of\
    \ wall-clock time and clock cycles), energy consumption and throughput (GFLOPs/s)\
    \ of GUST vs Serpens for the preprocessing and calculation phases on matrices\
    \ introduced in Table 3. The preprocessing is done using Intel core i7-\n\n| 10750H<br>CPU\
    \ with a power consumption of 45 W. |          |         |          |        \
    \ |          |\n|-------------------------------------------------|----------|---------|----------|---------|----------|\n\
    | Matrix                                          | Metric   | GUST    |     \
    \     | Serpens |          |\n| ID                                           \
    \   |          | Pre.    | Calc.    | Pre.    | Calc.    |\n|                \
    \                                 | Time     | 4.32 s  | 0.59 ms  | 9.34 s  |\
    \ 0.93 ms  |\n|                                                 | # Cycles | -\
    \       | 57K      | -       | 208K     |\n| (1)                             \
    \                | Energy   | 194 J   | 34.3 mJ  | 420 J   | 44.6 mJ  |\n|   \
    \                                              | GFLOPS   | -       | 47.8   \
    \  | -       | 30.5     |\n|                                                 |\
    \ Time     | 4.56 s  | 0.67 ms  | 13.3 s  | 0.85 ms  |\n|                    \
    \                             | # Cycles | -       | 64K      | -       | 190K\
    \     |\n| (2)                                             | Energy   | 205 J\
    \   | 42.5 mJ  | 598 J   | 40.7 mJ  |\n|                                     \
    \            | GFLOPS   | -       | 49.4     | -       | 35.6     |\n|       \
    \                                          | Time     | 4.72 s  | 0.83 ms  | 7.86\
    \ s  | 0.73 ms  |\n|                                                 | # Cycles\
    \ | -       | 80K      | -       | 163K     |\n| (3)                         \
    \                    | Energy   | 212 J   | 51.9 mJ  | 353 J   | 35.0 mJ  |\n\
    |                                                 | GFLOPS   | -       | 39.0\
    \     | -       | 44.4     |\n|                                              \
    \   | Time     | 8.69 s  | 1.10 ms  | 23.8 s  | 1.37 ms  |\n|                \
    \                                 | # Cycles | -       | 106K     | -       |\
    \ 306K     |\n| (4)                                             | Energy   | 391\
    \ J   | 70.6 mJ  | 1071 J  | 65.7 mJ  |\n|                                   \
    \              | GFLOPS   | -       | 50.1     | -       | 42.2     |\n|     \
    \                                            | Time     | 8.74 s  | 1.45 ms  |\
    \ 26.2 s  | 1.37 ms  |\n|                                                 | #\
    \ Cycles | -       | 139K     | -       | 306K     |\n| (5)                  \
    \                           | Energy   | 393 J   | 91.1 mJ  | 1179 J  | 65.7 mJ\
    \  |\n|                                                 | GFLOPS   | -       |\
    \ 40.0     | -       | 42.2     |\n|                                         \
    \        | Time     | 9.79 s  | 1.34 ms  | 39.7 s  | 2.09 ms  |\n|           \
    \                                      | # Cycles | -       | 129K     | -   \
    \    | 466K     |\n| (6)                                             | Energy\
    \   | 440 J   | 82.4 mJ  | 1786 J  | 100.4 mJ |\n|                           \
    \                      | GFLOPS   | -       | 31.5     | -       | 31.1     |\n\
    |                                                 | Time     | 13.53 s | 1.52\
    \ ms  | 70.0 s  | 2.05 ms  |\n|                                              \
    \   | # Cycles | -       | 146K     | -       | 457K     |\n| (7)            \
    \                                 | Energy   | 608 J   | 91.6 mJ  | 3150 J  |\
    \ 98.4 mJ  |\n|                                                 | GFLOPS   | -\
    \       | 48.8     | -       | 37.0     |\n| (8)                             \
    \                | Time     | 5.58 s  | 1.42 ms  | 9.29 s  | 1.87 ms  |\n|   \
    \                                              | # Cycles | -       | 136K   \
    \  | -       | 417K     |\n|                                                 |\
    \ Energy   | 251 J   | 116.3 mJ | 417 J   | 89.8 mJ  |\n|                    \
    \                             | GFLOPS   | -       | 21.1     | -       | 14.71\
    \    |\n|                                                 | Time     | 10.97 s\
    \ | 3.26 ms  | 56.7 s  | 4.52 ms  |\n| (9)                                   \
    \          | # Cycles | -       | 313K     | -       | 1.01M    |\n|         \
    \                                        | Energy   | 493 J   | 193.8 mJ | 2554\
    \ J  | 217 mJ   |\n|                                                 | GFLOPS\
    \   | -       | 19.8     | -       | 14.29    |\n\nstable across varying density\
    \ levels, and that GUST's efectiveness is density-independent, which is in agreement\
    \ with 12. However, the speedup varies between diferent matrix structures with\
    \ the same density. GUST does a much better job dealing with matrices with uniform\
    \ distribution than it does with power-law and K-mean. Depending on how well the\
    \ NZ elements are spread out, we get a diferent standard deviation for #NZ elements\
    \ in rows and column partitions (STD). As discussed in Section 3. 5, high STD\n\
    \nnegatively afects the performance of GUST. Load balancing helps reducing the\
    \ high STD, but to some extent.\n\nEnergy efciency gain also follows a (1/) trend,\
    \ but unlike speedup, it is not signifcantly afected by matrix structure. The\
    \ reason for this is that NZ operations and data movement are the primary contributors\
    \ to energy consumption, while the energy consumption from dynamic power during\
    \ active device operation plays a less signifcant role.\n\n## 5.5 Scalability\n\
    \nLooking at the energy efciency gain of length-256 and - 87 GUST with EC/LB in\
    \ Figure 8, we notice that length-87 achieves a higher energy efciency gain. This\
    \ may seem surprising since length-256 ofers more speedup. This is due to the\
    \ fact that the power consumption of the crossbar connector in GUST increases\
    \ superlinearly with length of GUST, as enumerated in Table 5. Furthermore, the\
    \ hardware overhead introduced by the crossbar increases quadratically with length\
    \ of GUST. This would be problematic if we were to use a long length GUST. One\
    \ possible solution to address this problem is to use a parallel arrangement of\
    \ multiple GUSTs rather than one big GUST; i.e., use length- GUSTs in parallel\
    \ rather than one length- GUST. GUST naturally gets parallelized really well since\
    \ it performs by taking in a set of rows, calculating the corresponding vector\
    \ elements, and then taking the next set of rows. Each set of rows will be processed\
    \ independtly and in parallel. Moreover, the Edge-Coloring schedule would not\
    \ need to change (schedule found for length- GUST is applicable to parallel length-\
    \ GUSTs).\n\nAlthough the parallel arrangement reduces the hardware overhead and\
    \ energy consumption, it will increase the execution for two reasons: 1. The main\
    \ purpose of GUST's hardware is to enable resource sharing among multiple rows\
    \ and columns. If we replace a length- GUST with length-, we virtually decrease\
    \ resource sharing among rows and columns to resource sharing among rows and columns.\
    \ 2. It is not guaranteed that the work will be divided equally among the parallel\
    \ GUSTs. Other factors such as number of arithmetic units and BW remains the same\
    \ between the parallel and single arrangement of GUST.\n\n# 6 Related Work\n\n\
    Many recent studies have shifted their focus towards accelerating SpMV of FPGAs.\
    \ For example, Copernicus [2] investigates the efects of employing various compression\
    \ formats, including LIL, for SpMV implementation on FPGAs. ReDESK [21] proposes\
    \ a representation specifcally designed for data prefetching on CPUs, allowing\
    \ streaming processing on FPGAs. Recent studies have also examined the computation\
    \ order, such as a work by Li et al.[19], which reorganizes the nonzero elements\
    \ to enhance data reuse, thereby optimizing memory requests. Customizing network\
    \ architecture using elementary blocks for low-level detail exploitation has\n\
    \nTable 5. Per-partition Resource Consumption of GUST– Resource consumption of\
    \ length-8, -87 and -256 GUST in terms of power (W) and number of units used by\
    \ the partitions of GUST. The arithmetic and I/O partitions of GUST have a resource\
    \ consumption increasing linearly with GUST's length while the crossbar connector\
    \ has a quadratic and superlinearly scaling hardware and power consumption.\n\n\
    | Seg   | length 8 |              | length 87 |               | length 256 | \
    \             |\n|-------|----------|--------------|-----------|---------------|------------|--------------|\n\
    | ment  | Power    | Units        | Power     | Units         | Power      | Units\
    \        |\n|       | 0.3      | LUT: 4229    | 3.5       | LUT: 46.0K    | 6.3\
    \        | LUT: 132K    |\n| Arith |          | Reg: 256     |           | Reg:\
    \ 2.8K     |            | Reg: 8.2K    |\n| metic |          | DSP: 16      |\
    \           | DSP: 174      |            | DSP: 512     |\n|       |         \
    \ | Carry8: 152  |           | Carry8: 1.6K  |            | Carry8: 4.8K |\n|\
    \ Cross | 1.0      | LUT: 772     | 3.6       | LUT: 17.3K    | 16.4       | LUT:\
    \ 756K    |\n| bar   |          | Reg: 256     |           | Reg: 2.8K     | \
    \           | Reg: 8.2K    |\n| IO    | 0.5      | IO Pins: 802 | 7.1       |\
    \ IO Pins: 8.9K | 28.1       | IO Pins: 27K |\n|       |          | Buf: 546 \
    \    |           | Buf: 6.2K     |            | Buf: 18K     |\n\nbeen explored\
    \ in an FPGA work [13], along with the use of tree structures like in Two-Step\
    \ [27] and Fafnir [1] for memory streaming. Sparstition [28] looks into the acceleration\
    \ of SpMV by examining partitioning schemes. Tree structures have been utilized\
    \ by several SpMV accelerators, such as MeNDA [9], a scalable multi-way merge\
    \ accelerator near DRAM that enhances performance and reduces energy consumption.\
    \ It adopts a merge sort-based algorithm with a highperformance hardware merge\
    \ tree and various techniques to exploit increased system memory bandwidth, achieving\
    \ remarkable speedup and efciency gains. A diferent treebased method includes\
    \ an accelerator design focusing on high-bandwidth utilization [20], with elaborate\
    \ FPGA implementation showing promising performance improvements.\n\nAdditionally,\
    \ a hardware/algorithm co-optimized accelerator for extensive SpMV problems is\
    \ presented in a recent paper [26], which proposes a data transfer-efcient algorithm\
    \ coupled with a specialized hardware model that includes an ASIC for multi-way\
    \ merge operation and integrates state-of-the-art 3D stacked HBM. SpaceA [35],\
    \ an SpMV accelerator, leverages processing-in-memory (PIM) architectures and\
    \ integrates features to overcome irregular memory access patterns, resulting\
    \ in substantial speedup and energy savings. A novel streaming implementation\
    \ for coordinate format (COO) sparse matrix-vector multiplication shows up to\
    \ 6× speedup and higher energy efciency in another work [24]. A study identifying\
    \ challenges in developing high-performance sparse linear algebra accelerators\
    \ on HBM-equipped FPGAs is presented in [8]. The case study on SpMV leads to HiSparse,\
    \ an accelerator displaying promising speedup and bandwidth efciency over existing\
    \ solutions. Serpens [29], another HBM-based FPGA accelerator, is designed for\
    \ general-purpose sparse matrix-vector multiplication, featuring memory-centric\
    \ processing engines.\n\n# 7 Conclusions and Future Work\n\nThrough innovative\
    \ hardware/software co-design, GUST achieves a hardware utilization of 33.67%,\
    \ and speedup and energy efciency gain of 411× and 137× over 1D systolic array\
    \ using the same number of arithmetic units for SpMV. Compared to Serpens, a state-of-the-art\
    \ FPGA-based SpMV accelerator, GUST achieves lower execution time for seven out\
    \ of nine matrices, and lower energy consumption for fve of the matrices. This\
    \ performance is achieved through resource sharing among multiple rows and columns,\
    \ which is enabled through separating partial sum and partial product calculation\
    \ in the hardware, and eliminating collisions with edge-coloring scheduling in\
    \ the software. Furthermore, the efectiveness is independent from sparsity sparsity,\
    \ and can be utilized in a parallel manner, making GUST a versatile accelerator\
    \ that can be used for a wide range of applications. One possible direction for\
    \ GUST would be to investigate the possibility of resource sharing in a 2-dimensional\
    \ GUST for sparse matrix-matrix multiplication. Moreover, the notion of edge-coloring\
    \ could be extended to other applications with shared resources for removing collisions.\n\
    \nThe ideas presented here is not limited to FPGAs only, and is applicable to\
    \ any hardware platform that can provide a set of multipliers and adders, and\
    \ a crossbar connector. For example, consider GPUs. Each block of threads in a\
    \ GPU has a shared memory that functions as a crossbar connector by design. Each\
    \ thread calculates a partial product and stores it in the shared memory. The\
    \ threads then synchronize and access the shared memory to update the partial\
    \ sums. Since a GPU is structured as multiple blocks of threads, each with a limited\
    \ amount of shared memory, the implementable GUST is a small length-k GUST for\
    \ each block. Of course there are more details to consider, especially since GPUs\
    \ are often memory-bound in the case of matrix-vector multiplication.\n\n# References\n\
    \n- [1] Bahar Asgari, Ramyad Hadidi, Jiashen Cao, Da Eun Shim, Sung-Kyu Lim, and\
    \ Hyesoon Kim. Fafnir: Accelerating sparse gathering by using efcient near-memory\
    \ intelligent reduction. In 2021 IEEE International Symposium on High-Performance\
    \ Computer Architecture (HPCA), pages 908–920, 2021.\n- [2] Bahar Asgari, Ramyad\
    \ Hadidi, Joshua Dierberger, Charlotte Steinichen, Amaan Marfatia, and Hyesoon\
    \ Kim. Copernicus: Characterizing the performance implications of compression\
    \ formats used in sparse workloads. In 2021 IEEE International Symposium on Workload\
    \ Characterization (IISWC), pages 1–12. IEEE, 2021.\n- [3] Bahar Asgari, Dheeraj\
    \ Ramchandani, Amaan Marfatia, and Hyesoon Kim. Maia: Matrix inversion acceleration\
    \ near memory. In 2022 32nd International Conference on Field-Programmable Logic\
    \ and Applications (FPL), pages 277–281. IEEE, 2022.\n- [4] R. P. Brent and H.\
    \ T. Kung. A regular layout for parallel adders. IEEE Trans. Comput., 31(3):260–264,\
    \ mar 1982.\n- [5] William Dally. The Future of Computing: Domain-Specifc Architecture.\
    \ hps://www.clsac.org/uploads/5/0/6/3/50633811/2021-clsacdally.pdf, 2021. [Online;\
    \ accessed May-2023].\n- [6] William Dally. On the Model of Computation: Point:\
    \ We Must Extend Our Model of Computation to Account for Cost and Location. hps://cacm.acm.org/magazines/2022/9/263792-on-the-modelof-computation-point/abstract,\
    \ 2022. [Online; accessed May-2023].\n- [7] Timothy A Davis and Yifan Hu. The\
    \ university of florida sparse matrix collection. ACM Transactions on Mathematical\
    \ Software (TOMS), 38(1):1–25, 2011.\n- [8] Yixiao Du, Yuwei Hu, Zhongchun Zhou,\
    \ and Zhiru Zhang. Highperformance sparse linear algebra on hbm-equipped fpgas\
    \ using hls: A case study on spmv. In Proceedings of the 2022 ACM/SIGDA International\
    \ Symposium on Field-Programmable Gate Arrays, pages 54–64, 2022.\n- [9] Siying\
    \ Feng, Xin He, Kuan-Yu Chen, Liu Ke, Xuan Zhang, David Blaauw, Trevor Mudge,\
    \ and Ronald Dreslinski. Menda: a near-memory multi-way merge solution for sparse\
    \ transposition and dataflows. In Proceedings of the 49th Annual International\
    \ Symposium on Computer Architecture, pages 245–258, 2022.\n- [10] Xin He, Kuan-Yu\
    \ Chen, Siying Feng, Hun-Seok Kim, David Blaauw, Ronald Dreslinski, and Trevor\
    \ Mudge. Squaring the circle: Executing sparse matrix computations on flextpu—a\
    \ tpu-like processor. In Proceedings of the International Conference on Parallel\
    \ Architectures and Compilation Techniques, PACT '22, page 148–159, New York,\
    \ NY, USA, 2023. Association for Computing Machinery.\n- [11] Xin He, Subhankar\
    \ Pal, Aporva Amarnath, Siying Feng, Dong-Hyeon Park, Austin Rovinski, Haojie\
    \ Ye, Yuhan Chen, Ronald Dreslinski, and Trevor Mudge. Sparse-tpu: Adapting systolic\
    \ arrays for sparse matrices. In Proceedings of the 34th ACM international conference\
    \ on supercomputing, pages 1–12, 2020.\n- [12] Yuwei Hu, Yixiao Du, Ecenur Ustun,\
    \ and Zhiru Zhang. Graphlily: Accelerating graph linear algebra on hbm-equipped\
    \ fpgas. In 2021 IEEE/ACM International Conference On Computer Aided Design (IC-CAD),\
    \ pages 1–9. IEEE, 2021.\n- [13] Abhishek Kumar Jain, Hossein Omidian, Henri Fraisse,\
    \ Mansimran Benipal, Lisa Liu, and Dinesh Gaitonde. A domain-specifc architecture\
    \ for accelerating sparse matrix vector multiplication on fpgas. In 2020 30th\
    \ International conference on field-programmable logic and applications (FPL),\
    \ pages 127–132. IEEE, 2020.\n- [14] Norm Jouppi, George Kurian, Sheng Li, Peter\
    \ Ma, Rahul Nagarajan, Lifeng Nai, Nishant Patil, Suvinay Subramanian, Andy Swing,\
    \ Brian Towles, Cliford Young, Xiang Zhou, Zongwei Zhou, and David A Patterson.\
    \ Tpu v4: An optically reconfgurable supercomputer for machine learning with hardware\
    \ support for embeddings. In Proceedings of the 50th Annual International Symposium\
    \ on Computer Architecture, ISCA '23, New York, NY, USA, 2023. Association for\
    \ Computing Machinery.\n- [15] Norman P. Jouppi, Clif Young, Nishant Patil, David\
    \ Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden,\
    \ Al Borchers, Rick Boyle, Pierre-luc Cantin, Cliford Chao, Chris Clark, Jeremy\
    \ Coriell, Mike Daley, Matt Dau, Jefrey Dean, Ben Gelb, Tara Vazir Ghaemmaghami,\
    \ Rajendra Gottipati, William Gulland, Robert Hagmann, C. Richard Ho, Doug Hogberg,\
    \ John Hu, Robert Hundt, Dan Hurt, Julian Ibarz, Aaron Jafey, Alek Jaworski, Alexander\
    \ Kaplan, Harshit Khaitan, Daniel Killebrew, Andy Koch, Naveen Kumar, Steve Lacy,\
    \ James Laudon, James Law, Diemthu Le, Chris Leary, Zhuyuan Liu, Kyle Lucke, Alan\
    \ Lundin, Gordon MacKean, Adriana Maggiore, Maire Mahony, Kieran Miller, Rahul\
    \ Nagarajan, Ravi Narayanaswami, Ray Ni, Kathy Nix, Thomas Norrie, Mark Omernick,\
    \ Narayana Penukonda, Andy Phelps, Jonathan Ross, Matt Ross, Amir Salek, Emad\
    \ Samadiani, Chris Severn, Gregory Sizikov, Matthew Snelham, Jed Souter, Dan Steinberg,\
    \ Andy Swing, Mercedes Tan, Gregory Thorson, Bo Tian, Horia Toma, Erick Tuttle,\
    \ Vijay Vasudevan, Richard Walter, Walter Wang, Eric Wilcox, and Doe Hyun Yoon.\
    \ In-datacenter performance analysis of a tensor processing unit. SIGARCH Comput.\
    \ Archit. News, 45(2):1–12, jun 2017.\n- [16] Konstantinos Kanellopoulos, Nandita\
    \ Vijaykumar, Christina Giannoula, Roknoddin Azizi, Skanda Koppula, Nika Mansouri\
    \ Ghiasi, Taha Shahroodi, Juan Gomez Luna, and Onur Mutlu. Smash: Co-designing\
    \ software compression and hardware-accelerated indexing for efcient sparse matrix\
    \ operations. In Proceedings of the 52nd Annual IEEE/ACM International Symposium\
    \ on Microarchitecture, MICRO '52, page 600–614, New York, NY, USA, 2019. Association\
    \ for Computing\n\nMachinery.\n\n- [17] Hsiang Tsung Kung and Charles E Leiserson.\
    \ Systolic arrays (for vlsi). In Sparse Matrix Proceedings 1978, volume 1, pages\
    \ 256–282. Society for industrial and applied mathematics Philadelphia, PA, USA,\
    \ 1979.\n- [18] Jure Leskovec and Andrej Krevl. SNAP Datasets: Stanford large\
    \ network dataset collection. hp://snap.stanford.edu/data, June 2014.\n- [19]\
    \ Shiqing Li, Di Liu, and Weichen Liu. Optimized data reuse via reordering for\
    \ sparse matrix-vector multiplication on fpgas. In 2021 IEEE/ACM International\
    \ Conference On Computer Aided Design (ICCAD), pages 1–9. IEEE, 2021.\n- [20]\
    \ Bowen Liu and Dajiang Liu. Towards high-bandwidth-utilization spmv on fpgas\
    \ via partial vector duplication. In Proceedings of the 28th Asia and South Pacific\
    \ Design Automation Conference, pages 33–38, 2023.\n- [21] Kai Lu, Zhaoshi Li,\
    \ Leibo Liu, Jiawei Wang, Shouyi Yin, and Shaojun Wei. Redesk: A reconfgurable\
    \ dataflow engine for sparse kernels on heterogeneous platforms. In 2019 IEEE/ACM\
    \ International Conference on Computer-Aided Design (ICCAD), pages 1–8. IEEE,\
    \ 2019.\n- [22] Euripides Montagne and Rina Surós. Systolic sparse matrix vector\
    \ multiply in the age of tpus and accelerators. In 2019 Spring Simulation Conference\
    \ (SpringSim), pages 1–10, 2019.\n- [23] Subhankar Pal, Siying Feng, Dong-hyeon\
    \ Park, Sung Kim, Aporva Amarnath, Chi-Sheng Yang, Xin He, Jonathan Beaumont,\
    \ Kyle May, Yan Xiong, Kuba Kaszyk, John Magnus Morton, Jiawen Sun, Michael O'Boyle,\
    \ Murray Cole, Chaitali Chakrabarti, David Blaauw, Hun-Seok Kim, Trevor Mudge,\
    \ and Ronald Dreslinski. Transmuter: Bridging the efciency gap using memory and\
    \ dataflow reconfguration. In Proceedings of the ACM International Conference\
    \ on Parallel Architectures and Compilation Techniques, PACT '20, page 175–190,\
    \ New York, NY, USA, 2020. Association for Computing Machinery.\n- [24] Alberto\
    \ Parravicini, Francesco Sgherzi, and Marco D Santambrogio. A reduced-precision\
    \ streaming spmv architecture for personalized pagerank on fpga. In Proceedings\
    \ of the 26th Asia and South Pacific Design Automation Conference, pages 378–383,\
    \ 2021.\n- [25] Dheeraj Ramchandani, Bahar Asgari, and Hyesoon Kim. Spica: Exploring\
    \ fpga optimizations to enable an efcient spmv implementation for computations\
    \ at edge. In 2023 IEEE International Conference on Edge Computing and Communications\
    \ (EDGE), pages 36–42. IEEE, 2023.\n- [26] Fazle Sadi, Larry Fileggi, and Franz\
    \ Franchetti. Algorithm and hardware co-optimized solution for large spmv problems.\
    \ In 2017 IEEE High Performance Extreme Computing Conference (HPEC), pages 1–7.\
    \ IEEE, 2017.\n- [27] Fazle Sadi, Joe Sweeney, Tze Meng Low, James C Hoe, Larry\
    \ Pileggi, and Franz Franchetti. Efcient spmv operation for large and highly sparse\
    \ matrices using scalable multi-way merge parallelization. In Proceedings of the\
    \ 52nd Annual IEEE/ACM International Symposium on Microarchitecture, pages 347–358,\
    \ 2019.\n- [28] Björn Sigurbergsson, Tom Hogervorst, Tong Dong Qiu, and Razvan\
    \ Nane. Sparstition: a partitioning scheme for large-scale sparse matrix vector\
    \ multiplication on fpga. In 2019 IEEE 30th International Conference on Application-specific\
    \ Systems, Architectures and Processors (ASAP), volume 2160, pages 51–58. IEEE,\
    \ 2019.\n- [29] Linghao Song, Yuze Chi, Licheng Guo, and Jason Cong. Serpens:\
    \ A high bandwidth memory based accelerator for general-purpose sparse matrix-vector\
    \ multiplication. In Proceedings of the 59th ACM/IEEE design automation conference,\
    \ pages 211–216, 2022.\n- [30] Linghao Song, Yuze Chi, Atefeh Sohrabizadeh, Young-kyu\
    \ Choi, Jason Lau, and Jason Cong. Sextans: A streaming accelerator for generalpurpose\
    \ sparse-matrix dense-matrix multiplication. In Proceedings of the 2022 ACM/SIGDA\
    \ International Symposium on Field-Programmable Gate Arrays, pages 65–77, 2022.\n\
    - [31] Nitish Srivastava, Hanchen Jin, Shaden Smith, Hongbo Rong, David Albonesi,\
    \ and Zhiru Zhang. Tensaurus: A versatile accelerator for mixed sparse-dense tensor\
    \ computations. In 2020 IEEE International Symposium on High Performance Computer\
    \ Architecture (HPCA), pages\n\nGUST: Graph Edge-Coloring Utilization for Accelerating\
    \ Sparse Matrix Vector Multiplication ASPLOS '24, April 27-May 1, 2024, La Jolla,\
    \ CA, USA\n\n689–702. IEEE, 2020.\n\n- [32] Nishil Talati, Kyle May, Armand Behroozi,\
    \ Yichen Yang, Kuba Kaszyk, Christos Vasiladiotis, Tarunesh Verma, Lu Li, Brandon\
    \ Nguyen, Jiawen Sun, John Magnus Morton, Agreen Ahmadi, Todd Austin, Michael\
    \ O'Boyle, Scott Mahlke, Trevor Mudge, and Ronald Dreslinski. Prodigy: Improving\
    \ the memory latency of data-indirect irregular workloads using hardware-software\
    \ co-design. In 2021 IEEE International Symposium on High-Performance Computer\
    \ Architecture (HPCA), pages 654–667, 2021.\n- [33] Minjin Tang, Mei Wen, Yasong\
    \ Cao, Junzhong Shen, Jianchao Yang, Jiawei Fei, Yang Guo, and Sheng Liu. Mentha:\
    \ Enabling sparse-packing computation on systolic arrays. In Proceedings of the\
    \ 51st International Conference on Parallel Processing, ICPP '22, New York, NY,\
    \ USA, 2023.\n\nAssociation for Computing Machinery.\n\n- [34] Vadim Georgievich\
    \ Vizing. Critical graphs with given chromatic class. Metody Diskret. Analiz.,\
    \ 5: 9–17. (In Russian.), 1965.\n- [35] Xinfeng Xie, Zheng Liang, Peng Gu, Abanti\
    \ Basak, Lei Deng, Ling Liang, Xing Hu, and Yuan Xie. Spacea: Sparse matrix vector\
    \ multiplication on processing-in-memory accelerator. In 2021 IEEE International\
    \ Symposium on High-Performance Computer Architecture (HPCA), pages 570–583. IEEE,\
    \ 2021.\n- [36] Zhekai Zhang, Hanrui Wang, Song Han, and William J Dally. Sparch:\
    \ Efcient architecture for sparse matrix multiplication. In 2020 IEEE International\
    \ Symposium on High Performance Computer Architecture (HPCA), pages 261–274. IEEE,\
    \ 2020."
- title: "MFIT: Multi-Fidelity Thermal Modeling for 2.5D and 3D Multi-Chiplet\n  Architectures"
  abstract: 'Rapidly evolving artificial intelligence and machine learning applications

    require ever-increasing computational capabilities, while monolithic 2D design

    technologies approach their limits. Heterogeneous integration of smaller

    chiplets using a 2.5D silicon interposer and 3D packaging has emerged as a

    promising paradigm to address this limit and meet performance demands. These

    approaches offer a significant cost reduction and higher manufacturing yield

    than monolithic 2D integrated circuits. However, the compact arrangement and

    high compute density exacerbate the thermal management challenges, potentially

    compromising performance. Addressing these thermal modeling challenges is

    critical, especially as system sizes grow and different design stages require

    varying levels of accuracy and speed. Since no single thermal modeling

    technique meets all these needs, this paper introduces MFIT, a range of

    multi-fidelity thermal models that effectively balance accuracy and speed.

    These multi-fidelity models can enable efficient design space exploration and

    runtime thermal management. Our extensive testing on systems with 16, 36, and

    64 2.5D integrated chiplets and 16x3 3D integrated chiplets demonstrates that

    these models can reduce execution times from days to mere seconds and

    milliseconds with negligible loss in accuracy.'
  url: http://arxiv.org/abs/2410.09188v4
  keywords: ''
  document: '# MFIT : Multi-FIdelity Thermal Modeling for 2.5D and 3D Multi-Chiplet
    Architectures


    LUKAS PFROMM<sup>∗</sup> and ALISH KANANI<sup>∗</sup> , University of Wisconsin–Madison,
    USA HARSH SHARMA, Washington State University, USA PARTH SOLANKI and ERIC TERVO,
    University of Wisconsin-Madison, USA JAEHYUN PARK, University of Ulsan, Republic
    of Korea JANARDHAN RAO DOPPA and PARTHA PRATIM PANDE, Washington State University,
    USA UMIT Y. OGRAS, University of Wisconsin–Madison, USA


    Rapidly evolving artificial intelligence and machine learning applications require
    ever-increasing computational capabilities, while monolithic 2D design technologies
    approach their limits. 2.5D/3D heterogeneous integration of smaller chiplets using
    advanced packaging has emerged as a promising paradigm for addressing this limit
    and meeting performance demands. These approaches offer a significant cost reduction
    and higher manufacturing yield than monolithic 2D integrated circuits. However,
    the compact arrangement and high compute density of these systems exacerbate thermal
    management challenges, potentially compromising performance. Addressing these
    thermal modeling challenges is critical, especially as system sizes grow and different
    design stages require varying levels of accuracy and speed. Since no single thermal
    modeling technique meets all these needs, this paper introduces MFIT, a range
    of multi-fidelity thermal models that effectively balance accuracy and speed.
    These multi-fidelity models can enable efficient design space exploration and
    runtime thermal management. Our extensive testing on systems with 16, 36, and
    64 2.5D integrated chiplets and 16×3 3D integrated chiplets demonstrates that
    these models can reduce execution times from days to mere seconds and milliseconds
    with negligible loss in accuracy.


    #### ACM Reference Format:


    Lukas Pfromm, Alish Kanani, Harsh Sharma, Parth Solanki, Eric Tervo, Jaehyun Park,
    Janardhan Rao Doppa, Partha Pratim Pande, and Umit Y. Ogras. 2025. MFIT : Multi-FIdelity
    Thermal Modeling for 2.5D and 3D Multi-Chiplet Architectures. 1, 1 (May 2025),
    [26](#page-25-0) pages.<https://doi.org/XXXXXXX.XXXXXXX>


    # 1 INTRODUCTION


    Massive data from different modalities, including text, images, video, and speech,
    are continuously produced by various sensors. At the same time, increasingly complex
    artificial intelligence (AI) and machine learning (ML) algorithms process this
    data to enable new applications that were previously impractical. This trend dictates
    the design of large-scale chips with high memory and compute capabilities, offering
    a high degree of parallelism [\[2,](#page-23-0) [6\]](#page-23-1). Traditional
    2D chip design and packaging technologies cannot sustain this need due to the
    low yield of large


    <https://doi.org/XXXXXXX.XXXXXXX>


    <sup>∗</sup>Both authors contributed equally to this research.


    Authors'' addresses: Lukas Pfromm, pfromm@wisc.edu; Alish Kanani, ahkanani@wisc.edu,
    University of Wisconsin–Madison, Madison, Wisconsin, USA; Harsh Sharma, Washington
    State University, Pullman, Washington, USA; Parth Solanki; Eric Tervo, University
    of Wisconsin-Madison, Madison, Wisconsin, USA; Jaehyun Park, University of Ulsan,
    Ulsan, Republic of Korea; Janardhan Rao Doppa; Partha Pratim Pande, Washington
    State University, Pullman, Washington, USA; Umit Y. Ogras, University of Wisconsin–Madison,
    Madison, Wisconsin, USA.


    Permission to make digital or hard copies of all or part of this work for personal
    or classroom use is granted without fee provided that copies are not made or distributed
    for profit or commercial advantage and that copies bear this notice and the full
    citation on the first page. Copyrights for components of this work owned by others
    than the author(s) must be honored. Abstracting with credit is permitted. To copy
    otherwise, or republish, to post on servers or to redistribute to lists, requires
    prior specific permission and/or a fee. Request permissions from permissions@acm.org.


    <sup>©</sup> 2025 Copyright held by the owner/author(s). Publication rights licensed
    to ACM. XXXX-XXXX/2025/5-ART \$15.00


    <span id="page-1-0"></span>![](_page_1_Figure_1.jpeg)


    (a) A 16 - 2.5D integrated chiplet-based system. The magnified view on the right
    shows a detailed structure of a single chiplet.


    ![](_page_1_Figure_3.jpeg)


    **Substrate**


    **µ-bumps C4 bumps**


    Fig. 1. 2.5D/3D integrated chiplet systems considered in this work, showing the
    chiplets, interposer, and part of the substrate.


    monolithic planar chips and the corresponding increase in fabrication cost [\[15\]](#page-23-2).
    Therefore, new design approaches are required to meet the increasing demand for
    computing power and memory capacity [\[2\]](#page-23-0).


    2.5D and 3D chiplet-based architectures have emerged as promising alternatives
    to traditional monolithic 2D chips due to their lower fabrication costs [\[29,](#page-24-0)
    [47,](#page-25-1) [51\]](#page-25-2). Compared to conventional monolithic systems,
    chipletbased systems integrate multiple small pre-fabricated chips (chiplets)
    on a silicon interposer, which facilitates data exchange, as illustrated in Fig.
    [1a.](#page-1-0) 3D packaged systems expand on this approach by stacking multiple
    chiplets vertically and connecting them with vertical vias, creating a more compact
    system as illustrated in Fig. [1b.](#page-1-0) The smaller size of these chiplets
    enables a higher yield and lower overall manufacturing cost than traditional monolithic
    dies [\[57\]](#page-25-3). Additionally, this modular approach facilitates scaling
    the system sizes and enables heterogeneous integration of different chiplet types,
    e.g., memory, processing, and processing-in-memory chiplets. Hence, emerging 2.5D
    and 3D architectures enable a new cost-effective avenue for compact scale-out
    implementations of various emerging compute- and data-intensive applications,
    including AI/ML. Indeed, these advantages have led to industrial adoption by companies
    including Intel [\[10,](#page-23-3) [60\]](#page-25-4), AMD [\[4,](#page-23-4)
    [8,](#page-23-5) [41\]](#page-24-1), and NVIDIA [\[47\]](#page-25-1).


    Thermal bottlenecks have long been a significant barrier to increasing the performance
    of computing systems. 2.5D and 3D integrated systems exacerbate this barrier due
    to their dense integration and unique physical structure [\[44\]](#page-24-2).
    Unlike monolithic chips, where heat spreads uniformly across a continuous die,
    2.5D chiplet-based systems conduct heat laterally through the interposer and vertically
    through the heat spreader. Although the interposer, often built with materials
    of relatively high thermal conductivity, can help alleviate localized thermal
    hotspots, chiplet-based designs introduce new sources of thermal stress. In particular,
    a single package can accommodate more active silicon by integrating multiple smaller
    chiplets, thereby increasing the total compute throughput and system power. If
    the total power consumption increases faster than the area, the package-level
    power density also becomes larger than a monolithic chip. Furthermore, while the
    interposer aids in thermal spreading, it also hosts numerous communication links
    whose resistive Joule heating contributes to additional thermal challenge [\[37\]](#page-24-3).
    Likewise, heat also flows vertically between adjacent stacked chiplets in a 3D
    chiplet-based system.


    These factors introduce unique challenges for effective thermal management in
    these systems. Traditional design flows and physical floorplanning focus on reducing
    wire lengths to meet timing constraints and minimizing area to reduce fabrication
    costs. However, these objectives could also lead to thermal crosstalk, thermal
    hotspots, and compromise performance. Chiplet-based systems introduce additional
    design parameters such as inter-chiplet link length, spacing, chiplet placement,
    sizing, inter-layer communication, and design partitioning. Tuning traditional
    and chiplet-based design parameters while maintaining thermal stability is critical
    to ensure a thermally-efficient design.


    #### MFIT : Multi-FIdelity Thermal Modeling for 2.5D and 3D Multi-Chiplet Architectures
    • 3


    <span id="page-2-0"></span>


    | Increasing error |                                                        |                                                              |
    Increasing execution time                            |                                                     |  |

    |------------------|--------------------------------------------------------|--------------------------------------------------------------|------------------------------------------------------|-----------------------------------------------------|--|

    |                  |                                                        |
    FEM Models                                                   | Analytical Models                                    |                                                     |  |

    |                  | 1. Fine-grained                                        |
    2. Abstracted                                                | 3. Thermal RC Model                                  |
    4. Discrete State-Space<br>(DSS)                    |  |

    | Features         | Most accurate (e.g., real<br>µbumps, links geometries) |
    Replaced micro-structures<br>with equivalent material blocks | Independent of
    specific<br>geometry, continuous time | Tuned for a specific<br>architecture,
    discrete time |  |

    | Error            | Golden reference                                       |
    < 0.5 °C                                                     | < 1.7 °C                                             |
    Same as thermal RC                                  |  |

    | Exe. time        | Not possible to model<br>entire package                |
    Days                                                         | Seconds                                              |
    Milliseconds                                        |  |

    | Use case         | Validate the abstracted<br>FEM models                  |
    Ground truth to tune C values in<br>Thermal RC model         | Thermal-aware DSE,
    reference<br>for DSS model        | Large-scale optimization,<br>thermal management     |  |

    | Described in     | Section 3                                              |
    Section 4.2                                                  | Section 4.3                                          |
    Section 4.4                                         |  |


    Fig. 2. Summary of the multi-fidelity thermal models. (1) Fine-grained FEM models
    capture precise geometry but are too complex to simulate the entire chiplet-based
    system. (2) Abstracted FEM models are derived from the fine-grained model to simulate
    large-scale systems with negligible impact on accuracy (< 0.5 ◦C, which is 0.5–1%
    around the temperatures of interest). (3) Since abstract FEM models are still
    too slow for DSE, they are used to tune thermal RC circuit models, which introduce
    less than 1.7◦C (1–3.5% around the temperatures of interest) error). (4) Further
    abstraction reduces the execution time to milliseconds using DSS models developed
    for specific system configurations, enabling runtime thermal management.


    The semiconductor chip design cycle spans multiple phases: system specification,
    architecture exploration, logic design, physical design and validation, fabrication,
    and post-silicon optimization/validation. Each phase has a unique set of design
    constraints and requirements. For example, the lack of a test chip during the
    pre-silicon phases requires simulation and analytical models. Finite Element Method
    (FEM) simulations offer the most accurate approach for pre-silicon thermal analysis
    [\[58\]](#page-25-5). They can serve as a reference and enable heat flow studies
    to guide the design process. However, they are too slow for practical architecture
    and design space exploration (DSE), as illustrated in Fig. [2.](#page-2-0) Modeling
    the package as a thermal RC (resistive-capacitive) network can significantly accelerate
    simulations with acceptable accuracy loss [\[52,](#page-25-6) [56\]](#page-25-7).
    Since each node in the thermal circuit corresponds to a specific location in the
    package, thermal RC models solve discretized versions of the FEM models in space.
    Hence, they enable thermally-aware DSE and optimization with a finite number of
    discrete hotspot nodes. However, the thermal resistance/capacitance values and
    the circuit topology must accurately reflect the chip geometry and material properties
    for reliable results. Since the thermal RC models solve continuous-time ordinary
    differential equations (ODEs), they have execution times in the order of seconds
    to minutes. Therefore, they cannot be used for runtime optimization tasks such
    as dynamic thermal and power management (DTPM). With a given sampling period,
    one can discretize them in the time domain. The resulting discrete state-space
    (DSS) models significantly reduce runtime at the cost of further abstracting the
    model from the physical package. Consequently, they apply only to the specific
    configurations for which they are developed.


    There is a strong need for tools to accurately analyze the thermal behavior of
    2.5D and 3D integrated systems and guide their design process. However, no single
    modeling technique can alone address the needs of all design phases. To address
    this critical gap, this paper proposes MFIT, a multi-fidelity thermal modeling
    framework that synergistically exploits the strengths of each class of models
    (FEM, thermal RC, and DSS). We use this framework to produce a set of thermal
    models that can guide the entire design cycle, unlike a point solution that can
    serve a specific portion of the design process. The elements of this set not only
    cover complementary parts of the design cycle but support each other and produce
    consistent results. We first develop a fine-grained FEM model of the target package
    as a reference. Since it is slow and computationally expensive, we next judiciously
    design an abstracted version of this fine-grained FEM model to simulate an entire
    package in days while maintaining accuracy. To enable fast DSE, MFIT also incorporates
    thermal RC circuit models verified against the reference FEM models. Our thermal
    RC models run in the order of seconds while leading to less than 1.7◦C error when
    the package


    temperature is around 100◦C, as summarized in Fig. [2.](#page-2-0) Hence, they
    can be used for pre-silicon architectural optimization, such as mapping the workloads
    to chiplets, network-on-interposer design, and chiplet placement for 2.5D and
    3D stacked systems. Finally, MFIT derives a final class of models by discretizing
    the thermal RC models, enabling runtime thermal management and large-scale DSE
    in the order of milliseconds. Since these discrete models are generated for a
    specific sampling period and configuration, they are regenerated from the RC model
    if the target configuration changes automatically in a few milliseconds. In summary,
    we obtain a set of multi-fidelity thermal models that guide and complement each
    other to cover all design phases.


    The key contributions of this work are as follows:


    - A novel thermal modeling approach that systematically abstracts fine-grained
    FEM models to produce abstract FEM, thermal RC, and DSS models to achieve varying
    speed and accuracy trade-offs,

    - A family of open-source multi-fidelity thermal models that span a broad accuracy
    (reference to 1.7◦C) and speed (days to milliseconds) range,

    - Extensive evaluations with 16, 36, and 64 2.5D and 16×3 3D integrated chiplets
    systems running AI/ML workloads to demonstrate the accuracy and speed-up benefits
    of our multi-fidelity thermal models,

    - Open-sourced code for thermal RC and DSS models at [github.com/AlishKanani/MFIT.](https://github.com/AlishKanani/MFIT)
    The FEM models created for this work are included as well.


    The remainder of the paper is organized as follows. Section [2](#page-3-0) and
    Section [3](#page-5-0) discuss related work and background on FEM. Section [4](#page-6-0)
    presents the proposed multi-fidelity thermal modeling framework. Finally, Section
    [5](#page-14-0) presents the experimental evaluation, and Section [6](#page-23-6)
    concludes the paper.


    #### <span id="page-3-0"></span>2 RELATED WORK


    2.5D and 3D integration-based systems are becoming mainstream due to higher performance
    and lower manufacturing costs than monolithic chips. Both domain-specific and
    general-purpose 2.5D and 3D architectures have been explored to date [\[4,](#page-23-4)
    [8,](#page-23-5) [19,](#page-23-7) [28,](#page-24-4) [47,](#page-25-1) [50,](#page-25-8)
    [51\]](#page-25-2). SIMBA is one of the first prototype multi-chip modules with
    36 chiplets designed for inference with deep models [\[47\]](#page-25-1). Similarly,
    Floret is a data center-scale architecture for accelerating convolutional neural
    network (CNN) inference tasks by exploiting dataflow patterns [\[51\]](#page-25-2).
    Loi et al. [\[36\]](#page-24-5) analyze the performance benefits of vertically
    integrated (3D) processor-memory hierarchy under thermal constraints. Similarly,
    Eckert et al. [\[19\]](#page-23-7) consider processing-in-memory (PIM) architectures
    implemented using 3D die stacking. They study the thermal constraints across different
    processor organizations and cooling solutions to identify viable solutions. Our
    proposed open-source thermal models catalyze similar thermal analysis and optimization
    studies for 2.5D and 3D integrated systems.


    The most accurate and direct thermal evaluation approach is temperature measurements
    on a hardware system. It can be performed using thermal imaging [\[5,](#page-23-8)
    [45\]](#page-24-6) or temperature sensors [\[65,](#page-25-9) [66\]](#page-25-10).
    However, the availability of the target hardware is a significant limitation.
    For example, large-scale 2.5D and 3D chiplet systems with tens of chiplets do
    not exist yet, while smaller prototypes and commercial systems provide limited
    insights applicable for larger systems [\[41,](#page-24-1) [64\]](#page-25-11).
    This limitation motivated FEM-based modeling as the most accurate way to analyze
    the heat flow and temperature [\[58\]](#page-25-5). Proprietary software, such
    as ANSYS Fluent [\[39\]](#page-24-7) and COMSOL [\[18\]](#page-23-9), are commonly
    used for FEM simulations. Since FEM suffers from computational cost, detailed
    FEM solutions are suitable only for small designs and validating analytical models
    [\[39\]](#page-24-7). For example, the authors of [\[68\]](#page-25-12) employ
    FEM to simulate a two-chiplet system on an interposer. They employ abstracted
    FEM models for both -bumps and C4 bumps to speed up the process, effectively reducing
    computational complexity before tackling the entire system.


    The computational overhead and impractically high execution time of FEM solvers
    motivate analytical models that enable rapid thermal evaluation in early design
    phases. The most common method involves constructing thermal RC networks and solving
    the corresponding system of ODEs. Popular thermal simulators such as


    |              | Non-uniform<br>grid | Anisotropic<br>materials | Non-homogeneous<br>layers
    | Heat dissipation from both<br>boundaries of the package | Flexible with<br>Architecture
    changes |

    |--------------|---------------------|--------------------------|---------------------------|---------------------------------------------------------|---------------------------------------|

    | Hotspot [53] | ×                   | ×                        | ✓                         |
    ✓                                                       | ✓                                     |

    | PACT [62]    | ×                   | ×                        | ✓                         |
    ×                                                       | ✓                                     |

    | 3D-ICE [56]  | ✓                   | ×                        | ×                         |
    ×1                                                      | ✓                                     |

    | Thermal RC   | ✓                   | ✓                        | ✓                         |
    ✓                                                       | ✓                                     |

    | DSS          | ✓                   | ✓                        | ✓                         |
    ✓                                                       | ×                                     |


    <span id="page-4-1"></span>Table 1. Qualitative analysis of existing analytical
    thermal models and our proposed thermal RC and DSS models. A detailed quantitative
    runtime comparison is completed in Section [5.](#page-14-0)


    HotSpot [\[52\]](#page-25-6) leverage this method, focusing on the microarchitectural
    layout blocks to facilitate design space exploration and early-stage thermal-aware
    layout and placement. Similarly, 3D-ICE [\[56\]](#page-25-7) models liquid cooling
    with microchannels embedded between silicon layers. PACT [\[62\]](#page-25-14)
    also employs a similar methodology by utilizing SPICE tools as solvers, focusing
    on standard-cell-level thermal analysis for 2.5D systems. However, these tools
    lack essential features required for accurately modeling chiplet-based heterogeneous
    systems and are not fast enough for large-scale, thermally-aware design space
    exploration (DSE) of multi-chiplet 2.5D and 3D integrated systems, as summarized
    in Table [1](#page-4-1) and detailed next.


    2.5D and 3D packages often involve materials with varying thermal conductivity
    across different directions. For example, the thermal conductivity of the C4 layer
    is higher in the vertical direction than in the lateral direction. The existing
    thermal models [\[52,](#page-25-6) [56,](#page-25-7) [62\]](#page-25-14) do not
    account for these anisotropic properties. Moreover, [\[52,](#page-25-6) [62\]](#page-25-14)
    assume a uniform grid size for all material layers (e.g., interposer, C4 bumps,
    chiplets). Non-uniform grids are useful for simulating regions with substantial
    spatial variation in temperature without unnecessarily increasing computational
    costs for simulating regions of moderate temperature variation. While 3D-ICE supports
    nonuniform grids for 3D architectures, it does not allow non-homogeneous layers,
    where different regions within the same layer can have distinct material properties.
    This limitation is particularly significant for emerging technologies, such as
    active interposer-based systems where chiplets are embedded [\[59\]](#page-25-15).
    In contrast, our multifidelity models address these limitations by enabling variable
    thermal conductivity across different directions and allowing flexible grid sizes
    for each layer and block. Additionally, our models support heat dissipation from
    both package boundaries, a crucial aspect overlooked by some existing approaches.
    This capability is essential for accurately modeling advanced cooling techniques,
    such as immersion cooling systems and flip-chip packages, which rely on efficient
    heat dissipation through both package boundaries to maintain optimal performance
    and reliability [\[23,](#page-24-8) [35\]](#page-24-9). Table [1](#page-4-1) provides
    a detailed qualitative comparison of existing thermal modeling tools. Detailed
    quantitative runtime comparisons between MFIT and state-of-the-art approaches
    are presented in Section [5.](#page-14-0)


    Architecture-level thermal RC models are commonly used for offline studies. For
    example, temperature sensor placement requires using a thermal model [\[16,](#page-23-10)
    [63\]](#page-25-16). Similarly, thermally-aware chiplet placement techniques in
    [\[20,](#page-23-11) [33,](#page-24-10) [38\]](#page-24-11) rely on the use of
    architectural thermal simulators to optimize the placement of chiplets and identify
    possible hotspots. While existing simulators can provide the functionality required
    for these studies, their prohibitive runtime prevents more extensive evaluations
    from being performed. Faster thermal simulation tools that do not sacrifice accuracy
    are required to enable more comprehensive work.


    However, existing architectural simulators are inadequate for dynamic thermal
    and power management (DTPM). DTPM requires a much faster temperature estimation
    time, on the order of milliseconds, for real-time temperature management [\[9,](#page-23-12)
    [48\]](#page-25-17). DSS models address this need by deriving a discrete-time
    linear time-invariant system that models the thermal dynamics at fixed locations
    as a function of the power consumption. For instance,


    <span id="page-4-0"></span><sup>1</sup>With Non-uniform grid, 3D-ICE does not
    support secondary heat flow path.


    TILTS [\[21\]](#page-24-12) discretizes the power inputs to the chip over fixed
    time intervals to accelerate thermal simulations. Hence, it needs to be reproduced
    when the timing requirements or the underlying hardware configuration change.
    The speedup gain offsets the loss of the explicit connection to the hardware parameters
    (e.g., thermal conductance and capacitance) and generality.


    The results of FEM, thermal RC, or other thermal simulations can also be used
    to train physics-informed machine learning techniques to model heat transfer in
    integrated circuits to reduce the thermal modeling effort [\[12\]](#page-23-13).
    For example, a recent technique collects data from numeric simulations and trains
    a random forest model to predict the convection heat transfer coefficients for
    a nonlinear heat transfer problem [\[30\]](#page-24-13). Similarly, Hwang et al.,
    [\[25\]](#page-24-14) present closed-form models derived from numerical simulations
    for tapered micro-channels to analyze the heat transfer performance as a function
    of the channel geometry. In contrast to individual classes of thermal models,
    this work proposes a framework to produce a family of multi-fidelity thermal models
    for 2.5D and 3D chiplet-based systems. The specific set of models designed with
    this framework covers a wide range of accuracy and execution time trade-offs,
    making them suitable for different design phases. Additionally, they can be augmented
    by additional models, such as physics-informed ML models, with complementary accuracy
    and execution time trade-offs.


    #### <span id="page-5-0"></span>3 FINITE ELEMENT METHOD FOR THERMAL ANALYSIS


    FEM analysis begins by dividing the problem domain into small finite elements,
    converting the continuous governing partial differential equations (PDEs) into
    algebraic equations. Next, the system''s geometry is broken down into a lattice
    of small discrete cells called a "mesh" which approximates a larger, continuous
    block [\[39\]](#page-24-7). After applying the PDEs and boundary conditions to
    each element, the equations are assembled into a global algebraic system, maintaining
    continuity between adjacent elements. This global system, representing the discretized
    PDEs over the whole domain, is solved numerically for the field variables at each
    mesh node. In this work, only the equation governing solid conduction is solved
    [\[58\]](#page-25-5):


    $$\nabla \cdot \left[ k \nabla T \right] + \dot{q} = \rho C\_v \frac{\partial
    T}{\partial t} \tag{1}$$


    where is the thermal conductivity, is the temperature, ¤ is the heat generation
    rate, is the density, and is the volumetric specific heat.


    ## 3.1 Stages of the FEM Simulation Pipeline


    Performing FEM simulations involves several key processing steps visualized in
    Fig. [3.](#page-6-1) First, the geometry, a 3D representation of the 2.5D or 3D
    integrated package, is created using computer-aided design tools. While increasing
    the detail in the geometry allows for more accuracy in modeling the system, it
    also increases the simulation time. This geometry should be as detailed as possible
    while allowing the setup and simulation to be completed within the given time
    constraints. Next, a volumetric mesh is generated by transforming the 3D model
    into one consisting of many individual cells on which the FEM software operates.
    A mesh sensitivity study is performed to determine if the mesh quality is sufficient.
    In this process, simulations are run with a progressively finer mesh while output
    parameters (such as temperature) are monitored. The mesh quality is considered
    to be sufficient when increasing the mesh granularity no longer impacts the temperature
    output.


    Once an acceptable mesh has been created, it is imported into the solver. The
    simulation is then set up, including boundary conditions, material parameters,
    power source terms, and other general model parameters. Our specific system setup
    is expanded upon in Section [5.](#page-14-0) Finally, the FEM software simulates
    the model by solving the governing equations.


    <span id="page-6-1"></span>![](_page_6_Figure_1.jpeg)


    Fig. 3. An illustration of the FEM pipeline.


    #### 3.2 Impracticality of FEM Simulations


    While FEM simulations offer high accuracy, they are impractical for DSE or runtime
    thermal management due to their time-consuming setup and operation. The process
    of geometry creation, meshing, solver setup, and execution is intricate and often
    exceeds the simulation runtime itself. Because the simulation process requires
    multiple iterations for reliable results, this time overhead quickly becomes prohibitive.
    The setup of 2.5D or 3D integrated systems is especially complex due to the large
    number of discrete power sources. These systems also involve numerous small and
    large bodies, dramatically increasing the computational complexity and the solver
    runtime [\[39\]](#page-24-7). Simulation times range from hours to days, directly
    impacted by geometric detail, complexity, size, and setup parameters such as the
    time step. Consequently, analyzing 2.5D or 3D integrated systems with intricate
    geometries and operating conditions using FEM simulations becomes prohibitively
    time-consuming, highlighting the need for alternative approaches.


    #### <span id="page-6-0"></span>4 MULTI-FIDELITY THERMAL MODELING


    #### 4.1 Overview of the Proposed Approach


    Our multi-fidelity thermal model set involves four individual models, visualized
    in Fig. [4.](#page-7-0) The process of creating these models is identical for
    any packaging technology. In this work, we consider 2.5D chiplet on silicon interposers
    and 3D systems integrated with -bumps [\[4,](#page-23-4) [32\]](#page-24-15).
    MFIT supports other integration methods and packaging technologies as well, as
    described below.


    We start by creating fine-grained FEM models of specific components within the
    package. For example, we model individual links within the interposer and -bumps
    which are used to connect chiplets to the interposer and between 3D stacked chiplets.
    The cost of this level of detail is the model complexity and execution time that
    limits the simulation scope. Therefore, these fine-grained models are used as
    a reference to design abstracted FEM models, as explained in the following subsection.
    This abstraction enables system-level FEM simulations of systems with significantly
    higher chiplet counts than would otherwise be possible, with negligible accuracy
    loss. All FEM simulations are performed using ANSYS Fluent [\[39\]](#page-24-7).
    The third class of model in the MFIT framework is the thermal RC model. Since
    these models are constructed using the geometry and material parameters of the
    system, a new RC model of a different system configuration can be created without
    re-running FEM simulations, allowing for rapid DSE. Finally, the continuous-time
    state-space equations that govern the thermal RC models are discretized with a
    given sampling period to create DSS models as detailed in Section [4.4.](#page-13-0)


    #### 4.2 Fine-Grained to Abstracted FEM Modeling


    First, fine-grained models of key system components are constructed with as much
    detail as possible. Fine-grained modeling of the entire system at the highest
    level of detail is infeasible due to the memory, CPU, and execution time requirements.
    The second step is systematically designing abstracted models by replacing detailed
    structures with homogeneous blocks. During this process, we find the material
    parameters for these blocks such that their thermal behavior matches the original
    structure.


    MFIT focuses on two structures within a chiplet-based package for this work: the
    -bumps connecting each chiplet to the interposer and 3D stacked chiplets to each
    other and the links that enable communication between chiplets. The rationale
    behind selecting these components is elaborated on in the following subsections.
    These two structures are present in both 2.5D and 3D chiplet-based packages, as
    shown in Figs. [5](#page-8-0) and [6,](#page-8-1) and the results of the abstraction
    experiments are applied to both the 2.5D and 3D full-system abstracted models.


    While we apply our abstract modeling approach to only two structures in this work,
    the same approach applies to other structures in the package, such as the substrate
    or C4 bumps. In addition to these abstractions, MFIT also models the heatsink
    as a heat transfer coefficient (HTC) instead of a physical model. This choice
    removes the need to model fluids in our simulations, as fluid flow in our simulations
    is used only for convective heat transfer in the heatsink.


    Next, we present our abstraction techniques and their validation instead of leaving
    them to the experimental evaluation to improve the readability and to justify
    larger-scale experiments performed using the abstract models.


    <span id="page-7-1"></span>4.2.1 -bump Abstracted Model. The -bumps are particularly
    important for thermal behavior due to their placement in the 2.5D and 3D integrated
    systems. In the 2.5D system, the -bumps are one of two paths to dissipate heat
    from a chiplet, as illustrated in Fig. [5.](#page-8-0) Similarly, in the 3D system,
    all heat flow for the lower two chiplets in each stack must travel through one
    of two -bump layers, as shown in Fig. [6.](#page-8-1) Due to the density and the
    total number of -bumps present, which number in the thousands, it is impractical
    to simulate an entire package with individually modeled -bumps in FEM. Therefore,
    a small block of the -bump layer, along with the associated upper and lower layer,
    is simulated in isolation to determine thermal properties that can be applied
    to the final abstracted models as illustrated in Fig. [7.](#page-8-2)


    First, the detailed block containing -bumps and underfill material is simulated
    with static heat flux and convection boundaries, which are applied to create a
    measurable thermal gradient across the -bump layer. Then, the thermal conductivity
    is calculated as:


    <span id="page-7-2"></span>

    $$k = \frac{\dot{q} \cdot l}{A \cdot \Delta T} \tag{2}$$


    <span id="page-7-0"></span>![](_page_7_Figure_7.jpeg)


    Fig. 4. The proposed workflow process to produce the multi-fidelity set of thermal
    models, starting with the most accurate yet slow models and deriving faster models.


    MFIT : Multi-FIdelity Thermal Modeling for 2.5D and 3D Multi-Chiplet Architectures
    • 9


    <span id="page-8-0"></span>![](_page_8_Figure_1.jpeg)


    Fig. 5. Cross section of a 2.5D integrated system on Si-interposer, showing abstracted
    blocks for the -bumps, C4 bumps, and link structures.


    <span id="page-8-1"></span>![](_page_8_Figure_3.jpeg)


    <span id="page-8-2"></span>Fig. 6. Cross section of a 3D integrated system, showing
    abstracted blocks for the -bumps, C4 bumps and link structures.


    ![](_page_8_Figure_5.jpeg)


    Fig. 7. Temperature contour of a -bump layer subsection.


    where ¤ is the heat flow rate, is the thickness of the material, is the cross-sectional
    area, and Δ is the temperature difference across the material [\[43\]](#page-24-16).
    Thermal capacitance and specific heat are calculated via weighted body average
    [\[31\]](#page-24-17). These parameters are applied to a model containing a homogeneous
    block in place of the previously modeled -bumps and underfill material. Finally,
    the same boundary conditions are used, as shown in Fig. [7.](#page-8-2)


    We observe identical temperature drop across the -bump layer of the abstracted
    model and less than a tenth of a degree difference in interface temperatures in
    this sub-block, as presented in Table [2,](#page-9-0) while achieving approximately
    1.5x speedup.


    4.2.2 Link Abstracted Model. A link refers to a group of wires embedded in the
    interposer that interconnect chiplets. Depending on the thermal crosstalk through
    these links, the Network-on-Interposer (NoI) architecture can influence the overall
    thermal behavior of a system. To understand the significance of accurately modeling
    links in full-system thermal simulations, we evaluated three configurations for
    a two-chiplet package:


    1. Full-detail link model: This configuration explicitly models each horizontal
    and vertical wire segment in the interposer layer, including their placement and
    density between the chiplets. Specifically, we implemented two


    | Model              | Upper surface | Lower surface | Temp.     |

    |--------------------|---------------|---------------|-----------|

    |                    | Temp. (◦C)    | Temp. (◦C)    | Drop (◦C) |

    | Detailed 𝜇-bumps   | 39.13         | 31.05         | 8.08      |

    | Abstracted 𝜇-bumps | 39.26         | 31.18         | 8.08      |


    <span id="page-9-0"></span>Table 2. Temperature results of the -bump block abstraction
    experiments. Only a single result is shown for brevity.


    ×64 Advanced Package Lane interfaces, following the bump map layout described
    in the UCIe specification [\[49\]](#page-25-18). This detailed model captures
    the realistic routing and thermal characteristics of Redistribution Layer (RDL)-based
    interposer connections.


    2. Abstracted block model: Here, the link is represented as a single thermal block
    with averaged properties, simplifying its spatial granularity.


    3. No-link model: In this case, the link is completely omitted from the thermal
    model.


    We use two different power configurations: (a) the power dissipation is static
    over time, and (b) it varies dynamically over time. These power consumption profiles
    are applied to one chiplet while the temperature of the other chiplet is calculated
    through FEM simulation. The mean absolute error (MAE) and average percentage error
    of the receiving chiplet temperature compared to the detailed model case are recorded
    in Table [3.](#page-9-1) Only a minimal accuracy loss is observed for both cases,
    while execution time savings are significant, as shown in Table [4.](#page-9-2)
    Therefore, we choose not to model links in our full-system simulations.


    <span id="page-9-1"></span>Table 3. MAE and Average Percentage Error comparison
    between abstracting and removing the links compared to detailed link modeling
    while operating between 60◦C and 100◦C.


    | Power            | Steady-State | Steady-State | Transient | Transient    |

    |------------------|--------------|--------------|-----------|--------------|

    |                  | MAE (◦C)     | Avg. % Error | MAE (◦C)  | Avg. % Error |

    | Abstracted links | 0.05◦C       | 0.083%       | 0.02◦C    | 0.045%       |

    | No links         | 0.34◦C       | 0.429%       | 0.13◦C    | 0.259%       |


    <span id="page-9-2"></span>Table 4. Execution time for Detailed, Abstract, and
    No Links experiments with steady and transient power inputs.


    | Power                      | Steady<br>Exe. time (min) | Transient<br>Exe. time
    (min) |  |

    |----------------------------|---------------------------|------------------------------|--|

    | Detailed links             | 489.23                    | 503.86                       |  |

    | Abstract links<br>No links | 164.29<br>123.80          | 172.64<br>132.13             |  |


    4.2.3 TSV Abstracted Model. Silicon interposers with through silicon vias (TSVs)
    are one of the prominent methods for integrating multiple chiplets into a single
    package. In silicon interposers and 3D stacked chips, TSVs are crucial structures
    to conduct signals and power through the chip to layers above and below. We develop
    and validate abstracted TSV models similar to the -bump modeling presented in
    Section [4.2.1.](#page-7-1)


    The first step is modeling a detailed block of the TSV layer, including the TSVs
    themselves and the surrounding silicon material. Additional blocks are placed
    above and below this layer to represent adjacent layers present in the full system.
    To model the thermal conductivity of the TSV later, heat flux and convection are
    applied to the


    top and bottom blocks, respectively. This standard process [\[43\]](#page-24-16)
    a temperature difference ΔT across the TSV layer, as visualized in Fig. 7 for
    the u-bump layer. Then, Equation [2](#page-7-2) is used to find the effective
    thermal conductivity .


    The second step is to create the abstracted thermal model, where the TSV layer
    is converted from a detailed representation to a single homogeneous block. The
    effective thermal conductivity found using the detailed model above is applied
    to this homogeneous block. Then, the same heat flux and convection which were
    applied to the detailed model are applied to this abstracted geometry. Finally,
    the temperature difference is measured again at the top and bottom of the TSV
    layer and compared to the temperature difference found using the detailed model.


    The results of these experiments are shown in Table [5](#page-10-0) for two separate
    TSV configurations. The first configuration considers 15m diameter TSVs with a
    pitch of 25m [\[61\]](#page-25-19). The second one considers 5m diameter TSVs
    with a pitch of 10m [\[46\]](#page-24-18). Results show that the temperature drop
    across the TSV layer is within 0.5 ◦C for both the large and small TSV diameter
    cases. Additionally, the speedup from modeling the detailed to abstract TSV case
    is approximately 1.8x, in line with the -bump abstraction speedup.


    <span id="page-10-0"></span>


    | TSV type                  | Model      | Top surface<br>temperature (◦C) | Bottom
    surface<br>Temperature (◦C) | Temperature<br>Drop (◦C) |

    |---------------------------|------------|---------------------------------|------------------------------------|--------------------------|

    | 15𝜇m diameter, 25𝜇m pitch | Detailed   | 85.03                           | 60.93                              |
    24.10                    |

    |                           | Abstracted | 84.77                           | 61.18                              |
    23.59                    |

    | 5𝜇m diameter, 10𝜇m pitch  | Detailed   | 85.95                           | 61.11                              |
    24.84                    |

    |                           | Abstracted | 85.87                           | 61.18                              |
    24.69                    |


    Table 5. Temperature results of the TSV block abstraction experiments. Results
    are shown


    <span id="page-10-2"></span>4.2.4 Heatsink Abstracted Model. FEM simulations that
    involve a heatsink must model the convective heat transfer from the system to
    the atmosphere using fluid models [\[68\]](#page-25-12). However, modeling the
    fluid dynamics dramatically increases the setup and runtime of FEM simulations.
    Additionally, the geometry must be modified for every different heatsink configuration,
    further increasing the time needed for design iteration. Due to their complexity,
    high-performance cooling methods, such as liquid cooling, are also difficult to
    model in FEM-based simulations.


    In MFIT, we remove the need to model the heatsink by abstracting the cooling solution
    to a single HTC. This coefficient is applied to the top of the lid where a heatsink
    is typically attached. Modeling a heatsink as a HTC is a common practice for many
    different cooling solutions [\[27,](#page-24-19) [42\]](#page-24-20). This approach
    allows for a great deal of flexibility in FEM modeling. Instead of completing
    the time-consuming pipeline in Fig. [3,](#page-6-1) the HTC can be easily modified
    to test the behavior of different cooling solutions.


    The experiments demonstrated in this work assume an active air-cooled heatsink.
    The value of the HTC of an air-cooled heatsink is determined by:


    <span id="page-10-1"></span>

    $$h\_{eq} = \frac{h\_{avg} \cdot A\_l \cdot \left(1 - \frac{N \cdot A\_f \cdot
    (1 - \eta\_f)}{A\_l}\right)}{LW} \tag{3}$$


    where is the total area of the heatsink, is the fin area, is the number of fins,
    is the fin efficiency, and and are the length and width of the base plate. The
    average convective HTC (ℎ) can be calculated using the Nusselt number [\[31\]](#page-24-17).
    We select values consistent with a basic copper heatsink with forced airflow provided
    by a typical commercial computer fan.


    While an active air-cooled heatsink is assumed in this work, similar equations
    to Equation [3](#page-10-1) for liquid cooling exist [\[31\]](#page-24-17). If
    the system designer chose to use liquid cooling for the simulation, a new HTC
    would be calculated and applied in place of the air cooling HTC described previously.


    While the above method of abstracting a heatsink is effective for systems where
    heat is dissipated primarily through the external boundaries of the package (top
    of the lid, bottom of the substrate), a different modeling approach may be used
    for other cooling methods, such as inter-tier liquid cooling, where microchannels
    contact the chip directly [\[25,](#page-24-14) [30\]](#page-24-13). In such an
    approach, heat is dissipated directly from the chip without moving to a heat spreader
    like the lid. Hence, additional heat transfer coefficients or abstraction techniques
    may be needed to capture the cooling behavior.


    #### 4.3 FEM to Thermal RC models


    This section describes the process of constructing a thermal RC model from the
    geometry of a given package. MFIT applies this technique to both 2.5D and 3D systems
    demonstrating the flexibility of the proposed methodology to different packaging
    technologies. However, this process can easily be applied to any package. Table
    [6](#page-11-0) provides the notation of the equations used in this and the following
    subsection.


    The package is first divided into horizontal layers, with the slicing process
    starting at the bottom substrate layer and ending at the top lid layer. Depending
    on the package design, each layer may be composed of a uniform material or various
    material blocks, resulting in either homogeneous or non-homogeneous layers. This
    flexibility enables the thermal RC model to simulate packages with heterogeneous
    designs where different chiplets are manufactured with various technologies, resulting
    in different material parameters in the same layer. Layers with uniform material
    properties are divided into a 2D grid of nodes, where the grid granularity can
    vary between layers. For non-homogeneous layers with different material blocks,
    each block can have a distinct grid granularity, resulting in a non-uniform grid
    that connects the entire package as a 3D network of thermal nodes, discretizing
    the chiplet geometry in space.


    Since a layer or material blocks may have anisotropic material, where thermal
    conductivity differs along the x, y, and z axes (represented by , , and ), we
    calculate the thermal conductance ( , , ) for each node using the following equations:


    $$G\_{\mathbf{x}} = \frac{k\_{\mathbf{x}} \cdot l\_{y} \cdot l\_{z}}{l\_{\mathbf{x}}},\
    G\_{y} = \frac{k\_{y} \cdot l\_{\mathbf{x}} \cdot l\_{z}}{l\_{u}},\ G\_{z} = \frac{k\_{z}
    \cdot l\_{\mathbf{x}} \cdot l\_{y}}{l\_{z}}\tag{4}$$


    <span id="page-11-0"></span>where and are the node lengths in the x and y dimensions,
    respectively, and represents the thickness of the layer. The thermal capacitance
    of each node is then calculated as = · · · · , where is material density and is
    the volumetric specific heat.


    | Notation          | Definition                                                 |

    |-------------------|------------------------------------------------------------|

    | 𝑘𝑥<br>, 𝑘𝑦,<br>𝑘𝑧 | Thermal conductivity of a layer along the x, y, and z axes
    |

    | 𝐺𝑥<br>, 𝐺𝑦,<br>𝐺𝑧 | Thermal conductance of a node along the x, y, and z axes   |

    | 𝜌                 | Material density                                           |

    | 𝐶𝑣                | Volumetric specific heat of a layer                        |

    | 𝐺𝑐𝑜𝑛𝑣             | Convection conductance of a boundary node                  |

    | ℎ𝑒𝑞               | Heat transfer coefficient of heatsink                      |

    | N                 | Total number of nodes in the RC and DSS models             |

    | T, T¤             | Nx1 temperature matrix and its derivative                  |

    | q¤                | Nx1 matrix of heat generation                              |

    | C, G              | NxN thermal capacitance and conductance matrices           |


    Table 6. Summary of the key parameters and notations used in this work.


    Heat is dissipated from the package primarily through a heatsink which is simulated
    using a convective heat transfer coefficient as detailed in Section [4.2.4.](#page-10-2)
    MFIT assumes forced convection is applied to the heatsink while passive convection
    occurs on the other external boundaries of the package. Consequently, convective
    conductance ( = ℎ · · ) is incorporated into the nodes of the top and bottom layers.


    The conductance between neighboring nodes and () of the same layers is determined
    by lateral conductance ( and ). Since our thermal RC model allows non-uniform
    grid sizes for different layers and blocks, a node in one layer can be connected
    to multiple nodes from adjacent layers. Thus, vertical conductance between nodes
    of different layers is calculated from , considering the overlap in the x-y plane.
    Once this RC network is established, we can formulate an ODE based on Kirchhoff''s
    current law for a node as:


    $$C\_i \frac{dT\_i}{dt} = \sum\_{j=1}^{N} (G\_{ij})(T\_j - T\_i) + \dot{q}\_i
    \tag{5}$$


    where = 0, if == or is not a neighbor node of .


    The heat generation (¤ ) from node is analogous to electric current, and temperature
    ( ) is analogous to voltage. Since only the chiplet layers consume power, heat
    generation for the nodes in other layers is zero. Solving the system of ODEs by
    forming a matrix is a well-studied approach. It can be represented by:


    <span id="page-12-0"></span>

    $$\mathbf{C} \times \dot{\mathbf{T}} = \mathbf{G} \times \mathbf{T} + \dot{\mathbf{q}}
    \tag{6}$$


    where T, T¤, and q¤ are × 1 matrices representing node temperatures, temperature
    derivative, and generated heat. C is a × diagonal matrix, where each element corresponds
    to a node''s thermal capacitance. The conductance matrix G can be expressed as:


    $$\mathbf{G} = \begin{bmatrix} -\sum\_{j=1}^{N} G\_{1j} & G\_{12} & \cdots & G\_{1N}
    \\ G\_{21} & -\sum\_{j=1}^{N} G\_{2j} & \cdots & G\_{2N} \\ \vdots & \vdots &
    \ddots & \vdots \\ G\_{N1} & G\_{N2} & \cdots & -\sum\_{j=1}^{N} G\_{Nj} \end{bmatrix}
    \tag{7}$$


    where, represents conductance between the neighboring nodes and .


    MFIT employs the backward Euler method to solve this system of ODEs, as its implicit
    formulation ensures stability when handling the stiff equations typical of thermal
    modeling. Given that the system''s matrices are highly sparse (each node connects
    to only a few neighbors), MFIT leverages this sparsity by integrating SuperLU
    [\[34\]](#page-24-21) as a sparse linear solver. Additionally, we incorporate
    the BLAS library [\[1\]](#page-23-14) alongside SuperLU to further accelerate
    computations. To balance flexibility and performance, we implement the front-end
    in Python to define the geometry and ODE Equations, while the core computations
    rely on optimized C-based implementations of SuperLU.


    Capacitance Tuning: While the thermal RC method provides very accurate steady-state
    temperature predictions, transient temperature calculations can exhibit relatively
    higher errors. This is a limitation of existing approaches as discussed in the
    experimental evaluations. To mitigate this issue, we fine-tune the capacitance
    values of each layer in the system using FEM results as a reference. Specifically,
    we introduce a scalar multiplier for each layer''s capacitance and optimize these
    values using the nonlinear optimizer Nelder-Mead [\[7\]](#page-23-15). This tuning
    process is performed on a small-scale system and is based on the number of layers
    and their material properties rather than chiplet placement or geometry. Once
    optimized, the same capacitance parameters are applied to larger system sizes
    for more accurate thermal modeling without increasing grid complexity, as we demonstrate
    in Section [5.4.](#page-18-0) Since most advanced packaging technologies rely
    on similar materials, re-tuning is rarely required. Tuning is only necessary when
    the number of layers or material properties changes, in which case fine-tuning
    capacitance


    values on a small-scale system ensures accurate temperature calculation. In this
    work, we tune two representative small-scale systems, one for 2.5D systems and
    one for 3D system. The capacitance values of these representative systems are
    applied to the larger systems shown in the experimental results.


    #### <span id="page-13-0"></span>4.4 Thermal RC to Discrete State Space models


    The thermal RC model can be discretized in the time domain to further reduce the
    execution time of the model with no cost of accuracy. The discretization of a
    thermal RC model is completed by automated tools and takes only milliseconds to
    run. The DSS model can be used for dynamic thermal management applications where
    the geometry is fixed, and faster thermal prediction is more important. However,
    a limitation of the DSS model is its dependence on an underlying continuous time
    model. It cannot be constructed directly without a thermal RC model as an intermediate
    step or system identification and measurement data. Additionally, a DSS model
    is specific to the geometry, materials, and sampling period used during the creation
    of the thermal RC model and the later discretization process. Therefore, the DSS
    model must be reconstructed if any design parameter changes. Only an existing
    RC model and a set time step are required to create a DSS model, with no direct
    information from the previous FEM model being needed.


    Rearranging the Equation [6](#page-12-0) into a state-space representation:


    $$

    \dot{\mathbf{T}} = \mathbf{A}\mathbf{T} + \mathbf{B}\dot{\mathbf{q}}, \quad \text{where
    } \mathbf{A} = \mathbf{C}^{-1}\mathbf{G}, \quad \mathbf{B} = \mathbf{C}^{-1}.
    \tag{8}

    $$


    The time evolution of this system over an interval ∈ [ , ( + 1) ], where denotes
    the sampling period, is determined by solving the differential equation.


    In the absence of power (q¤ = 0), the homogeneous solution is given as:


    <span id="page-13-1"></span>

    $$\mathbf{T}(t) = e^{\mathbf{A}(t - kT\_s)} \mathbf{T}(kT\_s) \tag{9}$$


    Assuming q¤ is present and remains constant over the interval under a zero-order
    hold (ZOH) approximation, the particular solution is given as:


    <span id="page-13-2"></span>

    $$\mathbf{T}(t) = \int\_{kT\_s}^{t} e^{\mathbf{A}(t-\tau)} \mathbf{B} \dot{\mathbf{q}}(\tau)
    \, d\tau \tag{10}$$


    Combining Equations [9](#page-13-1) and [10:](#page-13-2)


    $$\mathbf{T}(t) = e^{\mathbf{A}(t - kT\_s)}\mathbf{T}(kT\_s) + \int\_{kT\_s}^{t}
    e^{\mathbf{A}(t - \tau)}\mathbf{B} \,d\tau \cdot \dot{\mathbf{q}}(kT\_s) \tag{11}$$


    Evaluating at = ( + 1) , the system is discretized as:


    $$\mathbf{T}((k+1)T\_s) = e^{\mathbf{A}T\_s}\mathbf{T}(kT\_s) + \left(\int\_0^{T\_s}
    e^{\mathbf{A}\tau} d\tau\right) \mathbf{B}\dot{\mathbf{q}}(kT\_s), \quad \text{where
    } \int\_0^{T\_s} e^{\mathbf{A}\tau} d\tau = \mathbf{A}^{-1}(e^{\mathbf{A}T\_s}
    - \mathbf{I}) \tag{12}$$


    Then, the discrete-time system matrices is defined as:


    $$\mathbf{A}\_{\mathbf{d}} = e^{\mathbf{A}T\_s}, \quad \mathbf{B}\_{\mathbf{d}}
    = \mathbf{A}^{-1}(\mathbf{A}\_{\mathbf{d}} - \mathbf{I})\mathbf{B} \tag{13}$$


    the resulting discrete-time state-space equation is given by:


    <span id="page-13-3"></span>

    $$\mathbf{T}[k+1] = \mathbf{A}\_{\mathbf{d}} \mathbf{T}[k] + \mathbf{B}\_{\mathbf{d}}
    \dot{\mathbf{q}}[k] \tag{14}$$


    Equation [14](#page-13-3) represents the discrete-time equivalent of the continuous-time
    thermal RC model (shown in Equation [6\)](#page-12-0). MFIT uses the zero-order
    hold (ZOH) method as given in Equation [10](#page-13-2) for the discretization
    process. When power is provided as discrete inputs at each sampling period, ZOH
    provides an exact match to the


    <sup>,</sup> Vol. 1, No. 1, Article . Publication date: May 2025.


    continuous time model. can be determined for discretization as a function of input
    power consumption and system dynamics.


    The DSS model consists only of multiply-accumulate operations, allowing for extremely
    fast operation, as shown in Section [5.](#page-14-0) The discretization process
    has only a one-time cost and is also nearly instantaneous, allowing for rapid
    DSS model creation when a thermal RC model is available.


    # <span id="page-14-0"></span>5 EXPERIMENTAL RESULTS


    ## <span id="page-14-2"></span>5.1 Experimental Setup


    We evaluate the accuracy of the proposed MFIT methodology on three 2.5D systems
    and one 3D system representative of their respective classes. Three separate 2.5D
    systems are studied to demonstrate the flexibility of the proposed approach for
    systems with different numbers of chiplets. A 3D system is considered to demonstrate
    the capability of the approach to model systems beyond a single planar layer of
    chiplets. Our thermal RC and DSS models are open-sourced to catalyze research
    in this domain. The rest of this section describes the parameters and geometry
    of the 2.5D and 3D systems considered in this paper.


    5.1.1 Package Overview. Both the 2.5D and 3D systems utilize a silicon interposer
    with chiplets placed upon it. The interposer is connected to the underlying substrate
    using C4 bumps. A copper lid covers the chiplets and acts as a heat spreader.
    The lid and substrate form the outer bounds of the package. Dimensions of the
    package for each system configuration are found in Table [7.](#page-14-1) Within
    the package, copper wires embedded in the interposer are used to connect neighboring
    chiplets. In both our 2.5D and 3D systems, each chiplet area is considered to
    be 2.25<sup>2</sup> , consistent with prior studies [\[28,](#page-24-4) [50,](#page-25-8)
    [51\]](#page-25-2). Each chiplet consists of multiple blocks. Each of these blocks
    corresponds to a component, such as a computational tile or a router used for
    inter-chiplet communication, as detailed in Fig. [1a.](#page-1-0) Each of these
    blocks which make up the chiplet has an individual power profile. This means that
    different power profiles can be applied to every computational tile and router
    port in each chiplet. In our experimentation, different levels of detail are applied
    to the chiplets in the 2.5D and 3D systems as described in the following sections.


    <span id="page-14-1"></span>


    | Parameter                                           | 16 2.5D | 36 2.5D | 64
    2.5D | 16x3 3D |  |  |  |  |  |

    |-----------------------------------------------------|---------|---------|---------|---------|--|--|--|--|--|

    | Package Geometry                                    |         |         |         |         |  |  |  |  |  |

    | Package Thickness (mm)                              | 1.855   | 1.855   | 1.855   |
    2.105   |  |  |  |  |  |

    | Package Length and Width (mm)                       | 15.5    | 21.5    | 27.5    |
    15.5    |  |  |  |  |  |

    | Package Top Area (mm2<br>)                          | 240.25  | 462.25  | 756.25  |
    240.25  |  |  |  |  |  |

    | Package Volume (mm3<br>)                            | 445.66  | 857.47  | 1402.84
    | 505.72  |  |  |  |  |  |

    | Power (100% utilization)                            |         |         |         |         |  |  |  |  |  |

    | Individual Chiplet Maximum Power (W)                | 3       | 3       | 3       |
    1.2     |  |  |  |  |  |

    | Total System Maximum Power (W)                      | 48      | 108     | 192     |
    57.6    |  |  |  |  |  |

    | Total System Maximum Power per lid area (W/mm2<br>) | 0.199   | 0.233   | 0.253   |
    0.239   |  |  |  |  |  |

    | Temperature                                         |         |         |         |         |  |  |  |  |  |

    | Maximum Chiplet Temperature (◦C)                    | 118.25  | 129.75  | 164.03  |
    142.01  |  |  |  |  |  |


    Table 7. Specifications of simulated systems in this work.


    2.5D System Specifics: The target 2.5D system consists of a grid of chiplets integrated
    on an interposer, as illustrated in Fig. [1a.](#page-1-0) Each chiplet is connected
    directly to the interposer via -bumps surrounded by a capillary underfill material.
    The physical dimensions of the router ports are compatible with Universal Chiplet
    Interconnect Express (UCIe) specification [\[49\]](#page-25-18). The entire package
    is covered by a copper lid, which contacts each chiplet through a thermal interface
    material (TIM).


    3D System Specifics: In the 3D system, three stacked chiplets are placed in a
    4x4 grid with equal spacing, consistent with [\[10\]](#page-23-3). Vertically
    stacked chiplets are connected to one another with -bumps surrounded by a capillary
    underfill material. The bottom chiplet of each stack is connected to the interposer
    with the same method. This integration is illustrated in Fig. [1b.](#page-1-0)
    The lid contacts only the top chiplet layer through a thermal interface material.


    # 5.2 Thermal RC Model Configuration


    The number of nodes in the thermal RC network determines the model complexity,
    runtime, and granularity at which temperature can be observed in the model. A
    higher node density is used in chiplets to optimize this trade-off, while fewer
    nodes are used in non-chiplet components, such as the interposer, lid, substrate,
    and so on. Each chiplet is divided into four equal quadrants. One node is placed
    within each quadrant to allow for granular temperature monitoring across each
    chiplet. An alternate node density is used for non-chiplet layers in each model,
    as described below. This easily configurable non-uniform node density enables
    higher thermal resolution in critical parts such as chiplets while decreasing
    the runtime with lower resolution in less critical structures such as the substrate
    and lid. The DSS models in our experimentation are created by discretizing the
    thermal RC models with = 0.01 sampling period. The sampling time can be chosen
    according to the application requirements.


    2.5D Thermal RC Model Specifics: For the 2.5D systems, the choice of 4 nodes per
    chiplet leads to 64, 144, and 256 nodes in the chiplet layer of the 16, 36, and
    64 chiplet systems, respectively. For all other layers, the number of nodes is
    equal to the total number of chiplets per layer. This allows the model to maintain
    higher thermal resolution in the critical chiplet layers while maintaining a fast
    execution time.


    3D Thermal RC Model Specifics: The node densities in the 3D system are adjusted
    similarly to the 2.5D models. The layers that contain chiplets use an 8×8 grid,
    implying 4 nodes per chiplet. All other layers have a 4×4 grid, leading to a lower
    node density. The entire 3D system consists of 48 chiplets in total. There are
    a total of 192 nodes counting all nodes within chiplets.


    5.2.1 Input Workloads and Power Consumption. Identical workloads are considered
    for the 2.5D and 3D systems, with differences in chiplet power density detailed
    in the following subsections. The target chiplet systems are analyzed under one
    synthetic (WL1) and five real AI/ML application workloads (WL2-WL6). The synthetic
    workload starts with a stress test that applies the maximum power to all chiplets
    to increase temperature beyond 100◦C. Then, a pseudo-random bit sequence (PRBS)
    is applied to each chiplet to emulate a wide range of dynamic variations. Finally,
    all chiplets are turned off to let the temperature return to the ambient state,
    as depicted in Fig. [9.](#page-19-0) Besides testing transient and steady-state
    behaviors, this power profile helps us to tune the thermal RC model.


    The remaining scenarios consider processing-in-memory (PIM)-based chiplets for
    accelerating ML workloads. The computational platform is resistive random access
    memory (ReRAM) based chiplets commonly used in literature [\[47,](#page-25-1)
    [50,](#page-25-8) [55\]](#page-25-20). We select this configuration due to its
    ability to efficiently implement matrix-vector multiplication, which is the predominant
    operation in any CNN workload. Each workload consists of a series of deep neural
    networks (DNNs) which run in series on the system. The workloads are listed in
    Table [8.](#page-16-0) The neural networks (NN) in these workloads consist of
    several networks such as ResNets, DenseNets, and VGG


    networks. For example, WL1 contains 16 ResNet34''s, followed by one VGG19, then
    5 ResNet50''s, and so on. Each workload contains from 20 to 40 individual networks.
    Workloads are mapped to the system as computing resources become available, meaning
    a new NN is mapped to chiplets when it completes the execution of a previous NN.
    Consequently, these workloads consist of NNs ranging from small NNs like ResNet18,
    which can be mapped to a single chiplet, to larger NNs such as DenseNet169, which
    are spread across multiple chiplets.


    After mapping the neural network workloads to the target chiplet-based system
    using a nearest-neighbor strategy inspired by the Simba [\[47\]](#page-25-1),
    we estimate power consumption in two parts: computation and communication. Computation
    power is obtained using NeuroSim [\[14\]](#page-23-16), while communication power
    is estimated using a custom version of BookSim that includes interconnect energy
    modeling [\[26\]](#page-24-22). We periodically sample the energy consumption
    from both tools at 10 ms intervals and convert these into dynamic power profiles
    for each chiplet. While these tools are not natively designed for chiplet systems,
    we follow a methodology adapted from SIAM that enables accurate per-chiplet power
    estimation for thermal simulations [\[28\]](#page-24-4). Importantly, while we
    use this methodology for our experiments, MFIT only requires power traces as input,
    which can also be obtained from other simulators or real on-chip counters.


    Differences in 2.5D and 3D system chiplet power: Different hardware parameters,
    such as voltage and frequency, are used for the 2.5D and 3D systems. This results
    in lower per-chiplet power consumption in the 3D system of 1.2W as compared to
    3W for the 2.5D system, detailed under the Power section of Table [7.](#page-14-1)
    Using these parameters, the total system power per lid area of the 3D system is
    between that of the 36 and 64 chiplet 2.5D system. This level means that the temperature
    of individual chiplets should be roughly equivalent between these systems, which
    is confirmed in Fig. [9.](#page-19-0)


    5.2.2 Configuration of other thermal simulators: We compare our proposed thermal
    RC and DSS models against the state-of-the-art analytical thermal modeling tools
    HotSpot [\[52\]](#page-25-6), 3D-ICE [\[22\]](#page-24-23), and PACT [\[62\]](#page-25-14)
    for all system sizes and workload configurations. Since HotSpot was originally
    designed for 2D chips, we utilize an extension that incorporates 3D modeling capabilities
    [\[40\]](#page-24-24) to support both 2.5D and 3D integrated systems. Identical
    geometry and material parameters are set across all tools, aligning them with
    our reference FEM model. None of these tools support thermal conductivity variations
    along the x, y, and z directions. Consequently, for anisotropic material layers
    (e.g., the C4 bump layer) in the chiplet package, we use an averaged thermal conductivity
    to approximate this configuration. Additionally, HotSpot and PACT do not allow
    non-uniform grid sizes across


    | Workload | Composition                                                                                                                                                                                                                                          |

    |----------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|

    | WL1      | Synthetic (see Fig. 9)                                                                                                                                                                                                                               |

    | WL2      | 16×ResNet34 (C), 1×VGG19 (C), 5×ResNet50 (C), 3×DenseNet40 (C), 1×ResNet152                                                                                                                                                                          |

    | WL3      | (C), 1×VGG19 (I), 4×<br>ResNet34 (I), 1×ResNet18 (I), 1×ResNet50
    (I), 1×VGG16 (I)<br>16×ResNet34 (I), 1×VGG19 (I), 5×ResNet50 (I), 3×DenseNet169
    (I), 1×ResNet110,<br>1×VGG19 (I), 4×ResNet101 (I), 1xResNet152 (I), 1×ResNet18
    (I), 1×ResNet50 (I), |

    |          | 1×Resnet152 (I)                                                                                                                                                                                                                                      |

    | WL4      | 16×ResNet34 (C), 2×VGG19 (I), 4×DenseNet169 (I), 3×DenseNet40 (C),
    5×ResNet50<br>(C), 3×ResNet101, 7×ResNet150 (I), 2×VGG19 (I), 4×ResNet101, 1×VGG19
    (C)                                                                                            |

    | WL5      | 16×Resnet34 (I), 1×ResNet152 (I), 1×ResNet110 (I), 3×ResNet101 (I),
    9×DenseNet169<br>(I), 4×ResNet34 (I), 12×ResNet18 (I), 5×ResNet50 (I), 1×ResNet152
    (I)                                                                                           |

    | WL6      | 3×DenseNet169 (I), 4×ResNet34 (I), 12×ResNet18 (I), 4×ResNet101 (I),
    2×VGG19 (I),<br>4×ResNet101 (I), 1×VGG19 (C), 3×DenseNet40 (C)                                                                                                                  |


    <span id="page-16-0"></span>Table 8. Descriptions of the workloads used in this
    work. (C) and (I) denote the CIFAR100 and ImageNet datasets, respectively.


    <span id="page-17-0"></span>![](_page_17_Figure_1.jpeg)


    Fig. 8. Execution times (in log scale) of the proposed thermal models and HotSpot
    [\[52\]](#page-25-6), PACT [\[62\]](#page-25-14), and 3D-ICE [\[56\]](#page-25-7)
    for 16, 36, 64 - 2.5D integrated and 16x3 - 3D integrated chiplet systems.


    different horizontal layers, so a uniform grid size matching our chiplet layer
    is used. We configure the non-uniform grid in 3D-ICE [\[22\]](#page-24-23) to
    be consistent with our thermal RC model, ensuring the same number of thermal nodes
    for each system. Additionally, PACT provides flexibility in solver selection by
    supporting various SPICE solvers from Xyce [\[24\]](#page-24-25). For consistency,
    we use the TRAP solver, as given in [\[62\]](#page-25-14). PACT also enables parallel
    execution, which we leverage by running all simulations with eight cores on an
    Intel 10700k CPU.


    # 5.3 Execution Time Evaluation


    This section evaluates the execution time of the proposed multi-fidelity thermal
    model set. All simulations are run on a dual Intel Xeon Gold 6242R system with
    40 processing cores. We use WL1 for our timing analysis since the execution time
    is comparable across all workloads.


    2.5D Evaluation: Abstracted FEM simulations take 2.4, 14.5, and 38.0 hours for
    16, 36, and 64 chiplet systems, respectively. While providing an accurate reference,
    these long simulation times and significant development effort make FEM impractical
    for DSE. Our thermal RC models fill this gap with execution times ranging from
    0.85 to 3.57 seconds for 16, 36, and 64 chiplets, as summarized in the first 3
    systems in Fig. [8.](#page-17-0) Coupled with the accuracy presented in the Section
    [5.4,](#page-18-0) the 10,000 to 40,000-fold speedup demonstrates their viability
    as a DSE tool.


    Thermal RC models are derived directly from the underlying geometry and material
    parameters, meaning they can be reconfigured for different hardware and design
    configurations without re-calibration from FEM simulations. Relaxing this physical
    system to model connection, our DSS models reduce execution time to 18, 26, and
    54 milliseconds for 16, 36, and 64 chiplets respectively. This speedup with respect
    to the RC model enables runtime temperature prediction, which can inform dynamic
    thermal power management (DTPM) decisions to increase system performance and reliability
    [\[11\]](#page-23-17). Although accurate real-time power estimation is a critical
    component of such DTPM frameworks, MFIT treats power as an external input and
    can be used with power traces obtained from simulators, profilers, or real hardware
    counters. For instance, physical current sensors that measure runtime power consumption
    at cluster or core granularities can provide direct input to MFIT [\[13,](#page-23-18)
    [17\]](#page-23-19). Another


    option is software-based power meters that estimate runtime power by converting
    performance counter readings into power values, such as Qualcomm''s profiler [\[3\]](#page-23-20).
    While maintaining the accuracy for a given configuration, DSS can be regenerated
    from the RC model in a few milliseconds if the sampling period or hardware configuration
    changes.


    3D Evaluation: The execution time results of the 3D system are summarized in the
    3D - 16x3 chiplet system of Fig. [8.](#page-17-0) FEM simulations take approximately
    3.3 hours for the single 3D system. The thermal RC model significantly reduces
    the runtime to 1.6 seconds. Similar to the 2.5D system, the 3D DSS model further
    reduces runtime to just 24 milliseconds. This substantial speedup is due to the
    DSS model relying solely on matrix multiplication operations, which are highly
    optimized on modern hardware through SIMD instructions and other acceleration
    techniques, such as efficient memory access.


    Execution time comparison to existing tools: The commonly used thermal modeling
    tools, HotSpot [\[53\]](#page-25-13), 3D-ICE [\[22\]](#page-24-23), and PACT [\[62\]](#page-25-14)
    belong to the same class of model as our thermal RC models. As shown in Fig. [8,](#page-17-0)
    our proposed thermal RC and DSS methods achieve significantly faster execution
    times compared to these tools.


    HotSpot and PACT lack support for non-uniform grid structures, resulting in more
    complex ODE formulations and increased computational cost. In particular, HotSpot
    relies on the classical Runge-Kutta 4 (RK4) method, which is known to be less
    efficient for stiff systems due to its small time step requirements to maintain
    stability and accuracy, leading to increased computation time. As a result, our
    thermal RC model achieves a 4235× speedup for 16 chiplets, 4082× for 36 chiplets,
    3630× for 64 chiplets, and 4050× for the 16×3 3D chiplet system compared to HotSpot.
    Against PACT, our thermal RC model is 132× faster for 16 chiplets, 205× for 36
    chiplets, 164× for 64 chiplets, and 167× for the 16×3 3D chiplet system.


    While 3D-ICE does support non-uniform grids and employs an implicit Euler method
    for solving transient heat equations, MFIT further improves efficiency by integrating
    BLAS-optimized routines within the ODE solver pipeline. These routines accelerate
    matrix-vector multiplications and factorization steps, enabling faster convergence
    when used alongside the SuperLU solver. In contrast, 3D-ICE relies solely on SuperLU
    without these enhancements. Consequently, our approach achieves a 3× speedup for
    16 chiplets, 2.3× for 36 chiplets, 1.9× for 64 chiplets, and 2.7× for the 16×3
    3D chiplet system over 3D-ICE. Moreover, our DSS method surpasses all RC-based
    models in execution efficiency, demonstrating the best performance among all evaluated
    tools. Finally, we emphasize that MFIT, our multi-fidelity thermal model set,
    covers a much wider range of accuracy and execution time trade-offs than a specific
    point solution, providing greater flexibility for different modeling needs.


    #### <span id="page-18-0"></span>5.4 Validation of Thermal RC and DSS model


    We validate the accuracy of our thermal RC and DSS models by comparing their temperature
    estimates to full-system FEM simulation results for the same workload and system
    configurations. This comparison is completed for each of the three 2.5D system
    sizes and the single 3D system. In addition to the visualization of the temperature
    estimate over time, shown in Fig. [9,](#page-19-0) three metrics are used to quantify
    the accuracy of our thermal RC and DSS models against the FEM results. The MAE
    metric measures the mean absolute error in temperature across the entire simulation
    duration. The second metric, average percentage error, expresses the temperature
    error as a percentage. This is particularly important because a given absolute
    error at lower temperatures represents a more significant inaccuracy than the
    same absolute error at higher temperatures. The third metric, Temperature Violation
    Prediction accuracy, measures the accuracy of our models in predicting temperature
    violations. Predicting temperature violations (e.g., tracking the time steps when
    the temperature exceeds the allowed threshold) is often used by DTPM algorithms.
    We set 85◦C as the maximum allowable temperature threshold for each system without
    loss of generality [\[67\]](#page-25-21). This metric first identifies the time
    steps in FEM simulations where temperature violations occur (temperature exceeds
    85◦C). Then, it computes the percentage of these violations captured by the thermal
    RC and DSS models (e.g., 100% means all violations are


    detected with perfect accuracy). The proposed models conservatively flag violations
    within one degree of the above-mentioned threshold temperature.


    To assist users in visualizing the thermal behavior of the system under test,
    the RC model also creates a heat map of each layer in the system. As an example,
    the heat map of the interposer layer of a 2.5D 64 chiplet system is shown in Fig.
    [10.](#page-20-0) This figure shows the temperature gradient that occurs between
    the center of the interposer, where the heat-producing chiplets are located, and
    the edges of the system, where there are no chiplets. These maps allow for quick
    visual verification of the system behavior instead of relying purely on numerical
    results.


    2.5D Validation Results: Figs. [9a, 9b,](#page-19-0) and [9c](#page-19-0) plot
    the temperature as a function of time for each 2.5D system size of a representative
    chiplet while running workload WL1. All three plots of 2.5D systems clearly show
    that the systematically constructed thermal RC and DSS models produce near-identical
    results to the FEM baseline. They closely follow the FEM results during the stress
    test (temperature increases until reaching the maximum point), randomly changing
    chiplet power consumption (middle portion), and cool-down periods.


    The first 3 columns of Table [9](#page-21-0) summarize the accuracy results for
    all 2.5D systems and workloads. . The worst-case mean absolute errors of our models
    are only 1.46◦C, 1.64◦C, and 1.35◦C for the 16-, 36-, and 64-chiplet systems,
    respectively. The corresponding worst-case average percentage errors are 1.95%,
    2.10%, and 1.79%, indicating that even the largest errors occur at higher temperatures
    where such deviations are less critical. In contrast, other tools exhibit significantly
    higher errors, as discussed later. These results indicate that the proposed


    <span id="page-19-0"></span>![](_page_19_Figure_5.jpeg)


    Fig. 9. Temperatures of representative chiplets from 2.5D and 3D systems while
    running WL1 are plotted as a function of time.


    models achieve excellent accuracy across different hardware configurations and
    workloads. Our models also achieve FEM-level high accuracy in predicting temperature
    violations as described earlier in Section [5.4.](#page-18-0) For example, the
    worst-case accuracy for the 16-chiplet system is 93.5% (i.e., only 6.5% of the
    time steps where violations are missed). With the exception of WL2 and 3, prediction
    accuracy exceeds 82% for all other workloads run on the 36 chiplet system. The
    same trend is seen with the 64-chiplet system.


    For some specific workload-system combinations, such as WL2 and 3 run on the 36
    chiplet system, prediction accuracy falls below the 80% mark for our models. These
    relatively lower accuracy values stem from sudden temperature spikes that lead
    to short-term temperature violations in these workload-system combinations. Temperature
    spikes in specific chiplets occur when several grouped chiplets experience a transient
    power spike simultaneously. This increases the temperature of the more central
    chiplets in the group. In these cases, the peak temperatures are mostly at or
    below 85 degrees with infrequent short-term violations, which can be tolerated.
    For example, FEM simulations indicate only 255 temperature violations across all
    chiplets in WL3 compared to 11 thousand violations in WL1. Hence, missing even
    a few violations impacts the accuracy heavily when the RC and DSS models do not
    capture these spikes. In contrast, our thermal RC and DSS models effectively detect
    more prolonged violations, as evidenced by WL1, WL3, WL4, WL5, and WL6.


    3D Validation Results: Fig. [9d](#page-19-0) plots the temperature as a function
    of time for the single 3D system of a representative chiplet running WL1. The
    plot shows similar behavior to the 2.5D comparison, where the RC and DSS match
    each other exactly and match the reference FEM results extremely closely during
    the stress test, random, and cool-down portions of the workload.


    The far-right column of Table [9](#page-21-0) summarizes the accuracy results
    for the single 3D system for each workload. The worst-case mean absolute error
    is only 1.52◦C, while the worst-case average percentage error is 2.36%. This result
    shows that the proposed models maintain high levels of accuracy even when applied
    to a stacked-die 3D package. This high degree of accuracy is repeated when predicting
    temperature violations. The worst-case temperature violation prediction accuracy
    is only 94.0%, occurring for workload 6.


    <span id="page-20-0"></span>![](_page_20_Figure_5.jpeg)


    Fig. 10. The heat map of (a) the interposer layer and (b) chiplets generated by
    the thermal RC model for 64 chiplet system.


    <span id="page-21-0"></span>Table 9. Comparison of thermal RC, DSS, HotSpot [\[52\]](#page-25-6),
    3D-ICE [\[56\]](#page-25-7), and PACT [\[62\]](#page-25-14) models based on mean
    absolute error, average percentage error, and accuracy in predicting temperature
    violations relative to the reference FEM models. The best MAE and percentage error
    for each system-workload combination are highlighted in red. Average operating
    temperatures are in the 60-80 (◦C) range.


    |     |                         | 2.5D - 16 Chiplets |               | 2.5D -
    36 Chiplets          |               |               | 2.5D - 64 Chiplets          |              |               |
    3D - 16x3 Chiplets          |              |               |                             |

    |-----|-------------------------|--------------------|---------------|-----------------------------|---------------|---------------|-----------------------------|--------------|---------------|-----------------------------|--------------|---------------|-----------------------------|

    | WL  | Model                   | MAE                | Avg.<br>Error | Temp. Violation<br>Accuracy
    | MAE           | Avg.<br>Error | Temp. Violation<br>Accuracy | MAE          |
    Avg.<br>Error | Temp. Violation<br>Accuracy | MAE          | Avg.<br>Error | Temp.
    Violation<br>Accuracy |

    |     | Thermal RC 1.23◦C 1.59% |                    |               | 93.5%                       |
    1.42◦C 1.62%  |               | 95.7%                       | 0.99◦C 1.26% |               |
    99.8%                       | 0.94◦C 1.11% |               | 98.1%                       |

    |     | DSS                     | 1.23◦C 1.59%       |               | 93.5%                       |
    1.42◦C 1.62%  |               | 95.7%                       | 0.99◦C 1.26% |               |
    99.8%                       | 0.94◦C 1.11% |               | 98.1%                       |

    | WL1 | HotSpot                 | 2.74◦C 3.78%       |               | 67.8%                       |
    1.64◦C        | 2.13%         | 95.2%                       | 2.16◦C 2.62% |               |
    95.7%                       | 2.37◦C 2.99% |               | 87.5%                       |

    |     | 3D-ICE                  | 1.65◦C 2.28%       |               | 89.5%                       |
    2.07◦C        | 2.63%         | 97.4%                       | 1.65◦C 2.07% |               |
    99.5%                       | 1.73◦C 2.21% |               | 96.9%                       |

    |     | PACT                    | 1.99◦C 2.76%       |               | 91.7%                       |
    1.61◦C        | 2.13%         | 99.1%                       | 1.70◦C 2.15% |               |
    99.4%                       | 2.09◦C 2.62% |               | 98.7%                       |

    |     | Thermal RC 0.86◦C 1.38% |                    |               | 96.0%                       |
    1.16◦C        | 1.89%         | 0.0%                        | 0.95◦C 1.56% |               |
    100.0%                      | 0.98◦C 1.58% |               | 100.0%                      |

    |     | DSS                     | 0.86◦C 1.38%       |               | 96.0%                       |
    1.16◦C        | 1.89%         | 0.0%                        | 0.95◦C 1.56% |               |
    100.0%                      | 0.98◦C 1.58% |               | 100.0%                      |

    | WL2 | HotSpot                 | 1.58◦C 2.80%       |               | 75.0%                       |
    1.24◦C        | 1.95%         | 0.0%                        | 1.01◦C 1.62% |               |
    100.0%                      | 1.74◦C 2.70% |               | 100.0%                      |

    |     | 3D-ICE                  | 1.36◦C 2.21%       |               | 85.1%                       |
    1.35◦C        | 2.13%         | 0.0%                        | 1.82◦C 2.93% |               |
    100.0%                      | 1.39◦C 2.21% |               | 100.0%                      |

    |     | PACT                    | 1.58◦C 2.38%       |               | 75.9%                       |
    0.91◦C 1.44%  |               | 16.7%                       | 1.55◦C 2.51% |               |
    100.0%                      | 2.10◦C 3.33% |               | 100.0%                      |

    |     | Thermal RC 1.02◦C 1.63% |                    |               | 100.0%                      |
    1.28◦C        | 1.93%         | 78.5%                       | 1.03◦C 1.57% |               |
    95.8%                       | 0.80◦C 1.23% |               | 100.0%                      |

    |     | DSS                     | 1.02◦C 1.63%       |               | 100.0%                      |
    1.28◦C        | 1.93%         | 78.5%                       | 1.03◦C 1.57% |               |
    95.8%                       | 0.80◦C 1.23% |               | 100.0%                      |

    | WL3 | HotSpot                 | 1.97◦C 3.02%       |               | 100.0%                      |
    1.18◦C        | 1.74%         | 29.7%                       | 1.12◦C 1.65% |               |
    67.4%                       | 1.79◦C 2.67% |               | 100.0%                      |

    |     | 3D-ICE                  | 1.15◦C 1.79%       |               | 100.0%                      |
    1.53◦C        | 2.27%         | 3.8%                        | 1.82◦C 2.75% |               |
    76.4%                       | 1.12◦C 1.72% |               | 100.0%                      |

    |     | PACT                    | 2.03◦C 3.12%       |               | 100.0%                      |
    1.09◦C 1.61%  |               | 23.4%                       | 1.75◦C 2.59% |               |
    66.7%                       | 1.80◦C 2.76% |               | 100.0%                      |

    |     | Thermal RC 1.46◦C 1.95% |                    |               | 96.6%                       |
    1.64◦C 2.10%  |               | 96.0%                       | 1.35◦C 1.74% |               |
    98.0%                       | 1.48◦C 1.85% |               | 98.0%                       |

    |     | DSS                     | 1.46◦C 1.95%       |               | 96.6%                       |
    1.64◦C 2.10%  |               | 96.0%                       | 1.35◦C 1.74% |               |
    98.0%                       | 1.48◦C 1.85% |               | 98.0%                       |

    | WL4 | HotSpot                 | 2.29◦C 3.36%       |               | 95.6%                       |
    7.39◦C 10.28% |               | 92.4%                       | 2.10◦C 2.59% |               |
    96.5%                       | 2.71◦C 3.36% |               | 96.2%                       |

    |     | 3D-ICE                  | 3.72◦C 4.72%       |               | 91.3%                       |
    2.33◦C        | 2.94%         | 97.2%                       | 2.68◦C 3.43% |               |
    97.3%                       | 2.46◦C 3.08% |               | 96.4%                       |

    |     | PACT                    | 3.56◦C 4.74%       |               | 93.8%                       |
    1.94◦C        | 2.46%         | 97.6%                       | 2.39◦C 3.06% |               |
    97.2%                       | 2.97◦C 3.79% |               | 96.3%                       |

    |     | Thermal RC 1.01◦C 1.62% |                    |               | 100.0%                      |
    1.25◦C        | 1.89%         | 78.4%                       | 0.96◦C 1.43% |               |
    100.0%                      | 0.87◦C 1.35% |               | 100.0%                      |

    | WL5 | DSS                     | 1.01◦C 1.62%       |               | 100.0%                      |
    1.25◦C        | 1.89%         | 78.4%                       | 0.96◦C 1.43% |               |
    100.0%                      | 0.87◦C 1.35% |               | 100.0%                      |

    |     | HotSpot                 | 1.95◦C 3.03%       |               | 100.0%                      |
    1.16◦C        | 1.71%         | 57.9%                       | 1.30◦C 1.88% |               |
    0.0%                        | 1.86◦C 2.77% |               | 100.0%                      |

    |     | 3D-ICE                  | 1.16◦C 1.82%       |               | 100.0%                      |
    1.53◦C        | 2.27%         | 15.8%                       | 1.79◦C 2.67% |               |
    50.0%                       | 0.87◦C 1.33% |               | 100.0%                      |

    |     | PACT                    | 1.99◦C 3.09%       |               | 100.0%                      |
    1.13◦C 1.68%  |               | 38.4%                       | 1.87◦C 2.72% |               |
    15.0%                       | 1.64◦C 2.51% |               | 100.0%                      |

    |     | Thermal RC 0.89◦C 1.44% |                    |               | 98.1%                       |
    1.30◦C        | 1.97%         | 84.3%                       | 1.21◦C 1.79% |               |
    89.7%                       | 1.52◦C 2.36% |               | 91.2%                       |

    | WL6 | DSS                     | 0.89◦C 1.44%       |               | 98.1%                       |
    1.30◦C        | 1.97%         | 84.3%                       | 1.21◦C 1.79% |               |
    89.7%                       | 1.52◦C 2.36% |               | 91.2%                       |

    |     | HotSpot                 | 1.62◦C 2.82%       |               | 85.9%                       |
    1.28◦C 1.88%  |               | 87.8%                       | 1.48◦C 2.18% |               |
    76.1%                       | 2.22◦C 3.37% |               | 82.3%                       |

    |     | 3D-ICE                  | 1.34◦C 2.17%       |               | 91.8%                       |
    1.69◦C        | 2.48%         | 74.7%                       | 2.38◦C 3.54% |               |
    69.8%                       | 1.61◦C 2.44% |               | 70.3%                       |

    |     | PACT                    | 1.67◦C 2.56%       |               | 89.3%                       |
    1.28◦C        | 1.91%         | 91.4%                       | 1.85◦C 2.75% |               |
    90.2%                       | 2.43◦C 3.69% |               | 66.9%                       |


    Accuracy comparison to existing tools: Table [9](#page-21-0) also lists the MAE
    of HotSpot, 3D-ICE, and PACT simulations compared to FEM simulations for the 2.5D
    and 3D systems. For the 2.5D system results, the average error of Hotspot, 3D-ICE,
    and PACT across all workloads are 0.7, 0.66, and 0.59 degrees greater than our
    thermal RC and DSS models. For the 3D system results, the average error of Hotspot,
    3D-ICE, and PACT across all workloads are 1.02, 0.43, and 1.07 degrees greater
    than our thermal RC and DSS models.


    There are several reasons for the improved accuracy of our models as compared
    to existing thermal modeling tools. First, PACT and 3D-ICE are not able to model
    convective heat transfer from more than one boundary of the package. In the case
    of 3D-ICE, this configuration would require a uniform grid node distribution,
    which would negatively impact execution time. Second, none of these existing simulators
    support the ability to provide different thermal conductivities along the x, y,
    and z-axis, further decreasing accuracy. Third, our models have their node capacitance
    values tuned, improving their transient thermal simulation accuracy. Lastly, the
    strength of the PACT simulator lies in circuit-level thermal simulations [\[62\]](#page-25-14).
    It assumes a much finer grid granularity compared to block granularity, meaning
    that each block contains many thermal nodes. However, in architectural-level thermal
    simulations, the focus of this work, each block contains significantly fewer nodes.
    This mismatch can lead to uneven node distribution within the PACT simulator,
    potentially reducing simulation accuracy.


    <span id="page-22-0"></span>![](_page_22_Figure_0.jpeg)


    MFIT : Multi-FIdelity Thermal Modeling for 2.5D and 3D Multi-Chiplet Architectures
    • 23


    Fig. 11. Thermal modeling of AMD MI300A, a CPU-XPU heterogeneous system (a) Block
    diagram of MI300A architecture [\[54\]](#page-25-22), (b) Heatmap of IO Die layer,
    (c) Heatmap of compute die layers, (d) Heatmap of bulk-si layer.


    # 5.5 Modeling of Heterogeneous Architecture


    To demonstrate the applicability of MFIT to complex heterogeneous architectures,
    we model AMD''s MI300A system in this section. The MI300A features a hybrid 2.5D/3D
    integration, where multiple chiplets are stacked across two tiers [\[54\]](#page-25-22).
    Specifically, IO dies (IODs) are placed on the bottom tier, with a mix of accelerator
    complex dies (XCDs) and CPU complex dies (CCDs) stacked directly above them. Our
    modeled configuration includes 6 XCDs and 3 CCDs. This compute structure is surrounded
    by 8 high-bandwidth memory (HBM) stacks and their corresponding controllers, as
    illustrated in Fig. [11\(](#page-22-0)a). We model this system using our thermal
    RC framework with a total of 500 nodes. As in Section [5.1,](#page-14-2) we assign
    higher node density to active components such as the XCDs and CCDs, and lower
    node density to passive components like the lid and substrate. This demonstrates
    MFIT''s capability to model complex 2.5D/3D structures with heterogeneous node
    densities.


    Conventional thermal simulators such as HotSpot, PACT, and 3D-ICE are not capable
    of accurately modeling such architectures. These tools typically assume homogeneous
    material layers and cannot model anisotropic materials or bidirectional heat dissipation,
    these capabilities that are critical for capturing the thermal behavior of high-performance
    industrial systems like MI300A. Fig. [11\(](#page-22-0)b-d) shows temperature
    heatmaps across three vertical stacks, with labeled components, highlighting the
    spatial thermal variations across the heterogeneous chiplet layout.


    # <span id="page-23-6"></span>6 CONCLUSION


    Due to increasing manufacturing costs, conventional monolithic 2D chips cannot
    sustain the increasing performance and compute capacity demands. 2.5D and 3D chipset
    systems have emerged as cost-effective solutions to continue the required scaling.
    However, substantial compute power in a small volume intensifies the power density,
    leading to severe heat dissipation and thermal challenges. There is a strong need
    for open-source thermal modeling tools that enable researchers to analyze thermal
    behavior and perform thermally aware optimizations. Re-purposing existing approaches
    developed for monolithic chips incurs accuracy and execution time penalties, while
    custom-designed singular solutions have limited scope. To fill this gap, this
    paper proposed MFIT, a set of multi-fidelity thermal models that span a wide range
    of accuracy and execution time trade-offs. Since the proposed models are consistent
    by construction, designers can use them throughout the design cycle, from system
    specification to design space exploration and runtime resource management.


    ## REFERENCES


    - <span id="page-23-14"></span>[1] [n. d.]. BLAS (Basic Linear Algebra Subprograms).<https://www.netlib.org/blas/>

    - <span id="page-23-0"></span>[2] 2021. Semiconductor Research Corporation, "Decadal
    Plan for Semiconductors". [https://www.src.org/about/decadal-plan/decadal-plan](https://www.src.org/about/decadal-plan/decadal-plan-full-report.pdf)[full-report.pdf,](https://www.src.org/about/decadal-plan/decadal-plan-full-report.pdf)
    Accessed on March 31, 2024.

    - <span id="page-23-20"></span>[3] 2025. Qualcomm® Profiler. [https://www.qualcomm.com/developer/software/qualcomm-profiler.](https://www.qualcomm.com/developer/software/qualcomm-profiler)
    Accessed: Apr. 25, 2025.

    - <span id="page-23-4"></span>[4] Rahul Agarwal, Patrick Cheng, Priyal Shah, Brett
    Wilkerson, Raja Swaminathan, John Wuu, and Chandrasekhar Mandalapu. 2022. 3D packaging
    for heterogeneous integration. In 2022 IEEE 72nd Electronic Components and Technology
    Conference (ECTC). IEEE, 1103–1107.

    - <span id="page-23-8"></span>[5] Hussam Amrouch and Jörg Henkel. 2015. Lucid
    infrared thermography of thermally-constrained processors. In Proc. of IEEE/ACM
    ISLPED. 347–352.

    - <span id="page-23-1"></span>[6] Semiconductor Industry Association. [n. d.].
    Intl. Technology Roadmap for Semiconductors 2015 Edition. [https://www.semiconductors.](https://www.semiconductors.org/resources/2015-Intl.-technology-roadmap-for-semiconductors-itrs/)
    [org/resources/2015-Intl.-technology-roadmap-for-semiconductors-itrs/.](https://www.semiconductors.org/resources/2015-Intl.-technology-roadmap-for-semiconductors-itrs/)

    - <span id="page-23-15"></span>[7] Russell R Barton and John S Ivey Jr. 1996.
    Nelder-Mead simplex modifications for simulation optimization. Management Science
    42, 7 (1996), 954–973.

    - <span id="page-23-5"></span>[8] Srikant Bharadwaj, Jieming Yin, Bradford Beckmann,
    and Tushar Krishna. 2020. Kite: A Family of Heterogeneous Interposer Topologies
    Enabled via Accurate Interconnect Modeling. In Proc. of ACM/IEEE DAC.

    - <span id="page-23-12"></span>[9] Ganapati Bhat, Gaurav Singla, Ali K. Unver,
    and Umit Y. Ogras. 2018. Algorithmic Optimization of Thermal and Power Management
    for Heterogeneous Mobile Platforms. IEEE Trans. VLSI Syst. 26, 3 (2018), 544–557.

    - <span id="page-23-3"></span>[10] Bryan Black, Murali Annavaram, Ned Brekelbaum,
    John DeVale, Lei Jiang, Gabriel H. Loh, Don McCaule, Pat Morrow, Donald W. Nelson,
    Daniel Pantuso, Paul Reed, Jeff Rupley, Sadasivan Shankar, John Shen, and Clair
    Webb. 2006. Die Stacking (3D) Microarchitecture. In 2006 39th Annual IEEE/ACM
    International Symposium on Microarchitecture (MICRO''06). 469–479.<https://doi.org/10.1109/MICRO.2006.18>

    - <span id="page-23-17"></span>[11] David Brooks, Robert P. Dick, Russ Joseph,
    and Li Shang. 2007. Power, Thermal, and Reliability Modeling in Nanometer-Scale
    Microprocessors. IEEE Micro 27, 3 (2007), 49–62.

    - <span id="page-23-13"></span>[12] Shengze Cai, Zhicheng Wang, Sifan Wang, Paris
    Perdikaris, and George Em Karniadakis. 2021. Physics-informed neural networks
    for heat transfer problems. Journal of Heat Transfer 143, 6 (2021), 060801.

    - <span id="page-23-18"></span>[13] Man Pun Chan and Philip KT Mok. 2013. On-chip
    digital inductor current sensor for monolithic digitally controlled DC-DC converter.
    IEEE Transactions on Circuits and Systems I: Regular Papers 60, 5 (2013), 1232–1240.

    - <span id="page-23-16"></span>[14] Pai-Yu Chen, Xiaochen Peng, and Shimeng Yu.
    2018. NeuroSim: A Circuit-Level Macro Model for Benchmarking Neuro-Inspired Architectures
    in Online Learning. IEEE TCAD-IC 37, 12 (2018), 3067–3080.

    - <span id="page-23-2"></span>[15] Y.-K. Cheng et al. 2020. Next-Generation Design
    and Technology Co-optimization (DTCO) of System on Integrated Chip (SoIC) for
    Mobile and HPC Applications. In Proc. of IEEE IEDM. 41.3.1–41.3.4.

    - <span id="page-23-10"></span>[16] Pavan Kumar Chundi et al. 2017. Hotspot monitoring
    and Temperature Estimation with miniature on-chip temperature sensors. In Proc.
    of IEEE/ACM ISLPED. 1–6.

    - <span id="page-23-19"></span>[17] Martin Cochet, Alberto Puggelli, Ben Keller,
    Brian Zimmer, Milovan Blagojevic, Sylvain Clerc, Philippe Roche, Jean-Luc Autran,
    and Borivoje Nikolić. 2016. On-chip supply power measurement and waveform reconstruction
    in a 28nm FD-SOI processor SoC. In 2016 IEEE Asian Solid-State Circuits Conference
    (A-SSCC). IEEE, 125–128.

    - <span id="page-23-9"></span>[18] COMSOL. [n. d.]. COMSOL Multiphysics Reference
    Manual. [https://comsol.com.](https://comsol.com)<https://www.comsol.com/>

    - <span id="page-23-7"></span>[19] Yasuko Eckert, Nuwan Jayasena, and Gabriel
    H Loh. 2014. Thermal feasibility of die-stacked processing in memory. In Proc.
    of WoNDP.

    - <span id="page-23-11"></span>[20] Furkan Eris, Ajay Joshi, Andrew B. Kahng,
    Yenai Ma, Saiful Mojumder, and Tiansheng Zhang. 2018. Leveraging thermally-aware
    chiplet organization in 2.5D systems to reclaim dark silicon. In 2018 Design,
    Automation & Test in Europe Conference & Exhibition (DATE). 1441–1446.<https://doi.org/10.23919/DATE.2018.8342238>


    - <span id="page-24-12"></span>[21] Yongkui Han, Israel Koren, and C Mani Krishna.
    2007. TILTS: A fast architectural-level transient thermal simulation method. Journal
    of Low Power Electronics 3, 1 (2007), 13–21.

    - <span id="page-24-23"></span>[22] Darong Huang, Luis Costero, and David Atienza.
    2024. An Evaluation Framework for Dynamic Thermal Management Strategies in 3D
    MultiProcessor System-on-Chip Co-Design. IEEE Transactions on Parallel and Distributed
    Systems (2024).

    - <span id="page-24-8"></span>[23] Shih-Chang Huang, Hong-Ping Cheng, Chun-Kai
    Lo, Yung-Chuan Tseng, and Chin-Chi Cheng. 2023. Experimental study of a two-phase
    immersion cooling system for the CPU in a PC and a 2U server operated at the overclocking
    frequency. IEEE Transactions on Components, Packaging and Manufacturing Technology
    13, 6 (2023), 859–864.

    - <span id="page-24-25"></span>[24] S Hutchinson, E Keiter, R Hoekstra, H Watts,
    A Waters, T Russo, R Schells, S Wix, and C Bogdan. 2002. The Xyce™ parallel electronic
    simulator–an overview. Parallel Computing: Advances and Current Issues (2002),
    165–172.

    - <span id="page-24-14"></span>[25] Leslie Hwang, Beomjin Kwon, and Martin Wong.
    2018. Accurate models for optimizing tapered microchannel heat sinks in 3D ICs.
    In Proc. of IEEE Computer Society Annual Symposium on VLSI (ISVLSI). 58–63.

    - <span id="page-24-22"></span>[26] Nan Jiang et al. 2013. A detailed and flexible
    cycle-accurate network-on-chip simulator. In Proc. of ISPASS. IEEE, 86–96.

    - <span id="page-24-19"></span>[27] Waqar Ahmed Khan, J. Richard Culham, and M.
    Michael Yovanovich. 2008. Modeling of Cylindrical Pin-Fin Heat Sinks for Electronic
    Packaging. IEEE Transactions on Components and Packaging Technologies 31, 3 (2008),
    536–545.

    - <span id="page-24-4"></span>[28] Gokul Krishnan et al. 2021. SIAM: Chiplet-based
    scalable in-memory acceleration with mesh for deep neural networks. ACM Transactions
    on Embedded Computing Systems (TECS) 20, 5s (2021), 1–24.

    - <span id="page-24-0"></span>[29] Gokul Krishnan et al. 2022. Big-Little Chiplets
    for In-Memory Acceleration of DNNs: A Scalable Heterogeneous Architecture. In
    2022 IEEE/ACM ICCAD. 1–9.

    - <span id="page-24-13"></span>[30] Beomjin Kwon, Faizan Ejaz, and Leslie K Hwang.
    2020. Machine learning for heat transfer correlations. International Communications
    in Heat and Mass Transfer 116 (2020), 104694.

    - <span id="page-24-17"></span>[31] Theodore L. Bergman, Adrienne S. Lavine, Frank
    P. Incropera, and David P. DeWitt. 2011. Fundamentals of Heat and Mass Transfer.
    John Wiley & Sons.

    - <span id="page-24-15"></span>[32] John H Lau. 2023. Chiplet design and heterogeneous
    integration packaging. Springer.

    - <span id="page-24-10"></span>[33] Fuping Li, Ying Wang, Yuanqing Cheng, Yujie
    Wang, Yinhe Han, Huawei Li, and Xiaowei Li. 2022. GIA: A Reusable General Interposer
    Architecture for Agile Chiplet Integration. In Proceedings of the 41st IEEE/ACM
    International Conference on Computer-Aided Design (San Diego, California) (ICCAD
    ''22). Association for Computing Machinery, New York, NY, USA, Article 42, 9 pages.
    [https://doi.org/10.1145/](https://doi.org/10.1145/3508352.3549464) [3508352.3549464](https://doi.org/10.1145/3508352.3549464)

    - <span id="page-24-21"></span>[34] Xiaoye S Li. 2005. An overview of SuperLU:
    Algorithms, implementation, and user interface. ACM Transactions on Mathematical
    Software (TOMS) 31, 3 (2005), 302–325.

    - <span id="page-24-9"></span>[35] Yuquan Li, R Wayne Johnson, Rui Zhang, Phillip
    Henson, Patrick Thompson, Tejpal Hooghan, and Jeremias Libres. 2010. Ti/au die
    backside metallization for flip chip heat spreader attachment. IEEE transactions
    on electronics packaging manufacturing 33, 1 (2010), 44–54.

    - <span id="page-24-5"></span>[36] Gian Luca Loi et al. 2006. A thermally-aware
    performance analysis of vertically integrated (3-D) processor-memory hierarchy.
    In Proc. of DAC. 991–996.

    - <span id="page-24-3"></span>[37] Xiaoning Ma, Qinzhi Xu, Chenghan Wang, He Cao,
    Jianyun Liu, Daoqing Zhang, and Zhiqiang Li. 2024. An Electrical–Thermal Co-Simulation
    Model of Chiplet Heterogeneous Integration Systems. IEEE Transactions on Very
    Large Scale Integration (VLSI) Systems (2024).

    - <span id="page-24-11"></span>[38] Yenai Ma, Leila Delshadtehrani, Cansu Demirkiran,
    José L Abellan, and Aiav Joshi. 2021. TAP-2.5 D: A thermally-aware chiplet placement
    methodology for 2.5 D systems. In Proc. of DATE. IEEE, 1246–1251.

    - <span id="page-24-7"></span>[39] John E Matsson. 2023. An Introduction to Ansys
    Fluent 2023. Sdc Publications.

    - <span id="page-24-24"></span>[40] Jie Meng, Katsutoshi Kawakami, and Ayse K
    Coskun. 2012. Optimizing energy efficiency of 3-D multicore systems with stacked
    DRAM under power and thermal constraints. In Proc. of DAC. 648–655.

    - <span id="page-24-1"></span>[41] Samuel Naffziger et al. 2021. Pioneering Chiplet
    Technology and Design for the AMD EPYC™ and Ryzen™ Processor Families : Industrial
    Product. In Proc. of ACM/IEEE ISCA. 57–70.

    - <span id="page-24-20"></span>[42] S. Narasimhan and J. Majdalani. 2002. Characterization
    of compact heat sink models in natural convection. IEEE Transactions on Components
    and Packaging Technologies 25, 1 (2002), 78–86.

    - <span id="page-24-16"></span>[43] Gregory Nellis and Sanford Klein. 2008. Heat
    transfer. Cambridge university press.

    - <span id="page-24-2"></span>[44] Jaehyun Park, Alish Kanani, Lukas Pfromm, Harsh
    Sharma, Parth Solanki, Eric Tervo, Janardhan Rao Doppa, Partha Pratim Pande, and
    Umit Y Ogras. 2024. Thermal Modeling and Management Challenges in Heterogenous
    Integration: 2.5 D Chiplet Platforms and Beyond. In 2024 IEEE 42nd VLSI Test Symposium
    (VTS). 1–4.

    - <span id="page-24-6"></span>[45] Sheriff Sadiqbatcha et al. 2019. Hot spot identification
    and system parameterized thermal modeling for multi-core processors through infrared
    thermal imaging. In Proc. of DATE. 48–53.

    - <span id="page-24-18"></span>[46] Suhas M Satheesh and Emre Salman. 2012. Power
    distribution in TSV-based 3-D processor-memory stacks. IEEE Journal on Emerging
    and Selected Topics in Circuits and Systems 2, 4 (2012), 692–703.


    - <span id="page-25-1"></span><span id="page-25-0"></span>[47] Yakun Sophia Shao
    et al. 2021. Simba: Scaling Deep-Learning Inference with Multi-Chip-Module-Based
    Architecture, In Proc. of IEEE/ACM MICRO. Commun. ACM 64, 6, 107–116.

    - <span id="page-25-17"></span>[48] Shervin Sharifi and Tajana Šimunić Rosing.
    2010. Accurate direct and indirect on-chip temperature sensing for efficient dynamic
    thermal management. IEEE TCAD-IC 29, 10 (2010), 1586–1599.

    - <span id="page-25-18"></span>[49] Debendra Das Sharma, Gerald Pasdast, Zhiguo
    Qian, and Kemal Aygun. 2022. Universal chiplet interconnect express (UCIe): An
    open industry standard for innovations with chiplets at package level. IEEE Transactions
    on Components, Packaging and Manufacturing Technology 12, 9 (2022), 1423–1431.

    - <span id="page-25-8"></span>[50] Harsh Sharma et al. 2022. SWAP: A Server-Scale
    Communication-Aware Chiplet-Based Manycore PIM Accelerator. IEEE TCAD-IC 41, 11
    (2022), 4145–4156.

    - <span id="page-25-2"></span>[51] Harsh Sharma et al. 2023. Florets for Chiplets:
    Data Flow-aware High-Performance and Energy-efficient Network-on-Interposer for
    CNN Inference Tasks. ACM Transactions on Embedded Computing Systems 22, 5s (2023),
    1–21.

    - <span id="page-25-6"></span>[52] Kevin Skadron and otehrs. 2004. Temperature-Aware
    Microarchitecture: Modeling and Implementation. ACM Trans. Archit. Code Optim.
    1, 1 (mar 2004), 94–125.

    - <span id="page-25-13"></span>[53] Kevin Skadron, Mircea R. Stan, Karthik Sankaranarayanan,
    Wei Huang, Sivakumar Velusamy, and David Tarjan. 2004. Temperature-Aware Microarchitecture:
    Modeling and Implementation. ACM Trans. Archit. Code Optim. 1, 1 (mar 2004), 94–125.
    [https://doi.org/10.](https://doi.org/10.1145/980152.980157) [1145/980152.980157](https://doi.org/10.1145/980152.980157)

    - <span id="page-25-22"></span>[54] Alan Smith, Gabriel H Loh, Michael J Schulte,
    Mike Ignatowski, Samuel Naffziger, Mike Mantor, Mark Fowler Nathan Kalyanasundharam,
    Vamsi Alla, Nicholas Malaya, Joseph L Greathouse, et al. 2024. Realizing the AMD
    exascale heterogeneous processor vision: industry product. In 2024 ACM/IEEE 51st
    Annual International Symposium on Computer Architecture (ISCA). IEEE, 876–889.

    - <span id="page-25-20"></span>[55] Linghao Song, Xuehai Qian, Hai Li, and Yiran
    Chen. 2017. Pipelayer: A pipelined reram-based accelerator for deep learning.
    In Proc. of HPCA. IEEE, 541–552.

    - <span id="page-25-7"></span>[56] Arvind Sridhar, Alessandro Vincenzi, David
    Atienza, and Thomas Brunschwiler. 2013. 3D-ICE: A compact thermal model for early-stage
    design of liquid-cooled ICs. IEEE Trans. Comput. 63, 10 (2013), 2576–2589.

    - <span id="page-25-3"></span>[57] Dylan Stow, Itir Akgun, Russell Barnes, Peng
    Gu, and Yuan Xie. 2016. Cost analysis and cost-driven IP reuse methodology for
    SoC design based on 2.5D/3D integration. In Proc. of IEEE/ACM ICCAD. 1–6.

    - <span id="page-25-5"></span>[58] Hameedah Sultan, Anjali Chauhan, and Smruti
    R. Sarangi. 2019. A Survey of Chip-level Thermal Simulators. ACM Comput. Surv.
    52, 2, Article 42 (apr 2019), 35 pages.

    - <span id="page-25-15"></span>[59] Pruek Vanna-Iampikul, Lingjun Zhu, Serhat
    Erdogan, Mohanalingam Kathaperumal, Ravi Agarwal, Ram Gupta, Kevin Rinebold, and
    Sung Kyu Lim. 2023. Glass Interposer Integration of Logic and Memory Chiplets:
    PPA and Power/Signal Integrity Benefits. In 2023 60th ACM/IEEE Design Automation
    Conference (DAC). IEEE, 1–6.

    - <span id="page-25-4"></span>[60] Martin Won. 2023. Agilex FPGAs Deliver a Game-Changing
    Combination of Flexibility and Agility for the Data-Centric World. Technical Report.
    Intel.

    - <span id="page-25-19"></span>[61] Aibin Yu, John H Lau, Soon Wee Ho, Aditya
    Kumar, Wai Yin Hnin, Wen Sheng Lee, Ming Ching Jong, Vasarla Nagendra Sekhar,
    Vaidyanathan Kripesh, Damaruganath Pinjala, et al. 2011. Fabrication of high aspect
    ratio TSV and assembly with fine-pitch low-cost solder microbump for Si interposer
    technology with high-density interconnects. IEEE transactions on components, packaging
    and manufacturing technology 1, 9 (2011), 1336–1344.

    - <span id="page-25-14"></span>[62] Zihao Yuan et al. 2021. PACT: An extensible
    parallel thermal simulator for emerging integration and cooling technologies.
    IEEE TCAD-IC 41, 4 (2021), 1048–1061.

    - <span id="page-25-16"></span>[63] Francesco Zanini, David Atienza, Colin N.
    Jones, and Giovanni De Micheli. 2010. Temperature sensor placement in thermal
    management systems for MPSoCs. In Proc. of ISCAS. 1065–1068.

    - <span id="page-25-11"></span>[64] Florian Zaruba, Fabian Schuiki, and Luca Benini.
    2020. A 4096-core RISC-V chiplet architecture for ultra-efficient floating-point
    computing. In Proc. of IEEE Hot Chips 32 Symposium. IEEE Computer Society, 1–24.

    - <span id="page-25-9"></span>[65] Jinwei Zhang et al. 2021. Full-chip power density
    and thermal map characterization for commercial microprocessors under heat sink
    cooling. IEEE TCAD-IC. 41, 5 (2021), 1453–1466.

    - <span id="page-25-10"></span>[66] Yufu Zhang, Ankur Srivastava, and Mohamed
    Zahran. 2008. Chip level thermal profile estimation using on-chip temperature
    sensors. In IEEE Intl. Conf. on Computer Design. 432–437.

    - <span id="page-25-21"></span>[67] Minxuan Zhou et al. 2020. Temperature-Aware
    DRAM Cache Management—Relaxing Thermal Constraints in 3-D Systems. IEEE TCAD-IC
    39, 10 (2020), 1973–1986.

    - <span id="page-25-12"></span>[68] Minghao Zhou, Li Li, Fengze Hou, Guoqiang
    He, and Jiaqi Fan. 2022. Thermal Modeling of a Chiplet-Based Packaging With a
    2.5-D Through-Silicon Via Interposer. IEEE Transactions on Components, Packaging
    and Manufacturing Technology 12, 6 (2022), 956–963.'
- title: "Messaging-based Intelligent Processing Unit (m-IPU) for next generation\n\
    \  AI computing"
  abstract: 'Recent advancements in Artificial Intelligence (AI) algorithms have sparked
    a

    race to enhance hardware capabilities for accelerated task processing. While

    significant strides have been made, particularly in areas like computer vision,

    the progress of AI algorithms appears to have outpaced hardware development, as

    specialized hardware struggles to keep up with the ever-expanding algorithmic

    landscape. To address this gap, we propose a new accelerator architecture,

    called messaging-based intelligent processing unit (m-IPU), capable of runtime

    configuration to cater to various AI tasks. Central to this hardware is a

    programmable interconnection mechanism, relying on message passing between

    compute elements termed Sites. While the messaging between compute elements is

    a known concept for Network-on-Chip or multi-core architectures, our hardware

    can be categorized as a new class of coarse-grained reconfigurable architecture

    (CGRA), specially optimized for AI workloads. In this paper, we highlight

    m-IPU''s fundamental advantages for machine learning applications. We illustrate

    the efficacy through implementations of a neural network, matrix

    multiplications, and convolution operations, showcasing lower latency compared

    to the state-of-the-art. Our simulation-based experiments, conducted on the

    TSMC 28nm technology node, reveal minimal power consumption of 44.5 mW with

    94,200 cells utilization. For 3D convolution operations on (32 x 128) images,

    each (256 x 256), using a (3 x 3) filter and 4,096 Sites at a frequency of 100

    MHz, m-IPU achieves processing in just 503.3 milliseconds. These results

    underscore the potential of m-IPU as a unified, scalable, and high-performance

    hardware architecture tailored for future AI applications.'
  url: http://arxiv.org/abs/2410.09961v1
  keywords: '**—Machine Learning, Hardware Accelerator, Matrix Multiplication, Convolution,
    Reconfigurable Computing.**'
  document: "# Messaging-based Intelligent Processing Unit (m-IPU) for next generation\
    \ AI computing\n\nMd. Rownak Hossain Chowdhury and Mostafizur Rahman, *Senior\
    \ Member, IEEE*\n\n*Abstract***— Recent advancements in Artificial Intelligence\
    \ (AI) algorithms have sparked a race to enhance hardware capabilities for accelerated\
    \ task processing. While significant strides have been made, particularly in areas\
    \ like computer vision, the progress of AI algorithms appears to have outpaced\
    \ hardware development, as specialized hardware struggles to keep up with the\
    \ everexpanding algorithmic landscape. To address this gap, we propose a new accelerator\
    \ architecture, called messaging-based intelligent processing unit (m-IPU), capable\
    \ of runtime configuration to cater to various AI tasks. Central to this hardware\
    \ is a programmable interconnection mechanism, relying on message passing between\
    \ compute elements termed Sites. While the messaging between compute element is\
    \ a known concept for the Network on Chip or multi-core architectures, our hardware\
    \ can be categorized as a new class of coarse-grained reconfigurable architecture\
    \ (CGRA) specially optimized for AI workloads. In this paper, we highlight m-IPU's\
    \ fundamental advantages for machine learning applications. We illustrate the\
    \ efficacy through implementations of a neural network, matrix multiplications\
    \ and convolution operations, showcasing lower latency compared to state-of-the-art.\
    \ Our simulation-based experiments, conducted on the TSMC 28nm technology node,\
    \ reveal minimal power consumption of 44.5 mW with 94200 cells utilization. For\
    \ 3D convolution operations on (32 X 128) images, each (256 X 256), using a (3\
    \ X 3) filter and 4096 Sites at a frequency of 100 MHz, m-IPU achieves processing\
    \ in just 503.3 milliseconds. Additionally, m-IPU delivers a throughput of 142.45\
    \ million images/second for small neural network comprising one convolution layer,\
    \ one pooling layer, and two fully connected layers processing 20K 5X5 images\
    \ at 1 GHz frequency utilizing only 48 SiteOs. These results underscore the potential\
    \ of m-IPU as a unified, scalable, and high-performance hardware architecture\
    \ tailored for future AI applications.**\n\n*Index Terms***—Machine Learning,\
    \ Hardware Accelerator, Matrix Multiplication, Convolution, Reconfigurable Computing.**\
    \ \n\n#### I. INTRODUCTION\n\nThe advent of Artificial intelligence (AI), particularly\
    \ Deep Neural Networks (DNNs) has prompted a paradigm shift in various fields\
    \ such as computer vision, natural language processing, robotics, and many more.\
    \ As a result, modern tech industries are aggressively integrating advanced neural\
    \ network architectures like AlexNet [1], VGGNet [2], GoogLeNet [3], ResNet [4],\
    \ LSTM [5], GRU [6], Transformers [7] to uplift customer experience and maintain\
    \ a competitive edge. However, the fast-paced advancements and escalating computational\
    \ demands of AI have exposed significant limitations in traditional computing\
    \ systems, especially as Dennard's scaling becomes obsolete [\\[8\\]](#page-10-0)\
    \ and Moore's law nears its end [\\[9\\].](#page-10-1) This spurs innovation in\
    \ designing specialized hardware architecture tailored to application domains,\
    \ rather than merely optimizing existing ones [\\[10\\],](#page-10-2) ushering\
    \ in a new era of domain-specific computer architecture development to improve\
    \ performance persistentl[y\\[11\\].](#page-10-3)\n\nRecent years have witnessed\
    \ substantial research efforts aiming at efficient of processing AI workloads.\
    \ As a result, several specialized hardware accelerators, such as TP[U\\[12\\\
    ],](#page-10-4) MEISS[A\\[13\\],](#page-10-5) SpArc[h\\[14\\],](#page-10-6) MatRapto[r\\\
    [15\\],](#page-10-7) CGRA-based Inference Engine [\\[16\\]](#page-10-8) have emerged\
    \ to address the growing demands of computationally intensive AI tasks. While\
    \ these architectures do outperform traditional CPU and GPU architectures, they\
    \ lack generality in terms of application. Reconfigurability either in logic or\
    \ routing capabilities is essential to map the variety of AI algorithms that are\
    \ emerging.\n\nIn this study, we present a novel programmable hardware architecture\
    \ distinguished by its messaging-based interconnection system. The m-IPU's configurability\
    \ stems from its flexible interconnections, which facilitate the spontaneous mapping\
    \ and processing of large data volumes via a message-passing scheme, eliminating\
    \ the need for host processor intervention. In conjunction with this innovative\
    \ computing framework, we have optimized the hardware design at the micro-architectural\
    \ level to enhance overall performance. Our hierarchical and modular design approach\
    \ ensures a scalable architecture that can adapt to the evolving demands of AI\
    \ applications. Consequently, our matrix multiplication method utilizing the m-IPU\
    \ engine exhibits lower latency compared to state-of-the-art designs such as MEISSA\
    \ and TPU, while also enabling high throughput for convolution operations through\
    \ messaging-based intelligence. The key contributions of this paper are as follows:\n\
    \n- Details of the mIPU architecture and its contrast with TPU and other reconfigurable\
    \ architectures such as FPGA and CGRA.\n- Mapping of AI tasks such as matrix multiplication,\
    \ convolution operation in m-IPU.\n- Architecture evaluation through synthesis\
    \ and emulation at TSMC 28nm technology node.\n- Comparison of key performance\
    \ metrics with other architectures\n\nThis paper is organized as follows. Section\
    \ II reviews the prior works in reconfigurable hardware architecture domain and\
    \ highlights the motivation to develop specialized hardware tailored for AI applications.\
    \ Section III delves into the m-IPU architecture with system-level overview and\
    \ demonstration of hardware hierarchy. Next, Section IV outlines the methodology\
    \ for performing matrix multiplication and convolution operations, describing\
    \ the data mapping strategy within the mIPU fabric. Section V elaborates on the\
    \ implementation set up to validate our design under TSMC 28nm technology and\
    \ highlights record performance of key design metrics such as resource utilization,\
    \ power profile, latency, and throughput. This section also provides a comparative\
    \ analysis with other cutting-edge architectures. Finally, section VI summarizes\
    \ the concluding remarks emphasizing key contributions of the m-IPU.\n\n# II.\
    \ PRIOR WORK & MOTIVATION\n\nThe merit of an AI accelerator hinges on the balance\
    \ between its efficiency and flexibility. Hence, the performance of an AI accelerator\
    \ lies in its ability to support diverse AI models ensuring high throughput and\
    \ energy optimization across a wide range of neural network configurations [\\\
    [20\\].](#page-10-9) This trade-off has driven AI accelerator research towards\
    \ two distinct computing paradigms: a) Reconfigurable computing and b) Task specific\
    \ Specialized computing. Specialized architectures are highly optimized for dedicated\
    \ tasks, but they often fall short in addressing the broader spectrum of application\
    \ requirements. For instance, SpArch and MatRaptor are fine-tuned for highly sparse\
    \ networks like the Amazon copurchase networ[k\\[21\\],](#page-10-10) while MEISSA\
    \ leads in low-latency tasks in edge devices like autonomous drone[s\\[22\\],](#page-10-11)\
    \ and TPUs dominate high-throughput scenarios like data center[s\\[12\\].](#page-10-4)\
    \  Despite their strengths, this rigid application-centric approach will lead\
    \ to shorter lifecycles and higher nonrecurring engineering costs while deploying\
    \ different AI workload[s\\[23\\]](#page-10-12)[\\[24\\].](#page-10-13)\n\nIn\
    \ contrast, reconfigurable architectures provide the required flexibility, to\
    \ dynamically adapt hardware for various computational task[s\\[25\\].](#page-10-14)\
    \ Hence, several reconfigurable computing concepts like Coarse-Grained Reconfigurable\
    \ Architectures (CGRAs[\\)\\[26\\],](#page-10-15) Field-Programmable Gate Arrays\
    \ (FPGAs[\\)\\[27\\],](#page-10-16) memristor-based reconfigurable circui[t\\\
    [28\\],](#page-10-17) finegrained polymorphic circui[t\\[29\\],](#page-10-18)\
    \ noise-based configurable computin[g\\[30\\],](#page-10-19) crosstalk built-in\
    \ memory [\\[31\\]](#page-10-20) have been introduced to enhance the computational\
    \ capabilities. While specialization enhances efficiency, incorporating reconfigurability\
    \ often incurs architectural overhead. For example, FPGAs offer fine-grained reconfigurability\
    \ at the logic gate level, allowing virtually any kind of circuit to be implemented,\
    \ but suffer from slower clock speeds and longer configuration time[s\\[32\\].](#page-10-21)\
    \ CGRAs, on the other hand, balances between flexibility and specialization by\
    \ offering a coarsergrained reconfiguration, positioning themselves as a middle\
    \ ground between FPGAs and more rigid, highly specialized ASIC[s\\[33\\].](#page-10-22)\n\
    \nCGRAs are specifically designed for data-parallel tasks such as digital signal\
    \ processing, image processing, and machine learning, utilizing an array of processing\
    \ elements (PEs) that can be reconfigured for specific operation[s\\[34\\].](#page-10-23)\
    \  However, despite their versatility, CGRAs pose significant deficiencies when\
    \ applied for machine learning or data analytics tasks. For example, the CGRA-based\
    \ VersatCNN architectur[e\\[35\\],](#page-10-24) while designed to provide efficient\
    \ acceleration for CNN inference in embedded systems, faces several limitations.\
    \ The frequent reconfiguration required for different CNN layers and operations,\
    \ controlled by multiple address generator units (AGUs), introduces significant\
    \ overhead, particularly during transitions between convolution, max-pooling,\
    \ and up sampling layers. This reconfiguration process, although flexible, impacts\
    \ real-time performance. Additionally, the fixed parallelism of 832 MAC units\
    \ is optimized for certain layers but may lead to underutilization in layers with\
    \ fewer computations or more irregular dataflows, reducing overall efficiency\
    \ [\\[36\\].](#page-10-25) Furthermore, while convolution layers are highly optimized,\
    \ non-convolutional operations such as post-processing routines (e.g., object\
    \ detection box drawing) and control-heavy functions remain handled by the RISC-V\
    \ CPU, limiting the speedup potential. These limitations affect the architecture's\
    \ adaptability to more diverse and complex workloads, ultimately restricting its\
    \ scalability across varying CNN models.\n\nThe CGRA-based EMAX architecture proposed\
    \ by Tanomoto et al. [\\[37\\]](#page-10-26) is centered around a grid of PEs,\
    \ each containing fixed 32-bit, 2-stage floating-point units (EX1, EX2), FIFOs,\
    \ and local memory to facilitate parallelism. However, the fixed nature of the\
    \ execution units leads to inefficiencies when dealing with complex or irregular\
    \ tasks, such as non-linear activations and pooling operations, resulting in underutilization\
    \ of hardware resources [\\[38\\].](#page-10-27) The static configuration, although\
    \ allowing flexible mapping of operations, lacks dynamic reconfigurability, further\
    \ limiting the system's ability to adapt to diverse and evolving CNN task[s\\\
    [39\\].](#page-10-28) Moreover, while pipeline parallelization is implemented\
    \ for both forward and backward propagation, the fixed PEs and the double-buffering\
    \ mechanism introduce synchronization delays, particularly in convolutional layers\
    \ with larger kernels, ultimately causing bottlenecks that slow down overall performance\
    \ in deeper neural networks.\n\nYin et al. [\\[40\\]](#page-10-29) present a reconfigurable\
    \ hybrid-neuralnetwork processor that achieves energy efficiency of 1.06-to-5.09\
    \ TOPS/W for deep learning applications, but its limitations stem from performance\
    \ trade-off between energy consumption and spee[d\\[41\\].](#page-10-30) The processor\
    \ operates at lower frequencies, as low as 10MHz in certain modes, sacrificing\
    \ real-time performance and throughput for energy savings. Additionally, it relies\
    \ on low-precision (8-16 bit) computations, making it less suitable for tasks\
    \ that require 32-bit precision, such as model training. The architecture's use\
    \ of heterogeneous processing elements, which enable specialized operations like\
    \ pooling and RNNs, adds complexity, necessitating dynamic partitioning of processing\
    \ elements (PEs). This partitioning, controlled by an external host, incurs significant\
    \ overhead, limiting the processor's adaptability in real-time environments.\n\
    \nThe Eyeriss architectur[e\\[42\\],](#page-10-31) though optimized for energy\
    \ efficiency, encounters limitations related to its reconfiguration and data movement\
    \ mechanisms. The 1794-bit scan chain used for reconfiguring the system between\
    \ CNN layers introduces overhead and limits real-time adaptability, making the\
    \ architecture less effective in deep networks requiring frequent reconfiguratio[n\\\
    [43\\].](#page-10-32) The Network-on-Chip (NoC) effectively reduces off-chip data\
    \ movement but struggles with synchronization delays during the processing of\
    \ larger convolutional layers, such as in VGG-16, resulting in reduced throughpu[t\\\
    [44\\].](#page-10-33) These limitations, including reconfiguration overhead, synchronization\
    \ bottlenecks in the NoC constrain Eyeriss's scalability and performance when\
    \ applied to more complex and computationally demanding neural network architectures.\n\
    \nGiven these constraints, future AI hardware architectures require innovative,\
    \ runtime-reconfigurable computing solutions, which the messaging-based intelligent\
    \ processing unit (mIPU) aims to fulfill. In mIPU, each message encodes both the\
    \ current and next opcodes, along with spatial addresses and the corresponding\
    \ data. This encoding allows the underlying hardware to not only execute the immediate\
    \ tasks but also inherently set up future configurations. While domainspecific\
    \ AI accelerators like Google's Tensor Processing Unit (TPU) are optimized for\
    \ specific computations (e.g., Multiply and Accumulate) within fixed data flows\
    \ (systolic arrays for matrix multiplication[\\)\\[12\\],](#page-10-4) the mIPU\
    \ stands apart by offering dynamic reconfigurability. mIPU leverages an agile\
    \ messagepassing system where data routes based on message content, enabling the\
    \ architecture to handle varied data dimensions and flows. Moreover, its internal\
    \ message generation and propagation mechanism further reduce expensive memory\
    \ read operations and data movement, enabling mIPU in adapting to diverse AI workloads,\
    \ overcoming the limitations of specialized accelerators.\n\nThe micro-architecture\
    \ of mIPU, like CGRAs, organizes its computational units in a 2D grid. However,\
    \ unlike CGRAs, which are designed to cater to a wide variety of applications,\
    \ mIPU is purpose-built to map various AI operations like convolution, RELU and\
    \ pooling. This makes mIPU a domainspecific, self-reconfigurable hardware accelerator\
    \ with a focus on AI tasks, while still retaining certain traits like domainagnostic\
    \ CGRAs. Moreover, m-IPU distinguishes itself from CGRA in its execution model\
    \ to achieve reconfigurability. In CGRAs, processing elements are configured either\
    \ statically [\\[45\\]](#page-10-34)[\\[46\\]](#page-10-35)[\\[47\\]](#page-10-36)[\\\
    [48\\]](#page-10-37) (at compile time) or dynamically [\\[49\\]](#page-11-0)[\\\
    [50\\]](#page-11-1)[\\[51\\]](#page-11-2)[\\[52\\]](#page-11-3) (based on runtime\
    \ states), where operations are either executed in a predefined order or based\
    \ on operand readiness. But this approach restricts CGRAs adaptability in AI applications\
    \ that requires frequent real-time adjustments. For example, autonomous driving\
    \ demands continuous decision updates from real-time sensor input, while NLP tasks\
    \ require swift adjustments to accommodate evolving model architectures during\
    \ executio[n\\[53\\].](#page-11-4) In contrast to CGRAs, mIPU embeds both present\
    \ and future execution instructions within each message, inherently enabling continuous\
    \ reconfiguration without external intervention. Thus, m-IPU allows for seamless\
    \ task management on-the-fly, offering more agility and faster reconfiguration\
    \ than CGRAs, which often struggle with real-time adjustments and dynamic workloads.\n\
    \nAnother key distinction of mIPU with other architectures lies in its flexible\
    \ interconnection mechanisms. The microarchitecture of mIPU employs a runtime\
    \ message-passing strategy, where messages flow between neighboring computational\
    \ elements via interconnected pathways and through adjacent tiles using a fine-grained\
    \ hierarchical bus structure. Unlike network on chips (NoCs), which rely on predefined\
    \ pathways and routing algorithms to optimize data flo[w\\[54\\],](#page-11-5)\
    \ m-IPU's message-driven routing paths enables flexible, real-time data transfer.\
    \ This dynamic, message-driven system not only simplifies data flow management\
    \ but also minimizes hardware overhead. By minimizing the interconnection architecture,\
    \ mIPU allows for a greater number of computational units to be integrated, enhancing\
    \ parallelism and achieving higher throughput— areas where rigid NoC structures\
    \ often fall short.\n\n#### III. M-IPU ARCHITECTURE\n\nMessaging based Intelligent\
    \ Processing Unit (mIPU) is a reconfigurable computing architecture whose computing\
    \ and memory elements are parallel and distributed. The cornerstone of our programmable\
    \ hardware architecture is flexible virtual interconnections, which is pivotal\
    \ for integrating messaging-\n\n![](_page_2_Figure_8.jpeg)\n\n**Figure 1 A) System\
    \ Overview B) Message Encoding Scheme C) Instruction Set Architecture of m-IPU.**\n\
    \nbased intelligence and thereby enabling efficient information processing. The\
    \ proposed interconnect configuration is analogous to message passing in a human\
    \ chai[n\\[55\\].](#page-11-6) For instance, if there are 5 people in a line and\
    \ they want to pass messages from person #1 (Source) to person #5 (Destination),\
    \ then persons #2, #3, and #4 form a virtual link between source and destination\
    \ in that message passing scheme. This source, destination-based message passing\
    \ scheme can be applied to computing cores as well. The m-IPU, leveraging this\
    \ sourcedestination based message passing mechanism can behave as a custom-ASIC\
    \ at runtime for each running AI applications to deliver the best performance.\n\
    \nA system overview of the m-IPU is depicted in Figure 1(A). Here, a host CPU\
    \ is required (like GPUs and other accelerators) to interpret high-level language\
    \ (e.g., C, Python, etc.) and translate them into messages that m-IPU can operate\
    \ upon and collect outputs form the m-IPU. Inside the m-IPU all communication\
    \ between computing elements is performed through messages. The encoding scheme\
    \ of a 64-bit message in m-IPU is shown in Figure 1 (B). A message can be segmented\
    \ generate new message. To program m-IPU on-the-fly, we developed a lightweight\
    \ instruction set architecture (ISA) as shown in Fig. 1(C) comprising only 13\
    \ instructions.\n\n#### *A. Hardware Details*\n\nThe m-IPU follows a modular and\
    \ hierarchical design approach enabling it to be scalable irrespective of data\
    \ size or model complexity. The construction and segmentation of the m-IPU architecture\
    \ is outlined in Figure 2 (A-E). Inside the m-IPU engine, there is an array of\
    \ Quads. A Quad is a collection of 4 Blocks, and a Block is a collection of 16\
    \ Tiles. Each Tile consists of 16 SiteMs and each SiteM incorporates 16 SiteOs.\
    \ The Quads, Blocks, Tiles, and SiteMs hierarchy allows task distribution and\
    \ parallel computing. The SiteOs are the core elements and are analogous to Threads\
    \ of GPUs or the Processing Elements (PEs) of TPUs. SiteOs are organized in rows\
    \ and columns and the programmable interconnections between SiteOs allows the\
    \ messages to be routed any cores. A SiteM collects all these messages and outputs\
    \ 12 messages (4 for its own Tile, 4 for other Tiles within the same row, and\
    \ 4 for different columns/Blocks) at a time. Like SiteMs organization\n\n![](_page_3_Figure_6.jpeg)\n\
    \n**Figure 2 Details of A) m-IPU architecture B) Tile C) SiteM D) SiteO E) Hierarchy**\n\
    \ninto 5 parts: a) Present Opcode (from bit position 0 to 3), b) Present Destination\
    \ (from bit position 4 to 15), c) Values to be stored / operated (from bit position\
    \ 16 to 47), d) Next Opcode (from bit position 48 to 51), and e) Next Destination\
    \ (from bit position 52 to 63). In this framework, messages are routed to the\
    \ desired hardware unit within the m-IPU based on the Present Destination, where\
    \ an operation is performed on the value embedded in the message according to\
    \ the Present Opcode. Subsequently, the Next Opcode and Next Destination specified\
    \ in the message are retained inside the m-IPU to in a Tile, a collection of Tiles\
    \ is called Blocks. The Blocks communicate with each other through local and global\
    \ buses. Each block also contains distributed embedded memory elements to store\
    \ further instructions.\n\nThe fundamental computing core of the m-IPU is SiteO,\
    \ which is responsible for both computation and message passing. The hardware\
    \ architecture of SiteO as shown in Figure 2(D) mainly comprises floating point\
    \ unit (FPU), FIFO, decoder, counter, and register. Associated with each SiteO\
    \ there is a small 8-word memory buffer to store the next set of instructions.\
    \ The SiteOs also contain SRAMs to store weights. The SiteOs execute 32-bit IEEE\
    \ 754 arithmetic operations, such as addition, multiplication, and subtraction,\
    \ utilizing the Floating-Point Unit (FPU). SiteOs can receive messages either\
    \ from the top or the left direction and they release outputs either at the right\
    \ or the bottom direction. They are also aware of their neighbors (i.e., addresses\
    \ of neighbor SiteOs in right, left, up, and down are stored in each SiteO). There\
    \ are 2 FIFOs (Left and Top) to store incoming messages and push them towards\
    \ execution or exit route in a pipelined manner. If the FIFOs are empty, the turnout\
    \ time for in and out for a message is 1 cycle. If the FIFOs are full, the senders\
    \ are sent a full signal to stop sending. The phase when stationary values are\
    \ first loaded is called programming. The instruction set architecture for the\
    \ m-IPU is depicted in Figure 1 (c), and a set of messages for an example neural\
    \ network is detailed in the Table IV of the appendix section as supplementary\
    \ file.\n\nWhen a message arrives at SiteO, it first checks whether the destination\
    \ of the message is its address, and if it matches, then the message is decoded\
    \ and the instruction embedded within the message is executed, otherwise, the\
    \ message is passed on. After decoding a message, a SiteO can perform either message\
    \ streaming or message forwarding. Streaming and forwarding are two different\
    \ tasks; in the case of streaming, the SiteO receiving the message send it to\
    \ its preferred neighbor by updating the message, whereas, in forwarding, the\
    \ SiteO just behaves as a buffer to pass messages without intervention. Each Site,\
    \ upon receiving or generating a message, checks whether the destination is within\
    \ the same row or not; if it is, then it sends the message to the right and to\
    \ down otherwise. Eventually, through hopping Sites, a message reaches its destination.\
    \ If the messages are to be routed/passed downward, those messages are labeled\
    \ as Tile message and if they are passed rightward (within the same SiteO row),\
    \ those are labeled as Local messages. To serve two different purposes such as\
    \ Data loading and mathematical operation, m-IPU needs just 13 instructions. Here,\
    \ 1 instruction (*Prog*) is required for loading data inside m-IPU and the remaining\
    \ 12 instructions (*UPDATE*, *A\\_ADD*, *A\\_ADDS*, *A\\_SUB*, *A\\_SUBS*, *A\\\
    _MUL*, *A\\_MULS*, *A\\_DIV*, *A\\_DIVS*, *Av\\_ADD*, *RELU*, *CMP*) perform mathematical\
    \ operations.\n\nA SiteM is designed arranging 16 SiteOs in rows and columns.\
    \ Each column and row of the SiteM is equipped with four vertical and horizontal\
    \ buses, respectively. The bus topology enables messages to be dispatched simultaneously\
    \ at multiple SiteO locations rather than hopping thereby improving latency. This\
    \ concept can be mimicked and extended to develop other hierarchical structures\
    \ like tile, block, and quad. A Tile can have messages destined to itself (i.e.,\
    \ coming from within the Tile or outside the Tile), called Tile messages, and\
    \ have incoming messages destined for other Tiles within the same row (called\
    \ Local messages with respect to Blocks) and same column (called Block messages).\n\
    \n### IV. M-IPU COMPUTATION\n\nMatrix multiplication and convolution operations\
    \ are key computational kernel in various state-of-the-art AI applications. Hence,\
    \ the performance of AI accelerators lies in the efficient execution of matrix\
    \ multiplication and convolution operations. As AI technology advances, the complexity\
    \ of neural networks continues to increase. For example, CNN architectures encompass\
    \ multiple layers with varying filter size, padding, and stride to convert input\
    \ image volume to output preserving class score[s\\[56\\].](#page-11-7) Ideally,\
    \ the convolution operation is a repetitive sliding dot product according to filter\
    \ size. However, this\n\n![](_page_4_Figure_7.jpeg)\n\n**Figure 3 Matrix Multiplication\
    \ Approach in m-IPU.**\n\nconventional approach requires many operations to be\
    \ performed; hence, effective data mapping is parallelly significant along with\
    \ efficient hardware architectures to accelerate convolution operations. Moreover,\
    \ the performance metrics and data processing capabilities of convolution operation\
    \ vary according to the application. Real-time systems such as autonomous vehicle[s\\\
    [57\\],](#page-11-8) smartphone[s\\[58\\],](#page-11-9) medical device[s\\[59\\\
    ],](#page-11-10) robotics [\\[60\\]](#page-11-11) necessitates instant decision\
    \ making based on sensory inputs. However, due to resource limitations, they are\
    \ capable of single-batch processing; hence, the key design parameter in those\
    \ applications is latency. On the other hand, applications like video processin[g\\\
    [61\\],](#page-11-12) language modelin[g\\[62\\],](#page-11-13) speech recognitio[n\
    \ 0](#page-11-14) process multiple batches at the same time; as a result, requires\
    \ high throughput. This section will explore both the matrix multiplication and\
    \ convolution operations in m-IPU, catering to the varying needs of these AI applications.\n\
    \n#### *A. Matrix-Matrix Multiplication*\n\nThe matrix multiplication operation\
    \ inside m-IPU can be divided into four steps: a) Data (Matrix A) Load, b) Data\
    \ (Matrix B) Load and multiply, c) Addition, and d) Offload. Figure 3 illustrates\
    \ a matrix-matrix multiplication example within mIPU utilizing a SiteM. Here,\
    \ the inputs for the matrix multiplication are Matrix A (4 X 3) and Matrix B (3\
    \ X 3). According to the data mapping indicated in Figure 3, matrix A will be\
    \ loaded only once following Map1. After that, each column of the matrix B will\
    \ be loaded sequentially according to Map2, Map3, and Map4. Thus, the matrix multiplication\
    \ can be surmised as three (3) matrix-vector multiplication operations. Therefore,\
    \ the matrix-vector multiplication between matrix A and each column of matrix\
    \ B will be repeated 3 times in one SiteM. The corresponding result of each matrix-vector\
    \ multiplication will be accumulated to achieve the final output. Alternatively,\
    \ the intended multiplication operation can also be completed using 3 SiteM fabrics\
    \ of the m-IPU; where, 3 SiteMs will perform 3 matrix-vector multiplications parallelly\
    \ in 3 SiteMs. Thus, the matrix-matrix multiplication can be acquired within the\
    \ same time of matrix-vector multiplication using 3 SiteMs. Mathematically, the\
    \ number of SiteOs and the latency needed to execute a matrix-matrix multiplication\
    \ of Matrix A (N X M) and Matrix B (M X P) can be calculated using the equation\
    \ (1) and (2) respectively.\n\n$$\\text{SiteO}\\_{MM} = \\{ (\\text{N } X \\,\
    \ M) + N \\} \\, X \\, P \\tag{1}$$\n\n$$T\\_{MM} = N + P + 2 \\tag{2}$$\n\n*B.\
    \ Convolution Neural Network*\n\nTo illustrate the functionality of a convolutional\
    \ neural\n\n| TABLE I<br>Network Configuration and performance of an Example<br>Convolutional\
    \ Neural Network considered for illustration |      |                |       |\
    \                               |       |                |         |         \
    \        |         |\n|----------------------------------------------------------------------------------------------------------------------------|------|----------------|-------|-------------------------------|-------|----------------|---------|-----------------|---------|\n\
    | Layer                                                                      \
    \                                                |      | Feature<br>Map |   \
    \    | NETWORK CONFIGURATION<br>Size |       | Kernel<br>Size | Stride  | Activation\
    \      |         |\n| IN                                                     \
    \                                                                    |      |\
    \ Image          | 1     |                               | 5X5   |           \
    \     | -       | -               | -       |\n| 2                           \
    \                                                                            \
    \                   | Conv |                | 4     |                        \
    \       | 3X3X4 |                | 3X3     | 0               | RELU    |\n| 3\
    \                                                                            \
    \                                              | Pool |                | 4   \
    \  |                               | 2X2X4 |                | 2X2     | 1    \
    \           | RELU    |\n| 4                                                 \
    \                                                                         | FC\
    \   |                | -     |                               | 16    |       \
    \         | -       | -               | RELU    |\n| OUT                     \
    \                                                                            \
    \                       | FC   |                | -     |                    \
    \           | 4     |                | -       | -               | SOFTMAX |\n\
    | HARDWARE PERFORMANCE (Precision FP32)                                      \
    \                                                |      |                |   \
    \    |                               |       |                |         |    \
    \             |         |\n|                                                 \
    \                                                                           |\
    \      | Freq.          |       |                               |       |    \
    \            | Images/ | Throughput      |         |\n| #SiteOs              \
    \                                                                            \
    \                          |      |                | (GHz) |                 \
    \              | Batch |                | Batch   | (Images/Second) |        \
    \ |\n| 48                                                                    \
    \                                                     |      | 1             \
    \ |       | 1                             |       |                | 20000   |\
    \ 142.45 X 106    |         |\n\n![](_page_5_Figure_10.jpeg)\n\n**Figure 4 Data\
    \ mapping for Convolutional Neural Network (CNN) in m-IPU.**\n\n![](_page_6_Figure_1.jpeg)\n\
    \n**Figure 5 Demonstration of messaging-based computation for Convolution Layer\
    \ in m-IPU.**\n\nnetwork (CNN) within the mIPU architecture, we consider a simple\
    \ network configuration. This network involves processing a 5x5 single-channel\
    \ image through a series of layers, starting with a convolution layer equipped\
    \ with four 3x3 filters (using a stride of 1 and no padding), and followed by\
    \ a 2x2 pooling layer (with a stride of 1) as shown in Table I.\n\nThe system\
    \ employs 3 SiteMs (SiteM0, SiteM1, SiteM2) to manage all network operations.\
    \ To optimize hardware performance, we have prudently analyzed CNN architecture's\
    \ behavior, aiming to devise an efficient mapping strategy. During weights loading,\
    \ we prioritize data that must travel longer intra-fabric distances. This ensures\
    \ such data is in place by the time data covering shorter distances is processed,\
    \ thereby minimizing overall latency and improving synchronization across the\
    \ computing nodes. Secondly, during convolution operation the same image is multiplied\
    \ with different filter values; therefore, different filters are loaded into a\
    \ single SiteM, allowing the same image values to be consistently transmitted\
    \ via the vertical bus. Next, we strategically send specific image values to specific\
    \ SiteMs to ensure maximum data reusability since every pooling values always\
    \ depends on certain blocks of images. Even though this approach is doing some\
    \ redundant operations, however, it does not deteriorate the performance significantly\
    \ as all SiteMs are being operated parallelly.\n\nThe operational strategy for\
    \ the convolutional neural network (CNN) can be divided into two main phases:\
    \ i) Phase-1: Weights Loading, and ii) Phase-2: Operations, which include multiplications,\
    \ additions, activations, and comparisons (max pooling). For the implementation\
    \ of CNN operations, two distinct message types as listed in Table IV of the appendix\
    \ section as supplementary file are utilized: i) Type-1, where the present opcode\
    \ is \"Prog\", responsible for weights loading, and ii) Type-2, where the present\
    \ opcode is not \"Prog\", responsible for carrying out operational tasks. Each\
    \ SiteM receives messages via two different routes: either through the top four\
    \ ports of the respective SiteM or through the vertical bus. Data flow within\
    \ the SiteM occurs through two primary mechanisms: i) Hopping, and ii) Vertical\
    \ Bus. During hopping, data travels to the intended SiteO locations across multiple\
    \ SiteOs, based on the address of the target SiteO location. In contrast, the\
    \ vertical bus facilitates the routing of the same data to multiple SiteOs within\
    \ the same SiteM. Data hopping proves advantageous during Phase-1 (Weights Loading),\
    \ where each SiteO requires programming with distinct values. In Phase-2 (Operations),\
    \ when identical values must be multiplied by different weights, data propagation\
    \ incorporates the use of the vertical bus.\n\nDuring Phase-1 (Weights Loading),\
    \ Type-1 messages are dispatched through the top ports of the respective SiteM\
    \ and then reach the intended SiteO locations via hopping. Consequently, the required\
    \ SiteOs are programmed with the appropriate weights within 4 clock cycles (CCs),\
    \ as all SiteMs operate in parallel, as illustrated in Figure 4. Post programming,\
    \ each SiteO retains the next opcode and destination address embedded in the message\
    \ in its internal register. Upon completion of Phase-1, Type-2 messages are simultaneously\
    \ transmitted to the SiteMs via the vertical bus every clock cycle, initiating\
    \ operations as dictated by the opcode. Following each operation, new messages\
    \ are generated using the next opcode and destination stored within the register\
    \ along with the operation results. Specifically, the initial operation involves\
    \ multiplication, with the results from the SiteOs being transferred to the desired\
    \ SiteO location for addition via the horizontal bus. Subsequently, a new message\
    \ for the RELU operation is internally generated. After executing the RELU operation,\
    \ another message is prepared for the comparison operation. This sequence is repeated\
    \ as necessary until all required comparisons are completed, culminating in the\
    \ offloading of results to memory. All messages required to complete the convolution\
    \ operation are included in Table IV of the Appendix section as supplementary\
    \ file. A segment of this message sequence is depicted in Figure 5. During the\
    \ 4th CC, twelve Type-1 messages (M1-M12) are dispatched to three SiteMs, targeting\
    \ SiteO locations 0 to 3 for SiteM0, 16 to 19 for SiteM1, and 32 to 35 for SiteM2,\
    \ assuming all values in a 3x3 filter are \"1.\" Consequently, each SiteO stores\
    \ the values, subsequent opcodes, and destinations according to messages M1 to\
    \ M12. At the 5th CC, nine Type-2 messages (M13-M21) with the opcode \"A\\_MULS\"\
    \ are sent, which multiply the incoming values with the previously stored weights,\
    \ generating nine internal messages using the stored next opcode and destination.\
    \ All these internal messages, sharing the same opcode (A\\_ADDS) and destination\
    \ (33), are sent to SiteO 33 and aggregated at the 6th CC. SiteO 33, holding the\
    \ next opcode (RELU) and destination (34), then generates a new internal message\
    \ with the addition results, directing it to SiteO 34 during the 7th CC. Following\
    \ the RELU operation, another internal message is generated and sent to SiteO\
    \ 35 during the 8th CC, continuing the cycle.\n\n#### V. VALIDATION\n\nTo validate\
    \ our design, we have developed a digital design flow based on CAD tools under\
    \ TSMC 28nm technology node. Initially, we designed the RTL/behavioral models\
    \ of m-IPU and iteratively checked whether the RTL is free from linting errors\n\
    \nand whether the RTL is synthesis friendly. During the RTL code development,\
    \ we utilized Xilinx Vivado platform for initial measurement and simulation. Later,\
    \ we followed a digital design flow based on standard methodologies and CAD tools\
    \ for implementing the mIPU design. We have used a highperformance compact mobile\
    \ computing plus (CLN28HPC+) process from TSMC 28nm commercial PDK with 8 metal\
    \ layers and supply voltage of 0.9V. Here, the RTL was verified for functional\
    \ correctness. We have also performed property checking to verify the RTL implementation\
    \ and that the specification matches. After that, we set the design environment,\
    \ including the technology file and other environmental attributes. We have also\
    \ defined the design constraints file for synthesis, usually called an SDC synopsys\\\
    _constraints or dc\\_synopsys\\_setup file, specific to the synthesis tool. We\
    \ have defined the environment by specifying operating conditions, wire load models,\
    \ and system interface characteristics. Operating conditions include temperature,\
    \ voltage, and process variations. Wire load models estimate the effect of wire\
    \ length on design performance. System interface characteristics include input\
    \ drives, input and output loads, and fan-out loads. Since the environment model\
    \ directly affects design synthesis results, we have methodically constrained\
    \ designs by describing the design environment, target objectives, and design\
    \ rules. We also constrained timing and area information and iteratively run the\
    \ synthesis on the design to meet the design specifications. Once the constraints\
    \ file is set, we have provided synthesis inputs to the Cadence Genus. The input\
    \ files are the library files (which have the functional/timing information available\
    \ for the standard cell library and the wire load models for the wires based on\
    \ the fan-out length of the connectivity), RTL files, and the design constraints\
    \ files. The synthesis tool performs the synthesis of the RTL files and maps and\
    \ optimizes them to meet the design constraints requirements.\n\n![](_page_7_Figure_5.jpeg)\n\
    \n**Figure 6 Simulation result of Matrix Multiplication between Matrix A (3 X\
    \ 3) and Matrix B (3 X 3)**\n\n| TABLE II                                    \
    \                                               |               |            \
    \             |             |             |           |             |        \
    \     |                       |\n|--------------------------------------------------------------------------------------------|---------------|-------------------------|-------------|-------------|-----------|-------------|-------------|-----------------------|\n\
    | A Summary of Matrix Multiplication Approach between TPU<br>[14], MEISSA<br>[14],\
    \ and m-IPU |               |                         |             |        \
    \     |           |             |             |                       |\n| TYPE\
    \                                                                            \
    \           | PROCESSING    | RESOURCE<br>UTILIZATION | SHAPE       |        \
    \     |           | DATA LOAD   |             |                       |\n|   \
    \                                                                            \
    \             | ELEMENT       |                         | Matrix<br>A | Matrix<br>B\
    \ | METHOD    | Matrix<br>A | Matrix<br>B | LATENCY               |\n|       \
    \                                                                            \
    \         |               |                         |             |          \
    \   |           |             |             |                       |\n| TPU \
    \                                                                            \
    \           | MAC           | Multipliers: N X P      | N X M       | M X P  \
    \     | Systolic  | Stored      | Left to     | N + 2M + P -2         |\n|   \
    \                                                                            \
    \             |               | Adders: M X P           |             |      \
    \       | Array     |             | Right       |                       |\n| MEISSA\
    \                                                                            \
    \         | Multipliers & | Multipliers: M X P      | N X M       | M X P    \
    \   | Systolic  | Stored      | Left to     | N + M + P + log (M) - |\n|     \
    \                                                                            \
    \           | Adder Trees   | Adders: P X (M - 1)     |             |        \
    \     | Array     |             | Right       | 2                     |\n| m-IPU\
    \                                                                            \
    \          | SiteO         | SiteOs:                 | N X M       | M X P   \
    \    | Messaging | Program     | Vertical    |                       |\n|    \
    \                                                                            \
    \            |               | {(N X M) + N} X P       |             |       \
    \      | based     |             | Bus         | N + P + 2             |\n\n![](_page_8_Figure_2.jpeg)\n\
    \n**Figure 7 Comparison of the total time steps for matrix multiplication when\
    \ A) n varies from 2 to 2048, m = 128, p = 128, B) m varies from 2 to 2048, n\
    \ = 128, and p =128, and c) p varies from 2 to 2048, n = 128, m = 128.**\n\n|\
    \ Table III                       |                |          |       |      \
    \ |  |  |  |\n|---------------------------------|----------------|----------|-------|-------|--|--|--|\n\
    | m-IPU (SiteM) Design parameters |                |          |       |      \
    \ |  |  |  |\n| Technology                      | TSMC 28nm      |          |\
    \       |       |  |  |  |\n| Process                         | HPC+         \
    \  |          |       |       |  |  |  |\n| Metal Layer                     |\
    \ 1P8M           |          |       |       |  |  |  |\n| Voltage (VDD)      \
    \             | 0.9V (nominal) |          |       |       |  |  |  |\n| Package\
    \                         | Wire Bond      |          |       |       |  |  |\
    \  |\n| Frequency                       | 100 MHz        |          |       |\
    \       |  |  |  |\n|                                 | Leakage        | Dynamic\
    \  |       | Total |  |  |  |\n| Power (mW)                      | 0.42      \
    \     | 44.08    |       | 44.5  |  |  |  |\n|                               \
    \  | Sequential     | Inverter | Logic | Total |  |  |  |\n| Cell Count      \
    \                | 25692          | 4203     | 64305 | 94200 |  |  |  |\n\nDesign\
    \ optimization constraints define timing and area optimization goals for Cadence\
    \ Genus. We specify these constraints. Genus optimizes the synthesis of the design\
    \ by these constraints, but not at the expense of the design rule constraints.\
    \ That is, Genus never attempts to violate the higherpriority design rules. We\
    \ had to modify the design constraint several times to meet the design requirements.\
    \ After performing the synthesis, we performed functional verification with the\
    \ synthesized netlist to confirm that the synthesis tool has not altered the functionality.\
    \ The throughput calculation of convolution operations utilizing available hardware\
    \ resources is appended in Algorithm 1 of the Appendix section as supplementary\
    \ file. We have measured the throughput for both 2D (Gray Image) and 3D (Color\
    \ Image) convolution and during our observation, we varied both image and filter\
    \ sizes. We have considered 32 data batches where each batch contains 128 images\
    \ with 100 MHz clock frequency and 4096 available SiteOs. The padding and stride\
    \ are kept 0 and 1 in both cases. For 3D convolution, we assumed that 64 filters\
    \ are involved in the convolution process.\n\n### A. Evaluation\n\nTo verify our\
    \ matrix-matrix multiplication approach, we have simulated our design with numerous\
    \ values, however, a matrixmatrix multiplication between matrix A (3X3) and matrix\
    \ B (3X3) is appended in Figure 6 for demonstrating our matrixmatrix multiplication\
    \ process. Here, 9 messages are sent initially at 3 different steps to program\
    \ 9 SiteOs with matrix A. In this stage, data reach to respective SiteOs through\
    \ hopping. After that, each column of matrix B is passed to the m-IPU using vertical\
    \ bus for multiplication. Once the multiplication is finished, results are sent\
    \ to the desired SiteO for addition. This process is repeated until all columns\
    \ are transferred and finally results are collected.\n\nTable II summarizes the\
    \ key characteristics including processing element, resource utilization, latency\
    \ of matrix multiplication approaches between matrix A (N X M) and matrix B (M\
    \ X P) in three different hardware accelerators (TPU, MEISSA, m-IPU). Both TPU\
    \ and MEISSA utilize systolic array to perform matrix multiplication whereas our\
    \ design incorporates messaging-based intelligent dataflow mechanism to program\
    \ hardware at run-time. The latency of our matrixmatrix multiplication unit has\
    \ been observed varying different (rows and columns) dimensions of matrix A and\
    \ matrix B. Figure 7 (A), (B), and (C) represents the comparison of latency among\
    \ MEISSA, TPU, and our design by varying N, M, and P individually from 4 to 2048\
    \ keeping other two parameters constant (128). In all cases, m-IPU outperforms\
    \ the remaining two state-of-the art matrix multiplication architectures.\n\n\
    Besides, we evaluate power consumption and resource utilization under TSMC 28nm\
    \ technology to demonstrate our architecture's efficiency. The key design parameters\
    \ of the m-IPU architecture are listed in Table III that indicates the design\n\
    \n![](_page_9_Figure_1.jpeg)\n\n**Figure 8 Throughput measurement by varying A)\
    \ Image Size B) Filter Size for 2D Convolution and C) Image Size D) Filter Size\
    \ for 3D Convolution.**\n\nconsumes only 44.08 mW dynamic power at 0.9V operating\
    \ voltage. The low dynamic power consumption suggests our design's efficient task\
    \ partitioning at runtime due to intelligent programmability and effective data\
    \ mapping. Moreover, messages are loaded only from one direction (Top) and after\
    \ multiplication the SiteOs transfer data through horizontal bus; as a result,\
    \ m-IPU involves less signal transmission at runtime.\n\nNext, we identify an\
    \ effective data mapping strategy to accomplish faster convolution operation with\
    \ less hardware resources. Finally, we benchmark our design by calculating throughput\
    \ for multi-batch convolution operation. Figure 8(A) represents throughput varying\
    \ image sizes from (128 X 128) to (1024 X 1024) maintaining a fixed filter size\
    \ of (3 X 3) for 2D convolution. On the other hand, figure 8(B) highlights throughput\
    \ for 2D convolution for different filter sizes (from 3 X 3 to 11 X 11) while\
    \ keeping a fixed image dimension of (256 X 256). Our methodology completes 2D\
    \ convolution operation of (32 X 128) images of size (1024 X 1024) with a filter\
    \ of size (3 X 3) utilizing 4096 SiteOs and 100 MHz frequency within just 702.5\
    \ milliseconds. Again, the time required to execute a 2D convolution operation\
    \ for (32 X 128) images of dimension (256 X 256) using a (11 X 11) filter keeping\
    \ the other parameters unchanged is only 49.2 milliseconds. Similarly, figure\
    \ 8 (C) and 8 (D) demonstrates the throughput for 3D convolution varying image\
    \ and filter size respectively. Here, we have only considered the throughput for\
    \ convolution operation as it consumes major time of the entire network and omitted\
    \ the data loading and feed forward network time during benchmarking. Further\
    \ validation, benchmarking, and identifying better data mapping algorithm are\
    \ part of our ongoing work.\n\n## VI. CONCLUSION\n\nThe trend of deploying machine\
    \ learning and deep learning algorithms in widespread applications necessitates\
    \ an efficient hardware accelerator for executing matrix multiplication operation\
    \ within a limited hardware budget. To do so, developing a unified hardware architecture\
    \ that features reconfigurability, low-power, low-latency, and highthroughput\
    \ is essential. In this work, we presented an innovative messaging-based computing\
    \ paradigm that inherently programs hardware units at runtime. In addition to\
    \ this, we proposed an ultra-low-latency matrix multiplier that outperforms existing\
    \ systolic array-based matrix multiplication architecture such as TPU and MEISSA.\
    \ Our design has been validated through simulations under TSMC 28nm technology.\
    \ We have also established a methodology to benchmark convolution operation utilizing\
    \ available hardware resources and achieved high throughput. Our future work will\
    \ explore embedding of mIPU in RISC-V based SoC platforms such as Rocket chi[p\\\
    [64\\]](#page-11-15) or Parrot [0.](#page-11-14)\n\n## REFERENCES\n\n- [1] A.\
    \ Krizhevsky, I. Sutskever, and G. E. Hinton, \"ImageNet Classification with Deep\
    \ Convolutional Neural Networks,\" Communications of the ACM, vol. 60, no. 6,\
    \ pp. 84–90, May 2012\n- [2] K. Simonyan and A. Zisserman, \"Very Deep Convolutional\
    \ Networks for Large-Scale Image Recognition,\" Computer Vision and Pattern Recognition,\
    \ Sep. 2014.\n- [3] C. Szegedy *et al*., \"Going deeper with convolutions,\" *2015\
    \ IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, Boston,\
    \ MA, USA, 2015, pp. 1-9.\n- [4] K. He, X. Zhang, S. Ren and J. Sun, \"Deep Residual\
    \ Learning for Image Recognition,\" *2016 IEEE Conference on Computer Vision and\
    \ Pattern Recognition (CVPR)*, Las Vegas, NV, USA, 2016, pp. 770-778.\n- [5] S.\
    \ Hochreiter and J. Schmidhuber, \"Long Short-Term Memory,\" Neural Computation,\
    \ vol. 9, no. 8, pp. 1735–1780, Nov. 1997.\n- [6] Chung, Junyoung, et al. \"Empirical\
    \ Evaluation of Gated Recurrent Neural Networks on Sequence Modeling.\" ArXiv.org,\
    \ 2014, arxiv.org/abs/1412.3555.\n- [7] A. Vaswani et al., \"Attention Is All\
    \ You Need,\" arXiv.org, Jun. 12, 2017. [https://arxiv.org/abs/1706.03762.](https://arxiv.org/abs/1706.03762)\n\
    - <span id=\"page-10-0\"></span>[8] R. H. Dennard, F. H. Gaensslen, H. -N. Yu,\
    \ V. L. Rideout, E. Bassous and A. R. LeBlanc, \"Design of ion-implanted MOSFET's\
    \ with very small physical dimensions,\" in *IEEE Journal of Solid-State Circuits*,\
    \ vol. 9, no. 5, pp. 256-268, Oct. 1974\n- <span id=\"page-10-1\"></span>[9] T.\
    \ N. Theis and H. . -S. P. Wong, \"The End of Moore's Law: A New Beginning for\
    \ Information Technology,\" in *Computing in Science & Engineering*, vol. 19,\
    \ no. 2, pp. 41-50, Mar.-Apr. 2017.\n- <span id=\"page-10-2\"></span>[10] R. Hameed\
    \ *et al.*, \"Understanding sources of inefficiency in generalpurpose chips,\"\
    \ *ACM SIGARCH Computer Architecture News*, vol. 38, no. 3, pp. 37–47, Jun. 2010.\n\
    - <span id=\"page-10-3\"></span>[11] J. L. Hennessy and D. A. Patterson, \"A new\
    \ golden age for computer architecture: Domain-specific hardware/software co-design,\
    \ enhanced security, open instruction sets, and agile chip development,\" *2018\
    \ ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA)*,\
    \ Los Angeles, CA, USA, 2018, pp. 27-29.\n- <span id=\"page-10-4\"></span>[12]\
    \ N. P. Jouppi *et al*., \"In-datacenter performance analysis of a tensor processing\
    \ unit,\" *2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture\
    \ (ISCA)*, Toronto, ON, Canada, pp. 1-12, 2017\n- <span id=\"page-10-5\"></span>[13]\
    \ B. Asgari, R. Hadidi and H. Kim, \"MEISSA: Multiplying Matrices Efficiently\
    \ in a Scalable Systolic Architecture,\" *2020 IEEE 38th International Conference\
    \ on Computer Design (ICCD)*, Hartford, CT, USA, pp. 130-137, 2020\n- <span id=\"\
    page-10-6\"></span>[14] Z. Zhang, H. Wang, S. Han and W. Dally, \"SpArch: Efficient\
    \ Architecture for Sparse Matrix Multiplication,\" in *2020 IEEE International\
    \ Symposium on High Performance Computer Architecture (HPCA)*, San Diego, CA,\
    \ USA, pp. 261-274, 2020.\n- <span id=\"page-10-7\"></span>[15] N. Srivastava,\
    \ H. Jin, J. Liu, D. Albonesi and Z. Zhang, \"MatRaptor: A Sparse-Sparse Matrix\
    \ Multiplication Accelerator Based on Row-Wise Product,\" *2020 53rd Annual IEEE/ACM\
    \ International Symposium on Microarchitecture (MICRO)*, Athens, Greece, pp. 766-780,\
    \ 2020.\n- <span id=\"page-10-8\"></span>[16] M. Liang, M. Chen, Z. Wang and J.\
    \ Sun, \"A CGRA based Neural Network Inference Engine for Deep Reinforcement Learning,\"\
    \ *2018 IEEE Asia Pacific Conference on Circuits and Systems (APCCAS)*, Chengdu,\
    \ China, 2018, pp. 540-543.\n- [17] A. Gholami, Z. Yao, S. Kim, C. Hooper, M.\
    \ W. Mahoney and K. Keutzer, \"AI and Memory Wall,\" in *IEEE Micro*, vol. 44,\
    \ no. 3, pp. 33-39, May-June 2024.\n- [18] \"Does AI have a hardware problem?\"\
    \ *Nature Electronics*, vol. 1, no. 4, pp. 205–205, Apr. 2018.\n- [19] J. Welser,\
    \ J. W. Pitera and C. Goldberg, \"Future Computing Hardware for AI,\" *2018 IEEE\
    \ International Electron Devices Meeting (IEDM)*, San Francisco, CA, USA, 2018,\
    \ pp. 1.3.1-1.3.6.\n- <span id=\"page-10-9\"></span>[20] V. Sze, Y.-H. Chen, T.-J.\
    \ Yang, and J. S. Emer, *Efficient Processing of Deep Neural Networks*. Cham:\
    \ Springer International Publishing, 2020.\n- <span id=\"page-10-10\"></span>[21]\
    \ J. Leskovec, L. A. Adamic, and B. A. Huberman, \"The Dynamics of Viral Marketing,\"\
    \ Trans. on the Web (TWEB), 2007.\n- <span id=\"page-10-11\"></span>[22] Autel,\
    \ Autel X-Star Quadcopter, 2020 (accessed March 15, 2024). [Online]. Available[:\
    \ https://www.autelrobotics.com/x-star-camera-drone.](https://www.autelrobotics.com/x-star-camera-drone)\n\
    - <span id=\"page-10-12\"></span>[23] T. Nowatzki, V. Gangadhan, K. Sankaralingam\
    \ and G. Wright, \"Pushing the limits of accelerator efficiency while retaining\
    \ programmability,\" *2016 IEEE International Symposium on High Performance Computer\
    \ Architecture (HPCA)*, Barcelona, Spain, 2016, pp. 27-39.\n- <span id=\"page-10-13\"\
    ></span>[24] M. Duranton, K. D. Bosschere, C. Gamrat, J. Maebe, H. Munk, and O.\
    \ Zendra. 2017. The HiPEAC Vision 2017. *In Proceedings of the European Network\
    \ of Excellence on High Performance and Embedded Architecture and Compilation*.12.\n\
    - <span id=\"page-10-14\"></span>[25] R. Tessier, K. Pocek and A. DeHon, \"Reconfigurable\
    \ Computing Architectures,\" in Proceedings of the IEEE, vol. 103, no. 3, pp.\
    \ 332-354, March 2015.\n- <span id=\"page-10-15\"></span>[26] Z. Li, D. Wijerathne,\
    \ and T. Mitra, \"Coarse Grained Reconfigurable Array (CGRA).\" Available: https://www.comp.nus.edu.sg/~tulika/CGRA-Survey.pdf.\n\
    - <span id=\"page-10-16\"></span>[27] Ian Kuon; Russell Tessier; Jonathan Rose,\
    \ *FPGA Architecture: Survey and Challenges*, 2008.\n- <span id=\"page-10-17\"\
    ></span>[28] H. Xiao, X. Hu, T. Gao, Y. Zhou, S. Duan and Y. Chen, \"Efficient\
    \ Low-Bit Neural Network with Memristor-Based Reconfigurable Circuits,\"\n\n*IEEE\
    \ Transactions on Circuits and Systems II: Express Briefs*, vol. 71, no. 1, pp.\
    \ 66-70, Jan. 2024.\n\n- <span id=\"page-10-18\"></span>[29] Md Arif Iqbal, Srinivas\
    \ Rahul Sapireddy, S. Dasari, Kazi Asifuzzaman, and M. Rahman, \"A review of crosstalk\
    \ polymorphic circuits and their scalability,\" Memories - *Materials Devices\
    \ Circuits and Systems*, vol. 7, pp. 100094–100094, Apr. 2024.\n- <span id=\"\
    page-10-19\"></span>[30] N. K. Macha, B. T. Repalle, M. A. Iqbal and M. Rahman,\
    \ \"Crosstalk-Computing-Based Gate-Level Reconfigurable Circuits,\" in *IEEE Transactions\
    \ on Very Large-Scale Integration (VLSI) Systems*, vol. 30, no. 8, pp. 1073-1083,\
    \ Aug. 2022.\n- <span id=\"page-10-20\"></span>[31] P. Samant, Naveen Kumar Macha,\
    \ and M. Rahman, \"A Neoteric Approach for Logic with Embedded Memory Leveraging\
    \ Crosstalk Computing,\" *ACM Journal on Emerging Technologies in Computing Systems*,\
    \ vol. 19, no. 1, pp. 1–16, Dec. 2022.\n- <span id=\"page-10-21\"></span>[32]\
    \ A. Podobas, K. Sano and S. Matsuoka, \"A Survey on Coarse-Grained Reconfigurable\
    \ Architectures from a Performance Perspective,\" in *IEEE Access*, vol. 8, pp.\
    \ 146719-146743, 2020.\n- <span id=\"page-10-22\"></span>[33] L. Liu *et al.*,\
    \ \"A Survey of Coarse-Grained Reconfigurable Architecture and Design,\" *ACM\
    \ Computing Surveys*, vol. 52, no. 6, pp. 1–39, Oct. 2019.\n- <span id=\"page-10-23\"\
    ></span>[34] Bingfeng Mei, S. Vernalde, D. Verkest, H. De Man and R. Lauwereins,\
    \ \"Exploiting loop-level parallelism on coarse-grained reconfigurable architectures\
    \ using modulo scheduling,\" *2003 Design, Automation and Test in Europe Conference\
    \ and Exhibition*, Munich, Germany, 2003, pp. 296-301.\n- <span id=\"page-10-24\"\
    ></span>[35] Fava, L. B. R. V. (2024). CGRA-based Deep Neural Network for Object\
    \ Identification. Instituto Superior Técnico.\n- <span id=\"page-10-25\"></span>[36]\
    \ M. Wijtvliet, L. Waeijen and H. Corporaal, \"Coarse grained reconfigurable architectures\
    \ in the past 25 years: Overview and classification,\" *2016 International Conference\
    \ on Embedded Computer Systems: Architectures, Modeling and Simulation (SAMOS)*,\
    \ Agios Konstantinos, Greece, 2016, pp. 235-244.\n- <span id=\"page-10-26\"></span>[37]\
    \ M. Tanomoto, S. Takamaeda-Yamazaki, J. Yao and Y. Nakashima, \"A CGRA-Based\
    \ Approach for Accelerating Convolutional Neural Networks,\" *2015 IEEE 9th International\
    \ Symposium on Embedded Multicore/Many-core Systems-on-Chip*, Turin, Italy, 2015,\
    \ pp. 73-80.\n- <span id=\"page-10-27\"></span>[38] Y. Liang, J. Tan, Z. Xie,\
    \ Z. Chen, D. Lin, and Z. Yang, \"Research on Convolutional Neural Network Inference\
    \ Acceleration and Performance Optimization for Edge Intelligence,\" *Sensors*,\
    \ vol. 24, no. 1, p. 240, Jan. 2024.\n- <span id=\"page-10-28\"></span>[39] .\
    \ Ando, S. Takamaeda-Yamazaki, M. Ikebe, T. Asai, and M. Motomura, \"A Multithreaded\
    \ CGRA for Convolutional Neural Network Processing,\" *Circuits and Systems*,\
    \ vol. 8, no. 6, pp. 149–170, Jun. 2017.\n- <span id=\"page-10-29\"></span>[40]\
    \ S. Yin *et al*., \"A 1.06-to-5.09 TOPS/W reconfigurable hybrid-neuralnetwork\
    \ processor for deep learning applications,\" *2017 Symposium on VLSI Circuits*,\
    \ Kyoto, Japan, 2017, pp. C26-C2.\n- <span id=\"page-10-30\"></span>[41] Y. Wang\
    \ *et al*., \"Benchmarking the Performance and Energy Efficiency of AI Accelerators\
    \ for AI Training,\" *2020 20th IEEE/ACM International Symposium on Cluster, Cloud\
    \ and Internet Computing (CCGRID)*, Melbourne, VIC, Australia, 2020, pp. 744-751.\n\
    - <span id=\"page-10-31\"></span>[42] Y. -H. Chen, T. Krishna, J. S. Emer and\
    \ V. Sze, \"Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional\
    \ Neural Networks,\" in IEEE Journal of Solid-State Circuits, vol. 52, no. 1,\
    \ pp. 127-138, Jan. 2017.\n- <span id=\"page-10-32\"></span>[43] Y.-H. Chen, T.-J.\
    \ Yang, J. Emer, and V. Sze, \"Eyeriss v2: A Flexible Accelerator for Emerging\
    \ Deep Neural Networks on Mobile Devices,\" *arXiv.org*, May 20, 2019.\n- <span\
    \ id=\"page-10-33\"></span>[44] H. Kwon, A. Samajdar and T. Krishna, \"Rethinking\
    \ NoCs for spatial neural network accelerators,\" *2017 Eleventh IEEE/ACM International\
    \ Symposium on Networks-on-Chip (NOCS)*, Seoul, Korea (South), 2017, pp. 1-8.\n\
    - <span id=\"page-10-34\"></span>[45] L. Liu, Z. Li, C. Yang, C. Deng, S. Yin\
    \ and S. Wei, \"HReA: An Energy-Efficient Embedded Dynamically Reconfigurable\
    \ Fabric for 13-Dwarfs Processing,\" in *IEEE Transactions on Circuits and Systems\
    \ II: Express Briefs*, vol. 65, no. 3, pp. 381-385, March 2018.\n- <span id=\"\
    page-10-35\"></span>[46] R. Prabhakar *et al*., \"Plasticine: A reconfigurable\
    \ architecture for parallel patterns,\" *2017 ACM/IEEE 44th Annual International\
    \ Symposium on Computer Architecture (ISCA)*, Toronto, ON, Canada, 2017, pp. 389-402.\n\
    - <span id=\"page-10-36\"></span>[47] O. Akbari, M. Kamal, A. Afzali-Kusha, M.\
    \ Pedram and M. Shafique, \"PX-CGRA: Polymorphic approximate coarse-grained reconfigurable\
    \ architecture,\" *2018 Design, Automation & Test in Europe Conference & Exhibition\
    \ (DATE)*, Dresden, Germany, 2018, pp. 413-418.\n- <span id=\"page-10-37\"></span>[48]\
    \ L. Duch, S. Basu, M. Peón-Quirós, G. Ansaloni, L. Pozzi and D. Atienza, \"i-DPs\
    \ CGRA: An Interleaved-Datapaths Reconfigurable Accelerator for\n\nEmbedded Bio-Signal\
    \ Processing,\" in *IEEE Embedded Systems Letters*, vol. 11, no. 2, pp. 50-53,\
    \ June 2019.\n\n- <span id=\"page-11-0\"></span>[49] M. Mishra, T. J. Callahan,\
    \ T. Chelcea, G. Venkataramani, S. C. Goldstein, and M. Budiu, \"Tartan,\" *ACM\
    \ SIGOPS Operating Systems Review*, vol. 40, no. 5, pp. 163–174, Oct. 2006.\n\
    - <span id=\"page-11-1\"></span>[50] J. D. Souza, L. Carro, M. B. Rutzig and A.\
    \ C. S. Beck, \"A reconfigurable heterogeneous multicore with a homogeneous ISA,\"\
    \ *2016 Design, Automation & Test in Europe Conference & Exhibition (DATE)*, Dresden,\
    \ Germany, 2016, pp. 1598-1603.\n- <span id=\"page-11-2\"></span>[51] T. Chen,\
    \ S. Srinath, C. Batten and G. E. Suh, \"An Architectural Framework for Accelerating\
    \ Dynamic Parallel Algorithms on Reconfigurable Hardware,\" *2018 51st Annual\
    \ IEEE/ACM International Symposium on Microarchitecture (MICRO)*, Fukuoka, Japan,\
    \ 2018, pp. 55-67.\n- <span id=\"page-11-3\"></span>[52] D. Voitsechov, O. Port\
    \ and Y. Etsion, \"Inter-Thread Communication in Multithreaded, Reconfigurable\
    \ Coarse-Grain Arrays,\" *2018 51st Annual IEEE/ACM International Symposium on\
    \ Microarchitecture (MICRO)*, Fukuoka, Japan, 2018, pp. 42-54.\n- <span id=\"\
    page-11-4\"></span>[53] C. Xu and J. McAuley, \"A Survey on Dynamic Neural Networks\
    \ for Natural Language Processing,\" *arXiv.org*, Feb. 24, 2023. [https://arxiv.org/abs/2202.07101.](https://arxiv.org/abs/2202.07101)\n\
    - <span id=\"page-11-5\"></span>[54] T. Bjerregaard and S. Mahadevan, \"A survey\
    \ of research and practices of Network-on-chip,\" *ACM Computing Surveys*, vol.\
    \ 38, no. 1, p. 1, Jun. 2006.\n- <span id=\"page-11-6\"></span>[55] M. Rahman,\
    \ A. Iqbal, and S. Rahul, \"A Messaging based Intelligent Computing Approach for\
    \ Machine Learning Applications.\" Accessed: Mar. 20, 2024. [Online]. Available:\
    \ https://computing-lab.com/wpcontent/uploads/2022/02/mIPU-v1.pdf.\n- <span id=\"\
    page-11-7\"></span>[56] T. Wiatowski and H. Bölcskei, \"A Mathematical Theory\
    \ of Deep Convolutional Neural Networks for Feature Extraction,\" in *IEEE Transactions\
    \ on Information Theory*, vol. 64, no. 3, pp. 1845-1866, March 2018.\n- <span\
    \ id=\"page-11-8\"></span>[57] A. Gupta, K. Illanko and X. Fernando, \"Object\
    \ Detection for Connected and Autonomous Vehicles using CNN with Attention Mechanism,\"\
    \ *2022 IEEE 95th Vehicular Technology Conference: (VTC2022-Spring)*, Helsinki,\
    \ Finland, 2022, pp. 1-6.\n- <span id=\"page-11-9\"></span>[58] Y. Li, J. Luo,\
    \ S. Deng and G. Zhou, \"CNN-Based Continuous Authentication on Smartphones with\
    \ Conditional Wasserstein Generative Adversarial Network,\" in *IEEE Internet\
    \ of Things Journal*, vol. 9, no. 7, pp. 5447-5460, 1 April1, 2022\n- <span id=\"\
    page-11-10\"></span>[59] D. R. Sarvamangala and R. V. Kulkarni, \"Convolutional\
    \ neural networks in medical image understanding: a survey,\" Evolutionary Intelligence,\
    \ vol. 15, Jan. 2021\n- <span id=\"page-11-11\"></span>[60] J. Guo, H. -T. Nguyen,\
    \ C. Liu and C. C. Cheah, \"Convolutional Neural Network-Based Robot Control for\
    \ an Eye-in-Hand Camera,\" in *IEEE Transactions on Systems, Man, and Cybernetics:\
    \ Systems*, vol. 53, no. 8, pp. 4764-4775, Aug. 2023\n- <span id=\"page-11-12\"\
    ></span>[61] V. Sharma, M. Gupta, A. Kumar, and D. Mishra, \"Video Processing\
    \ Using Deep Learning Techniques: A Systematic Literature Review,\" in *IEEE Access*,\
    \ vol. 9, pp. 139489-139507, 2021.\n- <span id=\"page-11-13\"></span>[62] S. Yang,\
    \ \"Natural Language Processing Based on Convolutional Neural Network and Semi\
    \ Supervised Algorithm in Deep Learning,\" *2022 International Conference on Artificial\
    \ Intelligence in Everything (AIE)*, Lefkosa, Cyprus, 2022, pp. 174-178.\n- [63]\
    \ O. Abdel-Hamid, A. -r. Mohamed, H. Jiang, L. Deng, G. Penn and D. Yu, \"Convolutional\
    \ Neural Networks for Speech Recognition,\" in *IEEE/ACM Transactions on Audio,\
    \ Speech, and Language Processing*, vol. 22, no. 10, pp. 1533-1545, Oct. 2014.\n\
    - <span id=\"page-11-15\"></span>[64] K. Asanović *et al.*, \"The Rocket Chip\
    \ Generator,\" 2016. Available: https://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-\
    \ 17.pdf.\n- <span id=\"page-11-14\"></span>[65] D. Petrisko *et al.*, \"BlackParrot:\
    \ An Agile Open-Source RISC-V Multicore for Accelerator SoCs,\" *IEEE Micro*,\
    \ vol. 40, no. 4, pp. 93–102, Jul. 2020.\n\n![](_page_11_Picture_19.jpeg)\n\n\
    **Md. Rownak Hossain Chowdhury**, received the B.Sc. degree in electrical and\
    \ electronic engineering from Khulna University of Engineering and Technology,\
    \ Khulna, Bangladesh, in 2010. He is currently pursuing the Ph.D. degree with\
    \ the Department of Electrical and Computer Engineering,\n\nUniversity of Missouri-Kansas\
    \ City, Kansas City, MO, USA. His current research interests focus on developing\
    \ innovative computing techniques for AI hardware accelerator.\n\n![](_page_11_Picture_22.jpeg)\n\
    \n**Mostafizur Rahman**, received the Ph.D. degree in electrical and computer\
    \ engineering from the University of Massachusetts Amherst, Amherst, MA, USA.\n\
    \nHe was with the Department of Computer Science and Electrical Engineering (CSEE),\
    \ University of Missouri–Kansas City, Kansas City, MO, USA, where he leads the\n\
    \nNanoscale Integrated Circuits Laboratory and is currently a Co-Lead for the\
    \ Center for Interdisciplinary Nanoscale Research. His group's research focuses\
    \ on transformative approaches for nanoelectronics to surpass the current limitations\
    \ of today's integrated circuits.\n\nDr. Rahman is currently the Publication Chair\
    \ for NANOARCH and a Guest Editor for special issue of the *IEEE Transactions\
    \ on Nanotechnolog*y. He is also a Program Committee Member for NANOARCH and VLSI\
    \ design conferences. He is currently a Reviewer for the *IEEE Transactions on\
    \ Nanotechnology*, *ACM Journal on Emerging Technologies in Computing Systems*,\
    \ the Journal of Parallel and Distributed Computing, NANOARCH, and other publications."
- title: "Work-in-Progress: Real-Time Neural Network Inference on a Custom RISC-V\n\
    \  Multicore Vector Processor"
  abstract: "Neural networks are increasingly used in real-time systems, such as automated\n\
    driving applications. This requires high-performance hardware with predictable\n\
    timing behavior. State-of-the-art real-time hardware is limited in memory and\n\
    compute resources. On the other hand, modern accelerator systems lack the\nnecessary\
    \ predictability properties, mainly due to interference in the memory\nsubsystem.\n\
    \  We present a new hardware architecture with an accompanying compiler-based\n\
    deployment toolchain to close this gap between performance and predictability.\n\
    The hardware architecture consists of a multicore vector processor with\npredictable\
    \ cores, each with local scratchpad memories. A central management\ncore facilitates\
    \ access to shared external memory through a static schedule\ncalculated at compile-time.\
    \ The presented compiler exploits the fixed data flow\nof neural networks and\
    \ WCET estimates of subtasks running on individual cores\nto compute this schedule.\n\
    \  Through this approach, the WCET estimate of the overall system can be\nobtained\
    \ from the subtask WCET estimates, data transfer times, and access times\nof the\
    \ shared memory in conjunction with the schedule calculated by the\ncompiler."
  url: http://arxiv.org/abs/2410.10340v1
  keywords: Real-Time Systems, Multicore Processor, Neural Network, Predictable Execution
  document: '# Work-in-Progress: Real-Time Neural Network Inference on a Custom RISC-V
    Multicore Vector Processor


    Maximilian Kirschner1,2, Konstantin Dudzik1,2, Jurgen Becker ¨ 1,2 *<sup>1</sup>FZI
    Research Center for Information Technology*, Karlsruhe, Germany *<sup>2</sup>Karslruhe
    Institute of Technology*, Karlsruhe, Germany


    {kirschner, dudzik, juergen.becker}@fzi.de


    *Abstract*—Neural networks are increasingly used in real-time systems, such as
    automated driving applications. This requires high-performance hardware with predictable
    timing behavior. State-of-the-art real-time hardware is limited in memory and
    compute resources. On the other hand, modern accelerator systems lack the necessary
    predictability properties, mainly due to interference in the memory subsystem.


    We present a new hardware architecture with an accompanying compiler-based deployment
    toolchain to close this gap between performance and predictability. The hardware
    architecture consists of a multicore vector processor with predictable cores,
    each with local scratchpad memories. A central management core facilitates access
    to shared external memory through a static schedule calculated at compile-time.
    The presented compiler exploits the fixed data flow of neural networks and WCET
    estimates of subtasks running on individual cores to compute this schedule.


    Through this approach, the WCET estimate of the overall system can be obtained
    from the subtask WCET estimates, data transfer times, and access times of the
    shared memory in conjunction with the schedule calculated by the compiler.


    *Index Terms*—Real-Time Systems, Multicore Processor, Neural Network, Predictable
    Execution


    # I. INTRODUCTION


    With the increasing use of machine learning in real-time systems, the performance
    requirements for the underlying hardware are increasing rapidly. Especially in
    driver assistance systems and automated driving, using neural networks is essential
    [\[1\]](#page-3-0), [\[2\]](#page-3-1). At the same time, certification requirements
    and limited resources necessitate worst-case execution time (WCET) analysis for
    these systems. Hence, there is a pressing demand for timing-predictable high-performance
    hardware architectures.


    Currently, Graphics Processing Units (GPUs) and specialized accelerator architectures
    are used to meet the high demand for computing power. However, these heterogeneous
    systems are optimized for average-case performance and have properties detrimental
    to timing analysis. Most importantly, they often share a memory with the host
    CPU to exchange instructions and data, which leads to interference and, consequently,
    unpredictable behavior.


    On the other hand, current real-time architectures that focus on predictability
    have a significant performance gap to GPUs and Machine Learning (ML) accelerator
    systems.


    The main contribution of this paper is a hardware architecture and its corresponding
    compiler toolchain, which addresses the need for the predictable execution of
    neural networks. The hardware architecture consists of multiple predictable vector
    processors with local memories and a central management core to access a shared
    main memory. Loads and stores to the external memory follow a static schedule,
    pre-calculated at compile-time by the presented tools.


    The remainder of this paper is structured as follows. Section [II](#page-0-0)
    gives a brief overview of the related work. Our concept for timing-predictable
    neural network inference is described in Section [III.](#page-1-0) This concept
    comprises a custom hardware architecture and a compiler-based deployment approach.
    Section [IV](#page-2-0) is concerned with the implementation details of these
    two components. Finally, [section V](#page-3-2) concludes the paper and discusses
    possible next steps.


    # II. RELATED WORK


    <span id="page-0-0"></span>There are fundamentally two prevalent approaches to
    predictable hardware architectures with high computational performance: multicore
    processors and vector processors. Our architecture combines both ideas into a
    multicore processor with a vector extension in each core. Vector processors follow
    the Single-Instruction Multiple-Data (SIMD) paradigm, where specialized execution
    units execute the same instruction on a fixed number of elements in a vector register.
    Vector processors can be implemented with a relatively simple architecture, where
    a single stream of instructions passes through a single pipeline. This property
    is beneficial for predictability. The Vicuna co-processor [\[3\]](#page-3-3) implements
    such a vector unit, which was designed with a focus on predictability and can
    be integrated into different scalar RISC-V processors. The Vicuna vector co-processor
    is also an integral component of our implementation. However, SIMD architectures
    only solve the problem to a certain extent if the applications provide vector
    operations that can be mapped to wide vector registers [\[4\]](#page-3-4). Multicore
    processors, in contrast, follow the Multiple-Instruction Multiple-Data (MIMD)
    paradigm. Multiple cores independently execute different instructions on distinct
    data. Several predictable multicore architectures [\[5\]](#page-3-5)–[\[7\]](#page-3-6)
    have been proposed. When designing a real-time multicore processor, the primary
    objective is timing-compositionality [\[8\]](#page-3-7) so that the execution
    times of tasks running on the individual cores can be estimated and then combined
    into a total WCET. The main challenge for timing-compositional systems is to ensure
    freedom from interference. The existing approaches, therefore, almost all use
    distributed memory so that each core can execute from its own local memory. TDMA
    arbitration on the interconnect level is the most common solution for access to
    shared resources such as off-chip memory or peripherals. Our concept also relies
    on local memories, but a schedule calculated at compile time is used for communication
    with the external memory. As a result, the available bandwidth of the interconnect
    and the memory can be used more flexibly, allowing for higher maximum throughput.


    An alternative approach would be to build applicationspecific circuits for FPGAs
    or ASICs. Tools such as hls4ml [\[9\]](#page-3-8) or FINN [\[10\]](#page-3-9)
    can be used to build such circuits. However, the development process for application-specific
    hardware is still very complex and time-consuming. Therefore, in this paper, we
    present a hardware architecture that supports generic neural networks.


    Approaches for the deployment of neural networks in realtime-critical environments
    can be found in the literature. These works present tools generating predictable
    code from machine learning models [\[11\]](#page-3-10), [\[12\]](#page-3-11).
    The generated code is targeted towards simple, predictable single-core architectures.
    In [\[13\]](#page-3-12), a library targeting SIMD architectures is presented.
    This library implements General Matrix Multiplication (GEMM) routines for predictable
    execution on COTS processors. In contrast, in this work, we present a custom multicore
    architecture with associated deployment tools that extend the deployment to a
    multicore platform, thereby enabling the inference of more complex models.


    # <span id="page-1-0"></span>III. CONCEPT FOR THE TIMING-PREDICTABLE INFERENCE
    OF NEURAL NETWORKS


    Our platform for the predictable execution of neural networks consists of two
    parts: a hardware architecture and a corresponding machine-learning compiler toolchain.
    The hardware offers a high degree of parallelization and guarantees freedom from
    interference by design, while the compiler is used to deploy neural networks and
    ensures an efficient execution on the predictable hardware. It partitions the
    neural network into small subtasks, maps these to the cores, vectorizes the subtasks,
    and calculates a schedule for main memory accesses.


    ## *A. Hardware Architecture*


    The hardware architecture for the predictable acceleration of neural networks
    is shown in [Figure 1.](#page-1-1) It consists of N processor cores with an integrated
    vector unit each, which we refer to as worker cores. A multicore vector processor
    allows parallelization in two dimensions, distributing computations across the
    cores and across the vector units in each core. Each core is equipped with two
    scratchpad memories for instructions and data, respectively. The cores can thus
    perform


    ![](_page_1_Figure_7.jpeg)


    <span id="page-1-1"></span>Fig. 1. Multicore hardware architecture with local
    scratchpad memories and vector units in each core


    computations independently on their local memory. The architecture also contains
    a shared external DRAM memory, which is used to store data and instructions. The
    external memory contains the input data and weights of the neural network and
    is used to store intermediate results, if necessary. A central management core
    orchestrates access to external memory and communication between cores. It copies
    instructions and data between the scratchpad memories and the external memory,
    following a static schedule. This schedule is computed by the compiler and specifies
    communication times for each data movement. A Direct Memory Access (DMA) component
    with exclusive access to the external memory and the interconnect is used for
    efficient data transfers. The scratchpad memories are dual-ported so that data
    can be transferred while the core is executing.


    Since the DMA is the only component that can access the external memory and the
    only one that can initiate transfers on the interconnect, freedom from interference
    is guaranteed by design. The cores themselves are predictable and operate on their
    own memory. This allows for separate WCET analyses, which can then be combined
    with the schedule, which also contains estimates for memory access times and data
    transfer times, to a total WCET estimation.


    # *B. Compiler-Based Deployment Approach*


    Our concept involves MLIR-based [\[14\]](#page-3-13) compiler tools for the deployment
    of neural networks on the predictable multicore platform. The tools parallelize
    the model and map it to the cores, vectorize the code for each core, and calculate
    a schedule for memory access. The input to our toolchain is a trained and quantized
    ML model. This is then compiled to an optimized RISC-V binary that can be deployed
    to the external memory.


    [Figure 2](#page-2-1) shows the compilation steps and generated artifacts. The
    compiler toolchain consists of two sections as well as an external tool for WCET
    analysis, which is beyond the scope of this work. The toolchain is split into
    two workflows, one regarding parallelization, mapping, and vectorization for the
    worker cores and the other regarding memory access scheduling for the management
    core.


    In the first step, after loading the model, hardwareindependent optimizations,
    like operator fusion or pruning, are applied to the model. The model is then split
    into subtasks that can be executed independently. The size of the subtasks depends
    on the size of the local memories and the number of cores. In the third step,
    the subtasks are mapped to the cores. The objective of the mapping is to minimize
    memory transfers by maximizing data reuse. In the mapping step, the network is
    traversed in reverse, from the result layer to the input layer, in order to determine
    dependencies between the subtasks. In a second pass, interdependent calculations
    are then mapped to the same core to keep as much data as possible in the local
    memory. Dependencies on subtasks with large amounts of data are prioritized. In
    the next steps, the programs for the individual cores are vectorized with RISC-V
    vector instructions and compiled into a RISC-V binary.


    An external WCET analysis tool calculates a WCET for each of these artifacts.
    These estimates and the mapping information are then used as input for the second
    compiler section.


    In the sixth step, a schedule for the subtasks is calculated. The fixed control
    flow of neural networks without inputdependent branches makes it possible to statically
    compute such a schedule. The model dictates the order of subtasks. Based on the
    WCET estimates of the worker core programs, start times for all subtasks are defined.
    In the seventh step, communication instructions are generated for the management
    core. Memory transactions are scheduled as early as possible, so that only one
    transaction takes place at a time. This is achieved using worst-case estimates
    for the DMA and interconnect transfers as well as the memory access times. If
    at one point in time several transactions could be executed, these are arranged
    in the schedule in a round-robin fashion. The communication and timing methods
    are then compiled into a RISC-V binary for the management core. In the final step,
    the binaries for the worker cores and the management binary are linked to decicated
    address sections to be deployed on the hardware.


    #### IV. IMPLEMENTATION


    <span id="page-2-0"></span>We implement the described hardware architecture and
    the machine learning compiler for extensive evaluations on an FPGA as part of
    future work. The following section describes the details of the implementation.


    #### *A. Hardware Architecture*


    The whole processor is implemented in SystemVerilog, targeting the Xilinx UltraScale+
    FPGA architecture. The worker cores are based on the open-source RISC-V core Ibex
    [\[15\]](#page-3-14), with an attached Vicuna [\[3\]](#page-3-3) vector co-processor.
    The Vicuna co-processor implements version 1.0 of the RISC-V vector extension.
    To be more precise, it implements the Zve32x extension, i.e. the version of the
    vector extension that is intended for embedded processors. Integer vector elements
    with 8, 16, and 32 bits are supported. Our implementation is therefore geared
    towards int8 quantized neural networks.


    ![](_page_2_Figure_7.jpeg)


    <span id="page-2-1"></span>Fig. 2. Compilation steps for the deployment of neural
    networks on the proposed hardware


    The vector register length of Vicuna can be configured. Our implementation currently
    uses 512-bit vector registers. We have dimensioned the implementation to 16 worker
    cores with 1 MiB scratchpad memory each, which is divided into dualported instruction
    and data memories. In this configuration, we target the execution of medium-sized
    convolutional neural networks like ResNet50 or YOLOv5-small. The central management
    core that is responsible for the data transfers into and out of the local memories
    is a scalar Ibex core that is connected to a modified PULP Data Movement Accelerator
    (iDMA) [\[16\]](#page-3-15) and a timer component. The Ibex core is closely coupled
    to the iDMA and the timer via custom control registers. It configures both components
    according to the precomputed schedule. The schedule is realized using control
    and status register instructions from the Zicsr extension. A TileLink Uncached
    Lightweight (TL-UL) [\[17\]](#page-3-16) crossbar connects the iDMA with the local
    memories of the worker cores. The iDMA is also connected to the DDR4 memory of
    the FPGA board via an AXI4 bus.


    # *B. Compiler-Based Deployment Approach*


    We implement the compiler-based concept for the deployment of neural networks
    on our predictable multicore architecture, using the Multi-Level Intermediate
    Representation (MLIR) [\[14\]](#page-3-13). MLIR is a modular framework for compiler
    development with the aim of reusability. [Table I](#page-3-17) lists the most
    important MLIR dialects for the compiler implementation. Open Neural Network Exchange
    (ONNX) models are the input of our compiler. These are transformed to onnx-mlir.
    We perform hardware-independent optimizations, tiling, and packing using the linalg
    dialect on the tensor representation. The tiles that are mapped to the individual
    cores are vectorized to use the SIMD instructions using the vector dialect. affine
    operations explicitly represent the memory


    #### TABLE I


    MLIR DIALECTS USED FOR THE COMPILER IMPLEMENTATION


    <span id="page-3-17"></span>


    | Dialect   | Description                                                     |

    |-----------|-----------------------------------------------------------------|

    | onnx-mlir | Implements the Open Neural Network Exchange (ONNX)              |

    |           | standard. An ONNX model can be transformed into this di         |

    |           | alect. Popular ML frameworks can export models in ONNX          |

    |           | format.                                                         |

    | tensor    | High-level dialect used for tensor representation and tensor    |

    |           | modification operations                                         |

    | linalg    | Linear Algebra operations, e.g., matrix multiplication, con     |

    |           | volution, or depthwise convolution that operate on buffer       |

    |           | or tensor type data. linalg is the entry layer for MLIR code    |

    |           | generation and is used for transformations like tiling, fusion,
    |

    |           | or distribution.                                                |

    | vector    | Represents higher-order vectors, closing the gap between        |

    |           | high-dimension tensors and native vectors in hardware.          |

    |           | It is used after tiling and mapping to cores, for similar       |

    |           | transformations as linalg, but on smaller problems from the     |

    |           | perspective of a single compute unit.                           |

    | memref    | Represent references to regions of memory, similar to buffer    |

    |           | pointers but with additional shape and type information. It     |

    |           | is used for memory planning and concrete data accesses.         |

    | affine    | Abstraction for affine operations. Among others, it can         |

    |           | represent load and store operations as affine maps, allowing    |

    |           | for data reordering without actual modification of data in the  |

    |           | memory.                                                         |

    | llvm      | MLIR dialect corresponding to the LLVM intermediate             |

    |           | representation (IR). This dialect can be translated into LLVM   |


    accesses. Finally, we lower to the llvm dialect and write the binary with the
    RISC-V backend of LLVM.


    IR for compilation to CPU architectures.


    For the evaluation we will implement support for the most relevant ONNX layer
    types, of common CNN architectures, in the compiler. CNNs are common in perception
    systems in realtime environments and are, therefore, of particular interest. The
    most compute- and data-intensive layers in CNNs are fully connected and convolution
    layers. It is important to optimize the mapping, vectorization, and communication
    schedules for these layers. If the compiler should later be extended with other
    layer types, it may be necessary to implement new tiling, mapping and vectorization
    methods for efficient execution.


    These layers are typically implemented using GEMM routines because they can be
    efficiently tiled and vectorized. We, therefore, choose to implement these layers
    in the same way. The subtasks mentioned in the concept, correspond to tiles in
    GEMM routines, which are mapped to the worker cores. The disadvantage of GEMM-based
    convolution is data duplication. As access to external memory is expensive in
    our hardware, the actual duplication of memory is only carried out in the scratchpad.


    #### V. CONCLUSION AND OUTLOOK


    <span id="page-3-2"></span>Interference in state-of-the-art accelerator systems
    used for the execution of neural networks poses a significant challenge for the
    timing analysis in real-time systems. On the other hand, available timing predictable
    processors do not provide the necessary compute and memory resources. In this
    work, we present a custom multicore hardware architecture with local memories
    and centrally orchestrated memory transactions. For the scheduling of these memory
    transactions, we propose a compiler-based deployment concept.


    Up until now, the main components of the hardware have been implemented, while
    the compiler-based scheduling and deployment are subject to ongoing work. In the
    next step, we plan to evaluate the hardware architecture in various configurations
    and assess the trade-offs between the configuration parameters. This includes
    the number of cores, the length of the vector registers in the cores, and the
    size of the local scratchpads. This hardware can then serve as a flexible platform
    for future work on the deployment of machine learning applications in real-time
    systems.


    #### ACKNOWLEDGMENT


    This work was supported by the German Federal Ministry of Education and Research
    (BMBF) within the project "MANNHEIM-CeCaS," funding number 16ME0818.


    #### REFERENCES


    - <span id="page-3-0"></span>[1] E. Yurtsever, J. Lambert, A. Carballo, and K.
    Takeda, "A survey of autonomous driving: Common practices and emerging technologies,"
    *IEEE Access*, vol. 8, pp. 58 443–58 469, 2020.

    - <span id="page-3-1"></span>[2] S. M. Grigorescu, B. Trasnea, T. T. Cocias, and
    G. Macesanu, "A survey of deep learning techniques for autonomous driving," *J.
    Field Robotics*, vol. 37, no. 3, pp. 362–386, 2020.

    - <span id="page-3-3"></span>[3] M. Platzer and P. P. Puschner, "Vicuna: A timing-predictable
    RISC-V vector coprocessor for scalable parallel computation," in *33rd Euromicro
    Conference on Real-Time Systems, ECRTS 2021, July 5-9, 2021*, vol. 196, 2021.

    - <span id="page-3-4"></span>[4] M. Perotti, M. A. Cavalcante, R. Andri, L. Cavigelli,
    and L. Benini, "Ara2: Exploring single- and multi-core vector processing with
    an efficient RVV 1.0 compliant open-source processor," *IEEE Trans. Computers*,
    vol. 73, no. 7, pp. 1822–1836, 2024.

    - <span id="page-3-5"></span>[5] M. Schoeberl *et al.*, "T-CREST: time-predictable
    multi-core architecture for embedded systems," *J. Syst. Archit.*, vol. 61, no.
    9, pp. 449–471, 2015.

    - [6] T. Ungerer *et al.*, "Parallelizing industrial hard real-time applications
    for the parmerasa multicore," *ACM Trans. Embed. Comput. Syst.*, vol. 15, no.
    3, pp. 53:1–53:27, 2016.

    - <span id="page-3-6"></span>[7] E. R. Jellum *et al.*, "InterPRET: a time-predictable
    multicore processor," in *Proceedings of Cyber-Physical Systems and Internet of
    Things Week, CPS-IoT Week 2023 Workshops*. ACM, 2023, pp. 331–336.

    - <span id="page-3-7"></span>[8] S. Hahn, J. Reineke, and R. Wilhelm, "Towards
    compositionality in execution time analysis: definition and challenges," *SIGBED
    Rev.*, vol. 12, no. 1, pp. 28–36, 2015.

    - <span id="page-3-8"></span>[9] J. Duarte *et al.*, "Fast inference of deep neural
    networks in FPGAs for particle physics," *JINST*, vol. 13, no. 07, p. P07027,
    2018.

    - <span id="page-3-9"></span>[10] M. Blott *et al.*, "Finn-r: An end-to-end deep-learning
    framework for fast exploration of quantized neural networks," *ACM Transactions
    on Reconfigurable Technology and Systems (TRETS)*, vol. 11, 2018.

    - <span id="page-3-10"></span>[11] R. Conlin, K. Erickson, J. Abbate, and E. Kolemen,
    "Keras2c: A library for converting keras neural networks to real-time compatible
    C," *Eng. Appl. Artif. Intell.*, vol. 100, 2021.

    - <span id="page-3-11"></span>[12] I. D. A. Silva, T. Carle, A. Gauffriau, and
    C. Pagetti, "ACETONE: predictable programming framework for ML applications in
    safetycritical systems," in *34th Euromicro Conference on Real-Time Systems, ECRTS*,
    2022.

    - <span id="page-3-12"></span>[13] I. D. A. Silva, T. Carle, A. Gauffriau, V.
    Jegu, and C. Pagetti, "A ´ predictable SIMD library for GEMM routines," in *30th
    IEEE Real-Time and Embedded Technology and Applications Symposium, RTAS*, 2024,
    pp. 55–67.

    - <span id="page-3-13"></span>[14] C. Lattner *et al.*, "MLIR: scaling compiler
    infrastructure for domain specific computation," in *IEEE/ACM International Symposium
    on Code Generation and Optimization, CGO*, 2021.

    - <span id="page-3-14"></span>[15] P. D. Schiavone *et al.*, "Slow and steady
    wins the race? A comparison of ultra-low-power RISC-V cores for internet-of-things
    applications," in *27th International Symposium on Power and Timing Modeling,
    Optimization and Simulation, PATMOS*, 2017, pp. 1–8.

    - <span id="page-3-15"></span>[16] T. Benz *et al.*, "A high-performance, energy-efficient
    modular DMA engine architecture," *IEEE Trans. Computers*, vol. 73, no. 1, pp.
    263– 277, 2024.

    - <span id="page-3-16"></span>[17] *SiFive TileLink Specification*, SiFive, Inc.,
    February 2023, rev. 1.9.3.'
- title: "Dynamic Power Control in a Hardware Neural Network with\n  Error-Configurable\
    \ MAC Units"
  abstract: 'Multi-Layer Perceptrons (MLP) are powerful tools for representing complex,

    non-linear relationships, making them essential for diverse machine learning

    and AI applications. Efficient hardware implementation of MLPs can be achieved

    through many hardware and architectural design techniques. These networks excel

    at predictive modeling and classification tasks like image classification,

    making them a popular choice. Approximate computing techniques are increasingly

    used to optimize critical path delay, area, power, and overall hardware

    efficiency in high-performance computing systems through controlled error and

    related trade-offs. This study proposes a hardware MLP neural network

    implemented in 45nm CMOS technology, in which MAC units of the neurons

    incorporate error and power controllable approximate multipliers for

    classification of the MNIST dataset. The optimized network consists of 10

    neurons within the hidden layers, occupying 0.026mm2 of area, with 5.55mW at

    100MHz frequency in accurate mode and 4.81mW in lowest accuracy mode. The

    experiments indicate that the proposed design achieves a maximum rate of 13.33%

    decrease overall and 24.78% in each neuron''s power consumption with only a

    0.92% decrease in accuracy in comparison with accurate circuit.'
  url: http://arxiv.org/abs/2410.10545v1
  keywords: Neural networks, hardware accelerators, approximate computing, low-power
    design, very large-scale integration, image classification
  document: "# Dynamic Power Control in a Hardware Neural Network with Error-Configurable\
    \ MAC Units\n\nM. Ghaderi A. Delavari F. Ghoreishy S. Mirzakuchaki\n\n*School\
    \ of Electrical Engineering*\n\n*Iran University of Science and Technology*\n\n\
    Tehran, Iran\n\n{maede ghaderi, arvin delavari, faraz ghoreishy}@elec.iust.ac.ir,\
    \ {m kuchaki}@iust.ac.ir\n\n*Abstract*—Multi-Layer Perceptrons (MLP) are powerful\
    \ tools for representing complex, non-linear relationships, making them essential\
    \ for diverse machine learning and AI applications. Efficient hardware implementation\
    \ of MLPs can be achieved through many hardware and architectural design techniques.\
    \ These networks excel at predictive modeling and classification tasks like image\
    \ classification, making them a popular choice. Approximate computing techniques\
    \ are increasingly used to optimize critical path delay, area, power, and overall\
    \ hardware efficiency in highperformance computing systems through controlled\
    \ error and related trade-offs. This study proposes a hardware MLP neural network\
    \ implemented in 45nm CMOS technology, in which MAC units of the neurons incorporate\
    \ error and power controllable approximate multipliers for classification of the\
    \ MNIST dataset. The optimized network consists of 10 neurons within the hidden\
    \ layers, occupying 0.026mm² of area, with 5.55mW at 100MHz frequency in accurate\
    \ mode and 4.81mW in lowest accuracy mode. The experiments indicate that the proposed\
    \ design achieves a maximum rate of 13.33% decrease overall and 24.78% in each\
    \ neuron's power consumption with only a 0.92% decrease in accuracy in comparison\
    \ with accurate circuit.\n\n*Index Terms*—Neural networks, hardware accelerators,\
    \ approximate computing, low-power design, very large-scale integration, image\
    \ classification\n\n#### I. INTRODUCTION\n\nMulti-Layer Perceptron (MLP) neural\
    \ networks are highly effective solutions for image classification tasks. Image\
    \ classification techniques simplify visual recognition by enabling the identification\
    \ of objects, faces, and even medical conditions from imaging data. The use of\
    \ MLP networks for image classification enhances efficiency, accuracy, and the\
    \ ability to process large-scale datasets [\\[1\\]](#page-5-0) that would be beyond\
    \ human capabilities. These Perceptrons are designed in order to learn to recognize\
    \ complex patterns in data, as they are made up of interconnected nodes, similar\
    \ to the neurons in a human brain. By adjusting the strengths of these connections\
    \ during training, MLPs can excel at tasks like image classification, speech recognition,\
    \ and financial forecasting [\\[2\\]](#page-5-1). The multilayered structure of\
    \ MLPs allows them to capture intricate relationships within data, making them\
    \ a powerful machine learning tool across various applications.\n\nRecently there\
    \ have been many investigations on hardware implementations of neural networks\
    \ and AI accelerators. The primary motivation for hardware implementation of neural\
    \ networks is to achieve improved performance compared to general-purpose CPUs.\
    \ Neural networks, particularly the computationally intensive parts like matrix\
    \ multiplications and convolutions, can be highly parallelized. FPGAs and ASICs\
    \ can be designed with a large number of processing elements that can perform\
    \ these operations concurrently, leading to performance improvements over CPU\
    \ execution. Specialized hardware designs can be tailored to the requirements\
    \ of neural network computations, reducing redundant operations and memory accesses.\
    \ This can result in lower power consumption compared to general-purpose processors\
    \ which is important for mobile, embedded, and edge computing applications. Integration\
    \ of processing elements in these designs can minimize data transfer time, leading\
    \ to lower end-to-end latency for neural network inference, which is crucial for\
    \ real-time applications. Hardware implementations can scale more easily to handle\
    \ larger network models and datasets compared to software-based approaches which\
    \ may be limited by the memory and computational capacity of processors. The high\
    \ performance and specialized nature of neural network hardware implementations\
    \ make them well-suited for deployment in edge devices. Recent trends indicate\
    \ there have been many progress in terms of hardware neural network design. Authors\
    \ in [\\[3\\]](#page-5-2) presented a novel hardware architecture for Feed-Forward\
    \ NNs that employs two multiplexed physical layers to facilitate parallel computation.\
    \ The design focuses on optimizing the architecture based on the largest layer's\
    \ neuron count, allowing it to adapt to different network sizes while minimizing\
    \ execution clock cycles. A novel composite hardware architecture for CNN accelerators\
    \ using FPGA that addresses throughput limitations through an optimized mapping\
    \ mechanism and efficient data supply strategies was also presented in [\\[4\\\
    ]](#page-5-3). In [\\[5\\]](#page-5-4), a method for implementing DNNs on FPGA\
    \ chips that leverages stochastic computing while addressing hardware constraints\
    \ is presented, showcasing the effectiveness in an image processing application.\
    \ Additionally, [\\[6\\]](#page-5-5) introduces an AIoT-empowered edge-cloud collaborative\
    \ computing system designed to enhance the efficiency in face detection and recognition\
    \ through the use of energy-efficient FPGA-based CNN accelerators.\n\nNevertheless,\
    \ even with specialized hardware design for the mentioned tasks, an overhead of\
    \ area and power consumption may still be forced into the system due to the huge\
    \ computational workload or large number of processing elements in neural networks.\
    \ To address this issue, approximate computing techniques can be employed to enhance\
    \ the efficiency of MLP neural networks. Approximations are allowed in applications\
    \ with inherent resilient to faults injected in the systems such as image processing\
    \ [\\[7\\]](#page-5-6), search engines [\\[8\\]](#page-5-7) and neural networks\
    \ which is the case in this research. By incorporating approximate multipliers\
    \ with configurable error and power in the multiply-accumulate (MAC) units of\
    \ the neurons, it is possible to reduce the hardware area and control power consumption\
    \ considerably, while retaining accuracy at a reasonable level. Main contributions\
    \ of this study are as follows:\n\n- Design of an optimized MLP neural network\
    \ for MNIST dataset classification.\n- Design and integration of an approximate\
    \ multiplier with different error levels and controllable power within the MAC\
    \ units of the neurons.\n- Analysis on the resulted data of the experiment in\
    \ terms of hardware efficiency, overall performance, and achieved accuracy.\n\n\
    The remainder of this paper is structured as follows: Section II presents an overview\
    \ of related and previous literature. Section III provides detailed explanations\
    \ about the architecture of the proposed design. Section IV analyzes the efficiency\
    \ in terms of accuracy and hardware of the proposed design with experimental results,\
    \ and finally, the paper is concluded in Section V.\n\n#### II. RELATED WORKS\n\
    \nInvestigations were conducted on the effectiveness of approximate computing\
    \ techniques in neural networks with different architectures. Approximate computing\
    \ aims to trade off a small amount of accuracy for significant gains in efficiency,\
    \ such as reduced energy consumption or faster processing. Approximation techniques\
    \ have also been utilized in generalpurpose processing cores, particularly in\
    \ embedded processors [\\[9\\]](#page-5-8), which often operate under stringent\
    \ power constraints. The authors in [\\[10\\]](#page-5-9) proposed an adaptive\
    \ solution that considers energy constraints in deep learning applications. By\
    \ selectively applying approximate computing techniques, this adaptive approach\
    \ can optimize the energy-accuracy tradeoff for deep neural networks. As Artificial\
    \ Neural Networks (ANNs) have a wide range of applications that are inherently\
    \ error-tolerant, [\\[11\\]](#page-5-10) proposes an approximate computing framework\
    \ that achieves energy savings by approximating computation and memory accesses\
    \ of less critical neurons.\n\nConvolutional Neural Networks (CNNs) have been\
    \ targeted for applying approximation techniques to present hardwareefficient\
    \ structures. Some researchers in [\\[12\\]](#page-5-11) explored techniques to\
    \ make CNN architectures more efficient by approximating certain computations\
    \ without significantly degrading accuracy. CNNs have been widely used for tasks\
    \ like image and speech recognition due to their ability to automatically extract\
    \ relevant features from the input data. [\\[13\\]](#page-5-12) presented a reconfigurable\
    \ architecture that utilizes approximate multipliers in the tensor multiplication\
    \ operations of a CNN-based speech recognition system. Approximate multipliers\
    \ can provide significant energy savings by trading off a small amount of precision.\
    \ In [\\[14\\]](#page-5-13), an approximate array multiplier utilizing carry-disregarding\
    \ techniques was employed in a CNN-based image classification setup. By approximating\
    \ the multiplier, this approach aimed to improve the energy efficiency of the\
    \ CNN model without greatly impacting its classification accuracy.\n\nDifferent\
    \ neural network solutions such as Support Vector Machines (SVM), Multi-Layer\
    \ Perceptrons (MLP), and Deep Convolutional Neural Networks (CNN) have been used\
    \ for classification of the MNIST dataset, which consists of images of handwritten\
    \ digits. [\\[15\\]](#page-5-14) explored and evaluated the performance of these\
    \ various neural network architectures on the MNIST dataset. This study will also\
    \ evaluate the MLPs in an image classification application using the same dataset,\
    \ with the implementation of dynamic approximation control through approximate\
    \ multipliers in MAC units within the neurons. The main goal of the project is\
    \ to showcase the effectiveness of approximate computing techniques by measuring\
    \ the power consumption in every dynamic configuration of the proposed design\
    \ and evaluating the final classification accuracy of the network at different\
    \ error tolerance levels.\n\n# III. PROPOSED NEURAL NETWORKS'S ARCHITECTURE\n\n\
    In this study, a simple MLP neural network with one hidden layer was implemented,\
    \ resulting in a total of three layers: an input layer, a hidden layer, and an\
    \ output layer. The input layer is composed of 62 nodes, corresponding to the\
    \ input features of MNIST dataset which are reduced from 748 in order to have\
    \ a more hardware-efficient design. It is important to note that feature reduction\
    \ algorithms can reduce the accuracy of neural networks, but for hardware implementations\
    \ due to area and power constraints they are highly utilized for inputs. As illustrated\
    \ in Fig. [1,](#page-1-0) the hidden layer comprises 30 neurons, where feature\
    \ extraction is performed and output layer of the network consists of 10 neurons.\n\
    \n![](_page_1_Figure_12.jpeg)\n\n<span id=\"page-1-0\"></span>Fig. 1. Proposed\
    \ fully connected MLP neural network (Blayer are for Bias factors and Wlayer are\
    \ for Weights).\n\nAll connections in the proposed MLP network are fully connected.\
    \ This means that each neuron in a given layer *i* is connected to every neuron\
    \ in the subsequent layer *i – 1*, with the output of each layer being passed\
    \ to the next layer with a specified weight. All weights and bias values of the\
    \ trained model are kept in memory and will enter the network through controller's\
    \ given signals.\n\n### *A. Error Controllable MAC Unit structure*\n\nMAC units\
    \ are crucial in neural networks as they perform the essential operations of multiplication\
    \ and accumulation, which are vital for computing weighted sums. In this design,\
    \ input values, weights, and biases are represented in signed magnitude format,\
    \ with the Most Significant Bit (MSB) indicating the sign. If the MSB is 0, the\
    \ number is positive; if it is 1, the number is negative. This representation\
    \ is particularly useful for neural networks, where both positive and negative\
    \ values are common.\n\nThe MAC unit employs XOR operations to manage the signs\
    \ of the resulting products from the multiplication of weights and inputs. The\
    \ XOR gate takes two inputs: the sign of the weight and the sign of the input.\
    \ If either input is negative (MSB = 1), the output will be negative (result =\
    \ 1 in terms of sign), while if both are positive (MSB = 0), the result is positive\
    \ (result = 0). The rest of the operations are all done utilizing unsigned arithmetic\
    \ since the sign bit is handled elsewhere.\n\nAll numbers processed in the MAC\
    \ unit are 8-bit, consisting of 7 bits for the value and 1 sign bit. The result\
    \ of each multiplication between an input and a weight is 14 bits wide (15 bits\
    \ including the sign bit). Since the input layer consists of 62 neurons and the\
    \ neural network is fully connected, 62 of these 15-bit numbers need to be summed\
    \ together. This accumulation results in a 21-bit output from the MAC unit, providing\
    \ sufficient range to avoid overflow when summing multiple inputs, even with high\
    \ values from multiple input combinations. [6: 0]\n\nFig. 2 illustrates the various\
    \ components of the MAC unit, including the multiplier, which is responsible for\
    \ multiplying the input and weight values, and the adder, which accumulates the\
    \ results of multiple multiplications to compute the final output. Additionally,\
    \ a subtractor handles cases where one of the operands is negative, while comparison\
    \ logic is used to determine the final sign of the accumulated result.\n\nThe\
    \ MAC unit integrated within the neurons are equipped with an approximate multiplier\
    \ with dynamic error control feature. The multiplier has an error-control signal\
    \ as an input in order to control output error. There are several error metrics\
    \ in approximate arithmetic circuits for comparing efficiency of the designs in\
    \ terms of accuracy, such as Error Rate (ER), Mean Relative Error Distance (MRED)\
    \ and Normalized Mean Error Distance (NMED) [\\[16\\]](#page-5-15) [\\[17\\]](#page-5-16).\
    \ Table I shows average, minimum and maximum of the resulted ER, MRED and NMED\
    \ of the proposed design in all 32 configurations. The proposed multiplier also\
    \ has an accurate configuration in which no error\n\n![](_page_2_Figure_7.jpeg)\n\
    \n<span id=\"page-2-0\"></span>Fig. 2. Proposed MAC unit with error controllable\
    \ approximate multiplier circuit\n\nis applied in the system by the arithmetic\
    \ circuits. The error controllable multiplier is the key solution to dynamic power\
    \ control and saving feature of the neural network. Accurate mode (configuration\
    \ zero) is excluded from values in Table I.\n\n<span id=\"page-2-1\"></span>TABLE\
    \ I ACCURACY EFFICIENCY CRITERIA FOR PROPOSED APPROXIMATE MULTIPLIER\n\n| ER [%]\
    \                      |         | MRED [%] |        | NMED [%] |        |\n|-----------------------------|---------|----------|--------|----------|--------|\n\
    | MIN                         | MAX     | MIN      | MAX    | MIN      | MAX \
    \   |\n| 9.9609                      | 61.8255 | 0.0548   | 3.6840 | 0.0028  \
    \ | 0.3643 |\n| Average (32 configurations) |         |          |        |  \
    \        |        |\n| 43.556                      |         | 2.125    |    \
    \    | 0.224    |        |\n\n#### *B. Neurons' structure*\n\n [7] [4: 0] S =\
    \ 0 As shown in (1) neurons process input data to produce an output by applying\
    \ weights and summing them, which is done utilizing MAC units, adding the bias\
    \ terms, and passing the result through an activation function. In [\\(1\\)](#page-2-2),\
    \ Wij represents defined weights between i th output of previous layer and j th\
    \ nodes of following layer, and b<sup>j</sup> denotes the j th respective biases.\
    \ According to the number of inputs (#Ni) in this setup, and the number of neurons\
    \ in the hidden layer (#Hn), the network will have #N<sup>i</sup> × #H<sup>n</sup>\
    \ hidden layer weights. Additionally, considering #O<sup>n</sup> neurons in output\
    \ layer, there are #H<sup>n</sup> × #O<sup>n</sup> output layer weights. Also,\
    \ the number of needful biases in hidden and output layers are equal to #H<sup>n</sup>\
    \ and #On, respectively.\n\n<span id=\"page-2-2\"></span>\n$$y\\_i = \\left(\\\
    sum\\_{i=0}^{n} W\\_{ij} \\times x\\_i + b\\_j\\right) \\tag{1}$$\n\nIn the proposed\
    \ design, each neuron contains a MAC unit, an adder to include the bias, an activation\
    \ function (ReLU in this paper), and a saturation section to limit the activation\
    \ output from 21 bits to 8 bits, ensuring compatibility with the input requirements\
    \ of the subsequent layer. The block diagram of neurons implemented in this MLP\
    \ network is shown in Fig. [3.](#page-3-0)\n\n![](_page_3_Figure_0.jpeg)\n\n<span\
    \ id=\"page-3-0\"></span>Fig. 3. Structure of neuron module in the proposed MLP\
    \ neural network with error controllable MAC units\n\nDue to necessity of activation\
    \ functions for their ability to introduce non-linearity into neural networks,\
    \ Rectified Linear Unit (ReLU) function, which is a non-linear function, is implemented\
    \ in the proposed design. ReLU is described as (2) allowing the network to learn\
    \ and capture intricate patterns by activating neurons only when their input is\
    \ positive. ReLU is used in hardware neural networks more frequently than software\
    \ approaches, due to its simplicity in implementation.\n\n<span id=\"page-3-1\"\
    ></span>\n$$ReLU(x) = \\max(0, x) \\tag{2}$$\n\n#### *C. Datapath and overall\
    \ structure*\n\n # × 8 # × 8 ℎ > Considering both hidden and output layers, #H<sup>n</sup>\
    \ + #O<sup>n</sup> neurons are totally needed. However, in order to save resources\
    \ only 10 hardware neurons are integrated, working in 4 separate cycles to execute\
    \ all neurons' operations. To handle the multicycle datapath, a controller is\
    \ designed, which will be discussed later. As is shown in Fig. 4, the input values,\
    \ weights, and biases are chosen within multiplexers based on their relevant field\
    \ with selection signals. For each of the 10 neurons in the hidden layer, 10 registers\
    \ with 8-bit width are incorporated to store the results, which then serve as\
    \ input for the output layer. Finally, by evaluating results of output layer,\
    \ the maximum output value, as shown in Fig. 4, is acquired and the prediction\
    \ label emerges. \n\n![](_page_3_Figure_6.jpeg)\n\n<span id=\"page-3-2\"></span>Fig.\
    \ 4. Datapath of the implemented MLP neural network\n\nThis approach ensures efficient\
    \ use of hardware resources while maintaining the accuracy and functionality of\
    \ the neural network. The design effectively manages the data flow and\n\ncomputation\
    \ across multiple cycles, enabling network to process inputs and generate predictions\
    \ as required.\n\n# *D. Controller*\n\nTo manage input data, weights and biases,\
    \ a finite state machine with 5 states is designed. This state machine coordinates\
    \ a multi-step process by flow control of the operations, including reading inputs,\
    \ selecting computation parameters (weights and biases), loading registers, and\
    \ enabling counters. It transitions through a series of states, ultimately signaling\
    \ when the entire process is complete.\n\n- State 0: The state machine starts\
    \ by enabling the reading of memory inputs and selecting the input source. It\
    \ sets the weight selections and bias selection to 0 to choose the proper weights\
    \ and biases which are related to first 10 neurons in hidden layer, and the results\
    \ of all neuron in this state will be stored into registers. The system then transitions\
    \ to State 1.\n- State 1: In this state, the machine continues reading memory\
    \ inputs and selecting the input source. The weight and bias selections are set\
    \ to 1, which select the parameters of the second set of 10 hidden layer neurons,\
    \ and the outputs from all neurons in the same state will be saved into registers.\
    \ The state machine then moves to State 2.\n- • State 2: The operations of reading\
    \ memory inputs and selecting the input source continues. The weight and bias\
    \ selections are updated to 2 to fetch the last 10 neurons' weights and biases\
    \ in hidden layer and the results of each neuron in the same state will be stored\
    \ in registers. The machine transitions to State 3.\n- .. • Sate 3: The state\
    \ machine updates the weight and bias selections to 3 in order to choose output\
    \ layer's variable. Then, the circuit for maximum value determination is enabled\
    \ to obtain the predicted label for input image. Furthermore, classified image\
    \ counter is activated, and if the counter value is less than number of input\
    \ images in dataset, the state machine loops back to State 0; otherwise, it progresses\
    \ to State 4.\n\t- State 4: In the following state, controller produces a signal\
    \ for concluding operations when all images of the dataset are classified.\n\n\
    # IV. EXPERIMENTAL RESULTS\n\n <sup>0</sup> <sup>10</sup> � The proposed neural\
    \ network was implemented in 45nm CMOS technology utilizing Synopsys Design Compiler\
    \ software. Power analysis was accomplished by implying related switching activity\
    \ files to the proposed MLP network, which was designed using Verilog HDL. The\
    \ dataset was loaded into an external memory space within the testbench, which\
    \ served as input for the proposed MLP. An integer variable within the testbench\
    \ was utilized to count the number of correct results, as indicated by the number\
    \ of high states of the prediction completion signal during each interval. The\
    \ accuracy of the proposed MLP is measured by dividing the count of correctly\
    \ predicted results by the total number of images. As depicted\n\nin Fig. [4,](#page-3-2)\
    \ the FSM is tasked with managing the sequence of input data (in each clock cycle)\
    \ sourced from memory and identifying the type of input, whether it is a weight,\
    \ bias, or standard input data. The proposed error and power controllable multiplier\
    \ in the MAC units have 32 configurations. Proposed design has reached an area\
    \ of 26084µm², operating in a frequency range of 100MHz to 330MHz. All power results\
    \ in this study were measured in the equal setup, where design is working at 100MHz\
    \ frequency at 1.1V. The ER and MRED of the proposed multiplier vary from 0.00%\
    \ to 61.82% and 0.00% to 3.68% regarding the dynamic error control feature of\
    \ the design. The average ER of the design is equal to 43.55% and average MRED\
    \ is 2.12%. The average Normalized Mean Error Distance (NMED) is 0.22. The multiplier\
    \ circuit was carefully designed in order to provide the maximum range of power\
    \ saving in different configurations, and minimum error injection to the system.\n\
    \nThe proposed design was evaluated using all 32 configurations of the MAC units,\
    \ testing each configuration across every set of 10 neurons. The results demonstrated\
    \ success in terms of power efficiency and final accuracy. Accuracy of the MLP\
    \ is extracted from the ratio of correctly classified images to the number of\
    \ all images in the preferred dataset. The maximum accuracy reached through this\
    \ system was 89.67% (according to hardware optimizations and approximation in\
    \ design process). The lowest accuracy reached through approximate mode of the\
    \ MAC units in the most inaccurate configuration was 88.75% which shows that accuracy\
    \ has fallen less than 1% in the worst case. The average accuracy of all 32 configurations\
    \ is 89.11% which has a difference of only 0.56% in comparison with accurate form\
    \ which indicates reasonable results. But the most important achievement in this\
    \ study is the power saved in configurations adhering approximation in MAC units.\n\
    \nFig. [5](#page-4-0) presents percentage of improvement in terms of power consumption,\
    \ as Fig. [6](#page-4-1) shows the power consumed in every configuration.\n\n\
    ![](_page_4_Figure_3.jpeg)\n\n<span id=\"page-4-0\"></span>Fig. 5. Improvements\
    \ in overall power consumption all 32 dynamic configurations of MAC units (neurons)\n\
    \nFig. [6](#page-4-1) is presenting power consumption based on accuracy in the\
    \ respected error level. In the worst accuracy configuration of the MAC unit in\
    \ which the multiplier has an ER of 61.82% and MRED of 3.68%, the accuracy is\
    \ 88.75% where the power consumed in all of the network is equal to 4.81mW.\n\n\
    ![](_page_4_Figure_7.jpeg)\n\n<span id=\"page-4-1\"></span>Fig. 6. Power consumption\
    \ based on accuracy of the MLP neural network in all 32 configurations of MAC\
    \ units (neurons)\n\nAccurate mode of MAC units resulted in a power consumption\
    \ of 5.55mW. In fact, the maximum power saved in the system is 740µW which is\
    \ equal to 13.33% improvement in all of the neural network, 44.36% in each MAC\
    \ unit and 24.78% in every neuron. The average improvement of all 32 configurations\
    \ is equal to 5.84% in total power consumption which is nearly 324µW. Average\
    \ improvements in power saved by MAC units is 40.89% and in neurons is 22.90%.\
    \ Fig. [7](#page-4-2) illustrates the trade-off between accuracy and power consumption\
    \ in the proposed design which has led to reasonable results, providing a promising\
    \ approach for power saving in hardware neural networks.\n\n![](_page_4_Figure_10.jpeg)\n\
    \n<span id=\"page-4-2\"></span>Fig. 7. Neural network accuracy trade-off with\
    \ overall power consumption of implemented hardware\n\nThe proposed design is\
    \ highly optimized through a combination of feature reduction and resource-saving\
    \ techniques. This involved reducing the number of neurons in both hidden and\
    \ output layers and applying operations across multiple clock cycles, leading\
    \ to a hardware efficient design. The key finding of this study is that employing\
    \ circuit-level approximation and hardware-based techniques in neural networks\
    \ can yield substantial power savings. Each neuron out of the 10 in the proposed\
    \ MLP achieved an average 22.90% power savings compared to the accurate version\
    \ solely. Furthermore, if scaled to larger neural network architectures, the impact\
    \ of these hardware approximation techniques would become even more obvious.\n\
    \n#### V. CONCLUSION\n\nThis paper introduced an MLP neural network with approximate\
    \ MAC units to enhance energy efficiency for image classification tasks demonstrated\
    \ on the MNIST dataset. Our design achieved a 13.33% improvement in power consumption\
    \ with only a 0.92% drop in accuracy compared to exact computation instance in\
    \ the least accurate configuration which is a remarkable result. These findings\
    \ illustrate the effectiveness of approximate computing techniques in improving\
    \ hardware efficiency while maintaining acceptable accuracy regarding the application.\
    \ The results point out the practicality of addressing the inefficiencies seen\
    \ in conventional MLP implementations by using approximation methods. The reduction\
    \ in hardware area, power consumption and critical path delay is demanded for\
    \ applications that require low-power and compact hardware solutions, such as\
    \ portable and embedded systems.\n\nFurthermore, the slight trade-off in accuracy\
    \ is acceptable for many practical applications, especially when the benefits\
    \ in terms of efficiency are considerable. The suggested accurate/approximate\
    \ MLP method is a strong choice for many machine learning tasks because it can\
    \ keep a reasonable degree of accuracy while enhancing hardware performance. Future\
    \ work will explore the application of approximate computing solutions to other\
    \ neural network architectures and datasets to validate their generalizability\
    \ and effectiveness. Additionally, further optimizations could be investigated\
    \ to minimize the accuracy trade-off. The integration of approximate arithmetic\
    \ circuits into more complex neural networks shall pave the way for more energy-efficient\
    \ AI solutions across various domains.\n\nIn conclusion, this study presents a\
    \ notable direction for enhancing the efficiency of neural network hardware through\
    \ the use of approximate computing techniques. By achieving a proper balance between\
    \ performance and resource utilization, the proposed approach offers significant\
    \ advantages for the development of efficient, high-performance machine learning\
    \ and artificial intelligence systems.\n\n#### REFERENCES\n\n- <span id=\"page-5-0\"\
    ></span>[1] C. Xu, \"Applying MLP and CNN on Handwriting Images for Image Classification\
    \ Task,\" *2022 5th International Conference on Advanced Electronic Materials,\
    \ Computers and Software Engineering (AEMCSE)*, Wuhan, China, 2022, pp. 830-835.\n\
    - <span id=\"page-5-1\"></span>[2] R. S. Chugh, V. Bhatia, K. Khanna and V. Bhatia,\
    \ \"A Comparative Analysis of Classifiers for Image Classification,\" *2020 10th\
    \ International Conference on Cloud Computing, Data Science & Engineering*, Noida,\
    \ India, 2020, pp. 248-253.\n- <span id=\"page-5-2\"></span>[3] El-Sharkawy, Mohamed\
    \ et al. \"Re-configurable parallel Feed-Forward Neural Network implementation\
    \ using FPGA.\" Integr. 97 (2024): 102176\n- <span id=\"page-5-3\"></span>[4]\
    \ W. Huang et al., \"FPGA-Based High-Throughput CNN Hardware Accelerator With\
    \ High Computing Resource Utilization Ratio,\" *in IEEE Transactions on Neural\
    \ Networks and Learning Systems*, vol. 33, no. 8, pp. 4069-4083, Aug. 2022.\n\
    - <span id=\"page-5-4\"></span>[5] Nobari, Maedeh and Hadi Jahanirad. \"FPGA-based\
    \ implementation of deep neural network using stochastic computing.\" Appl. Soft\
    \ Comput. 137 (2023): 110166.\n- <span id=\"page-5-5\"></span>[6] X. Liu et al.,\
    \ \"Collaborative Edge Computing With FPGA-Based CNN Accelerators for Energy-Efficient\
    \ and Time-Aware Face Tracking System,\" *in IEEE Transactions on Computational\
    \ Social Systems*, vol. 9, no. 1, pp. 252-266, Feb. 2022\n- <span id=\"page-5-6\"\
    ></span>[7] G. Anusha and P. Deepa, \"Design of approximate adders and multipliers\
    \ for error tolerant image processing,\" *Microprocessors and Microsystems*, vol.\
    \ 72, p. 102940, 2020.\n- <span id=\"page-5-7\"></span>[8] S. Mittal, \"A survey\
    \ of techniques for approximate computing,\" *ACM Comput. Surv.*, vol. 48, no.\
    \ 4, pp. 62:1-62:33, Mar. 2016\n- <span id=\"page-5-8\"></span>[9] A. Delavari,\
    \ F. Ghoreishy, H. S. Shahhoseini and S. Mirzakuchaki, \"A Reconfigurable Approximate\
    \ Computing RISC-V Platform for Fault-Tolerant Applications,\" *2024 27th Euromicro\
    \ Conference on Digital System Design (DSD)*, Paris, France, 2024, pp. 81-89.\n\
    - <span id=\"page-5-9\"></span>[10] N. TaheriNejad and S. Shakibhamedan, \"Energy-aware\
    \ adaptive approximate computing for deep learning applications,\" *in Proc. IEEE\
    \ Comput.Soc. Annu. Symp. VLSI (ISVLSI)*, Jul. 2022, p. 328.\n- <span id=\"page-5-10\"\
    ></span>[11] Q. Zhang, T. Wang, Y. Tian, F. Yuan and Q. Xu, \"ApproxANN: An approximate\
    \ computing framework for artificial neural network,\" *2015 Design, Automation\
    \ & Test in Europe Conference & Exhibition (DATE)*, Grenoble, France, 2015, pp.\
    \ 701-706.\n- <span id=\"page-5-11\"></span>[12] M. F. Tolba, H. Saleh, M. Al-Qutayri,\
    \ A. Hroub and T. Stouraitis, \"Efficient CNN Hardware Architecture Based on Linear\
    \ Approximation and Computation Reuse Technique,\" *2023 International Conference\
    \ on Microelectronics (ICM)*, Abu Dhabi, United Arab Emirates, 2023, pp. 7-10.\n\
    - <span id=\"page-5-12\"></span>[13] J. Qian, Y. Jiang, Z. Zhang, R. Zhang, Z.\
    \ Wang and B. Liu, \"Reconfigurable Approximate Multiplication Architecture for\
    \ CNN-Based Speech Recognition Using Wallace Tree Tensor Multiplier Unit,\" *2021\
    \ IEEE/ACM International Symposium on Nanoscale Architectures (NANOARCH)*, AB,\
    \ Canada, 2021, pp. 1-6.\n- <span id=\"page-5-13\"></span>[14] S. Shakibhamedan,\
    \ N. Amirafshar, A. S. Baroughi, H. S. Shahhoseini and N. Taherinejad, \"ACE-CNN:\
    \ Approximate carry disregard multipliers for energy-efficient CNN-Based image\
    \ classification,\" *in IEEE Transactions on Circuits and Systems I: Regular Papers*,\
    \ 2024.\n- <span id=\"page-5-14\"></span>[15] N. Mohamed, R. Josphineleela, S.\
    \ Rakhamaji Madkar, J. Vasantha Sena, B. S. Alfurhood and B. Pant, \"The Smart\
    \ Handwritten Digits Recognition Using Machine Learning Algorithm,\" *2023 3rd\
    \ International Conference on Advance Computing and Innovative Technologies in\
    \ Engineering (ICACITE)*, Greater Noida, India, 2023, pp. 340-344.\n- <span id=\"\
    page-5-15\"></span>[16] A. G. M. Strollo, E. Napoli, D. D. Caro, N. Petra, and\
    \ G. D. Meo, \"Comparison and extension of approximate 4–2 compressors for lowpower\
    \ approximate multipliers,\" *IEEE Trans. Circuits Syst. I, Reg. Papers*, vol.\
    \ 67, no. 9, pp. 3021–3034, Sep. 2020.\n- <span id=\"page-5-16\"></span>[17] P.\
    \ Yin, C. Wang, H. Waris, W. Liu, Y. Han, and F. Lombardi, \"Design and analysis\
    \ of energy-efficient dynamic range approximate logarithmic multipliers for machine\
    \ learning,\" *IEEE Trans. Sustain. Comput.*, vol. 6, no. 4, pp. 612–625, Oct.\
    \ 2021."
- title: "Theoretical Analysis of the Efficient-Memory Matrix Storage Method for\n\
    \  Quantum Emulation Accelerators with Gate Fusion on FPGAs"
  abstract: 'Quantum emulators play an important role in the development and testing
    of

    quantum algorithms, especially given the limitations of the current FTQC era.

    Developing high-speed, memory-optimized quantum emulators is a growing research

    trend, with gate fusion being a promising technique. However, existing gate

    fusion implementations often struggle to efficiently support large-scale

    quantum systems with a high number of qubits due to a lack of optimizations for

    the exponential growth in memory requirements. Therefore, this study proposes

    the EMMS (Efficient-Memory Matrix Storage) method for storing quantum operators

    and states, along with an EMMS-based Quantum Emulator Accelerator (QEA)

    architecture that incorporates multiple processing elements (PEs) to accelerate

    tensor product and matrix multiplication computations in quantum emulation with

    gate fusion. The theoretical analysis of the QEA on the Xilinx ZCU102 FPGA,

    using varying numbers of PEs and different depths of unitary and local data

    memory, reveals a linear increase in memory depth with the number of qubits.

    This scaling highlights the potential of the EMMS-based QEA to accommodate

    larger quantum circuits, providing insights into selecting appropriate memory

    sizes and FPGA devices. Furthermore, the estimated performance of the QEA with

    PE counts ranging from $2^2$ to $2^5$ on the Xilinx ZCU102 FPGA demonstrates

    that increasing the number of PEs significantly reduces the computation cycle

    count for circuits with fewer than 18 qubits, making it significantly faster

    than previous works.'
  url: http://arxiv.org/abs/2410.11146v1
  keywords: quantum emulator, field-programmable-gatearrays, quantum computing
  document: '# Theoretical Analysis of the Efficient-Memory Matrix Storage Method
    for Quantum Emulation Accelerators with Gate Fusion on FPGAs


    Le Tran Xuan Hieu 1,2, Pham Hoai Luan <sup>3</sup> , Vu Tuan Hai <sup>3</sup>
    , Le Vu Trung Duong <sup>3</sup> and Yasuhiko Nakashima<sup>3</sup>


    <sup>1</sup> University of Information Technology, Ho Chi Minh City, 700000, Vietnam.


    <sup>2</sup> Vietnam National University, Ho Chi Minh City, 700000, Vietnam.


    <sup>3</sup> Nara Institute of Science and Technology, 8916–5 Takayama-cho, Ikoma,
    Nara 630-0192, Japan.


    Email: pham.luan@naist.is.jp


    *Abstract*—Quantum emulators play an important role in the development and testing
    of quantum algorithms, especially given the limitations of the current FTQC era.
    Developing high-speed, memory-optimized quantum emulators is a growing research
    trend, with gate fusion being a promising technique. However, existing gate fusion
    implementations often struggle to efficiently support large-scale quantum systems
    with a high number of qubits due to a lack of optimizations for the exponential
    growth in memory requirements. Therefore, this study proposes the EMMS (Efficient-Memory
    Matrix Storage) method for storing quantum operators and states, along with an
    EMMS-based Quantum Emulator Accelerator (QEA) architecture that incorporates multiple
    processing elements (PEs) to accelerate tensor product and matrix multiplication
    computations in quantum emulation with gate fusion. The theoretical analysis of
    the QEA on the Xilinx ZCU102 FPGA, using varying numbers of PEs and different
    depths of unitary and local data memory, reveals a linear increase in memory depth
    with the number of qubits. This scaling highlights the potential of the EMMS-based
    QEA to accommodate larger quantum circuits, providing insights into selecting
    appropriate memory sizes and FPGA devices. Furthermore, the estimated performance
    of the QEA with PE counts ranging from 2 2 to 2 5 on the Xilinx ZCU102 FPGA demonstrates
    that increasing the number of PEs significantly reduces the computation cycle
    count for circuits with fewer than 18 qubits, making it significantly faster than
    previous works.


    *Index Terms*—quantum emulator, field-programmable-gatearrays, quantum computing


    ## I. INTRODUCTION


    Nowadays, the use of quantum algorithms on FTQC computers has great potential
    for speeding up computation [\[1\]](#page-7-0)–[\[3\]](#page-7-1). The expense
    of performing tests on real quantum computers via cloud services such as AWS Braket,
    however, is an obstacle. As a result, scientists frequently turn to employing
    simulation programs such as Qiskit [\[4\]](#page-7-2) and Pennylane [\[5\]](#page-7-3)
    to test their models and methods locally. Although this method can be used for
    preliminary verification, it is inefficient since it takes exponentially more
    computer power to simulate and store the state of a large number of qubits (#Qubits).
    Moreover, the functions performed by a quantum simulator always entail massive
    matrix multiplications and tensor products. The dimensions of these matrices rise
    exponentially with the #Qubits, posing significant challenges with processing
    time and memory usage. A potential solution to these challenges is the growing
    use of quantum emulators built on specialized hardware. These emulators take advantage
    of the hardware''s strengths, such as enhanced parallel processing and more flexible
    memory management, allowing them to efficiently simulate larger qubit systems.
    By focusing on specialized hardware, these emulators can drive long-term progress,
    enabling quicker discovery and testing of quantum algorithms.


    Numerous studies have been conducted to enhance the performance of quantum circuit
    simulation systems, aiming to achieve high simulation speeds and support computations
    for a larger #Qubits. For example, the study in [\[6\]](#page-7-4) used the Cori
    II supercomputer, with 0.5 PBs of memory, to efficiently boost the simulation
    speed for a 45-qubit quantum circuit simulator using scheduling algorithms and
    automated code generation. However, its huge memory demands limited scalability
    and accessibility for larger quantum circuits. Another emerging trend is the use
    of graphics processing units (GPUs) for quantum circuit modeling acceleration.
    As described in [\[7\]](#page-7-5), the NVIDIA cuQuantum SDK provides useful building
    blocks for tensor network and state vector-based simulators to accelerate computations
    by leveraging the parallel processing power of GPUs. Another work presented in
    [\[8\]](#page-7-6) focuses on GPU-based simulation optimization using active state
    amplitude modulation, pruning, and compression techniques. Despite achieving high
    performance, they still can not optimize memory usage effectively, making it difficult
    to support quantum simulation systems with a large #Qubits. Notably, FPGAs have
    emerged as a promising hardware platform for quantum circuit emulations. For instance,
    the efficient computation quantum emulator for unitary transformations proposed
    in [\[9\]](#page-7-7) utilizes a pipeline design, floating-point arithmetic, and
    hardware minimization techniques to simulate Quantum Fourier Transform (QFT) [\[10\]](#page-7-8)
    with a high #Qubits. This approach aims to improve processing speed, accuracy,
    and memory usage. In addition, the re-configurable quantum emulator proposed in
    [\[11\]](#page-7-9) modifies Grover''s algorithm for dynamic multi-pattern search
    and estimates the capability of simulating up to 32 qubits with high accuracy
    and resource optimization techniques. However, the scalability of these FPGA-based
    emulators is still limited


    ![](_page_1_Figure_0.jpeg)


    Figure 1: Computational path in quantum computation model.


    by the large hardware resources required. Furthermore, the simulation time increases
    exponentially with #Qubits, and efficiently simulating complex algorithms can
    be challenging.


    A potential solution to the problem of enormous storage requirements in quantum
    simulators is gate fusion, as proposed and optimized in [\[6\]](#page-7-4), [\[12\]](#page-7-10),
    [\[13\]](#page-7-11). Accordingly, this technique reduces the number of gates
    in a quantum circuit by merging multiple gates into a single gate, which in turn
    reduces the number of matrix multiplications needed to simulate the circuit. As
    a result, memory consumption is decreased, and simulation performance is increased.
    Gate fusion can be very helpful in reducing the number of matrix operations, but
    it might not be able to completely address the memory use problem, particularly
    when working with high #Qubits. The prior quantum states also take a significant
    amount of memory to store, and the aggregated matrices can still be huge.


    To address the existing challenges, this paper introduces the Efficient-Memory
    Matrix Storage (EMMS) method, designed to minimize storage requirements and maximize
    parallel processing speed for gate fusion quantum simulation. Furthermore, a quantum
    emulation accelerator (QEA) based on the EMMS method is proposed to be implemented
    on FPGA system-on-chips (SoCs) to demonstrate the effectiveness of the method.
    The EMMS''s effectiveness in reducing storage resources and improving processing
    speed is exhibited through evaluation results incorporating variations in memory
    resources, #PEs and the #Qubits.


    ## II. BACKGROUND KNOWLEDGE


    ## <span id="page-1-4"></span>*A. Quantum computation model*


    The quantum computation model can be expressed as a unitary U parameterized by
    θ, act on a reference state |ψ (0)⟩ [\[14\]](#page-7-12) as Eq. [1:](#page-1-0)


    <span id="page-1-0"></span>

    $$\langle \psi^{(m)} \rangle = \mathcal{U}(\boldsymbol{\theta}) \times |\psi^{(\mathbf{0})}\rangle
    = \left(\prod\_{j=1}^{m} U^{(\mathbf{t})}(\boldsymbol{\theta}^{(\mathbf{t})})\right)
    \times |\psi^{(\mathbf{0})}\rangle,\tag{1}$$


    where |ψ (m) ⟩ represents the target quantum state, and U(θ) denotes the quantum
    operator. Whenever the quantum computation model is performed, it constitutes
    a Quantum Evaluation (QE). This quantum operator can be decomposed not only into
    Matrix Multiplication (MM) between sub-operators but also into mˆ gates (considering
    single-qubit gates and two-qubit gates) application. These gates G = {gj} can
    be applied on |ψ (0) ⟩ sequentially to achieve the same |ψ (m) ⟩.


    The strategy for solving Eq. [1](#page-1-0) is choosing the break points in gate
    sequence g1g<sup>2</sup> . . . gm<sup>ˆ</sup> to break it as group {G(t)}, satisfy
    single parameter condition (each sub-operator U (t) (θ (t) ) has as many gates
    as possible but only contains one parameterized gate), then pass {G(t)} into Tensor
    Product (TP) function T : G(t) ∈ g <sup>k</sup> → U (t) ∈ C <sup>N</sup>×<sup>N</sup>
    = N<sup>|</sup>G(t) | <sup>j</sup>=1 g<sup>j</sup> with g<sup>j</sup> ∈ G(t) .
    The term U (m) . . . U(1) |ψ (0) ⟩ can be re-written as Eq. [2:](#page-1-1)


    $$\left(\mathcal{T}(G^{(m)})\left(\ldots\left(\mathcal{T}(G^{(1)})|\psi^{(\mathbf{0})}\rangle\right)\right)\right),\tag{2}$$


    <span id="page-1-1"></span>which require m × TP and m × MM for execution. Note
    that, U (t) is the unitary matrix, satisfy U (t) (U (t) ) † = I.


    ## *B. Sparse matrix computations*


    *1) Coordinate Format:* A matrix is typically regarded as sparse when the number
    of zero elements is greater than or equal to the number of non-zero elements.
    In the context of quantum emulation, the enormous size of these matrices poses
    a challenge, as they contain so many elements that conventional memory systems
    cannot accommodate them. To efficiently manage this, we employ a sparse matrix
    storage method known as the Coordinate (COO) Format [\[15\]](#page-7-13). This
    format focuses on storing only the non-zero values, thereby significantly reducing
    the amount of storage required. By adopting the COO Format, we also exclude the
    need for redundant computations involving zero values, which in turn leads to
    enhanced computational performance. The format achieves this efficiency by representing
    a sparse matrix using three primary components: row indices, column indices, and
    the actual non-zero values. Each non-zero element is mapped precisely to its location
    within the tensor using these indices, allowing for efficient data retrieval and
    manipulation.


    <span id="page-1-3"></span>*2) Tensor product:* One of the computations in quantum
    emulation is TP between two quantum operators, also known as the Kronecker product.
    Denoted by g⊗h, it is an operation on two matrices of arbitrary size resulting
    in a block matrix.


    As we discussed, COO format is not only used for storing sparse matrices but also
    for computing. Accordingly, each nonzero element gi,j ∈ g is denoted as (i, j,
    gi,j ) where i, j, gi,j are the row index, column index, and value, respectively.
    The TP is expressed as looping through all possible combinations between {gi,j}
    and {hk,l}, as Algo. [1.](#page-1-2) Fig. [2](#page-2-0) illustrates the detailed
    sparse TP between two 2 × 2 tensors, which are transformed into two corresponding
    arrays of tuples. The resulting computed tuples are then merged into a new array.


    <span id="page-1-2"></span>


    | Algorithm 1 TP between two gates in COO.      |  |

    |-----------------------------------------------|--|

    | ( g × h )×( g × h )<br>Require: g, h, res ← 0 |  |

    | for (i, j, gi,j<br>) in g do                  |  |

    | for (k, l, hk,l) in h do                      |  |

    | ← gi,j<br>× hk,l<br>resi× h +l,j× h +k        |  |

    | end for                                       |  |

    | end for                                       |  |

    | return res                                    |  |

    |                                               |  |


    <span id="page-2-0"></span>1 0 0 −1 = 0 − 0 0 0 0 0 0 0 0 0 0 − 0 <sup>⊗</sup>
    <sup>0</sup> − 0 0,0,1 1,1, −1 0,1, 1,0, − 0,0,1 0,0,1 1,1, −1 1,1, −1 ⊗ 0,1,
    ⊗ 1,0, − ⊗ 0,1, ⊗ 1,0, − = 0,1, = 1,0, − = 2,3, − = 3,2, × 1/ 2 0 0 1/ 2 = 0 −/
    2 −/ 2 0 × 1,0,0 × 0,0,1/ 2 × 3,0,1/ 2 × 2,0,0 = 0,0,0 = 1,0, −/ 2 = 2,0, −/ 2
    = 3,0,0


    Figure 2: Conducting both TP and MM (T ({Z, Y })|ψGHZ⟩) in COO format.


    <span id="page-2-1"></span>


    | Algorithm 2 MM between operator and state in COO.                 |

    |-------------------------------------------------------------------|

    | (t)<br>(t−1)<br>n×1<br>Require: U<br>,  ψ<br>⟩, res ← 0           |

    | for (i, j, U(t)<br>(t) do<br>i,j ) in U                           |

    | (t−1)<br>for (k, 0, αk) in  ψ<br>⟩ do                             |

    | (t)<br>if j == k then resi<br>← resi<br>+ U<br>i,j × αk<br>end if |

    | end for                                                           |

    | end for                                                           |

    | return res                                                        |


    *3) Matrix multiplication:* In addition to the TP, quantum emulation employs the
    MM working as presented in Algo. [2.](#page-2-1) Similarly, the COO format is
    utilized for this computation, where all non-zero elements of matrices will be
    represented as constant or non-constant tuples as described in [II-B2.](#page-1-3)
    When performing U (t) × |ψ (t−1) ⟩, non-zero elements of U (t) are multiplied
    with corresponding non-zero elements of |ψ (t−1) ⟩ based on ESC algorithm [\[16\]](#page-7-14).
    Specifically, each tuple (i, j, U(t) i,j ) is computed with each amplitude αk,
    provided that j = k. All tuples with the same (i, k) are accumulated to obtain
    the final result as shown in Fig. [2.](#page-2-0)


    ## III. PROPOSED METHOD AND HARDWARE ARCHITECTURE


    This section introduces the Efficient-Memory Matrix Storage method to reduce memory
    usage and improve processing speed, along with our proposed architecture for efficient
    quantum emulation on FPGAs.


    ## <span id="page-2-3"></span>*A. Efficient-Memory Matrix Storage (EMMS) Method*


    The challenge when simulating Eq. [1](#page-1-0) is the exponential scaling of
    both computation time and resources based on (#Qubits) n. In detail, {Uj} can
    be represented as N × N complex matrix (N = 2<sup>n</sup>). Fig. [3](#page-3-0)
    illustrates the relationship between the #Qubits in a quantum circuit and the
    memory usage (in GB) for U (t) . It is observed that the memory requirements for
    simulating quantum circuits scale exponentially with the #Qubits. This phenomenon
    becomes more pronounced as the #Qubits increases. For instance, simulating a circuit
    with 20 qubits requires 0.02 GB of memory. However, as the qubit count increases
    to 32, the memory requirements skyrocket to approximately 68.72 GB. In almost
    all simulators, only |ψ (t) ⟩ is being saved during the whole computation process.
    Our target is to reducing both computation time and resources to polynomial complexity
    at a certain #Qubits.


    The challenge for processing U (t+1) = T (G(t+1) ) is that the output size is
    a N × N matrix, which is soon to be overloaded for local memory. Our strategy
    is decomposing T (G(t+1) ) into T (G) ⊗ T (G) as Eq. [3.](#page-2-2) With n (N
    = 2n) is the dividing point which depends on the type of quantum gates and the
    hardware resources, detailed in Section [III-B.](#page-3-1) The gate-chosen strategy
    is priority grouping sparse gates (all gates excluding {H, RX(.), R<sup>Y</sup>
    (.), SX}), for dense gates, we group it with I. This leads to the number of non-zero
    elements in each row in T (G) is 1.


    <span id="page-2-2"></span>

    $$\begin{aligned} \mathcal{T}(G^{(\mathbf{t}+\mathbf{1})})|\psi^{(\mathbf{t})}\rangle
    &= \left[\mathcal{T}(\overline{G}) \otimes \mathcal{T}(G)\right]|\psi^{(\mathbf{t})}\rangle\\
    &= \left[\left(\bigotimes\_{j=0}^{\overline{n}-1} g\_j\right) \otimes \left(\bigotimes\_{j=\overline{n}}^{n-1}
    g\_j\right)\right] \begin{bmatrix} \alpha\_0^{(\mathbf{t})}\\ \alpha\_1^{(\mathbf{t})}\\
    \vdots\\ \alpha\_N^{(\mathbf{t})} \end{bmatrix} \end{aligned} (3)$$


    ![](_page_2_Figure_11.jpeg)


    The right side of Eq. [3](#page-2-2) (- T (G) ⊗ T (G) |ψ (t) ⟩) is executed by
    reiterating TP and MM through all tuples in T (G) as Algo. [3,](#page-3-2) instead
    of Algo. [2.](#page-2-1) Note that multiple PEs can handle this process parallelly.
    A more comprehensive example is presented in Appendix. [V.](#page-7-15)


    This methodology effectively reduces memory usage by removing redundant zero elements
    using the COO format. Furthermore, decomposing T (G(t+1) ) into T (G) and T (G)
    reduces the storage requirements from N to N + N N . Under ideal conditions, where
    n = ⌈n/2⌉, the expression N + N N simplifies to 2N ≪ N 2 , resulting in a significant
    memory


    <span id="page-3-0"></span>![](_page_3_Figure_0.jpeg)


    Figure 3: Traditional gate fusion (U (t) ) storage method in [\[12\]](#page-7-10).


    <span id="page-3-3"></span>![](_page_3_Figure_2.jpeg)


    Figure 4: Proposed gate fusion''s storage.


    efficiency improvement of 2 n−1 . Fig. [4](#page-3-3) shows the estimated memory
    space from 20 to 32 qubits with different n. The optimal cases lie on the diagonal
    line. It is observed that selecting an appropriate value for n allow the EMMS
    method to limit the storage usage to a maximum of 3 MB.


    <span id="page-3-2"></span>


    | Algorithm 3 Basic state-evolution operation                                                                                                                                      |                         |   |

    |----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------|---|

    | (t)<br>(t+1)<br>Require:  ψ<br>⟩,  ψ<br>⟩ ← [0, 0, , 0]                                                                                                                          |                         |   |

    | for i in [0, 1, , N − 1] do<br>(t+1)<br> ψ<br>⟩i<br>←<br>N<br>:(i+1) N<br>−1<br>N<br>N<br>(t)<br>
    ψ<br>⟩j<br>N<br>:(j+1) N<br>−1<br>N<br>N<br>end for<br>(t+1)<br>return  ψ<br>⟩
    | <br><br>× T (G)<br>Gi,j | × |


    # <span id="page-3-1"></span>*B. EMMS based-Quantum Emulation Accelerators*


    According to the methodology discussed in Section [III-A,](#page-2-3) we present
    an overview of the hardware architecture, focusing on the memory organization
    for T (G), T (G), |ψ t ⟩ and |ψ <sup>t</sup>+1⟩ as detailed in Section [III-B1.](#page-3-4)
    Additionally, the parallel execution of TP and MM is outlined in Section [III-B2,](#page-3-5)
    while the design of a Complex ALU optimized for Sparse Matrix Operations is explained
    in Section [III-B3.](#page-4-0)


    <span id="page-3-4"></span>*1) Overview Hardware Architecture:* Fig. [5](#page-3-6)
    shows the overview architecture of a QEA at the system-on-chip (SoC) level, which
    consists of two main parts: the processing system (PS) and programmable logic
    (PL). The PS communicates with PL through a 128-bit AXI bus.


    The QEA''s PS, separated into user and kernel space, is in charge of running the
    GNU/Linux operating system. Programmers can compute cos and sin values, as well
    as


    <span id="page-3-6"></span>![](_page_3_Figure_10.jpeg)


    Figure 5: Hardware architecture of the quantum emulating accelerator.


    create a Instructions list of gate names and positions, using the user space.
    Configuration, control, and computational data of Instructions programs are compiled
    into machine code in the kernel space by an QEA compiler with GCC assistance.
    Through the use of direct memory access (DMA) or programmable input/output (PIO)
    over the AXI bus, the FPGA driver transfers the compiled data from the PS to the
    programmable logic (PL). Since control data, including start or finish signals
    are small and configuration data are transferred only once, it is sufficient to
    transfer both types of data using a 64-bit width PIO.


    The main component, QEA is realized within the PL of the FPGA and consists of
    seven parts: an AXI mapper, a T (G) Memory, a COO Matrix Generator, a PEA Controller,
    Gate Memory, a Write Arbiter, Read Arbiter and a Processing Element Array (PEA).
    The AXI Mapper oversees data exchange between the PS and the Local Data Memories
    of PEs. The Gate Memory is used to store the instructions received from the processing
    system (PS). The Processing Element Array (PEA) controller instructs the COO Matrix
    Generator to generate COO List gate matrices, as shown in Table [I,](#page-7-16)
    using the instruction list stored in the Gate Memory. The matrix data is transferred
    to the PEA for the computation of T (G), which is then stored in the T (G) Memory.
    Simultaneously, this process also computes T (G), which is stored in the PEA.
    The value of T (G) is then transferred back to the PEA to compute U and the |ψ
    t ⟩ stored in DDRAM is transferred to the PEA for calculating the next state,
    |ψ (t+1) ⟩. This updated state is then returned to DDRAM for storage.


    <span id="page-3-5"></span>*2) Processing Element for Parallel TP and MM Computations:*
    Despite the reduction in memory requirements, the computational demands of matrix
    operations remain substantial. To address this challenge, we integrate processing
    elements (PEs) with local data memory. This architecture ensures efficient execution
    of MM and TP operations, facilitating parallel processing and reducing overall
    latency.


    Fig. [6](#page-4-1) illustrates the architecture of a Processing Element


    <span id="page-4-1"></span>![](_page_4_Figure_0.jpeg)


    Figure 6: Hardware architecture of the processing element.


    (PE) designed for matrix operations. The Complex ALU responsible for TP and MM
    computations will be described in Section [III-B3.](#page-4-0) In addition to
    the basic logic blocks used to synchronize and pipeline the computation between
    PEs, we introduce three Local Data Memories (LDMs). The computations of T (G)
    and T (G) are performed sequentially, their results are continuously stored in
    T (G) Memory and LDM<sup>1</sup> respectively. After completing the computation
    of T (G), the data in the form of (i, j, Gi,j ) is transferred from the T (G)
    Memory to the Complex ALU for the computation of U. The state |ψ t ⟩ is then transferred
    to the LDM<sup>2</sup> to facilitate the computation of the following state, |ψ
    (t+1) ⟩, which is then stored in LDM3.


    <span id="page-4-0"></span>*3) Complex ALU for Sparse Matrix Operations:* As outlined
    in Section [II,](#page-1-4) the efficiency of Quantum Emulation procedures is
    predominantly dependent on TP and MM, which form the core of all principal operations.
    Therefore, enhancing the performance of Quantum Emulation is intrinsically linked
    to the optimization of those computations.


    Fig. [7](#page-4-2) illustrates the utilization of the proposed ALU such as COO
    Processing, TP and MM. Specifically, COO Processing and TP are employed for T
    (G) and T (G) computations, simultaneously COO Processing, TP and MM utilized
    to compute U and |ψ (t) ⟩ as described in Section [III-A.](#page-2-3) Three 128-bit
    input tuples (i, j, {Re, Im}) belong to T (G), T (G) and |ψ (t) ⟩ where i, j are
    32-bit unsigned integers, and {Re, Im} are 32 bit fixed-point real and imaginary
    parts of a complex number (2 signed integer bits and 30 fractional bits). Additionally,
    the "Matrix Size" signal is the size of the matrix representation of gate g<sup>j</sup>
    (Nˆ), given that our proposed architecture exclusively supports Nˆ ∈ {2, 4}. All
    computational units, including Addition, Multiplication, Subtraction, and Left
    Shift Operations, are designed in pipelined architecture. This ensures that all
    matrix computations can be executed with a throughput of approximately one cycle
    per operation. Four multiplexers are employed to select the output from one of
    two computational processes.


    ## IV. ANALYSIS AND EVALUATION


    ## *A. Quantitative Analysis of Hardware Resources*


    In this section, we provide theoretical hardware resource estimates for our proposed
    QEA architecture using the EMMS


    <span id="page-4-2"></span>![](_page_4_Figure_8.jpeg)


    Figure 7: Microarchitecture of the complex ALU.


    method, taking into account the maximum (#Qubits) supported. The analysis focuses
    on the impact of hardware resources when increasing the number of PEs (#PEs) and
    adjusting the depth of the LDM. This approach enables users to select the most
    suitable architecture for their specific FPGA type. Since the primary costs in
    QEA development arise from memory usage and multiplication operations, our resource
    analysis will concentrate on two key components: BRAM and DSP utilization.


    Fig. [8](#page-5-0) illustrates the trade-off between BRAM consumption and the
    maximum #Qubits support by QEA configurations with varying #PEs and different
    memory depths of T (G). Specifically, as shown in Fig. [8](#page-5-0) (a), BRAM
    usage increases linearly with the #PEs and LDM memory depth, and also grows proportionally
    with the T (G) Memory depth. This relationship demonstrates the scalability of
    the system, allowing for more PEs at the cost of increased memory to support additional
    qubits. As shown in Fig. [8](#page-5-0) (b), the maximum #Qubits supported increases
    linearly with the #PEs, reflecting the doubling of LDM depth as the #Qubits grows.
    This linear trend highlights QEA''s scalability in supporting larger quantum circuits
    with predictable memory growth, enabling the selection of suitable FPGAs easily.


    Fig. [9](#page-5-1) presents the estimated #DSP usage for the QEA architecture
    as the #PEs increases. The eight multipliers of the ALU inside each PE occupy
    DSP resources. Accordingly, the #DSP usage correlates with the increasing #PEs.
    At lower PE numbers, such as 2 2 and 2 3 , the DSP usage remains minimal, under
    500 units. Meanwhile, as the PE number grows to 2 5 and 2 6 , the DSP usage rises
    sharply, reaching approximately 2000 units at 2 6 . Even with such high DSP usage,
    it can still be supported by many FPGAs, indicating the feasibility of increasing
    the #PEs to accelerate the TP


    <span id="page-5-0"></span>![](_page_5_Figure_0.jpeg)


    Figure 8: Total estimated #BRAM usage (a) and maximum #Qubits support (b) for
    QEA configurations with 2 2 to 2 <sup>5</sup> PEs, across T (G) Memory depths
    of 2 12 , 2 <sup>14</sup>, and 2 16 .


    <span id="page-5-1"></span>![](_page_5_Figure_2.jpeg)


    Figure 9: Estimated #DSPs usage for QEA as #PEs increases.


    and MM calculations. Overall, the EMMS method enables QEA to support up to 32
    qubits while maintaining BRAM and DSP resource requirements that can be accommodated
    by many mid-range FPGAs


    ## *B. Quantitative Analysis of Performance*


    This section analyzes the system-level performance of QEA to clarify how the EMMS
    method and the #PEs affect processing efficiency. To accurately estimate performance,
    we evaluate QEA on the Xilinx Zynq UltraScale+ ZCU102 FPGA. Based on the available
    resources of the ZCU102 FPGA, we selected the four best versions of QEA, with
    2 2 to 2 5 PEs, each configured with the same T (G) Memory depth of 2 <sup>1</sup>6
    and different LDM depths. Since this paper focuses on theoretical analysis and
    no actual hardware has been designed using Verilog, we assess performance based
    on the number of execution cycles.


    For benchmarking, we choose a quantum computation model with a quantum circuit
    that involves *M* computations for the target quantum state |ψ (m) ⟩. In cases
    where the #Qubits used are fewer than the total LDM depth of all PEs, the number
    of execution cycles for the quantum circuit (CycleQC), which can compute state
    vectors consecutively in LDM<sup>2</sup> and LDM<sup>3</sup> without continuous
    transmission, can be calculated as Eq. [4:](#page-5-2)


    <span id="page-5-2"></span>

    $$\mathbf{C\_{QC}} = \mathbf{C\_{Write\ |\psi^{(0)}\rangle}} + \sum\_{t=0}^{m-1}
    \left( \mathbf{C\_{TP}^{U^{(t+1)}}} + \mathbf{C\_{MM}^{U^{(t+1)}}} \right) + \mathbf{C\_{Recall\
    |\psi^{(m)}\rangle}} \tag{4}$$


    Where CWrite|ψ(0)⟩ is the number of cycles (#Cycles) of the first |ψ (0) ⟩ transmission,
    C<sup>U</sup> (t+1) TP is #Cycles for computing the TP for U (t+1) , C<sup>U</sup>
    (t+1) MM is the #Cycles for computing the matrix multiplication of U (t+1) with
    |ψ (t) ⟩, and CRead <sup>|</sup>ψ(m)⟩ is #Cycles for reading final |ψ (m) ⟩ result.
    According to our experimental results on the ZCU102 FPGA, the highperformance
    port from the processing system to QEA allows 128-bit transmission and reception
    per cycle, which is equivalent to one 128-bit COO value of an element α (j) in
    |ψ (t) ⟩. As a result, CWrite <sup>|</sup>ψ(t)⟩ and CRead <sup>|</sup>ψ(t+1)⟩
    are equal to N, where n is the used #Qubits. Meanwhile, C<sup>U</sup> (t+1) TP
    and C<sup>U</sup> (t+1) MM can be shortened by paralleling the computation through
    increasing the #PEs, as calculated in Eq. [5:](#page-5-3)


    <span id="page-5-3"></span>

    $$\mathbf{C}\_{\mathbf{TP}}^{U^{(\mathfrak{t}+\mathbf{1})}} = (\overline{N} +
    N/\overline{N})/\#\text{PEs}; \mathbf{C}\_{\mathbf{MM}}^{U^{(\mathfrak{t}+\mathbf{1})}}
    = N/\#\text{PEs}. \quad (5)$$


    In cases where the #Qubits used are larger than the total LDM depth of all PEs,
    CQC increases dramatically because the computed state vectors |ψ (t) ⟩ and |ψ
    (t+1) ⟩ are only partially stored in LDM<sup>2</sup> and LDM3. This requires the
    system to continuously write a portion of |ψ (t) ⟩ and read a portion of |ψ (t+1)
    ⟩ from DDRAM to the QEA, and vice versa, to complete the update of the entire
    |ψ (t+1) ⟩. As a result, CWrite <sup>|</sup>ψ(t)⟩ and CRead <sup>|</sup>ψ(t+1)⟩
    become obvious bottlenecks in the system, causing CQC to increase rapidly following
    the *M* computations, as calculated in Eq. [6.](#page-5-4)


    <span id="page-5-4"></span>

    $$\mathbf{C\_{QC}} = \sum\_{t=0}^{m-1} \left( \mathbf{C\_{Write}}\_{|\psi^{(t)}\rangle}
    + \mathbf{C\_{TP}^{U^{(t+1)}}} + \mathbf{C\_{MM}^{U^{(t+1)}}} + \mathbf{C\_{Red}}\_{|\psi^{(t+1)}\rangle}
    \right) \tag{6}$$


    Based on the analysis in Eq. [4](#page-5-2) and [6,](#page-5-4) we estimate the
    number of computation cycles for the four QEA versions for


    <span id="page-6-0"></span>![](_page_6_Figure_0.jpeg)


    Figure 10: Estimated cycle count of four different versions of QEA on the Xilinx
    ZCU102 FPGA, required for performing 100 (m = 100) computations for the target
    quantum state across various #Qubits.


    100 (m = 100) computations of the target quantum state with #Qubits ranging from
    2 to 26, as shown in Fig. [10.](#page-6-0) Note that the Xilinx ZCU102 FPGA has
    only 4GB of DDR4 memory, so the maximum number of effectively supported #Qubits
    is limited to 26, as storing |ψ (t) ⟩ and |ψ (t+1) ⟩ for 26 qubits requires 2GB.
    Accordingly, from 2 to 18 qubits, |ψ (t) ⟩ and |ψ (t+1) ⟩ can be stored entirely
    in the LDM<sup>2</sup> and LDM<sup>3</sup> of the PEs, so the #Cycles for the
    QEA versions decreases as the #PEs increases. The exceptions are the QEA version
    with 2 <sup>5</sup> PEs and LDM depths of 2 <sup>10</sup>, which performs best
    for fewer than 15 qubits, and the QEA version with 2 <sup>4</sup> PEs and LDM
    depths of 2 <sup>12</sup>, which performs best for fewer than 16 qubits. Beyond
    19 qubits, increasing the #PEs does not significantly reduce the total number
    of computation cycles, as CWrite <sup>|</sup>ψ(t)⟩ and CRead <sup>|</sup>ψ(t+1)⟩
    account for 66% of the processing time. As a result, the #Cycles for all four
    QEA versions becomes largely insignificant and depends primarily on the transmission
    bandwidth between DDR4 and the QEA.


    Overall, the EMMS method makes QEA computationally feasible for large #Qubits.
    For computations with fewer than 18 qubits, increasing the #PEs significantly
    accelerates performance. However, for more than 18 qubits, system performance
    becomes heavily dependent on the transmission bandwidth between the QEA and DDRAM.
    This limitation can be addressed by adopting modern FPGAs with high-bandwidth
    memory support, which would improve data transfer rates and overall system performance.


    ## *C. Comparison with State-of-the-Art Works*


    This section compares the performance of QEA with quantum emulation hardware studies.
    To our knowledge, no quantum emulation hardware with the gate fusion approach
    is available to date, so the performance of the EMMS method cannot be compared
    with FPGA-based works. Instead, we compare the performance of QEA with quantum
    emulation hardware dedicated to updating traditional quantum state vectors, as
    described in [\[9\]](#page-7-7), [\[11\]](#page-7-9), which does not use gate
    fusion. For a fair comparison, we convert the execution time reported in [\[9\]](#page-7-7),
    [\[11\]](#page-7-9) into cycles and benchmark the QFT application.


    Fig. [11](#page-6-1) compares the computational cycles of three hardware architectures
    across qubits ranging from 2 to 16 in QFT


    <span id="page-6-1"></span>![](_page_6_Figure_7.jpeg)


    Figure 11: #Cycles between QEA and other FPGA-based quantum emulation from [\[9\]](#page-7-7),
    [\[11\]](#page-7-9) on QFT algorithm.


    applications. The two hardware designs in [\[9\]](#page-7-7), [\[11\]](#page-7-9)
    exhibit a steep increase in cycle count as the #Qubits grows. This is because
    these designs do not utilize the COO storage method, leading to exponential growth
    in computational complexity as the size of U (t) expands with the #Qubits. In
    contrast, by virtue of the COO storage method, our QEA implementation with 16
    PEs shows a significant reduction in the #Cycles across all qubit counts compared
    to [\[9\]](#page-7-7), [\[11\]](#page-7-9). Besides, the QEA effectively leverages
    parallelism by increasing the #PEs to 16, resulting in a 16× acceleration in computations.
    Overall, this comparison highlights the dramatic impact of COO storage and PE-based
    parallelism on reducing computational cycles.


    ## V. CONCLUSION & FUTURE WORK


    This study introduced the EMMS method and QEA architecture to address memory and
    computational challenges in quantum emulation. By leveraging multiple PEs, the
    QEA significantly accelerates tensor product and matrix multiplication tasks.
    Theoretical analysis on the Xilinx ZCU102 FPGA shows linear scalability with qubit
    count and substantial reductions in computation cycles for circuits with fewer
    than 18 qubits, outperforming previous designs. However, for circuits exceeding
    19 qubits, performance becomes bottlenecked by the data transfer bandwidth between
    DDR4 memory and the QEA, which consumes 66% of the total execution time.


    In future work, since QEA has only been evaluated theoretically, we will design
    QEA in Verilog and implement it practically on the Xilinx ZCU102 FPGA. Additionally,
    we will implement QEA on the Alveo U280 FPGA, which features high-bandwidth memory
    of up to 460 GB/s, to address the bottleneck issue in data transmission and reception
    for calculations involving more than 18 qubits.


    ## ACKNOWLEDGMENT


    This work was supported by JST-ALCA-Next Program Grant Number JPMJAN23F5, Japan.
    The research has been partly executed in response to the support of JSPS, KAKENHI
    Grant No. 22H00515, Japan.


    ## REFERENCES


    - <span id="page-7-0"></span>[1] J. Lau, K. Lim, H. Shrotriya *et al.*, "Nisq
    computing: where are we and where do we go?" *AAPPS Bulletin*, vol. 32, p. 27,
    2022. [Online]. Available:<https://doi.org/10.1007/s43673-022-00058-z>

    - [2] G. Q. AI, "Suppressing quantum errors by scaling a surface code logical
    qubit," *Nature*, vol. 614, no. 7949, pp. 676–681, Feb 2023. [Online]. Available:<https://doi.org/10.1038/s41586-022-05434-1>

    - <span id="page-7-1"></span>[3] D. Bluvstein, S. J. Evered, A. A. Geim, S. H.
    Li, H. Zhou, T. Manovitz, S. Ebadi, M. Cain, M. Kalinowski, D. Hangleiter, J.
    P. Bonilla Ataides, N. Maskara, I. Cong, X. Gao, P. Sales Rodriguez, T. Karolyshyn,
    G. Semeghini, M. J. Gullans, M. Greiner, V. Vuletic, and M. D. Lukin, ´ "Logical
    quantum processor based on reconfigurable atom arrays," *Nature*, vol. 626, no.
    7997, pp. 58–65, Feb 2024. [Online]. Available: <https://doi.org/10.1038/s41586-023-06927-3>

    - <span id="page-7-2"></span>[4] A. Javadi-Abhari and et al, "Quantum computing
    with qiskit," 2024. [Online]. Available:<https://arxiv.org/abs/2405.08810>

    - <span id="page-7-3"></span>[5] V. Bergholm and et al, "Pennylane: Automatic
    differentiation of hybrid quantum-classical computations," 2022. [Online]. Available:
    <https://arxiv.org/abs/1811.04968>

    - <span id="page-7-4"></span>[6] T. Haner and D. S. Steiger, "0.5 petabyte simulation
    of a ¨ 45-qubit quantum circuit," in *Proceedings of the International Conference
    for High Performance Computing, Networking, Storage and Analysis*, ser. SC ''17.
    ACM, Nov. 2017. [Online]. Available: <http://dx.doi.org/10.1145/3126908.3126947>

    - <span id="page-7-5"></span>[7] Bayraktar and et al, "cuquantum sdk: A high-performance
    library for accelerating quantum science," in *2023 IEEE International Conference
    on Quantum Computing and Engineering (QCE)*, vol. 01, 2023, pp. 1050–1061.

    - <span id="page-7-6"></span>[8] Y. Zhao, Y. Guo, Y. Yao, A. Dumi, D. M. Mulvey,
    S. Upadhyay, Y. Zhang, K. D. Jordan, J. Yang, and X. Tang, "Q-gpu: A recipe of
    optimizations for quantum circuit simulation using gpus," in *2022 IEEE International
    Symposium on High-Performance Computer Architecture (HPCA)*, 2022, pp. 726–740.

    - <span id="page-7-7"></span>[9] N. Mahmud, B. Haase-Divine, A. Kuhnke, A. Rai,
    A. MacGillivray, and E. El-Araby, "Efficient computation techniques and hardware
    architectures for unitary transformations in support of quantum algorithm emulation,"
    *Journal of Signal Processing Systems*, vol. 92, pp. 57–74, 2020.

    - <span id="page-7-8"></span>[10] D. Wakeham and M. Schuld, "Inference, interference
    and invariance: How the quantum fourier transform can help to learn from data,"
    2024. [Online]. Available:<https://arxiv.org/abs/2409.00172>

    - <span id="page-7-9"></span>[11] N. Mahmud, B. Haase-Divine, A. MacGillivray,
    B. Srimoungchanh, A. Kuhnke, N. Blankenau, A. Rai, and E. El-Araby, "Modifying
    quantum grover''s algorithm for dynamic multi-pattern search on reconfigurable
    hardware," *Journal of Computational Electronics*, vol. 19, no. 3, pp. 1215–1231,
    Sep 2020. [Online]. Available: <https://doi.org/10.1007/s10825-020-01489-3>

    - <span id="page-7-10"></span>[12] H. Hiroshi and D. Jun, "Optimization of quantum
    computing simulation with gate fusion," IBM Quantum, IBM Research Tokyo, IBM Quantum,
    IBM Research Tokyo, Tech. Rep. 23, mar 2021.

    - <span id="page-7-11"></span>[13] M. Smelyanskiy, N. P. Sawaya, and A. Aspuru-Guzik,
    "qhipster: The quantum high performance software testing environment," *arXiv
    preprint arXiv:1601.07195*, 2016.

    - <span id="page-7-12"></span>[14] M. Schuld and F. Petruccione, *Machine learning
    with quantum computers*. Springer, 2021, vol. 676.

    - <span id="page-7-13"></span>[15] P. A. Tew, "An investigation of sparse tensor
    formats for tensor libraries," *Master''s Thesis*, pp. 17–18, 2016.


    <span id="page-7-16"></span>


    |  | Table I: Compressed sparse gates (first 11 rows) and dense |  |  |  |  |

    |--|------------------------------------------------------------|--|--|--|--|

    |  | gates (last 2 rows) in COO format.                         |  |  |  |  |


    |        | COO |   |         |         |        | COO |   |         |         |  |

    |--------|-----|---|---------|---------|--------|-----|---|---------|---------|--|

    | Gate   | i   | j | Re      | Im      | Gate   | i   | j | Re      | Im      |  |

    | P(λ)   | 0   | 1 | 1       | 0       | CX     | 0   | 0 | 1       | 0       |  |

    |        | 1   | 1 | ∗<br>a  | ∗<br>b  |        | 1   | 1 | 1       | 0       |  |

    | X      | 0   | 1 | 1       | 0       |        | 2   | 3 | 1       | 0       |  |

    |        | 1   | 0 | 1       | 0       |        | 3   | 2 | 1       | 0       |  |

    | Y      | 0   | 1 | 0       | −1      |        | 0   | 0 | 1       | 0       |  |

    |        | 1   | 0 | 0       | 1       |        | 1   | 1 | 1       | 0       |  |

    | Z      | 0   | 0 | 1       | 0       | CY     | 2   | 3 | 1       | −1      |  |

    |        | 1   | 1 | −1      | 0       |        | 3   | 2 | 1       | 1       |  |

    | S      | 0   | 0 | 1       | 0       |        | 0   | 0 | 1       | 0       |  |

    |        | 1   | 1 | 0       | 1       | CZ     | 1   | 1 | 1       | 0       |  |

    | SDG    | 0   | 0 | 1       | 0       |        | 2   | 2 | 1       | 0       |  |

    |        | 1   | 1 | 0       | −1      |        | 3   | 3 | −1      | 0       |  |

    | T      | 0   | 0 | 1       | 0       |        | 0   | 0 | 1       | 0       |  |

    |        | 1   | 1 | ∗<br>q  | ∗<br>q  | CP(λ)  | 1   | 1 | 1       | 0       |  |

    | TDG    | 0   | 0 | 1       | 0       |        | 2   | 2 | 1       | 0       |  |

    |        | 1   | 1 | ∗<br>−q | ∗<br>q  |        | 3   | 3 | ∗<br>a  | ∗<br>b  |  |

    | RZ(θ)  | 0   | 0 | ∗<br>c  | ∗<br>−d |        | 0   | 0 | 1       | 0       |  |

    |        | 1   | 1 | ∗<br>c  | ∗<br>d  | CRY(θ) | 1   | 1 | 1       | 0       |  |

    |        | 0   | 0 | 1       | 0       |        | 2   | 2 | ∗<br>c  | 0       |  |

    | CRZ(θ) | 1   | 1 | 1       | 0       |        | 2   | 3 | ∗<br>−d | 0       |  |

    |        | 2   | 2 | ∗<br>c  | ∗<br>−d |        | 3   | 2 | ∗<br>d  | 0       |  |

    |        | 3   | 3 | ∗<br>c  | ∗<br>d  |        | 3   | 3 | ∗<br>c  | 0       |  |

    |        | 0   | 0 | 1       | 0       |        | 0   | 0 | 1       | 0       |  |

    |        | 1   | 1 | 1       | 0       |        | 1   | 1 | 1       | 0       |  |

    | CRX(θ) | 2   | 2 | ∗<br>c  | 0       | CH     | 2   | 2 | ∗<br>q  | 0       |  |

    |        | 2   | 3 | 0       | ∗<br>−d |        | 2   | 3 | ∗<br>q  | 0       |  |

    |        | 3   | 2 | 0       | ∗<br>−d |        | 3   | 2 | ∗<br>q  | 0       |  |

    |        | 3   | 3 | ∗<br>c  | 0       |        | 3   | 3 | ∗<br>−q | 0       |  |

    |        | 0   | 0 | ∗<br>q  | 0       |        | 0   | 0 | ∗<br>c  | 0       |  |

    | H      | 0   | 1 | ∗<br>q  | 0       | RX(θ)  | 0   | 1 | 0       | ∗<br>−d
    |  |

    |        | 1   | 0 | ∗<br>q  | 0       |        | 1   | 0 | 0       | ∗<br>−d
    |  |

    |        | 1   | 1 | ∗<br>−q | 0       |        | 1   | 1 | ∗<br>q  | 0       |  |

    | SX     | 0   | 0 | ∗<br>p  | ∗<br>p  | RY(θ)  | 0   | 0 | ∗<br>c  | 0       |  |

    |        | 0   | 1 | ∗<br>p  | ∗<br>−p |        | 0   | 1 | ∗<br>−d | 0       |  |

    |        | 1   | 0 | ∗<br>p  | ∗<br>−p |        | 1   | 0 | ∗<br>d  | 0       |  |

    |        | 1   | 1 | ∗<br>p  | ∗<br>p  |        | 1   | 1 | ∗<br>c  | 0       |  |

    |        |     |   |         |         |        |     |   | √       |         |  |


    a : cos(λ), b<sup>∗</sup> : sin(λ), c<sup>∗</sup> : cos(θ/2), d<sup>∗</sup> :
    sin(θ/2), q<sup>∗</sup> : 1/ 2, p<sup>∗</sup> : 1/2


    <span id="page-7-14"></span>[16] S. Dalton, L. Olson, and N. Bell, "Optimizing
    sparse matrix–matrix multiplication for the gpu," *ACM Transactions on Mathematical
    Software*, vol. 41, no. 4, pp. Article 25, pages 4–6, 2015.


    ## <span id="page-7-15"></span>APPENDIX A: EXAMPLE FOR EQ. [3](#page-2-2)


    Consider X-gate act on 0 th qubit in 3-qubits system |ψ (t) ⟩. We know that T
    (G) = X<sup>0</sup> = {(0, 1, 1),(1, 0, 1)} and T (G) = T (I1⊗I2) = {(0, 0, 1),(1,
    1, 1),(2, 2, 1),(3, 3, 1)}. Following Algo. [3,](#page-3-2) with N = 2, at i =
    0 and i = 1:


    $$\begin{aligned} \left[\overline{G}\_{0,1} \times \mathcal{T}(G)\right] \times
    |\psi^{(\mathbf{t})}\rangle\_{4:7} &= |\psi^{(\mathbf{t}+\mathbf{1})}\rangle\_{0:3},
    \\ \left[\overline{G}\_{1,0} \times \mathcal{T}(G)\right] \times |\psi^{(\mathbf{t})}\rangle\_{0:3}
    &= |\psi^{(\mathbf{t}+\mathbf{1})}\rangle\_{4:7}, \end{aligned} \tag{7}$$


    respectively, the final result obtained through concatenation.


    ## APPENDIX B: COMPRESSED GATES


    The detailed list of quantum gates in COO format has been shown in Table [I.](#page-7-16)'
- title: "Taming Performance Variability caused by Client-Side Hardware\n  Configuration"
  abstract: 'Many online services running in datacenters are implemented using a

    microservice software architecture characterized by strict latency

    requirements. Consequently, this popular software paradigm is increasingly used

    for the performance evaluation of server systems. Due to the scale and

    complexity of datacenters, the evaluation of server optimization techniques is

    usually done on a smaller scale using a client-server model. Although the

    experimental details of the server side are excessively described in most

    publications, the client side is often ignored. This paper identifies the

    hardware configuration of the client side as an important source of performance

    variation that can affect the accuracy and the correctness of the conclusions

    of a study that analyzes the performance of microservices. This is partially

    attributed to the strict latency requirements of microservices and the small

    scale of the experimental environment. In this work we present, using a widely

    used online-service, several examples where the accuracy and the trends of the

    conclusions differ based on the configuration of the client-side. At the same

    time we show that the experimental evaluation time can be significantly

    affected by the hardware configuration of the client. All these provoke the

    discussion of the right way to configure the experimental environment for

    assessing the performance of microservices.'
  url: http://arxiv.org/abs/2410.11554v1
  keywords: ''
  document: "# Taming Performance Variability caused by Client-Side Hardware Configuration\n\
    \nGeorgia Antoniou, Haris Volos, Yiannakis Sazeides University of Cyprus {ganton12,\
    \ hvolos01, yanos}@ucy.ac.cy\n\narXiv:2410.11554v1 [cs.AR] 15 Oct 2024\n\nAbstract—Many\
    \ online services running in datacenters are implemented using a microservice\
    \ software architecture characterized by strict latency requirements. Consequently,\
    \ this popular software paradigm is increasingly used for the performance evaluation\
    \ of server systems. Due to the scale and complexity of datacenters, the evaluation\
    \ of server optimization techniques is usually done on a smaller scale using a\
    \ client-server model. Although the experimental details of the server side are\
    \ excessively described in most publications, the client side is often ignored.\
    \ This paper identifies the hardware configuration of the client side as an important\
    \ source of performance variation that can affect the accuracy and the correctness\
    \ of the conclusions of a study that analyzes the performance of microservices.\
    \ This is partially attributed to the strict latency requirements of microservices\
    \ and the small scale of the experimental environment.\n\nIn this work we present,\
    \ using a widely used onlineservice, several examples where the accuracy and the\
    \ trends of the conclusions differ based on the configuration of the client-side.\
    \ At the same time we show that the experimental evaluation time can be significantly\
    \ affected by the hardware configuration of the client. All these provoke the\
    \ discussion of the right way to configure the experimental environment for assessing\
    \ the performance of microservices.\n\nIndex Terms—performance variability, client-side,\
    \ hardware configuration, microservices\n\n#### I. Introduction\n\nOnline applications\
    \ running in today's datacenters, such as social networks and web search, have\
    \ moved from a monolithic to a microservice-based architecture. In this architecture,\
    \ a monolithic application is decomposed into smaller, interconnected services\
    \ that communicate explicitly with each other over the network through well-defined\
    \ interfaces. These microservices can be independently developed, deployed, and\
    \ scaled. However, due to the increased network overhead arising from the need\
    \ for communication among services, each service must now adhere to stricter Quality-of-Service\
    \ (QoS) constraints compared to its monolithic counterpart. Previous work reports\
    \ tight QoS constraints for individual services, with 99th percentile latency\
    \ targets that range from 250us to 500us [\\[6\\]](#page-11-0), [\\[7\\]](#page-11-1),\
    \ [\\[22\\]](#page-12-0), [\\[49\\]](#page-13-0).\n\nGiven the rising prevalence\
    \ of latency-critical applications based on microservices in today's datacenters,\
    \ the scientific community has increasingly turned to microservices to evaluate\
    \ the performance of proposals targeting modern datacenter systems. This includes\
    \ widely-used services, such as Memcached [\\[1\\]](#page-11-2), which is typically\
    \ deployed as a distributed caching service to accelerate user-facing applications\
    \ [\\[5\\]](#page-11-3), and\n\n<span id=\"page-0-0\"></span>\n\n|  |  | TABLE\
    \ I: Hardware characterization in previous work. |  |  |  |\n|--|--|------------------------------------------------------|--|--|--|\n\
    |--|--|------------------------------------------------------|--|--|--|\n\n| Characterization\
    \  | Publications |\n|-------------------|--------------|\n| Client only     \
    \  | 0            |\n| Server only       | 8            |\n| Client and server\
    \ | 2            |\n| None              | 10           |\n| Total            \
    \ | 20           |\n\nnew benchmark suites, such as MicroSuite [\\[38\\]](#page-12-1),\
    \ DeathStar [\\[14\\]](#page-12-2), and TrainTicket [\\[52\\]](#page-13-1), which\
    \ implement representative applications based on microservices.\n\nExperimental\
    \ evaluation utilizing the above frameworks typically entails deploying them on\
    \ a small test cluster, following a client-server model. The test cluster often\
    \ has fewer machine nodes compared to larger-scale production clusters, primarily\
    \ due to complexity, scale, and cost constraints [\\[4\\]](#page-11-4), [\\[8\\\
    ]](#page-11-5), [\\[26\\]](#page-12-3), [\\[46\\]](#page-12-4). Under this deployment,\
    \ the test cluster comprises a set of server-side and client-side machines. Server-side\
    \ machines are usually configured to host a few services rather than the whole\
    \ application to keep the test cluster size under control while still achieving\
    \ a representative setup. Client-side machines host the workload generator, which\
    \ (i) generates a representative workload for the application to process, and\
    \ (ii) accurately measures the end-to-end performance of the system under a target\
    \ load, such as average response latency and tail latency (e.g., 99th percentile).\n\
    \nWe observe that while experimental evaluations typically specify the server-side\
    \ hardware configuration to ensure reproducibility of results, they often overlook\
    \ the client-side configuration. Table [I](#page-0-0) surveys the client- and\
    \ server-side hardware configuration in recent publications (from the years 2021,\
    \ 2022, and 2023) across various system and architecture conferences, including\
    \ ISPASS, IISWC and MICRO. We find that only 10% of the papers studied specify\
    \ the client-side hardware configuration. We attribute this limitation partially\
    \ to the implicit assumption that the end-to-end response latency is dominated\
    \ mainly by the server-side execution time. This assumption is rooted in past\
    \ practices in experimental evaluations that were based on monolithic applications\
    \ with millisecond-scale response latencies. However, this assumption no longer\
    \ holds with microservices having microsecond-scale response latencies where any\
    \ client-side microsecond-scale overhead can significantly impact the response\
    \ latency. For example, waking up of a client-side processor core from a\n\npower\
    \ sleep state takes from 2us to 200us [\\[48\\]](#page-13-2) (depending on the\
    \ sleep state). This overhead can significantly impact the response latency of\
    \ a microsecond-scale microservice, such as Memcached with an average server-side\
    \ processing time of 10us [\\[4\\]](#page-11-4), [\\[7\\]](#page-11-1), resulting\
    \ to an end-to-end response latency that can reach up to hundreds of microseconds.\n\
    \nTo analyze the effect of client-side hardware configuration on performance evaluation,\
    \ we conduct an experimental study based on representative, microservice-based\
    \ services and applications with microseconds/few milliseconds response latencies,\
    \ including Memcached [\\[1\\]](#page-11-2), HDSearch from MicroSuite [\\[38\\\
    ]](#page-12-1), Social Network from DeathStar [\\[14\\]](#page-12-2) and synthetic\
    \ workloads. Our experimental analysis reveals that client-side microsecondscale\
    \ hardware overheads, such as waking up from a power sleep state and dynamic voltage\
    \ frequency scaling (DVFS [\\[15\\]](#page-12-5)), can impact the accuracy of\
    \ the end-to-end measurements leading to incorrect conclusions and additionally\
    \ introducing performance variation. We find that the extent of the impact depends\
    \ on a combination of (i) workload generator design, (ii) hardware configuration\
    \ parameters, and (iii) server-side processing latency.\n\nThis behavior has several\
    \ ramifications. In an academic setup, an analysis without consideration of the\
    \ client-side hardware configuration can lead to inaccurate or wrong conclusions.\
    \ At the same time, it renders the work unrepeatable as important details are\
    \ missing from the experimental methodology of the paper. Finally, it degrades\
    \ the validity of comparisons among techniques optimizing similar metrics in similar\
    \ environments. In an industrial environment, performance evaluation is crucial\
    \ for determining the load a machine can sustain without any QoS violations and\
    \ guiding resource allocation for data centers [\\[32\\]](#page-12-6), [\\[33\\\
    ]](#page-12-7). Ignoring clientside hardware configuration in this context can\
    \ result in overprovisioning or underprovisioning of resources.\n\nIn summary,\
    \ we make the following key contributions:\n\n- We identify client-side configuration\
    \ as a key source of performance variation in experimental evaluation.\n- We demonstrate\
    \ experimentally how and when client-side configuration can influence the accuracy\
    \ and validity of the conclusions.\n- We analyse the impact of different client-side\
    \ configurations on the experimental evaluation time.\n- We provide recommendations\
    \ for how an experimental environment should be configured to improve representativeness\
    \ and thus mitigate measurement inaccuracy caused by client-side configuration.\n\
    \n# II. Client-caused Performance Variability\n\n<span id=\"page-1-1\"></span>Due\
    \ to the large scale of datacenters and web-based applications, researchers and\
    \ practitioners typically evaluate data center related optimizations on test clusters\
    \ with few nodes before propagating the optimization to the rest of the infrastructure.\
    \ Measuring the performance of a service typically involves using a workload generator\
    \ running on a set of client machines, as illustrated in Figure [1.](#page-1-0)\n\
    \n<span id=\"page-1-0\"></span>![](_page_1_Figure_10.jpeg)\n\nFig. 1: Typical\
    \ experimental methodology.\n\nA workload generator is a software component that\
    \ acts as a client that (i) generates requests for the service under study following\
    \ a representative workload, and (ii) accurately measures the end-to-end latency\
    \ (i.e., average latency, 99th percentile latency). Workload generators include\
    \ the load intensity which represents the inter-arrival time of requests and resource\
    \ demands which represents characteristics like the type and size of a request.\
    \ Most previous work on experimental evaluation focuses on the workload generator\
    \ (design and configuration), often neglecting the configuration of the client\
    \ machines on which the generator runs.\n\nOur key hypothesis is that client machine\
    \ configuration can significantly impact workload and measurement accuracy, and\
    \ the derived conclusions. The extent of the impact depends on a combination of\
    \ (i) workload generator design, (ii) hardware configuration parameters, and (iii)\
    \ service latency. Below, we qualitatively discuss how each such dimension may\
    \ impact measurement inaccuracy. Later, in Section [V,](#page-4-0) we present\
    \ empirical evidence supporting our key premise.\n\nWorkload generator design:\
    \ A workload generator timestamps generated requests and corresponding replies\
    \ to model a target workload and measure end-to-end latency. This timestamp-based\
    \ design makes the generator sensitive to timing inaccuracy in two ways. First,\
    \ an open-loop generator models an infinite number of requests [\\[24\\]](#page-12-8),\
    \ sending requests to the target service according to an inter-arrival time distribution\
    \ that represents the time between successive requests. Any inaccuracy in timing\
    \ can disrupt the interarrival times, causing requests to shift in time and deviate\
    \ from the target distribution. A closed-loop generator further limits the number\
    \ of outstanding requests to model a finite number of blocking clients [\\[24\\\
    ]](#page-12-8). Because the timing of the next request depends on when the response\
    \ to the previous request arrives, any timing inaccuracy can further impact the\
    \ time when a successive request is sent. Overall, for both generator types, any\
    \ timing inaccuracy can impact the timing of requests, causing the generated workload\
    \ to deviate from the target workload.\n\nSecond, a workload generator can measure\
    \ latency at various points in the system, such as the network interface card\
    \ (NIC), the in-kernel socket layer, or the generator itself, collectively referred\
    \ to as points of measurement [\\[24\\]](#page-12-8). With most typical workload\
    \ generators, the measurement point resides within the workload generator itself.\
    \ Therefore, measuring end-to-end latency depends on when the response reaches\
    \ the generator and when the generator timestamps the response accordingly, rendering\
    \ the measurement accuracy susceptible to any delay.\n\nHardware configuration\
    \ parameters: The client-side hardware configuration refers to different configuration\
    \ settings of the client-side system, including hyperthreading, turbo mode, C-states\
    \ and CPU frequency. Such hardware settings can impact timing accuracy, potentially\
    \ impacting both the generated workload and latency measurements in combination\
    \ with the design of the workload generator.\n\nFor example, consider a time-sensitive\
    \ workload generator with a point of measurement inside the generator itself,\
    \ operating on a system with enabled c-states, allowing the system to sleep when\
    \ idle. Upon issuing a request, the system may enter a sleep state until the corresponding\
    \ response arrives. When the response arrives, the system must first wakeup and\
    \ ramp up its frequency before the workload generator can timestamp the response\
    \ and measure the end-to-end latency, consequently increasing the measured response\
    \ time. Although this scenario may seem straightforward to avoid by configuring\
    \ the client system to disable hardware features affecting timing accuracy to\
    \ eliminate any variability, this approach may not always align with the target\
    \ environment. For a target environment enabling c-states for low power, the point\
    \ of measurement shall include any latency introduced by sleep state transitions.\
    \ Otherwise, the experimental analysis may not be representative in terms of end-to-end\
    \ latency. Since these types of analysis estimate the speedup of an optimization\
    \ and ultimately guide the number of resources required to serve a target load,\
    \ an inaccurate experimental environment may cause either overprovision or underprovision\
    \ of resources. Unfortunately, enabling c-states with a time-sensitive workload\
    \ generator to capture a representative point of measurement, may disrupt the\
    \ generated workload, thus leading to conflicting choices.\n\nAdditionally to\
    \ HW configuration parameters, kernel parameters (e.g., choice of idle governor),\
    \ or compiler optimization flags can possibly cause similar effects on the accuracy\
    \ of the end-to-end measurements. In this work we focus only on HW configuration\
    \ parameters.\n\nService latency: With the emergence of the microservice software\
    \ paradigm based on which applications are decomposed to several interconnected\
    \ smaller services, the QoS of latency critical applications like search and social\
    \ network has reduced significantly from milliseconds to microseconds while the\
    \ request rates requirements have remained the same. Hardware overheads that are\
    \ negligible for monolithic applications are now detrimental for the performance\
    \ of microservicebased applications. As a result, microservices are especially\
    \ vulnerable to the overhead introduced by the configuration of the client side\
    \ since it is in the same order as the response time of the microservice (i.e.,\
    \ 250us). For example C-state transition overhead can take from 2us up to 200us\
    \ depending on the\n\nprocessor, while legacy DVFS takes several microseconds\
    \ (i.e., 30us [\\[15\\]](#page-12-5)). In the case of monolithic applications\
    \ or microservice-based applications with higher response time the client side\
    \ HW configuration shouldn't affect significantly the accuracy.\n\n### III. Statistics\
    \ Primer\n\n<span id=\"page-2-0\"></span>In this section we present the background\
    \ of the statistical methods used in our analysis.\n\nConfidence Intervals (CI):\
    \ When we display values for summarized datasets such as mean and average it is\
    \ important to quantify their accuracy. In other words, since we gather empirical\
    \ statistics in our experiments, confidence intervals (CI) [\\[24\\]](#page-12-8),\
    \ [\\[25\\]](#page-12-9) offer some confidence that the empirical distribution\
    \ collected experimentally is close to the actual distribution of the measured\
    \ population. CI are ranges in which, we are x% sure that the population mean\
    \ lies, where x represents the confidence level. A sampled mean of 20, x=95% and\
    \ CI of 19.8 - 20.2, means that the true mean of the population distribution lies\
    \ within 1% error from the estimated sampled mean. In order to be confident that\
    \ a mean is higher than another, their CI should not overlap.\n\nDepending on\
    \ the distribution of the collected samples, we can either use a parametric or\
    \ a non-parametric CI expression. Parametric expressions assume that the sampled\
    \ data are derived from a known distribution (i.e., normal/Gaussian) whereas non-parametric\
    \ expressions assume that the distribution of the sampled data is unknown. Many\
    \ studies have demonstrated that data collected experimentally in computer systems,\
    \ do not follow a normal distribution [\\[21\\]](#page-12-10), [\\[29\\]](#page-12-11),\
    \ [\\[47\\]](#page-13-3). This is partially inline with what we have observed\
    \ in our analysis (see Section [V-C\\)](#page-8-0). To avoid assumptions of normality\
    \ we use non-parametric confidence intervals (and other statistical methods) unless\
    \ noted for the rest of the paper.\n\nNon-parameric CI are computed based on the\
    \ median instead of the mean. The following equations are used to compute the\
    \ confidence intervals bounds for the median.\n\n$$Lower\\\\_bound = \\lfloor\
    \ \\frac{n - z\\sqrt{n}}{2} \\rfloor \\tag{1}$$\n\n$$Upper\\\\_bound = \\lceil\
    \ 1 + \\frac{n + z\\sqrt{n}}{2} \\rceil \\tag{2}$$\n\nWhere n is the number of\
    \ samples in the set and z is the standard score which depends on the target confidence\
    \ level. For a confidence level of 95%, z equals 1.96. Deriving confidence intervals\
    \ involves first sorting the set of measurements, then using the above equations\
    \ to determine the indices of the measurements corresponding to the lower and\
    \ upper bounds of the confidence interval. The sample's median should be within\
    \ the CI bounds.\n\nIID samples: CI require the samples of a set to be independent\
    \ and identically distributed (iid) [\\[24\\]](#page-12-8), [\\[25\\]](#page-12-9).\
    \ In the case of an experiment where the collection metric is latency, the samples\
    \ are identically distributed since latency measurements come from the same server.\
    \ Regarding independence, in the analysis presented below we collect one sample\
    \ per run. In between runs we reset the environment and as a result the measured\
    \ samples are independent. When there is doubt for the iid-ness of samples, several\
    \ methods can be used with the standard one being autocorrelation. Autocorrelation\
    \ is a method that calculates the degree of similarity between a time series and\
    \ a lagged version of itself. The output of the analysis can be anything between\
    \ -1 and 1, where 1 represents a positive correlation, -1 a negative correlation\
    \ and values close to 0 indicate no correlation among samples. Other methods used\
    \ for assessing the iid-ness of samples include Lag-Plots and Turning Point Test.\n\
    \nHypothesis Testing - Shapiro-Wilk Test: Hypothesis testing [\\[27\\]](#page-12-12)\
    \ is a systematic procedure used in statistics to assess whether characteristics\
    \ of a population occur by chance or not. The first step in hypothesis testing,\
    \ is to define a null hypothesis like for example two populations are equal. Then\
    \ identify a test statistic that can evaluate the hypothesis. In our analysis\
    \ we use a Shapiro-Wilk Test [\\[37\\]](#page-12-13) in order to test whether\
    \ the sampled data follow a normal distribution. Based on the test statistic results\
    \ a p-value is calculated. P-values represent the probability of finding the observed\
    \ results of a test statistic if the null hypothesis is true. The p-value is then\
    \ compared with a significance level, if it is less than the significance level\
    \ then we reject the null hypothesis. Conventionally 5% and 1% confidence levels\
    \ have been used, which means that there is less than 1 in 20 and 1 in 100 chance\
    \ of being wrong respectively.\n\nSample Size for Determining Mean/Median: The\
    \ confidence level and accuracy of a CI depends on the number of samples. The\
    \ higher the number of samples the better the associated confidence level and\
    \ accuracy. In this section we describe 2 methods (1 parametric [\\[18\\]](#page-12-14),\
    \ 1 non parametric [\\[29\\]](#page-12-11)) that can be used to determine what\
    \ is the minimum required number of samples (repetitions in our case) that are\
    \ required to achieve a confidence level with a certain accuracy.\n\nEquation\
    \ [3](#page-3-0) [\\[18\\]](#page-12-14) calculates the iterations for parametric\
    \ distributions:\n\n<span id=\"page-3-0\"></span>\n$$m = (\\frac{100zs}{r\\alpha})^2\\\
    tag{3}$$\n\nwhere z is the normal variate of the desired confidence level (1.96\
    \ for 95% confidence), s is the standard deviation, r is the error % from the\
    \ mean and x is the mean of the collected samples.\n\nFor non-parametric distributions,\
    \ the CONFIRM method [\\[29\\]](#page-12-11) is used. To calculate the number\
    \ of repetitions with CONFIRM: (i) for a set size n, randomly select a subset\
    \ s <= n and estimate non-parametric CI, (ii) shuffle set, select another subset,\
    \ and estimate CI, (iii) repeat this procedure c times and then calculate the\
    \ means for all the lower bounds of CI and upper bounds of CI, and (iv) if error\
    \ is less or equal to 1% then size of the subset equals the number of repetitions,\
    \ otherwise increase subset size and repeat. The original CONFIRM paper uses c=200\
    \ and s >= 10 assuming that smaller subsets cannot estimate non-parametric CIs\
    \ reliably.\n\n# IV. Experimental Methodology\n\n#### A. System\n\nTo conduct\
    \ our experiments we use the c220g5 cluster of the Wisconsin site from the CloudLab\
    \ [\\[13\\]](#page-12-15) infrastructure. Our baseline system is a 2 socket server\
    \ with 2 Skylake-based (Intel Xeon Silver 4114) processors. There are 20 physical\
    \ cores and 40 hardware threads. The nominal frequency is 2.2GHz with the minimum\
    \ frequency reaching 0.8 GHz and the maximum Turbo Boost frequency 3 GHz. The\
    \ server is equipped with 192 GB DDR4 DRAM. The operating system used is UBUNTU\
    \ 18.04.\n\n#### B. Benchmarks\n\nTwo representative microservice-based latency\
    \ critical applications are used in the analysis:\n\nMemcached [\\[1\\]](#page-11-2)\
    \ is a lightweight key-value store that is widely deployed as a distributed caching\
    \ service to accelerate user-facing applications with strict latency requirements.\
    \ Memcached has been the focus of numerous studies, including efforts to provide\
    \ low microsecond-scale tail latency. In our experiments, we run a memcached instance\
    \ with 10 worker threads pinned on a single socket. We use an extended version\
    \ of Mutilate [\\[26\\]](#page-12-3), as a workload generator. Following the taxonomy\
    \ of Section [II,](#page-1-1) Mutilate is an open-loop workload generator; it\
    \ implements time-sensitive interarrival times using a block-wait event loop that\
    \ waits for elapsed time, with the point of measurement residing within the generator\
    \ itself. We run Mutilate on 5 machines, one for the master process and 4 for\
    \ the workload-generator clients, establishing a total of 160 connections. We\
    \ configure the workload generator to recreate the ETC workload from Facebook\
    \ [\\[5\\]](#page-11-3).\n\nHDSearch [\\[38\\]](#page-12-1) is one of the four\
    \ information-retrieval services of the MicroSuite microservice-based benchmark\
    \ suite. HDSearch is an image similarity search service written in C++, which\
    \ is structured as a three-tier service using RPC for communication between tiers.\
    \ It returns images from a large dataset whose feature vectors are near to the\
    \ query's feature vector. It uses Locality-Sensitive Hash (LSH) tables to traverse\
    \ the search space of the problem efficiently. We use the accompanying open-loop\
    \ client, which generates requests with inter-arrival times drawn from a Poisson\
    \ distribution, as a workload generator. Following the taxonomy of Section [II,](#page-1-1)\
    \ the client is an open-loop workload generator; it implements time-insensitive\
    \ interarrival times using a busy-wait loop that actively polls for elapsed time,\
    \ with the point of measurement residing within the generator itself. We use 3\
    \ machines to run the benchmark, 1 for each type of process: client, midtier and\
    \ bucket. Our benchmark configuration follows the configuration of the MicroSuite\
    \ paper [\\[38\\]](#page-12-1). Finally, we pin the processes onto specific cores\
    \ to eliminate process migration. Social Network is a microservice-based application\
    \ from the DeathStar [\\[14\\]](#page-12-2) benchmark suite, consisting of multiple\
    \ interconnected services. We deploy the benchmark on a single\n\nnode using Docker\
    \ Swarm. We initialize the social graph using the provided small dataset namely\
    \ \"Reed98 Facebook Networks\" [\\[36\\]](#page-12-16). We use the accompanying\
    \ open-loop client, which is an extended version of the wrk2 workload generator.\
    \ We configure the client to (i) establish 20 connections with the server, (ii)\
    \ send requests using an exponential distribution, and (iii) only use read-user-timeline\
    \ requests. Following the taxonomy of Section [II,](#page-1-1) the client is an\
    \ open-loop workload generator; it implements time-sensitive interarrival times\
    \ using a block-wait event loop that waits for elapsed time, with the point of\
    \ measurement residing within the generator itself. Finally, before each run we\
    \ fill the database of the application with posts using compose-post queries.\n\
    \nSynthetic Workload is a program with tunable service latency, implemented to\
    \ perform a sensitivity analysis. It can accept an input parameter, the value\
    \ of which specifies by how long the processing time of a request should be extended.\
    \ The processing time is implemented using a busy wait loop instead of a normal\
    \ wait loop to prevent the core from serving other requests, as the additional\
    \ wait time should be accounted as service time rather than sleep time. We run\
    \ our service instance with 10 worker threads pinned on a single socket. Following\
    \ the taxonomy of Section [II,](#page-1-1) the client of the synthetic workload,\
    \ is an open-loop workload generator; it implements time-sensitive interarrival\
    \ times using a blockwait event loop that waits for elapsed time, with the point\
    \ of measurement residing within the generator itself.\n\nUnless stated otherwise,\
    \ each experiment is the average of 50 runs. The duration of each run is 2 minutes.\
    \ We collect several metrics during the execution of each experiment with the\
    \ most important one being the average response time and 99th tail latency. We\
    \ use the non-parametric expressions to calculate CI with a confidence level of\
    \ 95%. In each experiment we tune several hardware knobs. The description of each\
    \ HW knob is mentioned in Section [IV-C](#page-4-1) and the scenarios evaluated\
    \ are mentioned in Section [IV-D.](#page-4-2)\n\n#### <span id=\"page-4-1\"></span>C.\
    \ Hardware Knobs\n\nIn this section we describe the different HW knobs of the\
    \ analysis and how we tune them.\n\nC-states [\\[16\\]](#page-12-17) are power\
    \ saving states that enable a core to reduce it's power consumption during idle\
    \ periods. Skylakebased processors support 4 C-states C0, C1, C1E and C6. We use\
    \ the intel\\_idle.max\\_cstate flag and the idle=poll flag to enable/disable\
    \ any C-states through the grub file.\n\nFrequency Driver [\\[23\\]](#page-12-18)\
    \ is a component of the CPUFreq subsystem of Linux that enables the OS to scale\
    \ the frequency and voltage. The frequency driver is responsible for communicating\
    \ the Frequency/Voltage settings to the hardware. Usually a linux system supports\
    \ 2 frequency drivers, intel\\_pstate and acpi-cpufreq. We pass the intel\\_pstate\
    \ flag to the grub file to enable/disable them.\n\nFrequency Governor [\\[23\\\
    ]](#page-12-18) is also a component of the CPUFreq subsystem. It is the component\
    \ responsible to decide the suitable frequency/voltage of the system based on\
    \ some heuristics. We use cpupower, which is a tool that act as a wrapper around\
    \ the sysfs kernel interface to specify a frequency driver.\n\nTurbo mode [\\\
    [16\\]](#page-12-17) is a feature in modern processors that allows CPU to dynamically\
    \ increase it's clock speed above its nominal frequency under certain conditions\
    \ (i.e., thermal capacity, number of active cores). We use the Model Specific\
    \ Register (MSR) 0x1a0 to enable/disable turbo mode.\n\nSimultaneous Multithreading\
    \ (SMT) [\\[42\\]](#page-12-19) is a feature in modern processors that allow multiple\
    \ threads to execute on the same physical core at the same time. We use the sys\
    \ interface to enable/disable this feature.\n\nUncore Frequency [\\[16\\]](#page-12-17)\
    \ refers to the operating frequency of the uncore components of the CPU. These\
    \ components include the Last Level Cache (LLC), IO interfaces etc. We use the\
    \ MSR 0x620 to tune the uncore frequency.\n\nTickless [\\[40\\]](#page-12-20)\
    \ is a characteristic of kernels that do not omit clock-scheduling interrupts\
    \ during idle periods. We pass the nohz flag to the grub file to enable/disable\
    \ this feature.\n\n#### <span id=\"page-4-2\"></span>D. Client/Server Configuration\n\
    \nIn our analysis we use 2 configurations for the client-side, the low-power (LP)\
    \ configuration and the high-performance (HP) configuration. The LP configuration\
    \ represents the default configuration of the system and thus the case where a\
    \ user is agnostic of the client-side configuration. The HP configuration represents\
    \ a configuration tuned empirically to achieve high performance. The details of\
    \ the configuration can be found in Table [II.](#page-4-3)\n\nThe server side\
    \ baseline configuration is presented also in Table [II.](#page-4-3) We choose\
    \ empirically a configuration that does not introduce high variability and achieves\
    \ good performance. In the experimental evaluation whenever a HW knob of the server\
    \ side changes it is explicitly mentioned.\n\n<span id=\"page-4-3\"></span>TABLE\
    \ II: Client- and server-side hardware configurations\n\n|                   \
    \    | Client Side  | Server Side  |              |  |\n|-----------------------|--------------|--------------|--------------|--|\n\
    | Configuration         | LP           | HP           | Baseline     |  |\n| C-states\
    \              | C0,C1,C1E,C6 | off          | C0,C1        |  |\n| Frequency<br>Driver\
    \   | intel pstate | acpi cpufreq | acpi cpufreq |  |\n| Frequency<br>Governor\
    \ | powersave    | performance  | performance  |  |\n| Turbo                 |\
    \ on           | on           | off          |  |\n| SMT                   | on\
    \           | on           | off          |  |\n| Uncore<br>Frequency   | dynamic\
    \      | fixed        | fixed        |  |\n| Tickless              | off     \
    \     | off          | on           |  |\n\nTable [III](#page-5-0) describes the\
    \ scenarios tested in the experimental analysis (Section [V\\)](#page-4-0) using\
    \ the terminology introduced in Section [II.](#page-1-1) The last column of the\
    \ table, indicates which one of the scenarios can cause wrong conclusions (e.g.\
    \ X) and the sections each scenario is evaluated in.\n\n# V. Experimental Analysis\n\
    \n<span id=\"page-4-0\"></span>We study the impact of client-side hardware configuration\
    \ on performance variation under different scenarios (Section [V-A\\)](#page-5-1)\
    \ and how this impact varies with services with\n\n|  | TABLE III: Scenarios Tested\
    \ in Section V. |  |  |  |\n|--|-------------------------------------------|--|--|--|\n\
    |--|-------------------------------------------|--|--|--|\n\n<span id=\"page-5-0\"\
    ></span>\n\n| Workload Generator<br>Design        |        | Client    | Response\
    \ | Risk/<br>Section |  |\n|-------------------------------------|--------|-----------|----------|------------------|--|\n\
    | inter.<br>point of<br>rate<br>meas. |        | Conf.     | Time     |      \
    \            |  |\n| open-loop<br>time-sensitive         | in-app | tuned    \
    \ | small    | (5.1,5.3)        |  |\n| open-loop<br>time-sensitive         |\
    \ in-app | not-tuned | small    | X(5.1,5.3)       |  |\n| open-loop<br>time-insensitive\
    \       | in-app | tuned     | big      | (5.2)            |  |\n| open-loop<br>time-insensitive\
    \       | in-app | not-tuned | big      | (5.2)            |  |\n\nhigher response\
    \ time (Section [V-B\\)](#page-5-2). Finally, we examine the impact of the client-side\
    \ configuration on the execution time of the experimental evaluation (Section\
    \ [V-C\\)](#page-8-0).\n\n#### <span id=\"page-5-1\"></span>A. Client-side Configuration\
    \ Impact\n\nWe present two case studies that aim to evaluate the impact of two\
    \ server-side features, specifically SMT and C-states, on the performance of the\
    \ Memcached service. Our findings demonstrate that the choice of client-side configuration\
    \ can lead to varying performance results and differing conclusions regarding\
    \ the effects of the features under study.\n\nSMT: The aim of the analysis is\
    \ to investigate whether SMT can improve the performance of Memcached under different\
    \ load (10K - 500K QPS) and corresponding utilization (5% - 55%). Figure [2](#page-6-0)\
    \ shows the performance evaluation of Memcached running on a server machine configured\
    \ with SMT disabled (baseline) or SMT enabled. Each server-side hardware configuration\
    \ is examined with two client configurations, namely LP (low power) and HP (high\
    \ performance).\n\nDepending on the client configuration, the end-to-end measurements\
    \ differ. Specifically, the LP end-to-end measurements are between 80% to 150%\
    \ higher than the end-to-end measurements of HP client, that is if we compare\
    \ similar server-side configurations. Additionally, the 99th percentile latency\
    \ is 33% to 200% higher for LP clients compared to HP clients. We argue that this\
    \ is a result of the additional overhead introduced by the client-side hardware\
    \ configuration. Since the point of measurement of the workload generator is inside\
    \ the generator itself, a query must experience at least a C-state transition\
    \ (2us - 200us), a DVFS transition (∼ 30us), and a context switch (∼25us) before\
    \ the workload generator is able to capture the timestamp that will mark the completion\
    \ of the query. This behavior is especially important in a datacenter setup. Let\
    \ us assume a service with a QoS of 99th percentile latency equal to 400us. The\
    \ LP client finds that the service can handle only 300K queries without violating\
    \ any QoS constraints. In contrast, the HP client finds that the service can handle\
    \ 500K queries. In other words, the LP client determines that a deployment will\
    \ require 1.6x more machines than the HP client, to satisfy the same load without\
    \ violating any QoS constraints.\n\nAnother important observation from Figure\
    \ [2c](#page-6-0) and Figure [2d](#page-6-0) is that the measured degradation\
    \ depends on the client-side hardware configuration. The LP client determines\n\
    \nthat enabling SMT on the server side improves the 99th percentile latency by\
    \ at most 3% (see Figure [2d](#page-6-0)). In contrast, the HP client determines\
    \ that the 99th percentile latency can improve by 13%. We believe this is partially\
    \ because the absolute performance improvement caused by SMT is more pronounced\
    \ for the HP client end-to-end time compared to the LP client.\n\nFinding 1: The\
    \ client-side hardware configuration can impact the accuracy of an experiment.\
    \ Specifically, it can (i) affect the end-to-end measurements, leading to higher\
    \ or lower measurements, and (ii) produce different speedups for the same feature\
    \ or technique under study.\n\nC1E: The aim of the analysis is to investigate\
    \ whether C1E can improve the performance of Memcached under different load (10K\
    \ - 500K QPS) and corresponding utilization (5% - 55%). Figure [3](#page-6-1)\
    \ shows the performance evaluation of Memcached running on a server machine configured\
    \ with C1E disabled (baseline) or C1E enabled. Each server-side hardware configuration\
    \ is examined with two client configurations, namely LP (low power) and HP (high\
    \ performance).\n\nSimilarly to the SMT study above, the choice of client configuration\
    \ leads to different end-to-end average response latency and 99th percentile latency.\
    \ Specifically, the average response latency differs from 64% to 145% and the\
    \ 99th percentile latency from 0% to 200%. Additionally, the observed slowdown\
    \ caused by C1E differs based on the client configuration. For the HP client,\
    \ the slowdown of C1E goes up to 19% for average latency and 18% for the 99th\
    \ percentile latency. For the LP client, the slowdown caused by C1E goes up to\
    \ 13% for the average latency and 7% for the 99th percentile latency.\n\nMore\
    \ importantly, the client choice shows different trends for high load (400K and\
    \ 500K QPS), leading to conflicting conclusions about the effect of C1E on performance.\
    \ The LP client reports that for high load the C1E enabled configuration is worse\
    \ than the C1E disabled (since the confidence intervals do not overlap). However,\
    \ the HP client reports that for all loads (except of the 10K QPS load) the C1E\
    \ enabled and C1E disabled configurations have the same performance.\n\nFinding\
    \ 2: The client-side hardware configuration can impact not only the accuracy but\
    \ also the observed trends of an experiment, leading to conflicting conclusions.\n\
    \n#### <span id=\"page-5-2\"></span>B. Impact Relative to Service Latency\n\n\
    We examine the impact of client-side hardware configuration on the performance\
    \ of applications with different endto-end latencies. We present three studies:\
    \ (i) a single-service study, which investigates the performance of a microservicebased\
    \ service benchmark, (ii) a multi-service application study, which investigates\
    \ the performance of a microservice-based application consisting of multiple services,\
    \ and (iii) a synthetic workload study, which performs a sensitivity analysis.\
    \ Our findings demonstrate that the client-side hardware configuration has minimal\
    \ impact on services with high response latency.\n\n<span id=\"page-6-0\"></span>![](_page_6_Figure_0.jpeg)\n\
    \nFig. 2: Performance evaluation of SMT impact on Memcached service latency with\
    \ LP and HP clients. (a) Average Response Time (median) for HP/LP client and SMT\
    \ ON/OFF server, (b) 99th Percentile Latency (median) for HP/LP client and SMT\
    \ ON/OFF server, (c) Slowdown (avg) caused by disabling SMT on the Average Response\
    \ Time for HP and LP client and (d) Slowdown (avg) caused by disabling SMT on\
    \ the 99th Percentile Latency for HP and LP client.\n\n<span id=\"page-6-1\"></span>![](_page_6_Figure_2.jpeg)\n\
    \nFig. 3: Performance evaluation of C1E impact on Memcached service latency with\
    \ LP and HP clients. (a) Average Response Time (median) for HP/LP client and C1E\
    \ ON/OFF server, (b) 99th Percentile Latency (median) for HP/LP client and C1E\
    \ ON/OFF server, (c) Slowdown (avg) caused by enabling C1E on the Average Response\
    \ Time for HP and LP client and (d) Slowdown (avg) caused by enabling C1E on the\
    \ 99th Percentile Latency for HP and LP client.\n\nSingle-Service: We use the\
    \ HDSearch service, which operates with millisecond-scale latency, to examine\
    \ the impact of clientside configuration on performance variation when analyzing\
    \ the performance of services with high response latency. Figure [4,](#page-7-0)\
    \ presents the performance evaluation of HDSearch running on a server machine\
    \ configured with SMT or C1E. Each server-side HW configuration is examined with\
    \ two client configurations, namely LP (low power) and HP (high performance).\n\
    \nSimilarly to Memcached, there is a difference in end-to-end measurements between\
    \ the HP and LP client for both average and 99th percentile latency, although\
    \ it is not as pronounced as in Memcached. Specifically, the average response\
    \ latency of LP is from 7% to 17% higher than HP. Regarding the 99th percentile\
    \ latency, LP has 5% to 29% higher 99th percentile latency than HP. Since HDSearch\
    \ has higher response latency than Memcached, we expect the difference between\
    \ the end to end measurements of the HP and LP clients which is a result of the\
    \ client configuration overhead, to be statistically less significant. Thus, we\
    \ expect LP and HP clients determine similar resource requirements to satisfy\
    \ a target load without violating any QoS constraints.\n\nContrary to Memcached,\
    \ the HP and LP clients measure same speedups (with similar trends) in the average\
    \ response latency for both the SMT and C1E server-side configurations. Even though\
    \ the LP measurements experience variability because of the client-side hardware\
    \ configuration, the high server-side processing time of HDSearch (400us) overshadows\
    \ the client-caused variability (∼20us in Figure [5b](#page-7-1)), thus minimally\
    \ impacting the observed speedup of the evaluated\n\n<span id=\"page-7-0\"></span>![](_page_7_Figure_0.jpeg)\n\
    \nFig. 4: Performance evaluation of SMT and C1E impact on HDSearch service latency\
    \ with LP and HP clients. (a) Average Response Time (median) for HP/LP client\
    \ and SMT ON/OFF server, (b) 99th Percentile Latency (median) for HP/LP client\
    \ and SMT ON/OFF server, (c) Average Response Time (median) for HP/LP client and\
    \ C1E ON/OFF server and (d) 99th Percentile Latency (median) for HP/LP client\
    \ and C1E ON/OFF server.\n\nserver configurations. Memcached server-side processing\
    \ time (∼10us) is in the same order as the client-caused variability (up to 10us\
    \ in Figure [5a](#page-7-1)), thus making the speedup of the evaluated server\
    \ configurations more sensitive to the clientside hardware configuration.\n\n\
    <span id=\"page-7-1\"></span>![](_page_7_Figure_3.jpeg)\n\nFig. 5: (a) Standard\
    \ Deviation of Memcached for the Average Response Time with LP/HP client configuration\
    \ and SMT ON/OFF server configuration, (b) Standard Deviation of HDSearch for\
    \ the Average Response Time with LP/HP client configuration and SMT ON/OFF server\
    \ configuration.\n\nOverall, we observe that HDSearch, a service with about 10\
    \ times higher end-to-end response latency than Memcached exhibits similar speedups\
    \ and trends when run with two different client configurations. Although the absolute\
    \ end-toend measurements are not the same, the difference is not as pronounced\
    \ as in Memcached.\n\nMulti-Service Application: We study the impact of the client-side\
    \ HW configuration on the performance of an application using Social Network from\
    \ the DeathStar benchmark\n\nsuite. Figure [6a](#page-8-1) presents the difference\
    \ in the end-to-end latency between the two client configurations LP and HP for\
    \ average and 99th percentile latency respectively. Similarly to HDSearch the\
    \ difference between the two clients gets smaller while the end-to-end latency\
    \ increases. Compared to HDSearch, the gap between the two clients is small (5%\
    \ vs 17% ) on the average response time due to the fact that Social Network has\
    \ higher end-to-end latency (∼2-3ms in Figure [6b](#page-8-1)) than HDSearch (∼1ms\
    \ in Figure [4\\)](#page-7-0). Surprisingly the impact of different clients on\
    \ the 99th percentile latency for Social Network, as shown in Figure [6c](#page-8-1),\
    \ is minimal. In other words, the 99th percentile reported by both clients, LP\
    \ and HP is the same. For end-to-end latencies over 10ms, the client-induced overhead\
    \ does not appear to affect the accuracy of the measurements, as illustrated in\
    \ Figure [6c](#page-8-1).\n\nOverall, we observe that DeathStar validates the\
    \ HDSearch analysis. Although the absolute measurements reported by the two clients\
    \ are not identical, the difference is not as pronounced as in Memcached and HDSearch.\n\
    \nSynthetic Workload: To examine the impact of client side HW configuration at\
    \ different latencies, we use the synthetic workload. Figure [7a](#page-9-0) and\
    \ Figure [7b](#page-9-0) present the performance evaluation of the synthetic workload\
    \ for different end-to-end latencies and QPS under two client configurations LP\
    \ and HP. Although the QPS presented in Figure [7](#page-9-0) are low compared\
    \ to the ones examined in previous sections, it is important to note that due\
    \ to the increase in processing time there is no opportunity to achieve higher\
    \ throughput. To determine the examined QPS we use Little's law and examine only\
    \ the QPS where the concurrency is less than the number of available cores (i.e.,\
    \ 10) for all possible values of the new parameter. Additionally, the results\
    \ presented in this section are the average of 20 runs.\n\nAs expected with the\
    \ increase of the end-to-end latency, the gap between the LP and HP reported end-to-end\
    \ mea-\n\n<span id=\"page-8-1\"></span>![](_page_8_Figure_0.jpeg)\n\nFig. 6: Performance\
    \ evaluation of HP and LP clients for Social Network. (a) Slowdown (avg) caused\
    \ by changing from HP to LP client on the Average Response Time and 99th Percentile\
    \ Latency, (b) Average Response Time Latency (median) for HP/LP client at different\
    \ QPS and (c) 99th Percentile Latency (median) for HP/LP client at different QPS.\n\
    \nsurements gets smaller. Specifically, the difference goes from 2.8x for 0 added\
    \ delay to 1.02x for 400 us added delay at 20K QPS. Similarly, for 99th percentile\
    \ latency the difference goes from 3.5x to 1x. Between 0 to 100us added delay\
    \ we observe the highest decrease in the difference between the end to end measurements\
    \ reported by HP and LP client for both average and 99th percentile. This analysis\
    \ confirms the findings of HDSearch, based on which the added client side overhead\
    \ becomes less statistically significant for benchmarks with higher latencies.\n\
    \nFigure [7c](#page-9-0), [7d](#page-9-0), [7e](#page-9-0) and [7f](#page-9-0)\
    \ demonstrate the absolute end-toend measurements for HP and LP client at 5K and\
    \ 20K QPS. At low QPS, where there is no queueing, the response time increases\
    \ linearly with the increase of the added delay which validates the implementation\
    \ of the synthetic workload. We observe that for average response time latencies\
    \ over 1ms the accuracy difference is less than 10% between HP and LP client.\
    \ For high QPS and high added delays (end to end over 2ms) the HP and LP clients\
    \ measurements converge. This is partially because of the variability of the experiment\
    \ being in the same order as the observed overhead introduced by the client side\
    \ hardware configuration (stdev∼100us). A major source of variability for high\
    \ QPS is the queueing caused in the server side. We conclude that when the end\
    \ to end latency is in the order of milliseconds, the impact of client side overhead\
    \ is less significant.\n\nFinding 3: The client-side hardware configuration has\
    \ minimal impact on services with high response latency. The client-side hardware\
    \ configuration causes performance variability when the processing time of an\
    \ application is in the same order of magnitude as the variability introduced\
    \ by the client side.\n\n#### <span id=\"page-8-0\"></span>C. Impact on Experimental\
    \ Evaluation Time\n\nIn this section, we investigate how the different client-side\
    \ hardware configurations affect the experimental evaluation time. By experimental\
    \ evaluation time, we mean the time required for an experiment to achieve a confidence\
    \ interval with at most 1% error at a 95% confidence level. Before estimating\
    \ the number of repetitions an experiment requires to gain statistical confidence,\
    \ we first check whether the collected samples follow a normal distribution. This\
    \ is because the closed-form expressions used to calculate the number of iterations\
    \ for an experiment assume that data follow a normal distribution.\n\nFigure [8](#page-9-1)\
    \ tests the normality of the data presented earlier in Section [V-A](#page-5-1)\
    \ using the Shapiro-Wilk test. Data points within a single configuration correspond\
    \ to varying loads (QPS), all collected from the same single server. The red dashed\
    \ line indicates the threshold below which configurations do not conform to a\
    \ normal distribution. We analyze a total of 42 configurations (six scenarios\
    \ each with seven QPS values), with each configuration comprising 50 runs. Approximately\
    \ 50% of these configurations adhere to a normal distribution, while the remaining\
    \ 50% do not.\n\nThe above normality test results are in line with previous work\
    \ that examines data normality on a single node. Specifically, within the LP-SMToff\
    \ scenario, all QPS configurations exhibit a normal distribution. Conversely,\
    \ none of the QPS configurations within the HP-SMTon scenario adhere to a normal\
    \ distribution. In the HP/LP-SMToff and HP/LP-C1Eon scenarios, approximately half\
    \ of the QPS configurations follow a normal distribution, while the remaining\
    \ half corresponding to the high QPS configurations do not adhere to a normal\
    \ distribution. In the HP/LP-SMToff and HP/LP-C1Eon scenarios, about half of the\
    \ QPS configurations conform to a normal distribution, whereas the other half,\
    \ comprising the high QPS configurations, do not. We attribute this non-normality\
    \ to queuing effects that are more pronounced for higher QPS and the reduced number\
    \ of logical threads (SMToff). Looking into the frequency charts of these high\
    \ QPS configurations, a large number of samples lies below and close to the median\
    \ of the distribution, whereas a small number of samples is scattered in a larger\
    \ range above the median, making the distribution skewed, as shown in Figure [9.](#page-9-2)\n\
    \nBased on the above normality test results, we use both parametric and non-parametric\
    \ (CONFIRM) methods to calculate the number of required iterations to achieve\
    \ a confidence interval with at-most 1% error and 95% confidence level for each\
    \ configuration (as explained in Section [III\\)](#page-2-0).\n\nTable [IV](#page-10-0)\
    \ presents the results of the two methods along with the Shapiro-Wilk test results.\
    \ The highest value of iterations estimated by CONFIRM is >50 since each experiment\
    \ is executed 50 times. The lowest value estimated by CONFIRM is 10, since the\
    \ method assumes that smaller subsets cannot estimate non-parametric CIs reliably.\
    \ The two methods do not produce exactly the same results partly due to the parametric\
    \ method's ability to reliably estimate values and provide tight\n\n<span id=\"\
    page-9-0\"></span>![](_page_9_Figure_0.jpeg)\n\nFig. 7: Performance evaluation\
    \ of HP and LP clients for different processing times. (a) Slowdown (avg) caused\
    \ by changing from HP to LP client on the Average Response Time, (b) Slowdown\
    \ (avg) caused by changing from HP to LP client on the 99th Percentile Latency,\
    \ (c) Average Response Time (median) for HP/LP client at 5K QPS, (d) 99th Percentile\
    \ Latency (median) for HP/LP client at 5K QPS, (e) Average Response Time (median)\
    \ for HP/LP client at 20K QPS and (f) 99th Percentile Latency (median) for HP/LP\
    \ client at 20K QPS.\n\n<span id=\"page-9-1\"></span>![](_page_9_Figure_2.jpeg)\n\
    \nFig. 8: Shapiro-Wilk p-value for configurations in Section [V-A.](#page-5-1)\n\
    \n<span id=\"page-9-2\"></span>![](_page_9_Figure_4.jpeg)\n\nFig. 9: Frequency\
    \ Chart for HP-SMToff 400K configuration. The red bar is where the median lies.\n\
    \nbounds with a fewer number of iterations, typically below 10, when the configuration\
    \ adheres to a normal distribution. As a result, there are several cases where\
    \ the parametric method estimates just one iteration, and the CONFIRM method requires\
    \ 10 iterations.\n\nNevertheless, the two methods support that different client\
    \ configurations require different number of iterations to produce tight statistically\
    \ confident results. For low QPS (10K - 100K), both methods agree that the LP\
    \ client requires a large number of iterations to achieve statistical confidence\n\
    \nwhereas the HP client requires much less. For high QPS (300K - 500K), the HP\
    \ client requires more iterations than the LP client. This behavior agrees with\
    \ our empirical observations, that want the LP client to have higher standard\
    \ deviation in low QPS than the HP client, and the HP client to have higher standard\
    \ deviation in high QPS (see Figure [5\\)](#page-7-1).\n\nFinding 4: The client-side\
    \ hardware configuration can affect the number of iterations needed for an experiment\
    \ because different configurations can exhibit different levels of performance\
    \ variability. Current experimental methods are effective at estimating the number\
    \ of iterations required to mitigate the performance variability caused by the\
    \ client.\n\n# VI. Configuration Recommendations\n\n Drawing from the taxonomy\
    \ of Section [II](#page-1-1) and the experimental analysis of Section [V,](#page-4-0)\
    \ we now discuss recommendations for how to best configure the client side in\
    \ an experimental evaluation based on latency-sensitive microservices. We focus\
    \ on the aspect of time-sensitivity caused by the interarrival time implementation\
    \ of open-loop workload generators, as we find this can play a key role in performance\
    \ variation.\n\nFor a time-sensitive interarrival time implementation, the client-side\
    \ hardware configuration should be tuned for performance. The performance configuration\
    \ mitigates the hardware timing overheads of power and energy optimizations (i.e.,\
    \ C-states, DVFS), allowing the workload generator to send requests as close as\
    \ possible to the time indicated by the interarrival time distribution. In this\
    \ case however, it is essential to consider how accurately the performance configuration\
    \ reflects the configuration within the target production cluster. If the configuration\
    \ deviates from the target production configuration, then it may over- or under-estimate\n\
    \n| Configuration | QPS  | Parametric | CONFIRM | Shapiro-Wilk |\n|---------------|------|------------|---------|--------------|\n\
    |               | 10K  | 288        | >50     | pass         |\n|            \
    \   | 50K  | 93         | >50     | pass         |\n|               | 100K | 15\
    \         | 37      | pass         |\n| LP-SMToff     | 200K | 3          | 11\
    \      | pass         |\n|               | 300K | 2          | 11      | fail\
    \         |\n|               | 400K | 5          | 19      | fail         |\n\
    |               | 500K | 19         | >50     | fail         |\n|            \
    \   | 10K  | 225        | >50     | pass         |\n|               | 50K  | 70\
    \         | >50     | pass         |\n|               | 100K | 17         | 34\
    \      | pass         |\n| LP-SMTon      | 200K | 4          | 16      | pass\
    \         |\n|               | 300K | 3          | 11      | pass         |\n\
    |               | 400K | 4          | 15      | pass         |\n|            \
    \   | 500K | 10         | 36      | pass         |\n|               | 10K  | 1\
    \          | 10      | pass         |\n|               | 50K  | 1          | 10\
    \      | fail         |\n|               | 100K | 1          | 10      | pass\
    \         |\n| HP-SMToff     | 200K | 2          | 11      | fail         |\n\
    |               | 300K | 27         | >50     | fail         |\n|            \
    \   | 400K | 123        | >50     | fail         |\n|               | 500K | 203\
    \        | >50     | fail         |\n|               | 10K  | 1          | 10\
    \      | fail         |\n|               | 50K  | 1          | 10      | fail\
    \         |\n|               | 100K | 1          | 10      | fail         |\n\
    | HP-SMTon      | 200K | 1          | 10      | fail         |\n|            \
    \   | 300K | 8          | 11      | fail         |\n|               | 400K | 39\
    \         | 41      | fail         |\n|               | 500K | 77         | 41\
    \      | fail         |\n|               | 10K  | 303        | >50     | pass\
    \         |\n|               | 50K  | 89         | >50     | pass         |\n\
    |               | 100K | 20         | >50     | pass         |\n| LP-C1Eon   \
    \   | 200K | 3          | 11      | pass         |\n|               | 300K | 1\
    \          | 10      | pass         |\n|               | 400K | 2          | 11\
    \      | pass         |\n|               | 500K | 9          | 21      | fail\
    \         |\n|               | 10K  | 2          | 11      | fail         |\n\
    |               | 50K  | 1          | 10      | fail         |\n|            \
    \   | 100K | 1          | 10      | pass         |\n| HP-C1Eon      | 200K | 1\
    \          | 10      | pass         |\n|               | 300K | 8          | 24\
    \      | fail         |\n|               | 400K | 21         | >50     | fail\
    \         |\n|               | 500K | 32         | >50     | pass         |\n\n\
    <span id=\"page-10-0\"></span>TABLE IV: Number of iterations to gain statistical\
    \ confidence and Shapiro-Wilk results.\n\nperformance metrics, such as end-to-end\
    \ time (Section [V-A\\)](#page-5-1), and consequently affect any conclusions drawn,\
    \ such as those related to resource provisioning.\n\nFor a time-insensitive interarrival\
    \ time implementation, the choice is guided by the target environment. The configuration\
    \ of the client should match the configuration of the target environment. When\
    \ the target configuration is unknown, a space exploration could be made to evaluate\
    \ a technique under several scenarios, using either homogeneous or heterogeneous\
    \ client and server machine configurations.\n\nAs far as the number of iterations\
    \ required for an experiment is concerned, well established methodologies [\\\
    [18\\]](#page-12-14), [\\[29\\]](#page-12-11) should be used based on the distribution\
    \ followed by the samples.\n\n# VII. Related Work\n\nTo the best of our knowledge,\
    \ this is the first study to investigate the impact of the client-side hardware\
    \ configuration on the accuracy and evaluation time of an experiment. Previous\
    \ works focus on quantifying variability arising from\n\nother sources, including\
    \ the order of experiments, process variation, and the server-side configuration.\
    \ Several studies propose techniques to mitigate variability, including increasing\
    \ the number of repetitions, using confidence intervals, and developing more robust\
    \ workload generators.\n\n#### A. Evaluating Performance using Microservices\n\
    \nIn recent years, latency-critical applications have moved from a monolithic\
    \ to a microservice-based software architecture to satisfy service-level objectives,\
    \ availability, scalability, and regular updates [\\[19\\]](#page-12-21), [\\\
    [35\\]](#page-12-22), [\\[43\\]](#page-12-23). In a microservice-based software\
    \ architecture, an application is decomposed into several services that communicate\
    \ with one another via the network through well-defined interfaces, such as gRPC\
    \ and REST APIs. The decoupled nature of these applications leads to stricter\
    \ QoS constraints per service compared to their monolithic counterparts, ranging\
    \ from 250us [\\[7\\]](#page-11-1), [\\[49\\]](#page-13-0) to 500us [\\[6\\]](#page-11-0),\
    \ [\\[22\\]](#page-12-0)), due to increased network communication overheads.\n\
    \nThe transition to a microservice software paradigm has prompted the community\
    \ to develop new benchmark suites, such as MicroSuite [\\[38\\]](#page-12-1) and\
    \ DeathStar [\\[14\\]](#page-12-2), and adopt existing services, such as Memcached\
    \ [\\[1\\]](#page-11-2), to effectively evaluate designs targeting microservices.\
    \ Memcached, in particular, has been the focus of numerous studies, including\
    \ taillatency optimizations [\\[30\\]](#page-12-24), collocation [\\[26\\]](#page-12-3),\
    \ request scheduling and consolidation, and C-states [\\[3\\]](#page-11-6), [\\\
    [4\\]](#page-11-4), [\\[48\\]](#page-13-2), due to its critical role in enhancing\
    \ response times of latency-critical applications as a lightweight caching service\
    \ [\\[28\\]](#page-12-25). To simplify the experimental environment and facilitate\
    \ reproducibility, deploying a single memcached server process for the experimental\
    \ evaluation has been common practice among previous works [\\[4\\]](#page-11-4),\
    \ [\\[8\\]](#page-11-5), [\\[26\\]](#page-12-3), [\\[46\\]](#page-12-4).\n\n####\
    \ B. Quantifying Performance Variability\n\nQuantifying variability has been the\
    \ focus of many works on datacenters, supercomputers and smartphones. Maricq et\
    \ al. [\\[29\\]](#page-12-11) investigate what is the inevitable variability across\
    \ nodes of the same architecture in a cluster. They conclude that variability\
    \ of up-to 10% can be attributed to the underlying hardware. Additionally, they\
    \ investigate the normality of performance samples across nodes and conclude that\
    \ the performance samples follow a non-parametric distribution across nodes. In\
    \ a similar setup, Duplyakin et al. [\\[12\\]](#page-11-7) investigate the variability\
    \ caused by the execution order of experiments. The rationale is that the sequence\
    \ in which experiments are conducted can alter the microarchitectural characteristics\
    \ of the machine, inevitably affecting the performance outcomes of each experiment.\
    \ If executed in a specific order, this bias will impact the outcome of the experiment.\
    \ Such variability can be categorized as a form of measurement bias. A measurement\
    \ bias [\\[31\\]](#page-12-26) is when a technique X speedups a system O by Z\
    \ but the speedup is not only a result of the technique but is also a bias of\
    \ the experimental setup. Several works have investigated this phenomenon in various\
    \ settings, including scheduling algorithms of supercomputers [\\[41\\]](#page-12-27),\
    \ architectural\n\nsimulations for multithreaded workloads [\\[2\\]](#page-11-8),\
    \ and O3 optimizations in SPEC CPU2006 workloads [\\[31\\]](#page-12-26). Additionally\
    \ to measurement bias, other works [\\[44\\]](#page-12-28), [\\[47\\]](#page-13-3)\
    \ have identified the network contention as a major contribution to the variability\
    \ observed by an application. Another work [\\[20\\]](#page-12-29) has developed\
    \ a methodology using the stress-ng [\\[9\\]](#page-11-9) tests that can estimate\
    \ the variability across machines of different architecture and as a result can\
    \ reproduce with some error an experiment outcome on a different machine. Finally,\
    \ Srinivasa et al. [\\[34\\]](#page-12-30) manage to create a methodology that\
    \ quantifies the process variation of smartphones at system level. Although different\
    \ sources of performance variability have been investigated there is no mention\
    \ of the configuration of the client side, even in survey papers [\\[12\\]](#page-11-7),\
    \ [\\[17\\]](#page-12-31), even though it can affect the accuracy of measurements.\n\
    \n#### C. Mitigating Performance Variability\n\nStrategies aiming to improve the\
    \ experimental methodology accuracy and mitigate the performance variability have\
    \ been proposed, many of which have been implemented inside a workload generator\
    \ [\\[10\\]](#page-11-10), [\\[24\\]](#page-12-8), [\\[26\\]](#page-12-3), [\\\
    [50\\]](#page-13-4). For example in Lancet [\\[24\\]](#page-12-8), the authors\
    \ try to create a workload generator that accurately captures the 99th percentile\
    \ latency of microservices by (i) minimizing the errors caused by excessive user\
    \ interference, (ii) using state of the art hardware-based techniques, and (iii)\
    \ using excessively statistical methods to accurately process the samples. Their\
    \ workload implements, among others, an Anderson Darling test to check the request\
    \ arrival distribution, an Augmented Dickey Fuller test to check the stationarity\
    \ of samples, and a Spearman test to check whether samples are independent. Apart\
    \ from workload generators, standalone tools have also been proposed, such as\
    \ CONFIRM [\\[29\\]](#page-12-11) and OrderSage [\\[12\\]](#page-11-7), aiming\
    \ to mitigate variability, in this case, by (i) calculating the CI for nonparametric\
    \ distributions, and (ii) randomizing the order of experiments. Another set of\
    \ works [\\[11\\]](#page-11-11), [\\[45\\]](#page-12-32), [\\[51\\]](#page-13-5)\
    \ fall under the umbrella of variability detection (anomaly detection or changepoint\
    \ detection). Other works aim to mitigate measurement bias by randomizing the\
    \ experimental setup, either through slight changes in the timing of requests\
    \ of the workload [\\[41\\]](#page-12-27) or modifications in the simulator to\
    \ change cache-miss penalty [\\[2\\]](#page-11-8). Finally, request batching [\\\
    [39\\]](#page-12-33) has been proposed for eliminating network variability in\
    \ Memcached. The proposals mentioned above are complementary to our work.\n\n\
    ## VIII. Conclusions\n\nTo the best of our knowledge, this is the first work that\
    \ examines the impact of the client-side configuration on the experimental evaluation\
    \ of microservices. Our evaluation reveals that under certain conditions that\
    \ concern the design of the workload generator and the characteristics of a microservice,\
    \ the client-side configuration can influence the accuracy of the end-to-end measurements\
    \ by up-to 150% for the average and 200% for the 99th percentile latency of Memcached.\
    \ Motivated by the above, we provide\n\nrecommendations regarding the experimental\
    \ environment configuration so that any unnecessary time bias is avoided and so\
    \ that the results of the experiment reflect closely the behaviour of the target\
    \ environment. These results support that the client-side configuration should\
    \ be considered when designing experiments.\n\n# Acknowledgments\n\nThe authors\
    \ would like to thank the anonymous reviewers for their insightful comments on\
    \ earlier versions of this manuscript. This project has received funding from\
    \ the European Union's Horizon 2020 research and innovation programme under the\
    \ Marie Skłodowska-Curie grant agreement No 101029391.\n\n# References\n\n- <span\
    \ id=\"page-11-2\"></span>[1] \"Memcached: A Distributed Memory Object Caching\
    \ System,\" online, accessed November 2021 [https://memcached.org/.](https://memcached.org/)\n\
    - <span id=\"page-11-8\"></span>[2] A. Alameldeen and D. Wood, \"Variability in\
    \ architectural simulations of multi-threaded workloads,\" in The Ninth International\
    \ Symposium on High-Performance Computer Architecture, 2003. HPCA-9 2003. Proceedings.,\
    \ 2003, pp. 7–18.\n- <span id=\"page-11-6\"></span>[3] G. Antoniou, H. Volos,\
    \ D. B. Bartolini, T. Rollet, Y. Sazeides, and J. H. Yahya, \"Agilepkgc: An agile\
    \ system idle state architecture for energy proportional datacenter servers,\"\
    \ in 2022 55th IEEE/ACM International Symposium on Microarchitecture (MICRO),\
    \ 2022, pp. 851–867.\n- <span id=\"page-11-4\"></span>[4] E. Asyabi, A. Bestavros,\
    \ E. Sharafzadeh, and T. Zhu, \"Peafowl: Inapplication CPU Scheduling to Reduce\
    \ Power Consumption of Inmemory Key-value Stores,\" in SoCC, 2020.\n- <span id=\"\
    page-11-3\"></span>[5] B. Atikoglu, Y. Xu, E. Frachtenberg, S. Jiang, and M. Paleczny,\
    \ \"Workload analysis of a large-scale key-value store,\" in Proceedings of the\
    \ 12th ACM SIGMETRICS/PERFORMANCE Joint International Conference on Measurement\
    \ and Modeling of Computer Systems, ser. SIGMETRICS '12. New York, NY, USA: Association\
    \ for Computing Machinery, 2012, p. 53–64. [Online]. Available:<https://doi.org/10.1145/2254756.2254766>\n\
    - <span id=\"page-11-0\"></span>[6] A. Belay, G. Prekas, M. Primorac, A. Klimovic,\
    \ S. Grossman, C. Kozyrakis, and E. Bugnion, \"The ix operating system: Combining\
    \ low latency, high throughput, and efficiency in a protected dataplane,\" ACM\
    \ Trans. Comput. Syst., vol. 34, no. 4, dec 2016. [Online]. Available: <https://doi.org/10.1145/2997641>\n\
    - <span id=\"page-11-1\"></span>[7] C.-H. Chou, L. N. Bhuyan, and D. Wong, \"\
    µDPM: Dynamic Power Management for the Microsecond Era,\" in HPCA, 2019.\n- <span\
    \ id=\"page-11-5\"></span>[8] C.-H. Chou, D. Wong, and L. N. Bhuyan, \"Dynsleep:\
    \ Fine-grained power management for a latency-critical data center application,\"\
    \ in Proceedings of the 2016 International Symposium on Low Power Electronics\
    \ and Design, ser. ISLPED '16. New York, NY, USA: Association for Computing Machinery,\
    \ 2016, p. 212–217. [Online]. Available:<https://doi.org/10.1145/2934583.2934616>\n\
    - <span id=\"page-11-9\"></span>[9] Colin Ian King, \"Stress-ng,\" online, accessed\
    \ June 2024, Oct 21, https://kernel.ubuntu.com/ cking/stress-ng/.\n- <span id=\"\
    page-11-10\"></span>[10] M. Curiel and A. Pont, \"Workload generators for web-based\
    \ systems: Characteristics, current status, and challenges,\" IEEE Communications\
    \ Surveys and Tutorials, vol. 20, no. 2, pp. 1526–1546, 2018.\n- <span id=\"page-11-11\"\
    ></span>[11] D. Duplyakin, A. Uta, A. Maricq, and R. Ricci, \"In datacenter performance,\
    \ the only constant is change,\" in 2020 20th IEEE/ACM International Symposium\
    \ on Cluster, Cloud and Internet Computing (CCGRID). Los Alamitos, CA, USA: IEEE\
    \ Computer Society, may 2020, pp. 370–379. [Online]. Available: [https://doi.ieeecomputersociety.org/10.](https://doi.ieeecomputersociety.org/10.1109/CCGrid49817.2020.00-56)\
    \ [1109/CCGrid49817.2020.00-56](https://doi.ieeecomputersociety.org/10.1109/CCGrid49817.2020.00-56)\n\
    - <span id=\"page-11-7\"></span>[12] D. Duplyakin, N. Ramesh, C. Imburgia, H.\
    \ F. A. Sheikh, S. Jain, P. Tekta, A. Maricq, G. Wong, and R. Ricci, \"Avoiding\
    \ the ordering trap in systems performance measurement,\" in 2023 USENIX Annual\
    \ Technical Conference (USENIX ATC 23). Boston, MA: USENIX Association, Jul. 2023,\
    \ pp. 373–386. [Online]. Available: <https://www.usenix.org/conference/atc23/presentation/duplyakin>\n\
    - <span id=\"page-12-15\"></span>[13] D. Duplyakin, R. Ricci, A. Maricq, G. Wong,\
    \ J. Duerig, E. Eide, L. Stoller, M. Hibler, D. Johnson, K. Webb, A. Akella, K.\
    \ Wang, G. Ricart, L. Landweber, C. Elliott, M. Zink, E. Cecchet, S. Kar, and\
    \ P. Mishra, \"The design and operation of CloudLab,\" in Proceedings of the USENIX\
    \ Annual Technical Conference (ATC), Jul. 2019, pp. 1–14. [Online]. Available:<https://www.flux.utah.edu/paper/duplyakin-atc19>\n\
    - <span id=\"page-12-2\"></span>[14] Y. Gan, Y. Zhang, D. Cheng, A. Shetty, P.\
    \ Rathi, N. Katarki, A. Bruno, J. Hu, B. Ritchken, B. Jackson, K. Hu, M. Pancholi,\
    \ Y. He, B. Clancy, C. Colen, F. Wen, C. Leung, S. Wang, L. Zaruvinsky, M. Espinosa,\
    \ R. Lin, Z. Liu, J. Padilla, and C. Delimitrou, \"An open-source benchmark suite\
    \ for microservices and their hardware-software implications for cloud & edge\
    \ systems,\" in Proceedings of the Twenty-Fourth International Conference on Architectural\
    \ Support for Programming Languages and Operating Systems, ser. ASPLOS '19. New\
    \ York, NY, USA: Association for Computing Machinery, 2019, p. 3–18. [Online].\
    \ Available:<https://doi.org/10.1145/3297858.3304013>\n- <span id=\"page-12-5\"\
    ></span>[15] A. Gendler, E. Knoll, and Y. Sazeides, \"I-dvfs: Instantaneous frequency\
    \ switch during dynamic voltage and frequency scaling,\" IEEE Micro, vol. 41,\
    \ no. 5, pp. 76–84, 2021.\n- <span id=\"page-12-17\"></span>[16] C. Gough, I.\
    \ Steiner, and W. Saunders, Energy Efficient Servers: Blueprints for Data Center\
    \ Optimization. Apress Berkeley, CA, 01 2015.\n- <span id=\"page-12-31\"></span>[17]\
    \ T. Hoefler and R. Belli, \"Scientific benchmarking of parallel computing systems:\
    \ twelve ways to tell the masses when reporting performance results,\" in SC '15:\
    \ Proceedings of the International Conference for High Performance Computing,\
    \ Networking, Storage and Analysis, 2015, pp. 1–12.\n- <span id=\"page-12-14\"\
    ></span>[18] R. Jain, The Art of Computer Systems Performance Analysis: Techniques\
    \ For Experimental Design, Measurement, Simulation, and Modeling, NY: Wiley, 04\
    \ 1991.\n- <span id=\"page-12-21\"></span>[19] James Lewis and Martin Fowler,\
    \ \"Microservices,\" online, accessed June 2024, March 2014, https://martinfowler.com/articles/microservices.html.\n\
    - <span id=\"page-12-29\"></span>[20] I. Jimenez, C. Maltzahn, J. Lofstead, A.\
    \ Moody, K. Mohror, R. Arpaci-Dusseau, and A. Arpaci-Dusseau, \"Characterizing\
    \ and reducing crossplatform performance variability using os-level virtualization,\"\
    \ in 2016 IEEE International Parallel and Distributed Processing Symposium Workshops\
    \ (IPDPSW), 2016, pp. 1077–1080.\n- <span id=\"page-12-10\"></span>[21] T. Kalibera,\
    \ L. Bulej, and P. Tuma, \"Benchmark Precision and Random Initial State,\" in\
    \ Proc. International Symposium on Performance Evaluation of Computer and Telecommunication\
    \ Systems (SPECTS). SCS, 2005, pp. 484–490.\n- <span id=\"page-12-0\"></span>[22]\
    \ H. Kasture and D. Sanchez, \"Tailbench: a benchmark suite and evaluation methodology\
    \ for latency-critical applications,\" in 2016 IEEE International Symposium on\
    \ Workload Characterization (IISWC), 2016, pp. 1–10.\n- <span id=\"page-12-18\"\
    ></span>[23] Kernel Development Team, \"CPU Performance Scaling - The Linux Kernel\
    \ documentation.\" online, accessed June 2024 [https://www.kernel.](https://www.kernel.org/doc/html/v4.14/admin-guide/pm/cpufreq.html)\
    \ [org/doc/html/v4.14/admin-guide/pm/cpufreq.html.](https://www.kernel.org/doc/html/v4.14/admin-guide/pm/cpufreq.html)\n\
    - <span id=\"page-12-8\"></span>[24] M. Kogias, S. Mallon, and E. Bugnion, \"\
    Lancet: A self-correcting latency measuring tool,\" in 2019 USENIX Annual Technical\
    \ Conference (USENIX ATC 19). Renton, WA: USENIX Association, Jul. 2019, pp. 881–896.\
    \ [Online]. Available: [https://www.usenix.org/conference/atc19/](https://www.usenix.org/conference/atc19/presentation/kogias-lancet)\
    \ [presentation/kogias-lancet](https://www.usenix.org/conference/atc19/presentation/kogias-lancet)\n\
    - <span id=\"page-12-9\"></span>[25] J.-Y. Le Boudec, Performance Evaluation of\
    \ Computer and Communication Systems. EPFL Press, 2010.\n- <span id=\"page-12-3\"\
    ></span>[26] J. Leverich and C. Kozyrakis, \"Reconciling high server utilization\
    \ and sub-millisecond quality-of-service,\" in Proceedings of the Ninth European\
    \ Conference on Computer Systems, ser. EuroSys '14. New York, NY, USA: Association\
    \ for Computing Machinery, 2014. [Online]. Available: <https://doi.org/10.1145/2592798.2592821>\n\
    - <span id=\"page-12-12\"></span>[27] G. Liu, H. Zhang, M. Feng, L. Wong, and\
    \ S.-K. Ng, \"Supporting exploratory hypothesis testing and analysis,\" ACM Trans.\
    \ Knowl. Discov. Data, vol. 9, no. 4, jun 2015. [Online]. Available: [https:](https://doi.org/10.1145/2701430)\
    \ [//doi.org/10.1145/2701430](https://doi.org/10.1145/2701430)\n- <span id=\"\
    page-12-25\"></span>[28] S. Luo, H. Xu, C. Lu, K. Ye, G. Xu, L. Zhang, Y. Ding,\
    \ J. He, and C. Xu, \"Characterizing microservice dependency and performance:\
    \ Alibaba trace analysis,\" in Proceedings of the ACM Symposium on Cloud Computing,\
    \ ser. SoCC '21. New York, NY, USA: Association for Computing Machinery, 2021,\
    \ p. 412–426. [Online]. Available: <https://doi.org/10.1145/3472883.3487003>\n\
    - <span id=\"page-12-11\"></span>[29] A. Maricq, D. Duplyakin, I. Jimenez, C.\
    \ Maltzahn, R. Stutsman, and R. Ricci, \"Taming performance variability,\" in\
    \ 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18).\
    \ Carlsbad, CA: USENIX Association, Oct. 2018, pp. 409–425. [Online]. Available:\
    \ <https://www.usenix.org/conference/osdi18/presentation/maricq>\n- <span id=\"\
    page-12-24\"></span>[30] A. Mirhosseini, B. L. West, G. W. Blake, and T. F. Wenisch,\
    \ \"Qzilla: A scheduling framework and core microarchitecture for tailtolerant\
    \ microservices,\" in 2020 IEEE International Symposium on High Performance Computer\
    \ Architecture (HPCA), 2020, pp. 207–219.\n- <span id=\"page-12-26\"></span>[31]\
    \ T. Mytkowicz, A. Diwan, M. Hauswirth, and P. F. Sweeney, \"Producing wrong data\
    \ without doing anything obviously wrong!\" in Proceedings of the 14th International\
    \ Conference on Architectural Support for Programming Languages and Operating\
    \ Systems, ser. ASPLOS XIV. New York, NY, USA: Association for Computing Machinery,\
    \ 2009, p. 265–276. [Online]. Available:<https://doi.org/10.1145/1508244.1508275>\n\
    - <span id=\"page-12-6\"></span>[32] P. Nikolaou, Y. Sazeides, A. Lampropulos,\
    \ D. Guilhot, A. Bartoli, G. Papadimitriou, A. Chatzidimitriou, D. Gizopoulos,\
    \ K. Tovletoglou, L. Mukhanov, and G. Karakonstantis, \"On the evaluation of the\
    \ totalcost-of-ownership trade-offs in edge vs cloud deployments: A wirelessdenial-of-service\
    \ case study,\" IEEE Transactions on Sustainable Computing, vol. 7, no. 2, pp.\
    \ 334–345, 2022.\n- <span id=\"page-12-7\"></span>[33] P. Nikolaou, Y. Sazeides,\
    \ A. Lampropulos, D. Guilhot, A. Bartoli, G. Papadimitriou, A. Chatzidimitriou,\
    \ D. Gizopoulos, K. Tovletoglou, L. Mukhanov, G. Karakonstantis, M. Kleanthous,\
    \ and A. Prat, Total Cost of Ownership Perspective of Cloud vs Edge Deployments\
    \ of IoT Applications. Cham: Springer International Publishing, 2022, pp. 141–161.\
    \ [Online]. Available: [\"https://doi.org/10.1007/978-3-030-74536-3\\\\_6\"](\"\
    https://doi.org/10.1007/978-3-030-74536-3_6\")\n- <span id=\"page-12-30\"></span>[34]\
    \ G. Prasad Srinivasa, S. Haseley, G. Challen, and M. Hempstead, \"Quantifying\
    \ process variations and its impacts on smartphones,\" in 2019 IEEE International\
    \ Symposium on Performance Analysis of Systems and Software (ISPASS), 2019, pp.\
    \ 117–126.\n- <span id=\"page-12-22\"></span>[35] Rob Brigham, \"DevOps at Amazon:\
    \ A Look at Our Tools and Processes.\" online, accessed June 2024, https://www.slideshare.net/AmazonWebServices/devops-at-amazon-alook-at-our-tools-and-processes.\n\
    - <span id=\"page-12-16\"></span>[36] R. A. Rossi and N. K. Ahmed, \"The network\
    \ data repository with interactive graph analytics and visualization,\" in AAAI,\
    \ 2015. [Online]. Available:<https://networkrepository.com>\n- <span id=\"page-12-13\"\
    ></span>[37] S. S. Shapiro and M. B. Wilk, \"An analysis of variance test for\
    \ normality (complete samples),\" Biometrika, vol. 52, no. 3/4, pp. 591–611, 1965.\
    \ [Online]. Available:<http://www.jstor.org/stable/2333709>\n- <span id=\"page-12-1\"\
    ></span>[38] A. Sriraman and T. F. Wenisch, \"µ Suite: A Benchmark Suite for Microservices,\"\
    \ in 2018 IEEE International Symposium on Workload Characterization (IISWC). Washington,\
    \ DC, USA: IEEE, 2018, pp. 1–12.\n- <span id=\"page-12-33\"></span>[39] A. Suresh\
    \ and A. Gandhi, \"Using variability as a guiding principle to reduce latency\
    \ in web applications via os profiling,\" in The World Wide Web Conference, ser.\
    \ WWW '19. New York, NY, USA: Association for Computing Machinery, 2019, p. 1759–1770.\
    \ [Online]. Available: <https://doi.org/10.1145/3308558.3313406>\n- <span id=\"\
    page-12-20\"></span>[40] K. D. Team, \"No\\_hz: Reducing scheduling-clock ticks.\"\
    \ online, accessed June 2024 [https://docs.kernel.org/timers/no\\\\_hz.html.](https://docs.kernel.org/timers/no_hz.html)\n\
    - <span id=\"page-12-27\"></span>[41] D. Tsafrir, K. Ouaknine, and D. G. Feitelson,\
    \ \"Reducing performance evaluation sensitivity and variability by input shaking,\"\
    \ in 2007 15th International Symposium on Modeling, Analysis, and Simulation of\
    \ Computer and Telecommunication Systems, 2007, pp. 231–237.\n- <span id=\"page-12-19\"\
    ></span>[42] D. M. Tullsen, S. J. Eggers, and H. M. Levy, \"Simultaneous multithreading:\
    \ maximizing on-chip parallelism,\" SIGARCH Comput. Archit. News, vol. 23, no.\
    \ 2, p. 392–403, may 1995. [Online]. Available: <https://doi.org/10.1145/225830.224449>\n\
    - <span id=\"page-12-23\"></span>[43] Twitter, \"Decomposing Twitter: Adventures\
    \ in Service Oriented Architecture.\" online, accessed June 2024, www.slideshare.net/InfoQ/decomposing-twitter-adventures-inserviceoriented-architecture.\n\
    - <span id=\"page-12-28\"></span>[44] A. Uta, A. Custura, D. Duplyakin, I. Jimenez,\
    \ J. Rellermeyer, C. Maltzahn, R. Ricci, and A. Iosup, \"Is big data performance\
    \ reproducible in modern cloud networks?\" in 17th USENIX Symposium on Networked\
    \ Systems Design and Implementation (NSDI 20). Santa Clara, CA: USENIX Association,\
    \ Feb. 2020, pp. 513–527. [Online]. Available: <https://www.usenix.org/conference/nsdi20/presentation/uta>\n\
    - <span id=\"page-12-32\"></span>[45] C. Wang, K. Viswanathan, L. Choudur, V.\
    \ Talwar, W. Satterfield, and K. Schwan, \"Statistical techniques for online anomaly\
    \ detection in data centers,\" in 12th IFIP/IEEE International Symposium on Integrated\
    \ Network Management (IM 2011) and Workshops, 2011, pp. 385–392.\n- <span id=\"\
    page-12-4\"></span>[46] Y. Wang, K. Arya, M. Kogias, M. Vanga, A. Bhandari, N.\
    \ J. Yadwadkar, S. Sen, S. Elnikety, C. Kozyrakis, and R. Bianchini, \"Smartharvest:\
    \ harvesting idle cpus safely and efficiently in the cloud,\" in Proceedings of\
    \ the Sixteenth European Conference on Computer Systems, ser. EuroSys '21. New\
    \ York, NY, USA: Association for Computing Machinery, 2021, p. 1–16. [Online].\
    \ Available:<https://doi.org/10.1145/3447786.3456225>\n- <span id=\"page-13-3\"\
    ></span>[47] N. J. Wright, S. Smallen, C. M. Olschanowsky, J. Hayes, and A. Snavely,\
    \ \"Measuring and understanding variation in benchmark performance,\" in 2009\
    \ DoD High Performance Computing Modernization Program Users Group Conference,\
    \ 2009, pp. 438–443.\n- <span id=\"page-13-2\"></span>[48] J. H. Yahya, H. Volos,\
    \ D. B. Bartolini, G. Antoniou, J. S. Kim, Z. Wang, K. Kalaitzidis, T. Rollet,\
    \ Z. Chen, Y. Geng, O. Mutlu, and Y. Sazeides, \"Agilewatts: An energy-efficient\
    \ cpu core idle-state architecture for latency-sensitive server applications,\"\
    \ in 2022 55th IEEE/ACM International Symposium on Microarchitecture (MICRO),\
    \ 2022, pp. 835–850.\n- <span id=\"page-13-0\"></span>[49] X. Zhan, R. Azimi,\
    \ S. Kanev, D. Brooks, and S. Reda, \"CARB: A C-state Power Management Arbiter\
    \ for Latency-critical Workloads,\" CAL, 2016.\n- <span id=\"page-13-4\"></span>[50]\
    \ Y. Zhang, D. Meisner, J. Mars, and L. Tang, \"Treadmill: attributing the source\
    \ of tail latency through precise load testing and statistical inference,\" in\
    \ Proceedings of the 43rd International Symposium on\n\nComputer Architecture,\
    \ ser. ISCA '16. IEEE Press, 2016, p. 456–468. [Online]. Available:<https://doi.org/10.1109/ISCA.2016.47>\n\
    \n- <span id=\"page-13-5\"></span>[51] Y. Zhao, D. Duplyakin, R. Ricci, and A.\
    \ Uta, \"Cloud performance variability prediction,\" in Companion of the ACM/SPEC\
    \ International Conference on Performance Engineering, ser. ICPE '21. New York,\
    \ NY, USA: Association for Computing Machinery, 2021, p. 35–40. [Online]. Available:<https://doi.org/10.1145/3447545.3451182>\n\
    - <span id=\"page-13-1\"></span>[52] X. Zhou, X. Peng, T. Xie, J. Sun, C. Xu,\
    \ C. Ji, and W. Zhao, \"Benchmarking microservice systems for software engineering\
    \ research,\" in Proceedings of the 40th International Conference on Software\
    \ Engineering: Companion Proceeedings, ICSE 2018, Gothenburg, Sweden, May 27 -\
    \ June 03, 2018, M. Chaudron, I. Crnkovic, M. Chechik, and M. Harman, Eds. ACM,\
    \ 2018, pp. 323–324. [Online]. Available: <https://doi.org/10.1145/3183440.3194991>"
- title: "An O(m+n)-Space Spatiotemporal Denoising Filter with Cache-Like Memories\n\
    \  for Dynamic Vision Sensors"
  abstract: 'Dynamic vision sensor (DVS) is novel neuromorphic imaging device that

    generates asynchronous events. Despite the high temporal resolution and high

    dynamic range features, DVS is faced with background noise problem.

    Spatiotemporal filter is an effective and hardware-friendly solution for DVS

    denoising but previous designs have large memory overhead or degraded

    performance issues. In this paper, we present a lightweight and real-time

    spatiotemporal denoising filter with set-associative cache-like memories, which

    has low space complexity of \text{O(m+n)} for DVS of $m\times n$ resolution. A

    two-stage pipeline for memory access with read cancellation feature is proposed

    to reduce power consumption. Further the bitwidth redundancy for event storage

    is exploited to minimize the memory footprint. We implemented our design on

    FPGA and experimental results show that it achieves state-of-the-art

    performance compared with previous spatiotemporal filters while maintaining low

    resource utilization and low power consumption of about 125mW to 210mW at

    100MHz clock frequency.'
  url: http://arxiv.org/abs/2410.12423v1
  keywords: ''
  document: "# An O(m+n)-Space Spatiotemporal Denoising Filter with Cache-Like Memories\
    \ for Dynamic Vision Sensors\n\n[Qinghang Zhao, Jiaqi Wang, Yixi Ji, Jinjian Wu,\
    \ and Guangming Shi](https://orcid.org/0000-0003-0116-8975) Xidian University,\
    \ China\n\nqhzhao@xidian.edu.cn\n\n#### Abstract\n\nDynamic vision sensor (DVS)\
    \ is novel neuromorphic imaging device that generates asynchronous events. Despite\
    \ the high temporal resolution and high dynamic range features, DVS is faced with\
    \ background noise problem. Spatiotemporal filter is an effective and hardware-friendly\
    \ solution for DVS denoising but previous designs have large memory overhead or\
    \ degraded performance issues. In this paper, we present a lightweight and real-time\
    \ spatiotemporal denoising filter with set-associative cache-like memories, which\
    \ has low space complexity of O(m+n) for DVS of × resolution. A two-stage pipeline\
    \ for memory access with read cancellation feature is proposed to reduce power\
    \ consumption. Further the bitwidth redundancy for event storage is exploited\
    \ to minimize the memory footprint. We implemented our design on FPGA and experimental\
    \ results show that it achieves state-of-the-art performance compared with previous\
    \ spatiotemporal filters while maintaining low resource utilization and low power\
    \ consumption of about 125mW to 210mW at 100MHz clock frequency.\n\n### CCS Concepts\n\
    \n• Hardware → Reconfigurable logic applications; Design modules and hierarchy;\
    \ • Computer systems organization → Embedded hardware.\n\n#### Keywords\n\nDynamic\
    \ Vision Sensor, Spatiotemporal Filter, Denoising, Memory Architecture, FPGA\n\
    \n#### ACM Reference Format:\n\nQinghang Zhao, Jiaqi Wang, Yixi Ji, Jinjian Wu,\
    \ and Guangming Shi. 2024. An O(m+n)-Space Spatiotemporal Denoising Filter with\
    \ Cache-Like Memories for Dynamic Vision Sensors. In IEEE/ACM International Conference\
    \ on Computer-Aided Design (ICCAD '24), October 27–31, 2024, New York, NY, USA.\
    \ ACM, New York, NY, USA, [9](#page-8-0) pages.<https://doi.org/10.1145/3676536.3676710>\n\
    \n#### 1 Introduction\n\nDynamic vision sensor (DVS) [\\[1\\]](#page-8-1) is a\
    \ kind of novel neuromorphic imaging device which is inspired by the principle\
    \ of biological retina. The pixel of DVS asynchronously generates bipolar ON or\
    \ OFF event when the change of luminous intensity of the receptive\n\nICCAD '24,\
    \ October 27–31, 2024, New York, NY, USA\n\n<https://doi.org/10.1145/3676536.3676710>\n\
    \nregion exceeds the positive or negative threshold. Hence the output of DVS is\
    \ usually represented as a quadruple (, , , ), which are column and row coordinate\
    \ of the pixel, timestamp, and polarity of the event, respectively. From the perspective\
    \ of working mechanism, the imaging process of DVS is in differential way. In\
    \ contrast, conventional CMOS image sensors (CIS) are based on the integral principle,\
    \ in which the capacitor in each pixel preserves the charge accumulation related\
    \ to the absolute light intensity during exposure time and the charge is further\
    \ converted to digital signal frame-by-frame synchronously. The period of charge\
    \ accumulation constrains the frame rate to tens for typical CIS. Besides, when\
    \ light intensity in some region is very high or low, the corresponding pixels\
    \ in CIS are difficult to yield valid signal, resulting in a moderate dynamic\
    \ range (<100dB). In comparison, owing to its differential imaging and asynchronous\
    \ readout characteristics, DVS has the advantages of high temporal resolution\
    \ (~s), high dynamic range (120dB - 160dB), and low power consumption [\\[1,](#page-8-1)\
    \ [2\\]](#page-8-2). In addition, the asynchronous output of DVS inherently fits\
    \ with Spiking Neural Network (SNN). Therefore, DVS has received a lot of attention\
    \ and been extensively exploited for a variety of computer vision tasks, such\
    \ as object recognition [\\[3\\]](#page-8-3), object tracking [\\[4\\]](#page-8-4),\
    \ video deblurring [\\[5\\]](#page-8-5), video frame interpolation [\\[6\\]](#page-8-6),\
    \ etc.\n\nHowever, along with the merits, the differential imaging manner of DVS\
    \ makes it more sensitive to the background activity (BA) caused by thermal noise\
    \ and junction leakage currents [\\[7\\]](#page-8-7), which has significant impact\
    \ on the quality of output signal and induces the communication bandwidth and\
    \ power consumption overhead. Therefore, denoising is of particular significance\
    \ in research of DVS. The fundamental principle of denoising is that the valid\
    \ events in the stream show spatiotemporal correlation and noise doesn't, since\
    \ the motion of object is continuous in space and time while noise is random.\
    \ In consideration of this characteristic, various methods have been proposed\
    \ to deal with the denoising problem of DVS. Spatiotemporal filter is an effective\
    \ way and is also easy to implement with hardware for online processing. The principal\
    \ is that if the event has correlation with antecedent ones within certain space\
    \ and time window, it passes the filter and is regarded as valid signal. Therefore,\
    \ maintaining a record of the coordinates and timestamps of events is vital. In\
    \ work [\\[8\\]](#page-8-8), the memory units are as as many as the sensor pixels\
    \ which are used to preserve the timestamp of most-recent event for correlation\
    \ assessment. This method achieves good performance but the memory overhead is\
    \ significant since the space complexity is O(mn) for a DVS of × resolution. The\
    \ work [\\[7\\]](#page-8-7) improves the spatiotemporal filter design taking into\
    \ account the number of correlated events, but still is an O(mn) scheme. In another\
    \ work [\\[9\\]](#page-8-9), two memory modules are adopted. The pixels in same\
    \ row share a common memory unit of one module and the incoming events belonging\
    \ to this row will\n\nPermission to make digital or hard copies of all or part\
    \ of this work for personal or classroom use is granted without fee provided that\
    \ copies are not made or distributed for profit or commercial advantage and that\
    \ copies bear this notice and the full citation on the first page. Copyrights\
    \ for components of this work owned by others than the author(s) must be honored.\
    \ Abstracting with credit is permitted. To copy otherwise, or republish, to post\
    \ on servers or to redistribute to lists, requires prior specific permission and/or\
    \ a fee. Request permissions from permissions@acm.org.\n\n<sup>©</sup> 2024 Copyright\
    \ held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-1077-3/24/10\n\
    \nupdate the same memory unit. And it's the same with column. This method significantly\
    \ reduces the memory complexity from O(mn) to O(m+n), but the performance is degraded.\n\
    \nOther denoising techniques can be categorized as offline methods. Specifically,\
    \ researchers have explored solutions using probabilistic undirected graph [\\\
    [10\\]](#page-8-10), event density [\\[11\\]](#page-8-11), Convolutional Neural\
    \ Network [\\[12\\]](#page-8-12), and Graph Neural Network [\\[13\\]](#page-8-13).\
    \ These methods require extensive computations and are very hard to implement\
    \ with hardware or to run on end device in real time. We focus on the online denoising\
    \ method and more detailed discussions of offline methods are beyond the scope\
    \ of this paper.\n\nIn this work, we present a novel spatiotemporal filter design\
    \ named Cache Like Filter (CLF), to resolve the contradiction of space complexity\
    \ and performance. CLF consists of symmetric Row Denoise Module and Column Denoise\
    \ Module. The modules utilize set-associative cache-like memory banks. Each block\
    \ of memory bank, analogous to a cache block, stores multiple events occurring\
    \ in same row or column of DVS. Therefore, the number of memory block is + for\
    \ a DVS of × resolution. In other words, the space complexity of CLF is O(m+n).\
    \ The detail of CLF design will be discussed in detail in Section [3.](#page-2-0)\
    \ The specific contributions of this work are listed as follows:\n\n- We propose\
    \ a novel spatiotemporal filter design, which utilizes the cache-like memories\
    \ for the first time and the space complexity is O(m+n) for a DVS of× resolution.\
    \ We prove that our design is a more generic design which includes previous spatiotemporal\
    \ filter designs. Therefore, design space can be explored to investigate the optimal\
    \ parameters for different scenarios.\n- We optimize the filter design in consideration\
    \ of the characteristics of DVS denoising. We design the pipeline structure for\
    \ memory access which employs a read cancellation technique to reduce power consumption.\
    \ Besides, based on theoretical analysis and simulations, the bitwidth of event\
    \ timestamp is reduced to minimize memory footprint.\n- We implement our design\
    \ on FPGA and conduct comprehensive experiment on both simulated data and recorded\
    \ data of DVS. The results demonstrate that the proposed method achieves comparable\
    \ and even better performance than O(mn) design while the resource utilization\
    \ and power consumption remain quite low.\n\nThe rest of this paper is organized\
    \ as follows. Section [2](#page-1-0) introduces the working principle of DVS and\
    \ how the spatiotemporal filters are used for denoising. Section [3](#page-2-0)\
    \ presents the cache-like spatiotemporal filter design and its optimizations.\
    \ Section [4](#page-4-0) reports the experimental results. Finally, Section [5](#page-7-0)\
    \ concludes this paper.\n\n#### <span id=\"page-1-0\"></span>2 Background\n\n\
    #### 2.1 Working Principle of DVS\n\nThe simplified circuit schematic of DVS pixel\
    \ is shown in Fig. [1\\(](#page-1-1)a). The pixel can be divided into three stages,\
    \ which are the photoreceptor, the charge amplifier, and the comparator. The photoreceptor\
    \ realizes photoelectric conversion. The MOSFET M1 in series with photodiode operates\
    \ in sub-threshold region. In this way, the output voltage <sup>1</sup> of this\
    \ stage is logarithmic with respect to the photocurrent , enhancing the dynamic\
    \ range of DVS. The charge amplifier\n\nemploys two capacitors to amplify the\
    \ change of <sup>1</sup> and its output <sup>2</sup> is proportional to <sup>1</sup>\
    \ 2 Δ log(). When the output voltage of charge amplifier exceeds the upper or\
    \ lower threshold, one comparator in third stage will yield high-level output,\
    \ referred to as ON or OFF event. If the control logic of DVS receives the event,\
    \ a pulse signal will be generated to reset the charge amplifier and its output\
    \ returns to default value. The process is repeated and discrete ON/OFF event\
    \ stream is generated. Fig. [1\\(](#page-1-1)b) illustrates the generation of\
    \ events with the light intensity changing. As can be seen, different with the\
    \ conventional CIS with fixed frame rate, DVS asynchronously generates bipolar\
    \ event signal and its firing rate reflects the variation of light intensity.\
    \ It can be known through the above analysis that the differential and asynchronous\
    \ nature of DVS imaging makes it high dynamic range, high time resolution, frame\
    \ free, and ultra fast. Besides, since the ADC is substituted by comparator, the\
    \ power consumption of DVS is significantly reduced.\n\n<span id=\"page-1-1\"\
    ></span>![](_page_1_Figure_13.jpeg)\n\nFigure 1: The (a) simplified circuit schematic\
    \ and (b) operational diagram of a DVS pixel.\n\n# <span id=\"page-1-2\"></span>2.2\
    \ Spatiotemporal Filters\n\nSpatiotemporal filter is a method for DVS denoising,\
    \ which is based on the principle that imaging of objects is highly correlated\
    \ in space and time, while noise is random. As shown in Fig. [2\\(](#page-2-1)a),\
    \ it is assumed that one event <sup>0</sup> is occurring at the current time <sup>0</sup>\
    \ at pixel coordinate (0, 0) with polarity 0, which is denoted by quadruple (0,\
    \ 0, 0,0). Event <sup>0</sup> is considered correlated with a preceding event\
    \ <sup>1</sup> denoted by (1, 1, 1, 1), if their temporal and spatial differences\
    \ fall within predefined thresholds, i.e., <sup>0</sup> − <sup>1</sup> ≤ ℎ, <sup>0</sup>\
    \ − <sup>1</sup> ≤ ℎ, and <sup>0</sup> − <sup>1</sup> ≤ ℎ. The spatiotemporal\
    \ filter determines whether an event is valid signal or noise according to the\
    \ number of correlated events <sup>0</sup> and a preset correlation criterion,\
    \ represented by the parameter . If <sup>0</sup> ≥ , the event is regarded as\
    \ valid signal; otherwise, it is categorized as noise.\n\n<span id=\"page-2-1\"\
    ></span>![](_page_2_Figure_2.jpeg)\n\nFigure 2: The (a) principle of spatiotemporal\
    \ filter and (b-d) illustration of different architectures.\n\nSpatiotemporal\
    \ filters is not only straightforward in principle but also easy to implement\
    \ with hardware. Various spatiotemporal filters have been designed in previous\
    \ work. The elementary version is shown in Fig. [2\\(](#page-2-1)b), in which\
    \ each sensor pixel corresponds to one memory unit to store the timestamp of event.\
    \ Delbruck proposed the background activity filter (BAF) [\\[8\\]](#page-8-8).\
    \ The timestamp of incoming event <sup>0</sup> compares to that of the corresponding\
    \ memory unit to determine whether it is noise or not. And then it will update\
    \ the timestamp of 3 × 3 nearest-neighbor memory units. Afterwards, Linares-Barranco\
    \ et al. [\\[14,](#page-8-14) [15\\]](#page-8-15) implemented BAF on FPGA. It\
    \ can be seen that in BAF, both the spatial threshold ℎ and the correlation criterion\
    \ is 1. Guo et al. [\\[7\\]](#page-8-7) proposed Spatiotemporal Correlation Filter\
    \ (STCF), in which can vary from 1 to 8 for 3 × 3 neighborhood, making the design\
    \ more adaptive. However, both BAF and STCF are O(mn) designs in terms of space\
    \ complexity. When sensor resolution gets high, the memory overhead gets larger.\
    \ For example, with 4 byte timestamp, the memory size for DVS of 1920×1080 resolution\
    \ [\\[16\\]](#page-8-16) will be about 3.9MB, which is rather expensive to implement\
    \ on chip. An improved spatiotemporal filter design is depicted in Fig. [2\\(](#page-2-1)c).\
    \ This scheme is referred to as Subsampling Shared Memory (SSM) because × sensor\
    \ pixels share one memory unit using subsampling technique. Except the memory\
    \ organization, SSM works in similar way to BAF and STCF. SSM reduces the memory\
    \ footprint by a factor of 2 , but still requires O(mn) memory. Khodamoradi et\
    \ al. [\\[9\\]](#page-8-9) proposed a spatiotemporal filter with O(n) space complexity\
    \ for the first time, which is known as the first noise filter that scales less\
    \ than O(n<sup>2</sup> ) in memory cost. As illustrated in Fig. [2\\(](#page-2-1)d),\
    \ two memory modules are adopted in this scheme. Each row of pixels share one\
    \ memory unit of Memory1, which stores column coordinate () and timestamp () of\
    \ incoming event in that row. Memory2 works in similar way. Considering the memory\
    \ architecture, we call it Row and Column Filter (RCF) for simplicity. The researchers\
    \ also realized RCF on FPGA and showed that is was effective for sparse DVS stream.\
    \ The RCF scheme greatly reduces memory requirement. However, when imaging scene\
    \ gets complex in which multiple objects or multiple parts of object are in motion,\
    \ RCF is not competent in correctly distinguishing signal and noise because only\
    \ one memory unit for each row or column is insufficient to record the past events.\
    \ In summary, previous spatiotemporal filter designs have made some valuable explorations\
    \ for DVS denoising. However, the design space has not been thoroughly explored,\
    \ and the trade-off between denoising performance and memory overhead requires\
    \ further considerations. In Section [3,](#page-2-0) we will elaborate on our\
    \ design to address the above research questions.\n\n#### <span id=\"page-2-0\"\
    ></span>3 Cache-Like Spatiotemporal Filter Design\n\n#### <span id=\"page-2-2\"\
    ></span>3.1 Overall Architecture\n\nTable 1: The List of Symbols and Notations.\n\
    \n| Symbol | Description                                       |\n|--------|---------------------------------------------------|\n\
    | \U0001D441\U0001D445\U0001D440    | Number of Row Memory Banks             \
    \           |\n| \U0001D441\U0001D436\U0001D440    | Number of Column Memory Banks\
    \                     |\n| \U0001D460\U0001D445\U0001D440    | Number of events\
    \ that can be stored in each block |\n|        | of row memory bank          \
    \                      |\n| \U0001D460\U0001D436\U0001D440    | Number of events\
    \ that can be stored in each block |\n|        | of column memory bank       \
    \                      |\n| \U0001D437\U0001D461ℎ    | Threshold to decide whether\
    \ events are spatially  |\n|        | correlated                             \
    \           |\n| \U0001D447\U0001D461ℎ    | Threshold to decide whether events\
    \ are temporally |\n|        | correlated                                    \
    \    |\n|        |                                                   |\n| \U0001D441\
    \U0001D436\U0001D445    | Threshold of the number of correlated events to   |\n\
    |        | decide event or noise                             |\n| \U0001D435\U0001D44A\
    \U0001D447    | Bitwidth of timestamp of event                    |\n\nFig. [3](#page-3-0)\
    \ shows the overall architecture of the proposed lightweight and hardware-friendly\
    \ spatiotemporal denoising filter, referred to as CLF. The symbols used in this\
    \ paper are listed in Table [1.](#page-2-2) The CLF consists of two symmetric\
    \ modules: the Row Denoising Module (RDM) and the Column Denoising Module (CDM),\
    \ which work in similar way. In each module, the memory organization is cache-like\
    \ and that's the meaning of CLF. We take RDM as example to explain our design\
    \ in detail. As Fig. [3](#page-3-0) shows, assuming the resolution of DVS is ×\
    \ , RDM includes independent memory banks, designated as RMB0, RMB1, . . . , RMB[\
    \ -1]. Each bank consists of <sup>l</sup> <sup>m</sup> memory blocks, which are\
    \ analogous to cache blocks. The memory is orchestrated in analogous to a set-associative\
    \ cache. If the row coordinate of one event is , the index of Row Memory Bank\
    \ to be stored is mod . For example, if = 4, the last two bits of coordinate is\
    \ exact the index of the memory set. As Fig. [3](#page-3-0) shows, the events\
    \ generated from pixels in row 0, 4, . . . , will be stored in RMB0, events in\
    \ row 1, 5, . . . , in RMB1, and so on. And the index of memory block to store\
    \ the event is <sup>j</sup> <sup>k</sup> . Each memory block stores events arising\
    \ from corresponding row. In our design, the storage location of each event in\
    \ memory block is not constrained. In this way, each Row Memory Bank behaves in\
    \ similar way with an -way set-associative cache. The difference is that in traditional\
    \ set-associative caches, each set consists of multiple cache blocks while in\
    \ the Row Memory Bank, each memory block stores multiple events. Since the mapping\
    \ relation between memory bank and memory block and pixels from certain row is\
    \ explicit, the row coordinate does not need to be saved. Therefore, the timestamp\n\
    \n<span id=\"page-3-0\"></span>![](_page_3_Figure_2.jpeg)\n\nFigure 3: The overall\
    \ architecture of the proposed spatiotemporal denoising filter.\n\n , column coordinate\
    \ , and polarity , if necessary, are to be stored in memory block. Usually is\
    \ much less than . Therefore, replacement policy of event in memory block should\
    \ be considered. Our design employs FIFO-based replacement strategy. First, it\
    \ is hardware-friendly. Besides, since the timestamp of current event is no less\
    \ than that of previous ones and more recent events are more likely to be used\
    \ for correlation judgement in future due to spatial and temporal locality, FIFO\
    \ policy behaves very much like LRU for DVS. To indicate which previous event\
    \ will be replaced, an index pointer for each memory block is required, of which\
    \ the bitwidth is ⌈2 ⌉. In our design, a small memory module named wpt is adopted\
    \ for each row memory bank, which contains the index pointer for each memory block.\
    \ Column memories are organized in similar way with row memories and the timestamp\
    \ and column index ( ) of the current event are stored in memory block. We will\
    \ leave the details to avoid repetition.\n\nThe right part of Fig. [3](#page-3-0)\
    \ illustrates the architecture of CLF and the workflow for denoising. In this\
    \ exemplary design, four Row Memory Banks are utilized ( = 4) and four events\
    \ can be stored in each memory block ( = 4). ℎ is set to be 1, which means the\
    \ previous events in 3×3 spatial window should be investigated. More specifically,\
    \ the adjacent two rows and two columns are checked. Assume the current input\
    \ event is represented as (, , , ) and lats two bits of is 00 in binary. requires\
    \ = ⌈2⌉ bits for storage, and needs = ⌈2⌉ bits. First, − 1 and + 1 are calculated,\
    \ of which the last two bits are 11 and 01, respectively. Therefore, RMB0, RMB3,\
    \ and RMB1 are to be read and the read address is most significant − 2 bits of\
    \ , − 1 and + 1, respectively. The read data from RMB0, rdata, contains information\
    \ of four previous events. This rdata is then exported to Event Decision Unit\
    \ (EDU), which first decides whether the input event is correlated with stored\
    \ events by comparing the column coordinate and timestamp difference and then\
    \ sums the correlated ones. In the meantime, the input event replaces previous\
    \ one from the memory unit in RMB0 on basis of FIFO policy. The replacement position\
    \ is indicated by wpt0 which increments by 1 at each time\n\nof memory update.\
    \ For example, if wpt0 is (11)2, it means that last update position is 3 and therefore\
    \ the input event will be written to position 0. Two other two EDUs read data\
    \ (rdatap and rdatan) from RMB3 and RMB1 and determine whether adjacent rows of\
    \ input event are carrying correlated events and calculate the sums. RDM combines\
    \ the output of three EDUs to generate total number of correlated events in all\
    \ three memory blocks. CDM works in the same way as RDM. The output of RDM and\
    \ CDM are summed up and then compared to the threshold parameter to generate final\
    \ result of CLF. It can be known that the proposed spatiotemporal filter can process\
    \ an input event and yield result in four clock cycles, indicating that our design\
    \ is a real-time scheme.\n\nFrom the above analysis, it can be known that for\
    \ 3 × 3 spatial window (ℎ = 1), three memory blocks should be simultaneously accessed\
    \ for both row and column memories. To realize this requirement, at least three\
    \ set of memory banks should be used. We employ four memory banks rather than\
    \ three for that these banks can be direct distinguished by the last two bits\
    \ of reference address and the modular operation of 3 is avoided. Similarly, if\
    \ × spatial window is adopted, the number of memory banks should be set to 2 ⌈2\
    \ ⌉ .\n\nIn fact, different spatiotemporal filters introduced in Section [2.2](#page-1-2)\
    \ can be regarded as specific design cases of CLF. If , , , and are all set to\
    \ 1, CLF degrades into RCF. When CDM is not used, is set to 1, and is set to ,\
    \ but the stored location of event is constrained to specific position of each\
    \ memory block, CLF degrades into BAF or STCF. Further, if multiple rows of pixels\
    \ are mapped to the same memory block, CLF becomes SSM. From this perspective,\
    \ our spatiotemporal filter design is more generic and therefore design space\
    \ exploration will be more effective.\n\n### 3.2 Pipeline Architecture for Memory\
    \ Access\n\nFor 3 × 3 spatial window, theoretically the correlation of the input\
    \ is more likely confirmed with the events in the same row or column since they\
    \ account for 5/9 of the neighbors. Considering this, we design the two-stage\
    \ pipeline architecture for memory access. As Fig. [3](#page-3-0) shows, RDM can\
    \ optionally adopt three registers, which are marked with dashed box, for cycle\
    \ delay of data and pipelining realization. Assume that four events, denoted as\
    \ 1-4, stream in CLF and the corresponding memory blocks reside in RMB0, RMB1,\
    \ and RMB3, respectively, the memory access process is illustrated in Fig. [4.](#page-4-1)\
    \ In the first stage of processing 1, RMB0 is read which is determined by 1. In\
    \ the second stage of 1, the memory block of RMB0 read in the first stage is updated\
    \ and memory blocks of RMB1 and RMB3 correspond to 1−1 and 1+1, respectively,\
    \ are read. In the meantime, the first-stage read of 2 is proceeded, i.e. RMB0\
    \ read. Subsequent read and write operations of row memory banks involve a similar\
    \ process. In the process, if the output of EDU generated in the first stage is\
    \ not zero which means correlated event exists, the read operation of the other\
    \ two memory banks are cancelled. Otherwise, the read goes on. This technique\
    \ of read cancellation is quite beneficial to reducing the energy consumption,\
    \ especially in ASIC implementation. We know from the above explanation that the\
    \ pipeline structure is only applicable to small cases because the read cancellation\
    \ leads to imprecise summation result of EDU. Nevertheless, = 1 is effective enough,\
    \ as will be shown in Section [4.](#page-4-0) It should be noted that concurrent\
    \ reads or concurrent read and write should be taken into consideration, as Fig.\
    \ [4](#page-4-1) shows. We make use of dual-port memory to meet the requirement\
    \ of concurrent access. CDM works in same way with RDM. Despite the implementation\
    \ of the pipeline architecture, the delay time of CLF is limited to only 5 clock\
    \ cycles.\n\n<span id=\"page-4-1\"></span>![](_page_4_Figure_3.jpeg)\n\nFigure\
    \ 4: The two-stage pipeline of memory access.\n\n# 3.3 Bitwidth Reduction of Event\
    \ Timestamp\n\nIn previous work [\\[9,](#page-8-9) [14\\]](#page-8-14), the bitwidth\
    \ of timestamp stored in memory remains the same as input event, which is usually\
    \ 4 byte. In fact, the timestamp field occupies the primary part of memory block\
    \ since either row or column coordinate of DVS can be represented within 11 bits\
    \ for sensor of resolution no more than 2048×2048. As shown in Fig. [5\\(](#page-4-2)a),\
    \ assume ℎ is the time threshold to discriminate event and noise and is range\
    \ that the timestamp field can represent, then when the first of the subsequent\
    \ input events in spatial window falls in the time range of [· , · + ℎ], noise\
    \ is misjudged as signal, or in other words false positive occurs. Previous\n\n\
    <span id=\"page-4-2\"></span>![](_page_4_Figure_7.jpeg)\n\nFigure 5: The relation\
    \ of the bitwidth of timestamp and false positive rate, indicating the potential\
    \ for bitwidth reduction.\n\nworks [\\[7,](#page-8-7) [9\\]](#page-8-9) show that\
    \ the DVS noise generally complies with Poisson distribution. Taking the Poisson\
    \ probability density function into consideration, we ran simulations to inspect\
    \ the relation of false positive rate and time. It should be noted that the time\
    \ here is in form of clock cycles for general expression. As can be seen from\
    \ Figure [5\\(](#page-4-2)b), when time exceeds a certain limit, the false positive\
    \ rate can be negligible for different value of ℎ. This indicates that the bitwidth\
    \ of is no need to be very large. We also collected the statistics of the time\
    \ difference between input event and the most recent one in 3 × 3 spatial window.\
    \ The result in Fig. [5\\(](#page-4-2)c) shows that different value of noise to\
    \ event ratio has little impact on ℎ.\n\n# <span id=\"page-4-0\"></span>4 Experimental\
    \ Evaluation\n\n# 4.1 Validation with Simulated Data\n\nIn this section, we will\
    \ validate the effectiveness of our spatiotemporal filter design. We implemented\
    \ CLF on Xilinx's (now AMD) low-cost XC7A35T FPGA of Artix-7 family at 100MHz\
    \ clock, which means the delay time is only 50ns. We realized various versions\
    \ to cover different design configurations, including / , , , and . All implementations\
    \ support DVS of maximum 1280×800 resolution. The resource utilization and power\
    \ consumption of different design configurations are listed in Table [2.](#page-6-0)\
    \ Among all the implementations, the configuration of / =4, =4, =4, and =32 requires\
    \ the most resources. However, it is still quite moderate in number. Besides,\
    \ if 8-bit timestamp is used, more than 40% resources are saved compared with\
    \ 32-bit version. On the whole, our design is resource efficient, making it applicable\
    \ on low-end FPGA devices or to implement with ASIC. The total power consumption\
    \ for different configurations ranges from 125mW to 210mW, which is quite low\
    \ for FPGA.\n\nDue to the characteristics of DVS, the number of events output\
    \ by DVS is much larger than image frames generated with traditional CIS for same\
    \ scene and recording time. For example, tens of million events will be generated\
    \ in several seconds with CeleX-V in typical scenarios. Therefore, it is almost\
    \ impossible to label the event data to provide ground truth. In consideration\
    \ of this challenge, we first conduct experiments with simulated event data,\n\
    \n<span id=\"page-5-0\"></span>![](_page_5_Figure_2.jpeg)\n\nFigure 6: Visualization\
    \ of clean and noisy event data generated with v2e simulator and denoised data\
    \ with CLF.\n\n<span id=\"page-5-1\"></span>![](_page_5_Figure_4.jpeg)\n\nFigure\
    \ 7: The denoising results of CLF in relation to the number of events that can\
    \ be stored in row memory block and column memory block (ℎ=1, ℎ=200s, =1).\n\n\
    which can be well controlled to be clean or noisy. We employ the v2e simulator\
    \ [\\[17\\]](#page-8-17), which synthesizes realistic DVS data from conventional\
    \ frame based video using an accurate DVS pixel model that includes DVS nonidealities.\
    \ We select four videos which are also provided by v2e, including box-moving-2,\
    \ box-moving-white, pendulum, and gemma-hpe, covering low to high complexity of\
    \ scene. The resolution of the former two videos is 800 × 600 and the latter two\
    \ is 346 × 260. We alter the simulator parameters to obtain the clean and noisy\
    \ data with different noise-to-signal ratio. The visualization of clean and noisy\
    \ event data generated with v2e simulator is shown in Fig. [6.](#page-5-0)\n\n\
    <span id=\"page-5-2\"></span>![](_page_5_Figure_7.jpeg)\n\nFigure 8: The denoising\
    \ results of CLF in relation to the time threshold ℎ (ℎ=1, =1, =4, =4).\n\nWe\
    \ altered the design parameters of CLF and carried out thorough denoising experiments.\
    \ Here, DVS denoising is regarded as classification problem and precision (P),\
    \ recall (R), and accuracy (A) are used for evaluation metrics. To investigate\
    \ the influence of parameters more apparently, we present typical results in graphs.\
    \ Fig. [7](#page-5-1) depicts the denoising results under different value of and\
    \ , i.e., whether RDM and CDM are both used and how many events can be stored\
    \ in each memory block. We can see that if either RDM or CDM is used, it is uncertain\
    \ which one is more effective. In other words, the event denoising problem is\
    \ directional. We think it is due to the directionality of moving object's trajectory.\
    \ We can also observe that under certain degree of resource constrains\n\n| Configuration1\
    \                                                              | Resource Utilization\
    \ | Power2 |       |      |  |  |\n|-----------------------------------------------------------------------------|----------------------|--------|-------|------|--|--|\n\
    |                                                                            \
    \ | LUT<br>LUTRAM        |        | FF    | (mW) |  |  |\n| 4-4-0-32         \
    \                                                           | 5073           \
    \      | 2624   | 2009  | 125  |  |  |\n| 4-0-4-32                           \
    \                                         | 5681                 | 2560   | 2502\
    \  | 131  |  |  |\n| 4-2-2-32                                                \
    \                    | 6506                 | 2592   | 3156  | 138  |  |  |\n\
    | 4-4-4-32                                                                   \
    \ | 10756                | 5184   | 4442  | 180  |  |  |\n| 4-4-4-8          \
    \                                                           | 5894           \
    \      | 2112   | 2880  | 131  |  |  |\n| 8-4-4-32                           \
    \                                         | 8205                 | 2972   | 3503\
    \  | 210  |  |  |\n| 8-4-4-8                                                 \
    \                    | 4990                 | 1244   | 2500  | 140  |  |  |\n\
    | Available                                                                  \
    \ | 20800                | 9600   | 41600 | \\    |  |  |\n| 1 The configuration\
    \ is in the format of \U0001D441\U0001D445\U0001D440<br>/\U0001D441\U0001D436\U0001D440\
    <br>-\U0001D460\U0001D445\U0001D440<br>-\U0001D460\U0001D436\U0001D440<br>-\U0001D435\
    \U0001D44A\U0001D447 |                      |        |       |      |  |  |\n\n\
    <span id=\"page-6-0\"></span>Table 2: Resource Utilization and Power Consumption\
    \ of FPGA Implementation of CLF.\n\n<sup>1</sup> The configuration is in the format\
    \ of / - - - .\n\n = 0 means only RDM is utilized and = 0 CDM. <sup>2</sup> Estimated\
    \ with Vivado based on the implemented design.\n\n( + =4), utilizing both RDM\
    \ and CDM ( =2 and =2) may not ensure the optimal results (e.g. gemma-hpe), but\
    \ achieves balanced performance for diverse scenarios. Increasing the capacity\
    \ of memory block ( =4 and =4) can improve the performance of CLF, even though\
    \ the marginal effect is diminishing. Fig. [8](#page-5-2) shows the influence\
    \ of threshold time to decide event correlation. We can see that as ℎ increases,\
    \ the performance is improved slowly and approaches to a flat and even slightly\
    \ degrades. For the adopted datasets, ℎ of 200s to 400s is sufficient. The impact\
    \ of the ℎ and is shown in Fig. [9.](#page-6-1) Generally, spatial window of 5\
    \ × 5 (ℎ = 2) achieves higher accuracy than 3 × 3 (ℎ = 1), which is consistent\
    \ with intuition. On the contrary, increasing usually induces accuracy decline.\
    \ However, it doesn't mean that larger is of no use at all. In fact, more strict\
    \ criterion is less likely to mistake noise as valid signal. From another point\
    \ of view, =1 is efficient for typical scenarios.\n\nWe choose BAF and RCF for\
    \ comparison, which represent designs with space complexity of O(mn) and O(m+n),\
    \ respectively. It should be noted that both BAF and RCF are realized with software\
    \ for sake of fairness because the hardware implementation of same design can\
    \ vary a lot and our realization of other scheme might be inferior to the original\
    \ version. Even though, the hardware overhead of different designs can be approximately\
    \ estimated according to its space complexity. is set to 1 for all methods, which\
    \ proves to be effective as shown above. In this case, STCF is same with BAF and\
    \ we therefore leave it. As for SSM, it can be seen as memory optimized version\
    \ of BAF with downgraded performance but is still O(mn) method essentially, which\
    \ is also not discussed for simplicity. Table [3](#page-7-1) shows the results\
    \ of different spatiotemporal filter design. Here, four configurations of CLF\
    \ are adopted in term of and / . We can observe that the performance of RCF is\
    \ inferior to BAF and our design, especially for complex scene. BAF and our design\
    \ have their own preponderances. For simple scenario like box-moving-2 and box-moving-white,\
    \ BAF achieves better results. For the other two complex situations, our design\
    \ achieves better performance of accuracy. The reason we think is that compared\
    \ with BAF the limited memory capacity of CLF leaves out some\n\n<span id=\"page-6-1\"\
    ></span>![](_page_6_Figure_8.jpeg)\n\nFigure 9: The denoising results of CLF in\
    \ relation to and ℎ ( =4, =4, ℎ=200s).\n\nnoises which avoids mistaking them as\
    \ events. As for different configurations of RCF, generally speaking, more bits\
    \ the timestamp is represented, and more events the memory unit stores, the better\
    \ the performance is. However, the performance gap between 8-bit and 32-bit timestamp\
    \ is trivial for most benchmarks, and sometimes 8-bit timestamp is even better\
    \ like pendulum. Therefore, reduction the bitwidth of timestamp is a good practice\
    \ to realize compact implementation on chip.\n\n### 4.2 Validation with Recorded\
    \ DVS Data\n\nWe further validate the performance of the proposed spatiotemporal\
    \ filter design with recorded DVS data. We employ CeleX-V [\\[16\\]](#page-8-16)\
    \ DVS camera for data acquisition. The resolution of sensor is 1280 × 800, which\
    \ can be supported by FPGA implementation of RCF. As can be seen from Fig. [10,](#page-7-2)\
    \ four scenarios are selected, including vehicles running on highway (Outdoor-1),\
    \ pedestrian bridge over road (Outdoor-2), waving hand (Indoor-1), and office\
    \ environment (Indoor-2). The former three are recorded with stationary camera\
    \ while the last one moving camera. 40.62M, 4.38M, 42.10M, and 65.23M events are\
    \ collected, respectively. Then BAF, RCF, and our method are applied for denoising.\
    \ The same ℎ and spatial window of 3 × 3 are adopted for different methods. For\
    \ our design, the configuration of / =4, =4, =4, and =8 is used. 36.95%, 76.33%,\
    \ 63.46%, and 55.92% of the events are removed with our design, respectively.\
    \ The visualization of denoising results is also shown in Fig[.10.](#page-7-2)\
    \ We can see that the noise of recorded data is effectively removed. However,\
    \ the quality of visualization image is difficult to judge. In addition, no ground\
    \ truth serves as evaluation criterion to quantitatively evaluate the performance\
    \ of different methods. Therefore, we utilize the pre-trained YOLOv5 model to\
    \ perform car detection on Outdoor-1, which can reflect the effectiveness of denoising\
    \ in some degree. mAP is calculated as 0.476, 0.453, and 0.486 for BAF, RCF, and\
    \ our design, respectively. Therefore,\n\n<span id=\"page-7-2\"></span>![](_page_7_Figure_2.jpeg)\n\
    \nFigure 10: Visualization of recorded raw DVS data and denoising results of CLF.\n\
    \n<span id=\"page-7-1\"></span>\n\n| Method      |                           \
    \    | moving-box-2        |       |                     |                   \
    \  |                     | moving-box-while |                     |          \
    \            |       |       |                     |       |  |\n|-------------|-------------------------------|---------------------|-------|---------------------|---------------------|---------------------|------------------|---------------------|----------------------|-------|-------|---------------------|-------|--|\n\
    |             |                               | #Noise/#Signal=1.29 |       |\
    \ #Noise/#Signal=6.44 |                     | #Noise/#Signal=5.47 |          \
    \        |                     | #Noise/#Signal=16.44 |       |       |      \
    \               |       |  |\n|             |                               |\
    \ P(%)                | R(%)  | A(%)                | P(%)                | R(%)\
    \                | A(%)             | P(%)                | R(%)             \
    \    | A(%)  | P(%)  | R(%)                | A(%)  |  |\n| BAF [8, 14] |     \
    \                          | 97.93               | 95.31 | 97.07             \
    \  | 87.39               | 86.27               | 96.48            | 97.37    \
    \           | 91.8                 | 98.35 | 80.35 | 79.28               | 98.59\
    \ |  |\n| RCF [9]     |                               | 97.71               |\
    \ 79.53 | 90.24               | 82.76               | 67.12               | 93.7\
    \             | 96.09               | 79.68                | 87.12 | 70.44 | 67.46\
    \               | 97.85 |  |\n|             | \U0001D435\U0001D44A\U0001D447<br>=32,\
    \ \U0001D460\U0001D445\U0001D440<br>/\U0001D460\U0001D436\U0001D440<br>=2 | 98.06\
    \               | 93.30 | 96.26               | 83.87               | 84.92  \
    \             | 95.78            | 98.21               | 87.17               \
    \ | 97.77 | 89.18 | 82.95               | 98.44 |  |\n| CLF         | \U0001D435\
    \U0001D44A\U0001D447<br>=32, \U0001D460\U0001D445\U0001D440<br>/\U0001D460\U0001D436\
    \U0001D440<br>=4 | 97.97               | 95.17 | 97.03               | 87.47 \
    \              | 85.76               | 96.44            | 98.23              \
    \ | 88.62                | 98.00 | 89.32 | 85.11               | 98.56 |  |\n\
    |             | \U0001D435\U0001D44A\U0001D447<br>=8, \U0001D460\U0001D445\U0001D440\
    <br>/\U0001D460\U0001D436\U0001D440<br>=2  | 98.47               | 86.81 | 93.64\
    \               | 87.96               | 78.51               | 95.67          \
    \  | 98.16               | 84.89                | 97.42 | 88.09 | 83.81      \
    \         | 98.42 |  |\n|             | \U0001D435\U0001D44A\U0001D447<br>=8,\
    \ \U0001D460\U0001D445\U0001D440<br>/\U0001D460\U0001D436\U0001D440<br>=4  | 98.2\
    \                | 88.97 | 94.46               | 84.59               | 86.21 \
    \              | 96.04            | 97.42               | 87.63              \
    \  | 97.73 | 87.42 | 87.42               | 98.56 |  |\n| Method      |       \
    \                        | pendulum            |       |                     |\
    \                     |                     | gemma-hpe        |             \
    \        |                      |       |       |                     |      \
    \ |  |\n|             |                               | #Noise/#Signal=0.51 |\
    \       |                     | #Noise/#Signal=1.69 |                     |  \
    \                | #Noise/#Signal=0.41 |                      |       |      \
    \ | #Noise/#Signal=1.63 |       |  |\n|             |                        \
    \       | P(%)                | R(%)  | A(%)                | P(%)           \
    \     | R(%)                | A(%)             | P(%)                | R(%)  \
    \               | A(%)  | P(%)  | R(%)                | A(%)  |  |\n| BAF [8,\
    \ 14] |                               | 98.54               | 40.92 | 60.39  \
    \             | 90.62               | 41.17               | 76.55            |\
    \ 82.58               | 25.4                 | 69.56 | 68.73 | 33.51         \
    \      | 68.89 |  |\n| RCF [9]     |                               | 98.36   \
    \            | 34.92 | 56.43               | 90.14               | 34.88     \
    \          | 74.38            | 97.79               | 46.17                | 60.99\
    \ | 84.1  | 45.01               | 75.83 |  |\n| CLF         | \U0001D435\U0001D44A\
    \U0001D447<br>=32, \U0001D460\U0001D445\U0001D440<br>/\U0001D460\U0001D436\U0001D440\
    <br>=2 | 99.04               | 39.21 | 59.39               | 95.37           \
    \    | 38.15               | 76.31            | 97.98               | 56.55  \
    \              | 68.27 | 90.01 | 52.62               | 79.74 |  |\n|         \
    \    | \U0001D435\U0001D44A\U0001D447<br>=32, \U0001D460\U0001D445\U0001D440<br>/\U0001D460\
    \U0001D436\U0001D440<br>=4 | 99.05               | 41.66 | 61.00             \
    \  | 95.60               | 41.13               | 77.40            | 97.94    \
    \           | 63.62                | 73.18 | 90.01 | 61.30               | 82.68\
    \ |  |\n|             | \U0001D435\U0001D44A\U0001D447<br>=8, \U0001D460\U0001D445\
    \U0001D440<br>/\U0001D460\U0001D436\U0001D440<br>=2  | 98.87               | 43.04\
    \ | 61.85               | 92.87               | 41.86               | 77.18  \
    \          | 98.44               | 55.55                | 67.77 | 91.03 | 52.10\
    \               | 79.81 |  |\n|             | \U0001D435\U0001D44A\U0001D447<br>=8,\
    \ \U0001D460\U0001D445\U0001D440<br>/\U0001D460\U0001D436\U0001D440<br>=4  | 98.74\
    \               | 48.71 | 65.53               | 92.62               | 48.20  \
    \             | 79.30            | 98.24               | 63.00               \
    \ | 72.89 | 89.65 | 61.23               | 82.55 |  |\n\n|  |  |  | Table 3: The\
    \ Denoising Results of Different Spatiotemporal Filters. |\n|--|--|--|---------------------------------------------------------------------|\n\
    |  |  |  |                                                                   \
    \  |\n\nwe have reason to believe that our RCF design also outperforms previous\
    \ spatiotemporal filters on real-world DVS data.\n\n# <span id=\"page-7-0\"></span>5\
    \ Conclusion\n\nThis work presents a novel spatiotemporal filter design for DVS\
    \ denoising, featuring a cache-like memory architecture that exhibits a low space\
    \ complexity of O(m+n). The pipelining structure of memory access and reduced\
    \ representation bitwidth of event further reduces power consumption and memory\
    \ capacity requirement. We implemented our design on FPGA and the experimental\
    \ results\n\nbased on both simulated data and recorded data with DVS validate\
    \ its effectiveness. The proposed spatiotemporal filter design provides a low-cost\
    \ solution for acquiring high-quality DVS event data in real time, thereby facilitating\
    \ the broader application of DVS technology.\n\n# 6 Acknowledgments\n\nThis work\
    \ was supported by the National Natural Science Foundation of China under Grant\
    \ 62104182.\n\n<span id=\"page-8-0\"></span>An O(m+n)-Space Spatiotemporal Denoising\
    \ Filter with Cache-Like Memories for Dynamic Vision Sensors ICCAD '24, October\
    \ 27–31, 2024, New York, NY, USA\n\n#### References\n\n- <span id=\"page-8-1\"\
    ></span>[1] Patrick Lichtsteiner, Christoph Posch, and Tobi Delbruck. A 128×128\
    \ 120 dB 15µs latency asynchronous temporal contrast vision sensor. IEEE Journal\
    \ of Solid-State Circuits, 43(2):566–576, 2008.\n- <span id=\"page-8-2\"></span>[2]\
    \ Thomas Finateu, Atsumi Niwa, Daniel Matolin, Koya Tsuchimoto, Andrea Mascheroni,\
    \ Etienne Reynaud, Pooria Mostafalu, Frederick Brady, Ludovic Chotard, Florian\
    \ LeGoff, Hirotsugu Takahashi, Hayato Wakabayashi, Yusuke Oike, and Christoph\
    \ Posch. A 1280×720 back-illuminated stacked temporal contrast event-based vision\
    \ sensor with 4.86µm pixels, 1.066GEPS readout, programmable event-rate controller\
    \ and compressive data-formatting pipeline. In 2020 IEEE International Solid-State\
    \ Circuits Conference - (ISSCC), pages 112–114, 2020.\n- <span id=\"page-8-3\"\
    ></span>[3] Mathias Gehrig and Davide Scaramuzza. Recurrent vision transformers\
    \ for object detection with event cameras. In Proceedings of the IEEE/CVF conference\
    \ on computer vision and pattern recognition, pages 13884–13893, 2023.\n- <span\
    \ id=\"page-8-4\"></span>[4] Nico Messikommer, Carter Fang, Mathias Gehrig, and\
    \ Davide Scaramuzza. Datadriven feature tracking for event cameras. In Proceedings\
    \ of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),\
    \ pages 5642–5651, 2023.\n- <span id=\"page-8-5\"></span>[5] Pei Zhang, Haosen\
    \ Liu, Zhou Ge, Chutian Wang, and Edmund Y. Lam. Neuromorphic imaging with joint\
    \ image deblurring and event denoising. IEEE Transactions on Image Processing,\
    \ 33:2318–2333, 2024.\n- <span id=\"page-8-6\"></span>[6] Yue Gao, Siqi Li, Yipeng\
    \ Li, Yandong Guo, and Qionghai Dai. Superfast: 200× video frame interpolation\
    \ via event camera. IEEE Transactions on Pattern Analysis and Machine Intelligence,\
    \ 45(6):7764–7780, 2023.\n- <span id=\"page-8-7\"></span>[7] Shasha Guo and Tobi\
    \ Delbruck. Low cost and latency event camera background activity denoising. IEEE\
    \ Transactions on Pattern Analysis and Machine Intelligence, 2022.\n- <span id=\"\
    page-8-8\"></span>[8] Tobi Delbruck. Frame-free dynamic digital vision. Intl.\
    \ Symp. on Secure-Life Electronics, Advanced Electronics for Quality Life and\
    \ Society, pages 21–26, 2008.\n- <span id=\"page-8-9\"></span>[9] Alireza Khodamoradi\
    \ and Ryan Kastner. O(n)-space spatiotemporal filter for reducing noise in neuromorphic\
    \ vision sensors. IEEE Transactions on Emerging Topics in Computing, 9(1):15–23,\
    \ 2021.\n- <span id=\"page-8-10\"></span>[10] Jinjian Wu, Chuanwei Ma, Leida Li,\
    \ Weisheng Dong, and Guangming Shi. Probabilistic undirected graph based denoising\
    \ method for dynamic vision sensor. IEEE Transactions on Multimedia, 9210(c):1–13,\
    \ 2020.\n- <span id=\"page-8-11\"></span>[11] Pei Zhang, Zhou Ge, Li Song, and\
    \ Edmund Y Lam. Neuromorphic imaging with density-based spatiotemporal denoising.\
    \ IEEE Transactions on Computational Imaging, 9:530–541, 2023.\n- <span id=\"\
    page-8-12\"></span>[12] R Baldwin, Mohammed Almatrafi, Vijayan Asari, and Keigo\
    \ Hirakawa. Event probability mask (EPM) and event denoising convolutional neural\
    \ network (EDnCNN) for neuromorphic cameras. In Proceedings of the IEEE/CVF Conference\
    \ on Computer Vision and Pattern Recognition, pages 1701–1710, 2020.\n- <span\
    \ id=\"page-8-13\"></span>[13] Yusra Alkendi, Rana Azzam, Abdulla Ayyad, Sajid\
    \ Javed, Lakmal Seneviratne, and Yahya Zweiri. Neuromorphic camera denoising using\
    \ graph neural networkdriven transformers. IEEE Transactions on Neural Networks\
    \ and Learning Systems, 35(3):4110–4124, 2024.\n- <span id=\"page-8-14\"></span>[14]\
    \ A. Linares-Barranco, F. Gomez-Rodriguez, V. Villanueva, L. Longinotti, and T.\
    \ Delbruck. A USB3.0 FPGA event-based filtering and tracking framework for dynamic\
    \ vision sensors. Proceedings - IEEE International Symposium on Circuits and Systems,\
    \ 2015-July:2417–2420, 2015.\n- <span id=\"page-8-15\"></span>[15] Alejandro Linares-Barranco,\
    \ Fernando Perez-Peña, Diederik Paul Moeys, Francisco Gomez-Rodriguez, Gabriel\
    \ Jimenez-Moreno, Shih-Chii Liu, and Tobi Delbruck. Low latency event-based filtering\
    \ and feature extraction for dynamic vision sensors in real-time FPGA applications.\
    \ IEEE Access, 7:134926–134942, 2019.\n- <span id=\"page-8-16\"></span>[16] Shoushun\
    \ Chen and Menghan Guo. Live demonstration: CeleX-V: A 1M pixel multi-mode event-based\
    \ sensor. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition\
    \ Workshops (CVPRW), pages 1682–1683. IEEE, 2019.\n- <span id=\"page-8-17\"></span>[17]\
    \ Yuhuang Hu, Shih-Chii Liu, and Tobi Delbruck. v2e: From video frames to realistic\
    \ DVS events. In Proceedings of the IEEE/CVF Conference on Computer Vision and\
    \ Pattern Recognition, pages 1312–1321, 2021."
- title: "Per-Bank Bandwidth Regulation of Shared Last-Level Cache for Real-Time\n\
    \  Systems"
  abstract: "Modern commercial-off-the-shelf (COTS) multicore processors have advanced\n\
    memory hierarchies that enhance memory-level parallelism (MLP), which is\ncrucial\
    \ for high performance. To support high MLP, shared last-level caches\n(LLCs)\
    \ are divided into multiple banks, allowing parallel access. However,\nuneven\
    \ distribution of cache requests from the cores, especially when requests\nfrom\
    \ multiple cores are concentrated on a single bank, can result in\nsignificant\
    \ contention affecting all cores that access the cache. Such cache\nbank contention\
    \ can even be maliciously induced -- known as cache bank-aware\ndenial-of-service\
    \ (DoS) attacks -- in order to jeopardize the system's timing\npredictability.\n\
    \  In this paper, we propose a per-bank bandwidth regulation approach for\nmulti-banked\
    \ shared LLC based multicore real-time systems. By regulating\nbandwidth on a\
    \ per-bank basis, the approach aims to prevent unnecessary\nthrottling of cache\
    \ accesses to non-contended banks, thus improving overall\nperformance (throughput)\
    \ without compromising isolation benefits of throttling.\nWe implement our approach\
    \ on a RISC-V system-on-chip (SoC) platform using\nFireSim and evaluate extensively\
    \ using both synthetic and real-world workloads.\nOur evaluation results show\
    \ that the proposed per-bank regulation approach\neffectively protects real-time\
    \ tasks from co-running cache bank-aware DoS\nattacks, and offers up to a 3.66$\\\
    times$ performance improvement for the\nthrottled benign best-effort tasks compared\
    \ to prior bank-oblivious bandwidth\nthrottling approaches."
  url: http://arxiv.org/abs/2410.14003v2
  keywords: ''
  document: '# Per-Bank Bandwidth Regulation of Shared Last-Level Cache for Real-Time
    Systems


    Connor Sullivan *University of Kansas* Lawrence, Kansas USA connor.sullivan13@ku.edu


    Alex Manley *University of Kansas* Lawrence, Kansas USA amanley97@ku.edu


    Mohammad Alian\* *Cornell University* Ithaca, New York USA malian@cornell.edu


    Heechul Yun *University of Kansas* Lawrence, Kansas USA heechul.yun@ku.edu


    *Abstract*—Modern commercial-off-the-shelf (COTS) multicore processors have advanced
    memory hierarchies that enhance memory-level parallelism (MLP), which is crucial
    for high performance. To support high MLP, shared last-level caches (LLCs) are
    divided into multiple banks, allowing parallel access. However, uneven distribution
    of cache requests from the cores, especially when requests from multiple cores
    are concentrated on a single bank, can result in significant contention affecting
    all cores that access the cache. Such cache bank contention can even be maliciously
    induced—known as cache bank-aware denial-ofservice (DoS) attacks—in order to jeopardize
    the system''s timing predictability.


    In this paper, we propose a per-bank bandwidth regulation approach for multi-banked
    shared LLC based multicore realtime systems. By regulating bandwidth on a per-bank
    basis, the approach aims to prevent unnecessary throttling of cache accesses to
    non-contended banks, thus improving overall performance (throughput) without compromising
    isolation benefits of throttling. We implement our approach on a RISC-V system-onchip
    (SoC) platform using FireSim and evaluate extensively using both synthetic and
    real-world workloads. Our evaluation results show that the proposed per-bank regulation
    approach effectively protects real-time tasks from co-running cache bank-aware
    DoS attacks, and offers up to a 3.66× performance improvement for the throttled
    benign best-effort tasks compared to prior bankoblivious bandwidth throttling
    approaches.


    ## I. INTRODUCTION


    Modern commercial-off-the-shelf (COTS) multicore processors are equipped with
    sophisticated memory hierarchies that support a high degree of memory-level parallelism
    (MLP). Because memory accesses often take significantly longer than actual computation,
    enabling high MLP across all levels of the memory hierarchy is crucial for achieving
    high performance in modern multicore architectures.


    To facilitate high MLP, shared last-level caches (LLCs) are often organized into
    multiple banks that can be independently accessed in parallel. For instance, the
    LLC of the ARM Cortex-A72 processor has two independent tag banks, each of which
    is further divided into four data banks [\[1\]](#page-11-0). Such a multi-bank
    cache design maximizes parallelism and throughput in accessing the cache, and
    is widely adopted in highperformance multicore architectures [\[1\]](#page-11-0)–[\[5\]](#page-11-1),
    including those that are used in safety-critical embedded real-time systems in
    automotive and aviation domains [\[6\]](#page-11-2), [\[7\]](#page-11-3).


    While most prior work on shared cache for real-time systems has focused on cache
    space partitioning, multiple studies have shown that partitioning cache space
    alone does not guarantee temporal isolation in accessing the cache [\[8\]](#page-11-4)–
    [\[12\]](#page-11-5). In particular, it has been shown that the performance of
    a multi-bank cache can degrade significantly when requests to the cache are unevenly
    distributed across the banks. In the worst-case scenario, when all requests are
    concentrated on a single cache bank, severe contention can arise. Such bank conflicts
    can disrupt the system''s temporal predictability and be leveraged as cache bank-aware
    denial-of-service (DoS) attacks [\[12\]](#page-11-5).


    To mitigate shared cache bank contention, the prior study [\[12\]](#page-11-5)
    suggested a software-based cache bandwidth throttling approach as a potential
    solution, which is based on MemGuard [\[13\]](#page-11-6) and uses hardware performance
    counters to monitor and regulate the LLC access bandwidth of the offending cores
    (those that generate excessive parallel requests to the LLC). However, such a
    software-based bandwidth throttling solution severely impacts the performance
    of the throttled cores. To provide sufficient isolation for the protected realtime
    task, it reportedly incurs up to 300× slowdown of the throttled tasks [\[12\]](#page-11-5),
    which may be unacceptable overhead for many applications. While hardware-based
    memory bandwidth throttling solutions [\[14\]](#page-11-7)–[\[16\]](#page-11-8),
    if used for LLC bandwidth throttling, can potentially reduce the overhead of software-based
    throttling, their effectiveness is still fundamentally limited because they are
    not aware of cache banks when regulating bandwidth, which makes them overly pessimistic.


    In this paper, we propose per-bank bandwidth regulation of shared LLCs for predictable
    and efficient use of the shared cache in multicore SoCs for real-time systems.
    Our approach is motivated by the observation that the worst-case bank contention
    arises when cache accesses are concentrated on a single cache bank rather than
    distributed across the banks. As such, instead of throttling bandwidth to the
    entire shared LLC, we apply bandwidth throttling on a per cache-bank basis to
    only throttle accesses when there is a bank conflict. This effectively multiplies
    the permissible cache access bandwidth of best-effort tasks without compromising
    the isolation benefits of bandwidth throttling to the protected real-time tasks.


    We implement the proposed per-bank throttling capability as an extension to an
    open-source hardware memory bandwidth


    <sup>\*</sup>This work was conducted while affiliated with the University of Kansas.


    regulator [\[16\]](#page-11-8) on a RISC-V system-on-chip (SoC) platform using
    Xilinx UltraScale+ VCU118 FPGA [\[17\]](#page-11-9) and FireSim [\[18\]](#page-11-10).
    We evaluate the effectiveness of the proposed approach in providing temporal isolation
    to the real-time victim tasks in the presence of cache bank-aware DoS attacks.
    We then demonstrate the efficiency benefits of per-bank regulation over prior
    approaches that throttle the aggregate bandwidth of all banks globally. We show
    that per-bank regulation can effectively protect victim tasks from the attack
    while providing best-effort tasks with up to a 3.66× performance improvement over
    the prior bank-oblivious regulation scheme.


    In summary, we make the following contributions:


    - We propose per-bank bandwidth regulation on shared LLC to effectively and efficiently
    defend against potential cache bank contention attacks (regardless of whether
    malicious or benign).

    - We present a prototype hardware design, which can be integrated into any RISC-V
    SoC that supports the standard TileLink interconnect, and analyze its ability
    to prevent cache bank-aware DoS attacks.

    - We implement our design on a realistic cycle-exact, FPGA-accelerated full-system
    simulator, and evaluate its performance improvements over prior bank-oblivious
    regulation approaches. We also provide our design as opensource[\\*](#page-1-0).


    The remainder of the paper is organized as follows. Section [II](#page-1-1) provides
    the necessary background. Section [III](#page-2-0) defines the threat model. Section
    [IV](#page-2-1) motivates the need for perbank regulation. We present our proposed
    per-bank regulation design in Section [V](#page-3-0) and the evaluation results
    in Section [VI.](#page-5-0) We discuss related work in Section [VII](#page-10-0)
    and conclude in Section [VIII.](#page-10-1)


    ## II. BACKGROUND


    <span id="page-1-1"></span>In this section we provide the necessary background
    on multi-banked caches, cache bank-aware DoS attacks, and bandwidth regulation
    methods.


    ## *A. Multi-Bank Cache Organization*


    The shared cache of a modern multicore processor is often composed of multiple
    independent banks (sometimes referred to as slices [\[19\]](#page-11-11)), which
    can be accessed in parallel. This multibank cache organization facilitates high
    MLP, which is crucial for high-performance multicore processors. In a multi-bank
    cache, a mapping function determines the bank from a given physical address. The
    mapping function can be as simple as using a subset of the memory address bits.


    Figure [1](#page-1-2) depicts the multi-bank LLC organization of the ARM Cortex-A72
    [\[1\]](#page-11-0). Note that it is comprised of two independent tag banks, each
    of which is further divided into four sub-banks called data banks. The tag banks
    are completely independent, allowing for two separate LLC accesses to be serviced
    in parallel. Likewise, the data banks allow for further interleaving of accesses.
    To index the tag and data banks,


    <span id="page-1-2"></span>![](_page_1_Figure_12.jpeg)


    Fig. 1: ARM Cortex-A72 LLC Organization [\[1\]](#page-11-0).


    physical address bits 4, 5 and 6 are used. Bit 6 indexes between the two tag banks,
    with bits 4 and 5 being used to index the data banks within each tag bank. For
    64 byte cache lines, each line is split into four sub-lines of 16 bytes that are
    striped across the data banks.


    It is important to note that these bits (4, 5 and 6) are in the lower 12 bits
    of an address, within the page offset. This means that these bits can be fully
    controlled from the user space without the need for elevated privileges or huge
    pages. With this understanding, an attacker can direct memory accesses to specific
    banks, opening the door for potential DoS attacks [\[12\]](#page-11-5).


    ## *B. Cache Bank-Aware DoS Attack*


    The feasibility of cache bank-aware DoS attacks was first demonstrated in a recent
    study [\[12\]](#page-11-5) on both ARM Cortex-A57 [\[2\]](#page-11-12) and Cortex-A72
    [\[1\]](#page-11-0) cores. Under the threat model described in Section [III,](#page-2-0)
    the study shows that by saturating a single cache bank of the shared L2 cache
    with many parallel requests, an attacker can cause up to a 10× cross-core slowdown
    on a victim task. This slowdown occurs even when the victim is running on a dedicated
    core in isolation, accessing a dedicated L2 cache (space) partition by means of
    page coloring. As the cache space is partitioned between the victim and the attacker,
    it demonstrates that the slowdown is not caused by cache evictions. Furthermore,
    it also shows that the contention occurs at the bank level—not at the bus level—as
    no slowdown is observed when the victim and attacker target separate cache banks.
    Lastly, the worst-case slowdown occurs when both the victim and the attacker access
    the same cache bank, suggesting that the cache bank bandwidth becomes the bottleneck
    in such a situation.


    ## *C. Cache Bandwidth Regulation*


    To mitigate cache bank-aware DoS attacks, the prior study [\[12\]](#page-11-5)
    proposed a software-based cache bandwidth regulation method, LLCGuard, which uses
    per-core performance counters to regulate (limit) each core''s LLC access bandwidth
    (as opposed to DRAM bandwidth regulation proposed by MemGuard [\[13\]](#page-11-6))
    at a regular time interval (e.g., 1ms). However, the software-based approach is
    known to incur very high performance cost to the throttled best-effort cores.
    Concretely, the study reports up-to 300× slowdown of the tasks on the throttled
    cores to ensure no more than 1.1× slowdown of the protected real-time tasks on
    the unregulated core. As discussed in [\[12\]](#page-11-5), part of the reason
    for such a massive performance loss is due to software implementation overhead.
    With the


    <span id="page-1-0"></span><sup>\*</sup><https://github.com/CSL-KU/per-bank-regulation-firesim>


    regulation period of 1ms, a large amount of LLC accesses can still occur in short
    bursts, which results in LLC bank contention.


    In contrast, a hardware-based cache bandwidth solution can operate at a much finer
    granularity (in cycles), which can help spread the LLC accesses more evenly across
    the entire throttle period, thereby reducing the negative performance impact of
    throttling best-effort cores. While existing hardwarebased bandwidth regulators,
    such as Intel RDT [\[14\]](#page-11-7) and ARM MPAM [\[15\]](#page-11-13), are
    mainly designed to regulate memory bandwidth, they can potentially be modified
    to regulate cache bandwidth to mitigate cache bank contention.


    Unfortunately, all aforementioned regulation schemes, both software and hardware,
    suffer from a common limitation they do not regulate at the *bank level*, where
    the actual contention occurs. Instead, they treat the entire cache (or DRAM) as
    a single resource and regulate its total access bandwidth. We henceforth refer
    to the latter as *all-bank* regulation. In the following, we show why this all-bank
    regulation is overly *pessimistic*.


    ## <span id="page-2-0"></span>III. THREAT MODEL OF CACHE BANK DOS ATTACKS


    In this work, we consider the same threat model used in [\[12\]](#page-11-5).
    That is, we assume: (1) a victim task and one or more attacker tasks are co-located
    on a multicore platform, which has a shared last-level cache (LLC) and main memory
    (DRAM); (2) the victim and the attackers are partitioned to run on dedicated CPU
    cores and LLC cache spaces; (3) the attackers have non-privileged access on the
    target platforms and can only execute code from the userspace; (4) the cache bank
    address mapping information is known beforehand either from datasheets [\[1\]](#page-11-0),
    [\[2\]](#page-11-12) or reverse engineering [\[20\]](#page-11-14), [\[21\]](#page-11-15).
    Following these conditions, our goal is to guarantee temporal isolation of the
    victim accessing the shared cache in the presence of co-scheduled attackers, while
    maximizing cache bandwidth throughput available to the attackers. ff. Bank


    ## IV. MOTIVATION


    <span id="page-2-1"></span>In this section, we first evaluate the effect of cache
    bankaware DoS attacks, synthetic workloads that generate severe cache bank contention
    [\[12\]](#page-11-5), on two embedded multicore platforms (Section [IV-A\)](#page-2-2).
    We then discuss the limitations of bankoblivious "all-bank" cache bandwidth regulation
    approaches in mitigating such attacks (Section [IV-B\)](#page-3-1).


    ## <span id="page-2-2"></span>*A. Effects of Cache Bank-Aware DoS Attacks*


    In this experiment, we use two contemporary embedded multicore platforms: Raspberry
    Pi 4 Model B [\[22\]](#page-11-16) and BeagleV Ahead [\[5\]](#page-11-1). The
    Raspberry Pi 4 is based on the Broadcom BCM2711 SoC and is equipped with four
    ARM Cortex-A72 [\[1\]](#page-11-0) cores with a 1MB shared L2 cache. Comparably,
    the BeagleV Ahead is based on the Alibaba T-Head TH1520 SoC, equipped with four
    Xuantie C910 RISC-V cores with a 1MB shared L2 cache. Table [I](#page-2-3) shows
    the basic characteristics of the two platforms.


    <span id="page-2-3"></span>


    | Platform        | Raspberry Pi 4 (B)       | Beagle V Ahead  |  |

    |-----------------|--------------------------|-----------------|--|

    | SoC             | BCM2711<br>TH1520        |                 |  |

    | Architecture    | ARMv8-A                  | RISC-V 64GC     |  |

    | CPU             | 4x Cortex-A72            | 4x Xuantie C910 |  |

    |                 | out-of-order             | out-of-order    |  |

    |                 | 1.5GHz<br>2.0GHz         |                 |  |

    |                 | 48KB(I)/32KB(D)          | 64KB(I)/64KB(D) |  |

    | Shared L2 Cache | 1MB                      | 1MB             |  |

    | Memory          | 4GB LPDDR4<br>4GB LPDDR4 |                 |  |

    |                 |                          |                 |  |


    TABLE I: COTS embedded multicore platforms.


    For software, the Pi 4 runs Raspberry Pi OS with Linux kernel 6.6, and the Beagle
    V runs Ubuntu 20.20 with Linux kernel 5.10. In both platforms, the kernels are
    patched with PALLOC [\[23\]](#page-11-17), a page coloring mechanism for Linux,
    to partition the L2 cache space equally between the victim and the attackers.


    For evaluation, we use the *BkPLL* workload from [\[12\]](#page-11-5). As both
    the victim and the attackers, *BkPLL* is a pointer chasing workload that can generate
    a configurable number of parallel memory requests targeting a specific cache bank.
    We first run the victim on one core in isolation and measure its performance.
    We then repeat the experiment in the presence of co-running attacker tasks on
    the other cores. We evaluate different combinations of target cache banks for
    the victim and the attackers: *Same Bank* refers to the case where the victim
    and the attacker target the same cache bank, whereas *Diff Bank* refers to the
    case where they target different banks.


    <span id="page-2-4"></span>![](_page_2_Figure_13.jpeg)


    Fig. 2: Effects of cache bank-aware DoS attacks


    Figure [2](#page-2-4) shows the normalized slowdowns of the victim on different
    cache bank mapping configurations. The dashed horizontal line denotes the baseline
    1.00× slowdown (in this case, solo performance). First we notice that, consistent
    with the findings in [\[12\]](#page-11-5), contention occurs at the cache bank
    level, not on the shared bus level. The victim sees no slowdown when the attackers
    target a different bank, whereas severe slowdown is observed when both target
    the same cache bank. Second, we observe up to 8.7× slowdown on the Pi 4 platform,
    which is considerably worse than the 8.3× reported worstcase slowdown on the same
    platform [\[12\]](#page-11-5). Interestingly, we find different target data bank
    selections for the victim and the attackers contribute to the increased worst-case
    slowdown. Third, the BeagleV platform shows similar trends but its worst-case
    slowdown is considerably less (3.5×) than that of the Pi 4 (8.7×). This is due
    to the differences in baseline performance—i.e., the CPU core''s ability to concurrently
    generate requests and the peak bandwidth of the cache. Note that Raspberry Pi
    4''s peak cache bandwidth is 2× higher than that of the BeagleV. In general, faster
    processors tend to suffer larger worst-case slowdowns.


    # <span id="page-3-1"></span>*B. Limitations of "All-Bank" Bandwidth Regulation*


    The results in the previous subsection show that the contention created by the
    DoS attack is not on the shared bus, but at the targeted cache bank. This indicates
    that, in order to mitigate the bank contention attack, we only need to limit (throttle)
    the traffic (bandwidth) going into the contended bank. Furthermore, the banks
    in the cache are independent of each other. As such, regulation should be applied
    on a perbank basis rather than applied unnecessarily across all banks. Unfortunately,
    existing bandwidth regulation approaches cannot be applied to individual cache
    banks.


    For example, BRU [\[16\]](#page-11-8) is a hardware-level bandwidth regulator
    inserted between the L1 caches and the shared L2 cache [\[16\]](#page-11-8). As
    such, it regulates the L2 access traffic of the subset of cores that may be executing
    the DoS attackers. However, BRU tracks all L2 access traffic, without consideration
    for the individual bank destination. In other words, it implements an "all-bank"
    bandwidth regulation scheme.


    > <span id="page-3-2"></span>Bank 1 Bank 2 Bank 1 Bank 2 Access One Access Two
    All-Bank Per-Bank Budget Budget Budget Budget Unit


    Fig. 3: All-bank vs. per-bank bandwidth regulation on multibank shared caches


    Figure [3](#page-3-2) depicts the high-level intuition illustrating why all-bank
    regulation is needlessly pessimistic. Consider two bandwidth regulation systems,
    one with all-bank regulation (left) and one with per-bank (right). Both systems
    have two cache banks. Suppose that we need to limit the traffic to 5 accesses
    to a cache bank per regulation period to mitigate the contention on the bank.
    In the case of all-bank regulation it is bank oblivious, thus the global cache
    access budget must be set to 5 accesses, to counter the worst-case where all traffic
    goes to one cache bank. This budget is deducted on every cache access, even though
    in reality, the two cache accesses are interleaved across two different banks.


    Per-bank regulation, in contrast, can apply the access budget of 5 to each bank
    separately, only then is a budget deducted when the specific bank is accessed.
    As such, when the two accesses are interleaved over the two banks, each bank''s
    budget is depleted by one, leaving a remaining budget of 4 for each bank, whereas
    only 3 would be left in the all-bank case. As more accesses are interleaved, per-bank
    regulation can provide higher aggregate bandwidth while still providing worst-case
    cache bank contention guarantees. With this intuition in mind, we now discuss
    our proposed per-bank cache bandwidth regulation system design.


    ## V. PER-BANK CACHE BANDWIDTH REGULATION


    <span id="page-3-0"></span>In this section, we describe the design and implementation
    details of the proposed per-bank cache bandwidth regulation approach.


    # *A. Design Overview*


    Our per-bank bandwidth regulation solution is implemented as an extension to an
    open-source hardware bandwidth regulator called BRU [\[16\]](#page-11-8), designed
    to drop into an SoC design between the cores/accelerators and the shared cache.
    Figure [4](#page-3-3) depicts a high-level view of our bandwidth regulation unit
    in a basic dual-core setup.


    <span id="page-3-3"></span>![](_page_3_Figure_13.jpeg)


    Fig. 4: High-level view of a regulation unit in a dual-core SoC


    BRU supports creation of multiple arbitrary domains, each of which may be composed
    of one or more cores. A domain is the primary entity that bandwidth regulation
    is applied to. In the original BRU design, each domain can be configured with
    a period (cycles) and a budget (number of memory requests). The budget is decremented
    for any memory request made to the shared cache (regardless of which bank it targets)
    and once the budget is depleted, all cores in the domain are denied access to
    the shared cache until the period expires and the budget is replenished. As discussed
    earlier, we call this all-bank regulation because it counts an access to any bank
    equally.


    Our modifications to BRU enable tracking and regulating the budget for each shared
    cache bank rather than for the entire cache. Specifically, for each request to
    the cache, we decode its destination cache bank address and charge it to the corresponding
    bank''s budget. This means, for an N bank shared cache, we have N separate bank
    budgets to keep track of. When any one bank''s budget is depleted, further access
    to the bank will be prevented(throttled) until the next period begins. Accesses
    to other banks can still occur as long as their budgets are not depleted.


    ## *B. Per-bank Bandwidth Regulation Interface*


    To enable fixed user-defined bandwidth regulation, our design exposes memory-mapped
    I/O (MMIO) registers. The *Access Budget Register (ABR)* is used to program the
    maximum number of accesses per period and the *Regulation Period Register (RPR)*
    sets the regulation period in cycles. Equation [1](#page-4-0) represents the bandwidth
    budget assigned to each bank given the values of the *ABR* and *RPR* registers.
    Note that each bank gets the budget *BW*, rather than it being evenly distributed
    among the banks. *TS* represents the *transaction size* and *f* the *clock frequency*.
    When a core accesses the cache, *TS* is equivalent to the line size (commonly
    64 bytes).


    <span id="page-4-0"></span>

    $$\text{BW} = \frac{ABR}{RPR} \times TS \times f \tag{1}$$


    The *RPR* is applied globally to all logical groupings of cores that are being
    regulated concurrently—we refer to these groupings as regulation *domains*. Each
    domain has an *ABR*, allowing unique per-domain budgets to be applied to each
    cache bank.


    Along with a budget configuration interface, fixed bandwidth regulation requires
    mechanisms to track accesses (perbank counters in our design) and regulate these
    accesses as necessary. We organize the per-bank access counters in a *Domain Control
    Interface (DCI)*, with a *Core Control Interface (CCI)* containing regulation
    enable registers and domain assignment registers.


    Domain Control Interface. Each domain has its own access counter registers. We
    denote these registers as *Bank Access Counters (BAC)*. A given domain has *N*
    of these registers, where *N* is equal to the number of banks in the shared cache.
    These registers are used solely for bandwidth regulation. Note that this interface
    also includes the user configurable per-domain *ABR* registers.


    Core Control Interface. This interface includes logic for assigning cores to domains
    and enabling regulation for a given core. Domain assignment is handled through
    the *Domain Assignment Registers (DAR)*, enabling each core to be configured to
    any one domain. A *Regulation Enable Register (RER)* is generated for each core
    and allows for regulation to be enabled or disabled seamlessly.


    Figure [5](#page-4-1) depicts the regulation unit''s control interface for an
    arbitrary quad-core system configured to have two regulation domains. In the *CCI*
    there is logic for the four cores. Cores 0-2 are assigned to Domain 0, with their
    *RERs* set high. Core 3 is


    <span id="page-4-1"></span>![](_page_4_Figure_9.jpeg)


    Fig. 5: Example of group regulation in a quad-core system. Core 0-2 belong to
    Domain 0, which is regulated. Core 3, on the other hand, belongs to Domain 1,
    which is not regulated. In this example, Domain 0 is for *best-effort* tasks while
    Domain 1 is for the *real-time* tasks.


    assigned to Domain 1, but its *RER* is kept low, meaning that the core''s accesses
    are not being regulated. Bracketed register names(i.e. [Domain]) indicate that
    they are memory mapped and user configurable.


    ## *C. Per-bank Bandwidth Monitoring Interface*


    In addition to per-bank regulation, we also provide a perbank bandwidth monitoring
    interface to enable software(OS) based fine-grained monitoring and adaptive bandwidth
    regulation capabilities. Specifically, our bandwidth regulation unit includes
    per-bank monitoring registers that are separate from the regulation interface.
    The per-bank monitoring registers form the *monitoring interface*. For every core
    in the system, a set of *N* counters is generated, where *N* is equal to the number
    of banks. Similar to typical performance counters, these counters can be reset
    and read by a user to determine the per-bank bandwidth and access pattern of a
    core.


    ## *D. Regulation and Monitoring Algorithms*


    Algorithm [1](#page-5-1) shows the pseudo-code of our regulation unit. At a high
    level, it manages the global period counter (lines 1-8), the per-bank access counters,
    and throttling (lines 9-22).


    Each clock cycle, *PeriodCounter* is incremented to advance the current regulation
    period (line 7). The *PeriodCounter* is reset when its value equals or exceeds
    the user defined *RegulationPeriod*. As part of this reset, all bank budgets are
    replenished by zeroing the *BankCounters* (lines 1-6).


    Lines 9-22 make up the main body of our regulation algorithm, with the logic being
    evaluated per-core and perbank (lines 9 and 10). In a given domain, if the budget
    of bank j is depleted, then all further accesses to that bank are stalled for
    the cores in that domain (lines 13-16). When a core sends a bank access, the corresponding
    domain''s bank access counter is incremented (lines 17-20). Specifically, bank
    accesses occur on Channel A (A(i).isAccess), a TileLink notation which we further
    explain below (Section [V-E\)](#page-5-2).


    The monitoring interface is handled similarly. Algorithm [2](#page-5-3) shows
    the pseudo-code for the monitoring interface. The logic happens per-core and per-bank
    (line 1 and 2). Lines 3-8


    ## <span id="page-5-1"></span>Algorithm 1 Per-bank Regulation Algorithm


    |     | 1: if PeriodCounter ≥ RegulationPeriod then  |

    |-----|----------------------------------------------|

    | 2:  | PeriodCounter ← 0                            |

    | 3:  | for all c in BankCounters do                 |

    | 4:  | c ← 0                                        |

    | 5:  | end for                                      |

    |     | 6: else                                      |

    | 7:  | PeriodCounter++                              |

    |     | 8: end if                                    |

    |     | 9: for i ← 0 to nCores - 1 do                |

    | 10: | for j ← 0 to nBanks - 1 do                   |

    | 11: | stall(i)(j) ← False                          |

    | 12: | AccessIsBank(i)(j) ← (j == bankBits)         |

    | 13: | if (BankCounters(Domain(i))(j)≥AccessBudget) |

    | 14: | and AccessIsBank(i)(j) then                  |

    | 15: | stall(i)(j) ← True                           |

    | 16: | end if                                       |

    | 17: | if A(i).isAccess and AccessIsBank(i)(j) then |

    | 18: | BankCounters(Domain(i))(j)++                 |

    | 19: | end if                                       |

    | 20: | end for                                      |

    |     | 21: end for                                  |


    are similar to the main body of [1,](#page-5-1) where a bank monitor counter is
    incremented when that specific bank is accessed. The difference being that there
    is no notion of domains or period in the monitoring interface.


    <span id="page-5-3"></span>


    | Algorithm 2 Per-bank Monitoring Algorithm          |  |  |

    |----------------------------------------------------|--|--|

    | 1: for i ← 0 to nCores - 1 do                      |  |  |

    | for j ← 0 to nBanks - 1 do<br>2:                   |  |  |

    | AccessIsBank(i)(j) ← (j == bankBits)<br>3:         |  |  |

    | if CoreAccess(i) and AccessIsBank(i)(j) then<br>4: |  |  |

    | BankMonitor(i)(j)++<br>5:                          |  |  |

    | end if<br>6:                                       |  |  |

    | end for<br>7:                                      |  |  |

    | 8: end for                                         |  |  |


    Note that our implementation is written in the Chisel hardware description language
    (HDL) [\[24\]](#page-11-18). This allows our design to support any number of domains
    and cache banks through configurable parameters, eliminating the need to modify
    the hardware design code.


    # <span id="page-5-2"></span>*E. Implementation*


    We implement our design using the Chipyard SoC Framework [\[25\]](#page-11-19).
    In this subsection, we discuss details of both the TileLink [\[26\]](#page-11-20)
    interconnect specification and the Rocket Chip SoC [\[27\]](#page-11-21) as they
    relate to our implementation.


    Our regulation unit interfaces with TileLink Cached (TL-C) edges. TL-C edges connect
    the cores to the shared memory subsystem and are cache coherent [\[26\]](#page-11-20).
    There are five channels of communication on TL-C edges: *A, B, C, D* and *E*.
    We focus on *Channel A*, which carries requests from the core''s private caches
    to the shared caches and memories.


    <span id="page-5-4"></span>![](_page_5_Figure_8.jpeg)


    Fig. 6: Dual-core Rocket SoC with per-bank regulation unit


    Figure [6](#page-5-4) depicts our bandwidth regulation unit in a generic dual-core
    Rocket Chip SoC design. The connections between the cores and the shared system
    bus are TL-C edges. When a data or instruction cache miss occurs in the private
    L1 caches of the cores, a request is sent over Channel A. By monitoring this channel
    we can track per-core accesses and regulate when a domain''s bank budget is depleted.


    For synchronizing messages on a given channel, TileLink uses a ready-valid interface
    for sender/receiver handshaking. To regulate a channel, we can simply set the
    ready and valid signals to low, effectively stalling the request. Channel A also
    carries information about the the memory address being requested. From this we
    can extract the bank address bits to count per-bank accesses. All TL-C channels
    going from the core to the shared system bus pass through the regulation unit.
    However, only Channel A is monitored for accesses and regulated via the ready-valid
    signals. All other channels and signals remain unmodified, passing through and
    connecting directly to the shared system bus.


    Along with connections to the system bus, the regulation unit also connects to
    the *periphery bus*. This bus is utilized by cores to read/write to the MMIO registers.
    Figure [6](#page-5-4) also depicts the *BAC* counter registers and monitoring
    interface registers.


    # VI. EVALUATION


    <span id="page-5-0"></span>In this section, we evaluate hardware bandwidth regulation''s
    ability to defend against the cache-bank DoS attack and show the benefits of per-bank
    over all-bank bandwidth regulation.


    # <span id="page-5-5"></span>*A. Experimental Setup*


    We use FireSim, an FPGA accelerated cycle-exact full system simulator [\[18\]](#page-11-10).
    This allows us to accurately evaluate the performance of the proposed hardware
    design when deployed in ASIC, which operates at a higher clock (e.g., >1GHz) while
    being simulated on a FPGA at an actual clock speed of 100MHz.


    <span id="page-6-0"></span>


    | Cores           | 1×BOOM, 1GHz, out-of-order, 3-wide, ROB:     |

    |-----------------|----------------------------------------------|

    |                 | 96, LSQ: 24/24, L1: 32K(I)/32K(D)            |

    |                 | 2×Rocket, 1GHz, in-order, L1: 16K(I)/16K(D), |

    |                 | attached with Mempress traffic generators    |

    | Shared L2 Cache | 1MB (16-way)                                 |

    | Memory          | 4GB DDR3                                     |


    TABLE II: Evaluation platform specifications


    Table [II](#page-6-0) shows the basic characteristics of the tri-core heterogeneous
    SoC we constructed on FireSim for evaluation. The SoC is composed of one out-of-order
    core, the Berkeley Out-of-Order Machine (BOOM) [\[28\]](#page-11-22), and two
    in-order Rocket [\[27\]](#page-11-21) cores, which are connected to Mempress traffic
    generators [\[29\]](#page-11-23). All cores share a 1MB L2 cache and a 4GB DDR3
    main memory subsystem.


    Note that cache bank-aware DoS attacks [\[12\]](#page-11-5) require outof-order
    CPU cores to be able to generate many concurrent memory requests on a specific
    target cache bank. As such, we initially attempted to construct a quad-core BOOM
    based SoC on FireSim using the *LargeBoom* configuration. However, due to physical
    constraints of our FPGA platform, we were unable to fit four large BOOM cores
    simultaneously in the FPGA. Furthermore, there is an unresolved bug in BOOM that
    results in a simulation hang when executing certain memory intensive workloads
    on multi-core configurations[\\*](#page-6-1). As such, in our simulation setup,
    we instead utilize the Mempress traffic generator [\[29\]](#page-11-23) to act
    as the cache bank attacker tasks.


    Mempress is a configurable hardware unit that can generate multiple parallel streams
    of requests to the shared memory at varying access patterns. Implemented as an
    on-chip RoCC accelerator [\[30\]](#page-11-24), Mempress has access to the shared
    memory subsystem. For all following experiments, the attackers will be two separate
    Mempress units targeting the same last-level cache (LLC) bank.


    The Mempress traffic generators are attached to two Rocket cores (one per core).
    Since Rocket cores are in-order, they cannot create significant contention in
    the shared cache on their own. However, Mempress enhances the cores by enabling
    them to generate parallel accesses through the traffic generators, all while still
    meeting FPGA space constraints. All targets are clocked at 1GHz.


    For the shared L2 cache, we use SiFive''s open-source inclusive cache [\[31\]](#page-11-25),
    which is a real synthesizable hardware cache design that supports a configurable
    number of cache banks. The bank mapping bits start at address bit 6 for a two
    bank design, while bits 6 and 7 are used in a four bank design. Throughout our
    experimentation, we vary the number of LLC banks to be either two or four, but
    the size and associativity remain constant. Cache lines are set to be 64 bytes.


    All simulations are run with the RISC-V version of Linux kernel 6.2. For synthetic
    workloads, we use *BkPLL* (described in section [IV-A\)](#page-2-2) and *Bandwidth*
    from [\[8\]](#page-11-4). *Bandwidth* accesses a chunk of memory sequentially,
    striding at a step size of a cache line. Both workloads can be configured to perform
    either read or write accesses. Lastly, the San Diego Vision Benchmark Suite (SD-VBS)
    [\[32\]](#page-11-26) with CIF input format is used for real-world evaluation.


    To ensure all slowdowns are solely due to bank contention and not impacted by
    set conflict misses, we apply the PALLOC patch to the Linux kernel [\[23\]](#page-11-17)
    to enable cache set partitioning. Using PALLOC, we create two partitions dividing
    the LLC of 1MB into equal segments of 512KB each. We assign one partition to victim
    tasks and one partition to best-effort (attacker) tasks.


    # <span id="page-6-4"></span>*B. SD-VBS Profiling*


    To guide our evaluation, we first profile the workloads from SD-VBS to find each
    workload''s LLC bandwidth and bank access pattern. With our implemented per-bank
    monitoring interface, we collect these results on a single-core BOOM system with
    a four bank LLC. It should be noted that we exclude *multi ncut* due to long simulation
    times.


    <span id="page-6-2"></span>


    | Workload     | LLC Read B/W | LLC Write B/W |

    |--------------|--------------|---------------|

    | Disparity    | 2663.1       | 1330.6        |

    | MSER         | 967.9        | 270.7         |

    | Sift         | 356.9        | 90.6          |

    | Stitch       | 795.1        | 405.1         |

    | Localization | 55.9         | 0.326         |

    | Tracking     | 405.8        | 173.1         |

    | SVM          | 179.6        | 45.5          |


    TABLE III: SD-VBS LLC bandwidth characteristics (MB/s)


    Table [III](#page-6-2) shows the collected bandwidth results. From this, we select
    *Disparity*, *MSER* and *Stitch* for best-effort task evaluation in section [VI-F,](#page-9-0)
    as workloads that do not make frequent accesses to the LLC will not be noticeably
    affected by regulation. Through experimentation, we determine 700MB/s to be a
    suitable bandwidth threshold.


    <span id="page-6-3"></span>


    | Workload     | Bank 1  | Bank 2  | Bank 3  | Bank 4  |

    |--------------|---------|---------|---------|---------|

    | Disparity    | 5723148 | 5694269 | 5679896 | 5693761 |

    | MSER         | 476464  | 467716  | 466571  | 468930  |

    | Sift         | 1938519 | 1908043 | 1868291 | 1922350 |

    | Stitch       | 3867787 | 3821181 | 3786533 | 3896458 |

    | Localization | 309455  | 1843    | 1647    | 1795    |

    | Tracking     | 496624  | 252594  | 251577  | 259686  |

    | SVM          | 540773  | 505279  | 557619  | 525794  |


    TABLE IV: SD-VBS per-bank LLC access counts


    Table [IV](#page-6-3) shows the per-bank access counts of the SD-VBS workloads
    across the four cache banks. In our simulated design, a four-bank LLC uses address
    bits 6 and 7 to index the banks. Of the seven workloads, *Disparity, MSER, Sift*
    and *Stitch* have an even access spread across all banks. On the other hand, *Localization*
    and *Tracking* have heavier traffic to specific banks. *Localization* specifically
    sees 58× more accesses directed at bank one than the other three banks combined
    and 187× more accesses than the least accessed bank (bank two). *Tracking* sees
    2× more accesses directed at bank one than the least accessed bank. These results
    show that, in most benign (not malicious) workloads, requests to the shared cache
    are generally distributed evenly across the cache banks, although there are some
    notable exceptions.


    <span id="page-6-1"></span><sup>\*</sup><https://github.com/riscv-boom/riscv-boom/issues/690>


    As such, if we use the all-bank regulation approach to defend against potential
    cache-bank DoS attacks, which target only one bank, we significantly under utilize
    the cache bandwidth when benign workloads are executed on the throttled best-effort
    cores, as we will show later in this section.


    ## <span id="page-7-1"></span>*C. Cache Bank-Aware DoS Attack on FireSim*


    To begin our experimentation, we first mount the cache bank-aware DoS attack [\[12\]](#page-11-5)
    in our simulation environment, establishing the maximum base-line slowdown for
    our setup. These results are collected on a system with two banks in the LLC.


    The experiments are set up as follows. We utilize the *BkPLL* workload described
    in Section [IV-A](#page-2-2) as our victim task. The victim is configured to perform
    reads (denoted as *BkPLLRead*), has a working-set-size (WSS) of 128KB and is executed
    on the BOOM core. The Mempress attackers are configured to each have a WSS of
    64KB. We first run the victim in isolation and measure its performance. The attackers
    are then co-run with the victim in order to see the attackers impact on the victim''s
    performance. The attackers are applied in the two following scenarios:


    - 1) Diff. Bank: The attackers and the victim target different (disjoint) cache
    banks (victim: bank 0, attacker: bank 1)

    - 2) Same Bank: Both the attackers and the victim target the same cache bank (both
    attacker and victim: bank 0).


    Victim Bank Isolation


    <span id="page-7-0"></span>![](_page_7_Figure_6.jpeg)


    Fig. 7: Impact of cache bank-aware DoS attack on synthetic read victim in the
    FireSim platform. The bank attackers target is varied.


    Figure [7](#page-7-0) shows the results. Note first that, when the attackers and
    victim target separate banks, the victim experiences no slowdown. Yet, when the
    same bank is targeted the victim experiences a 3.52× slowdown from the write attackers.
    The results are very similar to what we have observed on the BeagleV platform
    in Section [IV-A,](#page-2-2) demonstrating the validity of our evaluation setup.
    To parallel the conclusion on the real platforms, we observe complete temporal
    isolation when the victim and the attackers target different banks. This confirms
    that any contention created by the attacker is at the *bank level*, not the interconnect
    (bus) level.


    Three key takeaways are: (1) each cache bank should be considered as an independent
    shared resource, which has limited bandwidth. If the bandwidth is over-saturated,
    then contention occurs and the subsequent requests to the bank will be delayed;
    (2) the targeted bank DoS attack is effective because it saturates the bank''s
    limited bandwidth; (3) Using bandwidth regulation to mitigate contention by preventing
    bandwidth saturation will be effective.


    # *D. Evaluation of Hardware Bandwidth Regulation*


    In this experiment, we evaluate the effectiveness of BRU''s bandwidth regulation
    in providing temporal isolation to the victim task in the presence of cache bank
    DoS attackers.


    For this experiment, we use all-bank regulation as implemented in the baseline
    BRU [\[16\]](#page-11-8). As discussed earlier, BRU allows for cores to be regulated
    alone or in groups using *domains*. Using this capability, we create a "real-time"
    domain for the victim task and a "best-effort" domain for the attackers. We then
    assign the BOOM core to the real-time domain and the two other Rocket cores, enhanced
    with the Mempress traffic generators, to the best-effort domain. As in Section
    [VI-C,](#page-7-1) Mempress instances are configured to generate overwhelming
    traffic to cache bank 0, to simulate the worstcase. For the victim tasks, we use
    *BkPLLRead* (synthetic) and *Disparity* (real-world), both configured to target
    cache bank 0. We vary the best-effort (attacker) domain''s bandwidth budget from
    640MB/s to 15.36GB/s, measuring the slowdown that each victim experiences normalized
    to the solo victim run (no attackers). The budget is set by programming the *Regulation
    Period Register* to 400 cycles (400 ns in our setup), and increasing the best-effort
    domain''s *Access Budget Register* from 16 accesses (640MB/s) per-period to 384
    accesses (15.36GB/s) per-period. Unless otherwise mentioned, all subsequent experiments
    use a regulation period of 400 cycles.


    <span id="page-7-2"></span>![](_page_7_Figure_14.jpeg)


    Fig. 8: Impact of increasing attacker bandwidth budget on BkPLLRead and Disparity.
    The WSS of attackers is 64KB.


    Figure [8](#page-7-2) shows the results. As we can see, up to an attacker budget
    of 1.28GB/s, the BkPLLRead victim experiences a 1.03× slowdown, which is small
    and acceptable in may applications. Beyond this threshold, however, the victim''s
    execution begins to be impacted, increasing to 1.12× at a budget of 2.56GB/s and
    growing to 3.52× at 15.36GB/s. Note that 15.36GB/s is bigger than the observed
    peak cache memory bandwidth of the attackers, which is effectively identical to
    not using the bandwidth regulation at all. When we repeat the experiment with
    *Disparity* as the victim, the performance degrades at a slower rate, peaking
    only at 1.39× slowdown when the attackers are allotted their full bandwidth. This
    is because Disparity, unlike BkPLLRead, generates fewer accesses to the shared
    cache that are more evenly distributed among the cache banks. In other words,
    Disparity is not the worst-case and its performance impact from the cache bank
    attack will be upper bounded by that of the BkPLLRead victim. Since Disparity
    has the highest measured bandwidth of the SD-VBS workloads (see section [VI-B\)](#page-6-4),
    all other workloads are similarly upper bounded.


    The key takeaways are (1) cache bandwidth regulation can effectively regulate
    the attackers to provide worst-case slowdown guarantees for the victim; (2) the
    regulation budget should be set based on the worst-case scenario when both the
    attackers and the victim target one single cache bank.


    ## <span id="page-8-2"></span>*E. All-Bank vs. Per-Bank Regulation*


    In the following experiments, we evaluate how different bandwidth regulation methods
    impact the performance of the victim and the attackers.


    We first show the impact of the all-bank and per-bank regulation methods in providing
    isolation guarantees to the victim task in the presence of the co-running cache-bank
    DoS attacks.


    The experiment setup is the same as before: the victim (BkPLLRead) runs on the
    real-time domain and the attackers run on the best-effort domain, which is regulated.
    The regulation budget of the best-effort domain is configured at 1.28GB/s (found
    to be the maximum allowable budget in the previous experiment) in both all-bank
    and per-bank regulation methods. Note that under per-bank regulation, each bank
    receives the budget of 1.28GB/s.


    <span id="page-8-0"></span>![](_page_8_Figure_6.jpeg)


    Fig. 9: Normalized slowdown of the victim when a 1.28GB/s regulation budget is
    applied to throttle write attackers under both all-bank and per-bank regulations.


    Figure [9](#page-8-0) shows the results. When running without regulation, we see
    the same 3.52× slowdown of the victim as was shown in the previous section. On
    the other hand, for both regulation schemes, the victim sees only a 1.03× slowdown
    with regulated attackers. This is because in both per-bank and all-bank regulation
    methods, only one cache bank is stressed by the attacks and the requests to the
    same bank are charged equally in both regulation methods.


    The results demonstrate that all-bank and per-bank bandwidth regulation methods
    are identical in protecting the victim in the worst-case (i.e., the cache bank
    DoS attackers are running on the best-effort domain). However, they will have
    very different effects in non worst-case scenarios as we will show in the following.


    Next, we evaluate the throughput impact of the regulation methods on the regulated
    cores. For this, we use the *Bandwidth* workload from [\[8\]](#page-11-4) as described
    in section [VI-A.](#page-5-5) We configure the workload to perform read accesses
    with a WSS of 128KB (4× the L1 size). The workload is pinned to the system''s
    BOOM core. We measure the slowdown normalized to a nonregulated run of the workload.
    This experiment is performed on four different LLC designs as follows:


    - 1) All-bank regulation with two LLC banks.

    - 2) Per-bank regulation with two LLC banks.

    - 3) All-bank regulation with four LLC banks.

    - 4) Per-bank regulation with four LLC banks.


    <span id="page-8-1"></span>![](_page_8_Figure_16.jpeg)


    Fig. 10: Normalized slowdown of *Bandwidth* using per-bank and all-bank regulations
    on two and four bank cache configurations. Regulation budget is 1.28GB/s. Workload
    is pinned to BOOM core.


    Figure [10](#page-8-1) shows the results. For the synthetic best effort task,
    regulating the entire cache as one unit (all-bank) results in performance degradation
    of 5.47× in the two-bank case and 5.49× in the four-bank case. In contrast, per-bank
    regulation sees a 2.94× and a 1.50× degradation in the two and four-bank cases,
    respectively. To directly compare, per-bank sees a 1.86× and 3.66× improvement
    over all-bank in the respective cases. Recall that this improvement is due to
    per-bank regulation allotting each bank a budget of 1.28GB/s. It should be noted
    that one would expect a 2× difference in the two bank cases and a 4× difference
    in the four bank cases. Of course, our prototype has some inefficiencies, however
    we deem these acceptable as the benefits of per-bank regulation are still clear.


    From this synthetic experiment, we draw two major conclusions. First, per-bank
    regulation demonstrates a clear improvement in best-effort task throughput compared
    to the overly pessimistic all-bank regulation. This throughput improvement is
    achieved all while guaranteeing the same temporal isolation of victim tasks. Second,
    performance benefits of our perbank implementation scale effectively as the number
    of banks increases.


    ## <span id="page-9-0"></span>*F. Impact of Per-Bank Regulation on Real-World
    Applications*


    The *Bandwidth* workload is a synthetic workload, not representative of real-world
    applications. We further evaluate the benefits of per-bank regulation over all-bank
    regulation using a set of benchmarks from SD-VBS [\[32\]](#page-11-26) and SPEC2017
    [\[33\]](#page-11-27). Specifically, we select *Disparity*, *MSER* and *Stitch*
    from SD-VBS (all *CIF* input) and *gcc*, *xalanc* and *mcf* from SPEC2017 (*ref*
    input). The SD-VBS benchmarks are chosen because they are relatively LLC bandwidth
    intensive workloads (see Section [VI-B\)](#page-6-4). Likewise, the SPEC2017 benchmarks
    are chosen as they are relatively cache sensitive and have fast simulation run
    times.


    For these experiments, we configure a system with one BOOM core to avoid any cross-core
    interference. We set a regulation budget of 1.28GB/s and measure each workload''s
    performance, computing the slowdown normalized to an unregulated run of the workload.
    As was done in section [VI-E,](#page-8-2) we evaluate using both two bank and
    four bank cache designs.


    Figure [11](#page-10-2) shows the results for our selected workloads. In general,
    we see improvement when using per-bank regulation over all-bank regulation. Moreover,
    performance scales well from two to four banks, such as in *Disparity* which suffers
    less slowdown as more cache banks are used. Specifically, in the two-bank case,
    *Disparity* sees a 1.85× and 1.73× slowdown under all-bank and per-bank regulation
    respectively. When the cache is configured with four banks, all-bank regulation
    creates the same 1.85× slowdown, while per-bank regulation has only a 1.28× slowdown.
    Thus, Disparity is an excellent example of the improvement of per-bank over all-bank
    regulation and the performance scalability as the number of banks increases. The
    results for *MSER, Stitch* and *gcc* also show similar improvement when going
    from an all-bank to per-bank regulation scheme.


    These experiments clearly demonstrate that, as with the synthetic case, real-world
    workloads see noticeable improvement when using per-bank regulation over all-bank
    regulation. Again, it must be emphasized that this improvement is accompanied
    by per-bank regulation''s guarantee of isolating victim tasks to the same degree
    as all-bank regulation. Overall, this highlights the superiority of per-bank regulation
    over all-bank.


    ## *G. Software vs. Hardware Bandwidth Regulation*


    In this experiment, we compare the software-based cache bandwidth regulation method
    proposed in [\[12\]](#page-11-5) with our hardware-based bandwidth regulation.


    Ideally, we would like to implement the software-based regulation approach directly
    on experimental platform. However, because we leverage Mempress traffic generators
    instead of using BOOM CPU cores for the attack, the software approach cannot be
    properly tested. Instead, we implement the software regulation approach on the
    BeagleV board, which is equipped with four RISC-V out-of-order cores (see Section
    [IV-A\)](#page-2-2) comparable to the BOOM core used in our testbed.


    On the BeagleV platform, we observe up to 76× slowdown of the best-effort (attacker)
    tasks (throttled at 100MB/s) for the software regulation method to protect the
    victim. These results are in-line with the up to 300× slowdown reported in [\[12\]](#page-11-5)
    on the Pi 4 platform. While using the hardware-based regulation methods in our
    FireSim setup, the worst-case slowdown of the attackers is up to 5.49× for all-bank
    regulation, and up to 2.94× slowdown for the per-bank regulation in 2-bank configuration.
    The slowdown is further reduced down 1.5× in the 4-bank configuration.


    Note that the CPU performance of the BeagleV platform is on-par with that of our
    simulation setup. As such, we posit that the dramatic performance differences
    we observe come from the superior effectiveness of hardware-based regulation over
    the software-based one.


    ## *H. Hardware Implementation Overhead*


    To evaluate the cost-to-performance benefit of our per-bank design, we synthesize
    a quad-core BOOM SoC and perform power and area analysis. We run place and route
    from the Cadence Suite''s Innovus tool, along with the Hammer [\[34\]](#page-11-28)
    VLSI flow scripts targeting the ASAP 7nm technology node [\[35\]](#page-11-29),
    to characterize the overhead and compare to the all-bank implementation in [\[16\]](#page-11-8).


    <span id="page-9-1"></span>


    | Implementation  | Regulation Unit (nm2<br>) | SoC (nm2<br>) | Percent |

    |-----------------|---------------------------|---------------|---------|

    | All-Bank [16]   | 429                       | 465305        | 0.09%   |

    | Per-Bank (Ours) | 1372                      | 466248        | 0.29%   |


    TABLE V: Comparative area analysis of the two regulation unit implementations.
    Percent is the area consumed by the implementation from the total SoC area.


    Table [V](#page-9-1) shows the area utilization of the two configurations after
    place and route has been completed. We find that the area overhead added in our
    per-bank design comes to 3.2× that of the all-bank implementation. However, it
    is still less than 0.3% of the entire SoC area.


    <span id="page-9-2"></span>


    | Design Under Test | Total Power (mW) | Percent |

    |-------------------|------------------|---------|

    | SoC               | 110              |         |

    | All-Bank [16]     | 0.67             | 0.6%    |

    | Per-Bank (Ours)   | 2.36             | 2.1%    |


    TABLE VI: Comparative power analysis of the two regulation unit implementations.
    Percent is the power consumed by the implementation from the total SoC power.


    Table [VI](#page-9-2) shows the power analysis results. As shown, the per-bank
    design again consumes 3.5× that of the all-bank design, yet it is still only just
    over 2% of the total power. It


    <span id="page-10-2"></span>![](_page_10_Figure_0.jpeg)


    Fig. 11: Comparison of all-bank and per-bank regulation when running real-world
    workloads. Each workload is run with a regulation budget of 1.28GB/s. Slowdown
    is in comparison to the unregulated run.


    <span id="page-10-0"></span>can be stated that the area and power overhead of
    our per-bank design is acceptable considering its significant performance benefits
    seen in previous sections.


    ## VII. RELATED WORK


    ffort SlowdownIn the real-time community, correctly estimating worst-case task
    execution timing is of paramount importance, yet it has been difficult to do so
    in multicore systems due to the vast and diverse set of shared hardware resources
    that can dramatically impact task execution timing. Microarchitectural DoS attacks
    on shared hardware resources are therefore important for the real-time community
    to study as they can shed light on the impacts on worst-case timing. Moscibroda
    et al. proposed the "memory performance attack" [\[36\]](#page-11-30), which exploits
    the DRAM controller''s FR-FCFS [\[37\]](#page-11-31) scheduling policy to induce
    contention. Attacks on shared cache space [\[38\]](#page-11-32), bus bandwidth
    [\[39\]](#page-11-33), shared cache MSHRs [\[8\]](#page-11-4) and write-back buffers
    [\[9\]](#page-11-34), shared GPU [\[40\]](#page-11-35), and shared cache between
    the CPU and the integrated GPU [\[41\]](#page-12-0) have been explored. Most recently,
    bank contention on multi-bank shared caches has shown to be an effective DoS attack
    avenue [\[12\]](#page-11-5), which we focus on in this work. Several studies have
    investigated the effects of these microarchitectural attacks in actual cyberphysical
    systems [\[42\]](#page-12-1), [\[43\]](#page-12-2).


    Providing strong isolation in multicore has long been a topic of intense research
    over the past two decades. This includes various software and hardware mechanisms
    to manage the shared resources [\[13\]](#page-11-6), [\[44\]](#page-12-3)–[\[53\]](#page-12-4).
    Broadly, these resource management studies fall into two categories: space partitioning
    and bandwidth throttling. Cache space partitioning has been extensively studied
    in the real-time community to prevent unwanted cache-line evictions of high-priority
    real-time tasks by lower priority tasks. Cache space partitioning can be realized
    in software, through page coloring [\[54\]](#page-12-5), or in hardware, such
    as the way-based partitioning capabilities found in Intel RDT [\[14\]](#page-11-7)
    and ARM MPAM [\[15\]](#page-11-13).


    Memory bandwidth throttling is another extensively studied mechanism for isolation.
    Most software-based memory bandwidth throttling techniques utilize the CPU core''s
    performance counters to monitor the bandwidth. Then the periodic timers and interrupts
    regulate the allowed bandwidth of the cores at fixed time intervals [\[13\]](#page-11-6),
    [\[53\]](#page-12-4). MemPol [\[55\]](#page-12-6) instead utilizes a dedicated
    real-time micro-controller unit (MCU) to asynchronously monitor and regulate the
    memory traffic through polling. This approach reduces the interrupt overhead at
    the expense of wasting the real-time MCU. Hardware-based bandwidth regulation
    can eliminate such software overhead and can be enforced at a very fine granularity
    (in cycles rather than in milliseconds). BRU [\[16\]](#page-11-8), Intel RDT [\[14\]](#page-11-7),
    ARM MPAM [\[15\]](#page-11-13) all provide memory bandwidth regulation capabilities
    in hardware.


    Until recently, cache bandwidth has received little attention as it was believed
    to be of less importance compared to cache space partitioning or memory bandwidth.
    However, a recent study demonstrated its implications in multi-bank caches within
    high-performance multicore architectures [\[12\]](#page-11-5). The study proposed
    a software cache bandwidth throttling mechanism as a potential mitigation solution,
    but acknowledged the unacceptably high overhead of such a software implementation.
    In this work, we present a hardware solution to manage cache bandwidth in real-time
    systems. To the best of our knowledge, we are the first to present a hardware-based
    cache bandwidth regulation solution. More importantly, we are the first to propose
    a per-bank cache bandwidth regulation approach that can significantly improve
    average throughput on the regulated cores.


    ## VIII. CONCLUSION


    <span id="page-10-1"></span>In this paper, we presented a per-bank cache bandwidth
    regulation approach to effectively and efficiently mitigate potential cache bank
    bandwidth contention. We make the observation that the contention occurs at the
    individual cache bank rather than at the interconnect (bus), therefore our key
    contribution is to apply a well-known bandwidth regulation mechanism at the cache
    bank level. We evaluate that this approach can effectively minimize the effect
    of worst-case cache bank contention while maximizing allowed cache bandwidth and
    guaranteeing the isolation. We implemented the proposed per-bank regulation solution
    in hardware by extending an open-source bandwidth regulator design. We demonstrated
    its effectiveness in providing isolation guarantees to critical real-time tasks
    in the presence of adversarial cache bank DoS attackers. Furthermore, we illustrated
    that our per-bank bandwidth regulation approach can significantly improve performance
    of throttled best-effort tasks without compromising isolation guarantees allotted
    to real-time tasks. Specifically, we achieved up to 3.66× throughput improvement
    over the baseline bank-oblivious bandwidth throttling approach. As future work,
    we plan to apply the proposed per-bank regulation approach to DRAM banks.


    ## ACKNOWLEDGMENTS


    This research is supported in part by NSF grants CPS-2038923, CCF-2239020, CCF-2403013,
    and ACE, one of the seven centers in JUMP 2.0, a Semiconductor Research Corporation
    (SRC) program sponsored by DARPA.


    ## REFERENCES


    - <span id="page-11-0"></span>[1] ARM, "ARM Cortex-A72 MPCore Processor Technical
    Reference Manual r0p3," ARM Holdings, Tech. Rep., 2024, accessed: 2024- 05-09.
    [Online]. Available: [https://developer.arm.com/documentation/](https://developer.arm.com/documentation/100095/0003/)
    [100095/0003/](https://developer.arm.com/documentation/100095/0003/)

    - <span id="page-11-12"></span>[2] ——, "ARM Cortex-A57 MPCore Processor Technical
    Reference Manual r1p3," ARM Holdings, Tech. Rep., 2024, accessed: 2024-05-09.
    [Online]. Available: <https://developer.arm.com/documentation/ddi0488/>

    - [3] ——, "ARM DynamIQ Shared Unit Technical Reference Manual," ARM Holdings,
    Tech. Rep. 100453, 2024, accessed: 2024-05-09. [Online]. Available: [https://developer.arm.com/documentation/100453/](https://developer.arm.com/documentation/100453/0401/)
    [0401/](https://developer.arm.com/documentation/100453/0401/)

    - [4] The Raspberry Pi Foundation, "Raspberry Pi 5 raspberrypi.com," [https:](https://www.raspberrypi.com/products/raspberry-pi-5/)
    [//www.raspberrypi.com/products/raspberry-pi-5/,](https://www.raspberrypi.com/products/raspberry-pi-5/)
    2024, accessed: 2024- 05-10.

    - <span id="page-11-1"></span>[5] BeagleBoard.org Foundation, "BeagleV Ahead -
    BeagleBoard.org," [https://www.beagleboard.org/boards/beaglev-ahead,](https://www.beagleboard.org/boards/beaglev-ahead)
    2024, accessed: 2024-05-03.

    - <span id="page-11-2"></span>[6] NXP, "T4240RM, T4240 QorIQ Integrated Multicore
    Communications Processor Family Reference Manual - Reference Manual," Tech. Rep.,
    2017.

    - <span id="page-11-3"></span>[7] ——, "QorIQ LX2160A Reference Manual," Tech.
    Rep., 2021.

    - <span id="page-11-4"></span>[8] P. K. Valsan, H. Yun, and F. Farshchi, "Taming
    Non-blocking Caches to Improve Isolation in Multicore Real-Time Systems," in *RTAS*,
    2016.

    - <span id="page-11-34"></span>[9] M. G. Bechtel and H. Yun, "Denial-of-Service
    Attacks on Shared Cache in Multicore: Analysis and Prevention," in *RTAS*, 2019.

    - [10] D. Iorga, T. Sorensen, J. Wickerson, and A. F. Donaldson, "Slow and steady:
    Measuring and tuning multicore interference," in *RTAS*, 2020.

    - [11] A. Li, M. Sudvarg, H. Liu, Z. Yu, C. Gill, and N. Zhang, "PolyRhythm: Adaptive
    Tuning of a Multi-Channel Attack Template for Timing Interference," in *RTSS*,
    2022.

    - <span id="page-11-5"></span>[12] M. Bechtel and H. Yun, "Cache bank-aware denial-of-service
    attacks on multicore arm processors," in *RTAS*, 2023.

    - <span id="page-11-6"></span>[13] H. Yun, G. Yao, R. Pellizzoni, M. Caccamo,
    and L. Sha, "MemGuard: Memory Bandwidth Reservation System for Efficient Performance
    Isolation in Multi-core Platforms," in *RTAS*, 2013.

    - <span id="page-11-7"></span>[14] Intel, "Intel® 64 and IA-32 Architectures Software
    Developer Manuals," Intel Corporation, Tech. Rep., 2024, accessed: 2024-05-09.
    [Online]. Available: [https://www.intel.com/content/www/us/en/developer/articles/](https://www.intel.com/content/www/us/en/developer/articles/technical/intel-sdm.html)
    [technical/intel-sdm.html](https://www.intel.com/content/www/us/en/developer/articles/technical/intel-sdm.html)

    - <span id="page-11-13"></span>[15] ARM, "Arm Memory System Resource Partitioning
    and Monitoring (MPAM) System Component Specification," ARM Holdings, Tech. Rep.,
    2024, accessed: 2024-05-09. [Online]. Available: [https://developer.](https://developer.arm.com/documentation/ihi0099/aa)
    [arm.com/documentation/ihi0099/aa](https://developer.arm.com/documentation/ihi0099/aa)

    - <span id="page-11-8"></span>[16] F. Farshchi, Q. Huang, and H. Yun, "Bru: Bandwidth
    regulation unit for real-time multicore processors," in *RTAS*, 2020.

    - <span id="page-11-9"></span>[17] "Amd virtex ultrascale+ vcu118 fpga," [https://www.xilinx.com/products/](https://www.xilinx.com/products/boards-and-kits/vcu118.html)
    [boards-and-kits/vcu118.html,](https://www.xilinx.com/products/boards-and-kits/vcu118.html)
    2024, accessed: 2024-05-13.

    - <span id="page-11-10"></span>[18] S. Karandikar, H. Mao, D. Kim, D. Biancolin,
    A. Amid, D. Lee, N. Pemberton, E. Amaro, C. Schmidt, A. Chopra, Q. Huang, K. Kovacs,
    B. Nikolic, R. Katz, J. Bachrach, and K. Asanovic, "FireSim: FPGA- ´ accelerated
    cycle-exact scale-out system simulation in the public cloud," in *ISCA*, 2018.

    - <span id="page-11-11"></span>[19] R. Balasubramonian, N. P. Jouppi, and N. Muralimanohar,
    *Multi-Core Cache Hierarchies*. Morgan & Claypool Publishers, 2011.

    - <span id="page-11-14"></span>[20] A. Farshin, A. Roozbeh, G. Q. Maguire Jr,
    and D. Kostic, "Make the ´ most out of last level cache in intel processors,"
    in *EuroSys*, 2019.

    - <span id="page-11-15"></span>[21] G. Irazoqui, T. Eisenbarth, and B. Sunar,
    "Systematic reverse engineering of cache slice selection in intel processors,"
    in *DSD*. IEEE, 2015.

    - <span id="page-11-16"></span>[22] The Raspberry Pi Foundation, "Raspberry Pi
    4 - raspberrypi.com," [https://www.raspberrypi.com/products/raspberry-pi-4-model-b/,](https://www.raspberrypi.com/products/raspberry-pi-4-model-b/)
    2024, accessed: 2024-05-10.

    - <span id="page-11-17"></span>[23] H. Yun, R. Mancuso, Z.-P. Wu, and R. Pellizzoni,
    "PALLOC: DRAM Bank-Aware Memory Allocator for Performance Isolation on Multicore
    Platforms," in *RTAS*, 2014.

    - <span id="page-11-18"></span>[24] J. Bachrach, H. Vo, B. Richards, Y. Lee, A.
    Waterman, R. Avizienis, ˇ J. Wawrzynek, and K. Asanovic, "Chisel: Constructing
    hardware in a ´ scala embedded language," in *DAC*, 2012.

    - <span id="page-11-19"></span>[25] A. Amid, D. Biancolin, A. Gonzalez, D. Grubb,
    S. Karandikar, H. Liew, A. Magyar, H. Mao, A. Ou, N. Pemberton, P. Rigge, C. Schmidt,
    J. Wright, J. Zhao, Y. S. Shao, K. Asanovic, and B. Nikoli ´ c, "Chipyard: ´ Integrated
    design, simulation, and implementation framework for custom socs," *IEEE Micro*,
    vol. 40, no. 4, 2020.

    - <span id="page-11-20"></span>[26] SiFive, "SiFive TileLink Specification," 2017,
    accessed: 2024-05-10.

    - <span id="page-11-21"></span>[27] K. Asanovic, R. Avizienis, J. Bachrach, S.
    Beamer, D. Biancolin, ´ C. Celio, H. Cook, D. Dabbelt, J. Hauser, A. Izraelevitz,
    S. Karandikar, B. Keller, D. Kim, J. Koenig, Y. Lee, E. Love, M. Maas, A. Magyar,
    H. Mao, M. Moreto, A. Ou, D. A. Patterson, B. Richards, C. Schmidt, S. Twigg,
    H. Vo, and A. Waterman, "The rocket chip generator," Tech. Rep. UCB/EECS-2016-17,
    Apr 2016. [Online]. Available: [http:](http://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-17.html)
    [//www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-17.html](http://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-17.html)

    - <span id="page-11-22"></span>[28] C. Celio, D. A. Patterson, and K. Asanovic,
    "The berkeley out- ´ of-order machine (boom): An industry-competitive, synthesizable,
    parameterized risc-v processor," Tech. Rep. UCB/EECS-2015-167, Jun 2015. [Online].
    Available: [http://www2.eecs.berkeley.edu/Pubs/](http://www2.eecs.berkeley.edu/Pubs/TechRpts/2015/EECS-2015-167.html)
    [TechRpts/2015/EECS-2015-167.html](http://www2.eecs.berkeley.edu/Pubs/TechRpts/2015/EECS-2015-167.html)

    - <span id="page-11-23"></span>[29] "Mempress," [https://github.com/ucb-bar/mempress/tree/main,](https://github.com/ucb-bar/mempress/tree/main)
    2024, accessed: 2024-05-09.

    - <span id="page-11-24"></span>[30] "RoCC Accelerators," [https://chipyard.readthedocs.io/en/stable/](https://chipyard.readthedocs.io/en/stable/Customization/RoCC-Accelerators.html)
    [Customization/RoCC-Accelerators.html,](https://chipyard.readthedocs.io/en/stable/Customization/RoCC-Accelerators.html)
    2024, accessed: 2024-05- 13.

    - <span id="page-11-25"></span>[31] "SiFive Inclusive Cache," [https://github.com/chipsalliance/](https://github.com/chipsalliance/rocket-chip-inclusive-cache/tree/main)
    [rocket-chip-inclusive-cache/tree/main,](https://github.com/chipsalliance/rocket-chip-inclusive-cache/tree/main)
    2024, accessed: 2024-05-09.

    - <span id="page-11-26"></span>[32] S. K. Venkata, I. Ahn, D. Jeon, A. Gupta,
    C. Louie, S. Garcia, S. Belongie, and M. B. Taylor, "SD-VBS: The San Diego Vision
    Benchmark Suite," in *IISWC*, 2009.

    - <span id="page-11-27"></span>[33] "SPEC CPU2017," [https://www.spec.org/cpu2017.](https://www.spec.org/cpu2017)

    - <span id="page-11-28"></span>[34] H. Liew, D. Grubb, J. Wright, C. Schmidt,
    N. Krzysztofowicz, A. Izraelevitz, E. Wang, K. Asanovic, J. Bachrach, and B. Nikoli
    ´ c,´ "Hammer: a modular and reusable physical design flow tool: invited," in
    *DAC*, 2022.

    - <span id="page-11-29"></span>[35] L. T. Clark, V. Vashishtha, L. Shifren, A.
    Gujja, S. Sinha, B. Cline, C. Ramamurthy, and G. Yeric, "Asap7: A 7-nm finfet
    predictive process design kit," *Microelectronics Journal*, vol. 53, 2016. [Online].
    Available: <https://doi.org/10.1016/j.mejo.2016.04.006>

    - <span id="page-11-30"></span>[36] T. Moscibroda and O. Mutlu, "Memory Performance
    Attacks: Denial of Memory Service in Multi-Core Systems," in *USENIX Security
    Symposium*, 2007.

    - <span id="page-11-31"></span>[37] S. Rixner, W. J. Dally, U. J. Kapasi, P. Mattson,
    and J. Owens, "Memory Access Scheduling," in *ACM SIGARCH Computer Architecture
    News*, 2000.

    - <span id="page-11-32"></span>[38] G. Keramidas, P. Petoumenos, S. Kaxiras, A.
    Antonopoulos, and D. Serpanos, "Preventing denial-of-service attacks in shared
    cmp caches," in *SAMOS*, 2006.

    - <span id="page-11-33"></span>[39] D. H. Woo and H. Lee, "Analyzing performance
    vulnerability due to resource denial of service attack on chip multiprocessors,"
    in *CMP-MSI*, 2007.

    - <span id="page-11-35"></span>[40] T. Yandrofski, J. Chen, N. Otterness, J. H.
    Anderson, and F. Smith, "Making Powerful Enemies on NVIDIA GPUs," in *RTSS*, 2022.

    - <span id="page-12-0"></span>[41] M. Bechtel and H. Yun, "Denial-of-Service Attacks
    on Shared Resources in Intel''s Integrated CPU-GPU Platforms," in *ISORC*, 2022.

    - <span id="page-12-1"></span>[42] A. Li, J. Wang, S. Baruah, B. Sinopoli, and
    N. Zhang, "An empirical study of performance interference: Timing violation patterns
    and impacts," in *RTAS*, 2024.

    - <span id="page-12-2"></span>[43] M. Bechtel and H. Yun, "Analysis and mitigation
    of shared resource contention on heterogeneous multicore: An industrial case study,"
    *IEEE Transactions on Computers*, vol. 73, no. 7, pp. 1753–1766, 2024.

    - <span id="page-12-3"></span>[44] R. Mancuso, R. Dudko, E. Betti, M. Cesati,
    M. Caccamo, and R. Pellizzoni, "Real-Time Cache Management Framework for Multi-core
    Architectures," in *RTAS*, 2013.

    - [45] H. Kim, A. Kandhalu, and R. Rajkumar, "A Coordinated Approach for Practical
    OS-Level Cache Management in Multi-core Real-Time Systems," in *ECRTS*, 2013.

    - [46] Y. Ye, R. West, Z. Cheng, and Y. Li, "Coloris: a Dynamic Cache Partitioning
    System Using Page Coloring," in *PACT*, 2014.

    - [47] N. Kim, B. C. Ward, M. Chisholm, J. H. Anderson, and F. D. Smith, "Attacking
    the One-Out-of-M Multicore Problem by Combining Hardware Management with Mixed-Criticality
    Provisioning," *Real-Time Systems*, 2017.

    - [48] S. Roozkhosh and R. Mancuso, "The Potential of Programmable Logic in the
    Middle: Cache Bleaching," in *RTAS*, 2020.

    - [49] P. Sohal, R. Tabish, U. Drepper, and R. Mancuso, "E-WarP: a Systemwide
    Framework for Memory Bandwidth Profiling and Management," in *RTSS*, 2020.

    - [50] F. Farshchi, P. K. Valsan, R. Mancuso, and H. Yun, "Deterministic Memory
    Abstraction and Supporting Multicore System Architecture," in *ECRTS*, 2018.

    - [51] W. Ali and H. Yun, "RT-Gang: Real-Time Gang Scheduling Framework for Safety-Critical
    Systems," in *RTAS*, 2019.

    - [52] M. Xu, L. T. X. Phan, H.-Y. Choi, Y. Lin, H. Li, C. Lu, and I. Lee, "Holistic
    Resource Allocation for Multicore Real-Time Systems," in *RTAS*, 2019.

    - <span id="page-12-4"></span>[53] A. Saeed, D. Dasari, D. Ziegenbein, V. Rajasekaran,
    F. Rehm, M. Pressler, A. Hamann, D. Mueller-Gritschneder, A. Gerstlauer, and U.
    Schlichtmann, "Memory utilization-based dynamic bandwidth regulation for temporal
    isolation in multi-cores," in *RTAS*, 2022.

    - <span id="page-12-5"></span>[54] S. Mittal, "A survey of techniques for cache
    partitioning in multicore processors," *ACM Computing Surveys (CSUR)*, vol. 50,
    no. 2, 2017.

    - <span id="page-12-6"></span>[55] A. Zuepke, A. Bastoni, W. Chen, M. Caccamo,
    and R. Mancuso, "Mempol: Policing core memory bandwidth from outside of the cores,"
    in *RTAS*, 2023.'
- title: 'IANUS: Integrated Accelerator based on NPU-PIM Unified Memory System'
  abstract: 'Accelerating end-to-end inference of transformer-based large language
    models

    (LLMs) is a critical component of AI services in datacenters. However, diverse

    compute characteristics of end-to-end LLM inference present challenges as

    previously proposed accelerators only address certain operations or stages

    (e.g., self-attention, generation stage, etc.). To address the unique

    challenges of accelerating end-to-end inference, we propose IANUS -- Integrated

    Accelerator based on NPU-PIM Unified Memory System. IANUS is a domain-specific

    system architecture that combines a Neural Processing Unit (NPU) with a

    Processing-in-Memory (PIM) to leverage both the NPU''s high computation

    throughput and the PIM''s high effective memory bandwidth. In particular, IANUS

    employs a unified main memory system where the PIM memory is used both for PIM

    operations and for NPU''s main memory. The unified main memory system ensures

    that memory capacity is efficiently utilized and the movement of shared data

    between NPU and PIM is minimized. However, it introduces new challenges since

    normal memory accesses and PIM computations cannot be performed simultaneously.

    Thus, we propose novel PIM Access Scheduling that manages normal memory

    accesses and PIM computations through workload mapping and scheduling across

    the PIM and the NPU. Our detailed simulation evaluations show that IANUS

    improves the performance of GPT-2 by 6.2$\times$ and 3.2$\times$, on average,

    compared to the NVIDIA A100 GPU and the state-of-the-art accelerator. As a

    proof-of-concept, we develop a prototype of IANUS with a commercial PIM, NPU,

    and an FPGA-based PIM controller to demonstrate the feasibility of IANUS.'
  url: http://arxiv.org/abs/2410.15008v1
  keywords: ''
  document: '# IANUS: Integrated Accelerator based on NPU-PIM Unified Memory System


    | Minseok Seo<br>Seoul National<br>University<br>South Korea | Xuan Truong<br>Nguyen<br>Seoul
    National<br>University<br>South Korea | Seok Joong<br>Hwang<br>SAPEON Inc.<br>South
    Korea | Yongkee<br>Kwon<br>SK hynix<br>South Korea                  | Guhyun Kim<br>SK
    hynix<br>South Korea          | Chanwook<br>Park<br>SK hynix<br>South Korea   |

    |------------------------------------------------------------|----------------------------------------------------------------------|---------------------------------------------------|-------------------------------------------------------------|------------------------------------------------|-----------------------------------------------|

    | Ilkon Kim<br>SK hynix<br>South Korea                       | Jaehan Park<br>SK
    hynix<br>South Korea                               | Jeongbin Kim<br>SK hynix<br>South
    Korea           | Woojae Shin<br>SK hynix<br>South Korea                      |
    Jongsoon Won<br>SK hynix<br>South Korea        | Haerang Choi<br>SK hynix<br>South
    Korea       |

    | Kyuyoung<br>Kim<br>SK hynix<br>South Korea                 | Daehan Kwon<br>SK
    hynix<br>South Korea                               | Chunseok<br>Jeong<br>SK hynix<br>South
    Korea      | Sangheon Lee<br>SAPEON Inc.<br>South Korea                  | Yongseok<br>Choi<br>SAPEON
    Inc.<br>South Korea | Wooseok<br>Byun<br>SAPEON Inc.<br>South Korea |

    |                                                            |                                                                      |
    Seungcheol<br>Baek<br>SAPEON Inc.<br>South Korea  | Hyuk-Jae Lee<br>Seoul National<br>University<br>South
    Korea | John Kim<br>KAIST<br>South Korea               |                                               |


    # Abstract


    Accelerating end-to-end inference of transformer-based large language models (LLMs)
    is a critical component of AI services in datacenters. However, diverse compute
    characteristics of end-to-end LLM inference present challenges as previously proposed
    accelerators only address certain operations or stages (e.g., self-attention,
    generation stage, etc.). To address the unique challenges of accelerating end-to-end
    inference, we propose IANUS – Integrated Accelerator based on NPU-PIM Unified
    Memory System. IANUS is a domainspecific system architecture that combines a Neural
    Processing Unit (NPU) with a Processing-in-Memory (PIM) to leverage both the NPU''s
    high computation throughput and the PIM''s high effective memory bandwidth. In
    particular, IANUS employs a unified main memory system where the PIM memory is
    used both for PIM operations and for NPU''s main memory. The unified main memory
    system ensures that


    This paper is an updated version of the paper that appeared in the Proceedings
    of the 29th ACM International Conference on Architectural Support for Programming
    Languages and Operating Systems (ASPLOS), April 2024.


    Permission to make digital or hard copies of part or all of this work for personal
    or classroom use is granted without fee provided that copies are not made or distributed
    for profit or commercial advantage and that copies bear this notice and the full
    citation on the first page. Copyrights for thirdparty components of this work
    must be honored. For all other uses, contact the owner/author(s).


    ASPLOS ''24, April 27-May 1, 2024, La Jolla, CA, USA © 2024 Copyright held by
    the owner/author(s). ACM ISBN 979-8-4007-0386-7/24/04. <https://doi.org/10.1145/3620666.3651324>


    memory capacity is efficiently utilized and the movement of shared data between
    NPU and PIM is minimized. However, it introduces new challenges since normal memory
    accesses and PIM computations cannot be performed simultaneously. Thus, we propose
    novel PIM Access Scheduling that manages normal memory accesses and PIM computations
    through workload mapping and scheduling across the PIM and the NPU. Our detailed
    simulation evaluations show that IANUS improves the performance of GPT-2 by 6.2×
    and 3.2×, on average, compared to the NVIDIA A100 GPU and the stateof-the-art
    accelerator. As a proof-of-concept, we develop a prototype of IANUS with a commercial
    PIM, NPU, and an FPGA-based PIM controller to demonstrate the feasibility of IANUS.


    ## CCS Concepts: • Computer systems organization → Heterogeneous (hybrid) systems;
    • Computing methodologies → Planning and scheduling.


    Keywords: Accelerators, Heterogeneous Architectures, Neural Processing Unit, Processing-in-memory,
    Large Language Model, Workload Mapping, Scheduling


    #### ACM Reference Format:


    Minseok Seo, Xuan Truong Nguyen, Seok Joong Hwang, Yongkee Kwon, Guhyun Kim, Chanwook
    Park, Ilkon Kim, Jaehan Park, Jeongbin Kim, Woojae Shin, Jongsoon Won, Haerang
    Choi, Kyuyoung Kim, Daehan Kwon, Chunseok Jeong, Sangheon Lee, Yongseok Choi,
    Wooseok Byun, Seungcheol Baek, Hyuk-Jae Lee, and John


    Kim. 2024. IANUS: Integrated Accelerator based on NPU-PIM Unified Memory System.
    In 29th ACM International Conference on Architectural Support for Programming
    Languages and Operating Systems, Volume 3 (ASPLOS ''24), April 27-May 1, 2024,
    La Jolla, CA, USA. ACM, New York, NY, USA, [16](#page-15-0) pages. [https://doi.org/10.1145/](https://doi.org/10.1145/3620666.3651324)
    [3620666.3651324](https://doi.org/10.1145/3620666.3651324)


    # 1 Introduction


    Transformer [\[44\]](#page-14-0), BERT [\[9\]](#page-13-0), and GPT [\[39\]](#page-14-1)
    have been widely used for natural language processing (NLP) services at datacenters.
    Although GPUs are commonly used to accelerate the inference of deep learning models,
    GPUs are less effective in handling transformer models because of multi-head attention
    and inference stages that are memory-bound [\[19\]](#page-13-1). To address the
    limitations of GPU for transformer models, many recent works [\[13,](#page-13-2)
    [14,](#page-13-3) [34,](#page-14-2) [45\]](#page-14-3) have proposed to accelerate
    multi-head attention through dedicated accelerators and algorithmic changes; however,
    these prior work do not fully address the challenges of end-to-end inference acceleration.
    Recently, DFX [\[19\]](#page-13-1) proposed an FPGA-based appliance that is designed
    for memory-bound transformer inference stages; however, it is sub-optimal for
    the compute-bound stages in end-to-end inference.


    One of the main challenges in accelerating end-to-end inference of transformer-based
    large language models (LLMs) is their diverse characteristics, which exhibit a
    broad range of computational intensities. For example, GPT includes complex vector
    operations, multi-head attention, and fullyconnected (FC) layers that present
    both compute-bound matrix-matrix multiplication as well as memory-bound matrixvector
    multiplication. Consequently, to accelerate end-to-end inference of LLMs, hardware
    must be capable of efficiently handling all these diverse operations.


    Neural processing units (NPUs) [\[6,](#page-13-4) [21,](#page-13-5) [23\]](#page-13-6)
    have been widely proposed to accelerate deep neural networks (DNNs). However,
    NPUs are often limited by memory-bound operations even when high-bandwidth memory
    is utilized. In comparison, processing-in-memory (PIM) [\[8,](#page-13-7) [29,](#page-14-4)
    [30\]](#page-14-5) minimizes data movement by enabling computation near memory
    and provides higher effective memory bandwidth. Recent PIM chips [\[29,](#page-14-4)
    [30\]](#page-14-5) are effective "domain-specific" memory as they accelerate memory-bound
    operations by guaranteeing full internal memory bandwidth utilization for processing
    units in memory on domain-specific kernels. However, computebound operations such
    as matrix-matrix computations or complex vector operations are not efficient on
    PIM because of the limitations of DRAM technology that is highly areaconstrained.


    To address the challenges of end-to-end LLM inference, we propose an NPU-PIM architecture
    that provides the benefit of both a domain-specific accelerator (i.e., NPU) as
    well as a domain-specific memory (i.e., PIM), effectively supporting a broad range
    of arithmetic intensities in LLMs. In particular, we propose IANUS – Integrated
    Accelerator based on


    NPU-PIM Unified Memory System. [1](#page-1-0) To the best of our knowledge, this
    is one of the first works that integrate a commercial NPU with a commercial PIM
    memory to enable a domain-specific system architecture. Previously proposed PIM-based
    systems view PIM as an "accelerator" [\[8,](#page-13-7) [24,](#page-14-6) [26,](#page-14-7)
    [31\]](#page-14-8) and employ a partitioned memory system that uses the dedicated
    memory for the xPU (e.g., GPU, CPU) and the PIM accelerator memory. This leads
    to inefficient memory capacity usage as shared data between xPU and PIM tend to
    be duplicated in both memories for optimal performance. This is especially problematic
    for LLM where parameters of FC layers represent a large portion of data that need
    to be shared between the NPU and the PIM.


    In light of these challenges, we propose a unified memory system where PIM memory
    also serves as the main memory for the NPU. This approach removes the need for
    any data duplication and movement of shared data. However, the unified memory
    system in an NPU-PIM system introduces new challenges as PIM computations and
    normal memory accesses cannot be performed concurrently. In this work, we propose
    a novel PIM Access Scheduling (PAS) that schedules PIM computations and normal
    memory accesses through mapping and scheduling of the workload on the NPU-PIM
    architecture with a unified memory system. The challenges of PIM computation in
    a unified memory system include memory resource conflict with normal memory accesses
    as well as the failure to leverage the potential for parallel execution with computations
    performed on the NPU. Thus, PAS takes into account both resource conflicts and
    parallelizability of operations between the NPU and PIM to fully exploit the parallelism
    across the different resources. We also demonstrate the proof-of-concept of IANUS
    by prototyping the system with an FPGA. In summary, the key contributions of this
    work include the following.


    - 1. Architecture: We propose IANUS, a novel heterogeneous architecture that combines
    a dedicated hardware accelerator (NPU) with a specialized memory (PIM), to accelerate
    operations with diverse characteristics in the end-to-end LLM inference.

    - 2. Unified Memory System & PIM Access Scheduling: Identifying about 90% of model
    parameters shared between the NPU and PIM in the LLM, we propose a unified memory
    system where the memory for the NPU and the PIM memory is shared to efficiently
    utilize the memory capacity. We also propose PIM Access Scheduling (PAS) that
    manages the challenges of the unified memory system with effective workload mapping
    and scheduling. Through a detailed simulation of IANUS, IANUS with PAS achieves
    6.2× and 3.2× speedup in


    <span id="page-1-0"></span><sup>1</sup> IANUS is a Roman god with two faces that
    represented the middle ground between both concrete and abstract dualities. The
    IANUS architecture in this work shares similarities as it represents a "middle
    ground" architecture between NPU and PIM architectures.


    IANUS: Integrated Accelerator based on NPU-PIM Unified Memory System ASPLOS ''24,
    April 27-May 1, 2024, La Jolla, CA, USA


    <span id="page-2-0"></span>![](_page_2_Figure_1.jpeg)


    Figure 1. (a) Structure of GPT with vector operations marked with (V) and (b)
    multi-head attention mechanism shown in detail.


    GPT-2 compared to the A100 GPU and the state-ofthe-art prior work (DFX [\[19\]](#page-13-1)),
    respectively.


    3. System integration and FPGA prototyping: To demonstrate the feasibility of
    IANUS, we build an integrated system including a commercial NPU, commercial PIM
    chips, and an FPGA-based PIM controller.


    # 2 Background


    #### 2.1 Transformer-based LLMs


    NLP usually consists of two stages: input token summarization stage (summarization)
    and output token generation stage (generation). While the summarization stage
    processes all input tokens collectively, the generation stage deals with one generated
    token per stage. In text generation tasks, the summarization stage initially handles
    all inputs, followed by the generation stage processing each produced token.


    Transformer-based LLMs, such as BERT and GPT, use multiple encoder or decoder
    blocks, followed by a task-specific head (Figure [1a\)](#page-2-0). Each block
    consists of multi-head attention module, feed-forward network (FFN) module, layer
    normalization [\[3\]](#page-13-8), and residual addition [\[15\]](#page-13-9).
    During the summarization stage, FC layers typically operate as matrix-matrix multiplication
    with multiple input tokens, while in the generation stage, they perform matrix-vector
    multiplication with a single token. The multi-head attention mechanism is depicted
    in Figure [1b.](#page-2-0) Input tokens () are multiplied with weight matrices
    to generate query (), key (), and value ( ) ( 1 ). In the generation stage, new
    and are concatenated with previous ones. For self-attention, , , and are splitted
    into multiple heads. The matrix product of query and transposed key ( ) ( 2 )
    are executed to compute attention score () ( 3 ) and then output ( ) ( 4 ) within
    each head is generated. Finally, the outputs of all heads are merged and processed
    by the following FC layer ( 5 ).


    <span id="page-2-1"></span>![](_page_2_Figure_10.jpeg)


    Figure 2. Generation stage of GPT-2 XL (a) Latency and FLOPs breakdown of decoders.
    (b) Latency breakdown of self-attention. Results are obtained using an A100 GPU.


    #### 2.2 Platforms for DNN Inference


    Domain-specific Accelerators: DNN accelerators [\[2,](#page-13-10) [5,](#page-13-11)
    [17,](#page-13-12) [23,](#page-13-6) [27,](#page-14-9) [33,](#page-14-10) [36\]](#page-14-11)
    mainly focus on accelerating convolution computation. Therefore, these accelerators
    often face bandwidth bottlenecks during the generation stage of LLMs, primarily
    involving matrix-vector multiplication. To tackle this problem, DFX [\[19\]](#page-13-1),
    an FPGA-based appliance, maximizes bandwidth utilization by designing peak FLOPS
    to match the memory bandwidth. However, while providing significant benefits on
    the generation stage, the benefits of DFX on the summarization stage are small
    because of limited FLOPS.


    Processing-in-Memory: PIM refers to the technology of implementing processing
    units inside memory to accelerate specific workloads or save energy consumption.
    Recently, PIM based on commercial DRAMs have been announced (Accelerator-in-Memory
    (AiM) [\[26,](#page-14-7) [30\]](#page-14-5), HBM-PIM [\[29,](#page-14-4) [31\]](#page-14-8),
    and UPMEM-PIM [\[8\]](#page-13-7)). They are suitable for memoryintensive workloads
    by utilizing all- or half-bank parallelism. As a result, they are considered promising
    solutions for generation stages of LLMs because LLMs include matrix-vector multiplication
    in generation stage.


    # 3 Motivations


    In this section, we show the diverse computation requirements of LLMs and present
    challenges in designing the accelerator system for their end-to-end inference.
    This motivates the need for a heterogeneous architecture that combines a domain-specific
    accelerator with high compute capability and a domain-specific memory with high
    memory bandwidth. We also demonstrate the motivation of a unified main memory
    organization in an NPU-PIM system for LLMs.


    #### 3.1 Diverse Computational Requirements of LLMs


    The generation and the summarization stages exhibit different computational characteristics
    as the generation stage


    of LLMs is often memory-bound with matrix-vector operations while summarization
    stage is compute-bound with matrix-matrix operations. While compute-bound components
    are well-matched to compute accelerators (e.g., NPU, GPU), memory-bound operations
    are not. For example, when generating two tokens with 512 input tokens, the generation
    stage requires 512× fewer FLOPs compared to the summarization stage. However,
    the execution time of the generation stage is 88.5% of the summarization stage
    on A100 GPU. As shown in Figure [2a,](#page-2-1) FCs and FFNs in the generation
    stage that consist of matrix-vector multiplications account for 45.4% of the total
    latency and are well-matched to be accelerated by PIM. In comparison, the summarization
    stage shows an even greater reliance on FCs and FFNs, which mainly employ matrix-matrix
    multiplications, thereby necessitating a compute accelerator (e.g., NPU) for effective
    acceleration. Thus, in this work, we propose a heterogeneous accelerator that
    integrates both an NPU with a PIM to address the diverse computation requirements
    in LLMs.


    In addition, LLMs also include vector operations such as layer normalization and
    non-computing operations such as transpose of matrix. As shown in Figure [2a,](#page-2-1)
    layer normalization and residual addition represent 13.2% of the total latency
    while representing less than 0.06% of the total FLOPs, raising a need for a dedicated
    vector processing unit. Additionally, a significant portion of self-attention
    latency in the decoder is attributed to non-computing operations within the self-attention,
    as in Figures [2a](#page-2-1) and [2b.](#page-2-1) Among operations in self-attention
    that accounts for 41.4% of the total decoder latency, non-computing operations
    occupy 66.1% of the total self-attention latency. This substantial impact of non-computing
    operations highlights the necessity for a domain-specific accelerator with flexible
    data manipulation.


    #### 3.2 Partitioned vs. Unified Memory Systems in LLMs


    Systems using commercial PIM [\[8,](#page-13-7) [24,](#page-14-6) [26,](#page-14-7)
    [31\]](#page-14-8) with CPU or GPU typically employ a partitioned main memory
    system where some main memory is dedicated for PIM accelerator''s memory while
    the remaining memory is used by the host (i.e., CPU or GPU). This approach can
    maximize parallelism as both PIM and the host can access their own memory. However,
    partitioned memory can be problematic if there is significant sharing of data
    between the host and the PIM accelerator as the same data need to be duplicated
    across both memories to maximize the parallelism. Without duplicating data, substantial
    data transfers between two memories are necessary, potentially deteriorating performance.


    In LLMs, the parameters of FC layers need to be shared between the NPU and the
    PIM since they are utilized both in the matrix-matrix and matrix-vector computation.
    Since the FC parameters constitute a large fraction of data required for inference
    (e.g., 91% in GPT-2), using a partitioned memory in the NPU-PIM system for LLMs
    results in inefficient usage of the memory. As a result, we employ a unified memory
    organization where the PIM is used as the main memory for both the PIM accelerator
    and the NPU – resulting in approximately 2× reduction in memory footprint compared
    to partitioned memory system.


    However, a unified memory presents new challenges, compared to the partitioned
    memory system, as the PIM memory is responsible for both "normal" memory accesses
    from the NPU as well as the PIM computation and these two steps cannot be executed
    in parallel. As naïve scheduling does not consider memory resource conflicts between
    PIM computations and normal memory accesses and fails to observe the parallelizability
    between PIM computations and other computations, it cannot exploit available parallelism
    across the NPU and the PIM. In this work, we propose PIM Access Scheduling that
    addresses such challenges of the unified memory system.


    # 4 IANUS Architecture


    To accelerate the end-to-end inference of transformer-based LLMs, we introduce
    IANUS (Integrated Accelerator based on NPU-PIM Unified Memory System) that integrates
    NPU and PIM (Figure [3\)](#page-4-0). This section describes the IANUS architecture,
    including the NPU and the PIM architecture that we leverage, details the transformer-aware
    microarchitecture within NPU and PIM, as well as introduces new microarchitectural
    components that we propose to enable IANUS with a unified memory system architecture.


    #### <span id="page-3-0"></span>4.1 NPU & PIM Architecture


    Computation Units in NPU: As in Figure [3,](#page-4-0) a single core of NPU comprises
    two computing units: the matrix unit (MU) and the vector unit (VU). The MU is
    built on a systolic array [\[25\]](#page-14-12) of 128×64 processing elements
    to accelerate matrixmatrix multiplication, such as FC layers. To enable efficient
    pre- or post-processing, the MU also supports output scaling and bias addition.
    The VU consists of sixteen 4-wide VLIW processors [\[11\]](#page-13-13). As it
    is designed to manage vector operations and general purpose operations that the
    MU cannot efficiently perform, the VU supports element-wise addition, layer normalization
    [\[3\]](#page-13-8), masking, and non-linear activation functions such as softmax
    [\[4\]](#page-13-14) and GELU [\[18\]](#page-13-15).


    Scratch-pad Memories in NPU: The activation scratchpad memory (AM) and the weight
    scratch-pad memory (WM) in the core of NPU supply data to the computing units.
    The WM provides weights, scales, and biases to the matrix unit. The AM serves
    as a data storage for both computing units, typically providing input or activation
    data. The AM adopts a transposed data addressing layout relative to the WM to
    fully exploit the benefits of the matrix unit''s systolic array. Moreover, the
    size of data accessed by a single address in each scratch-pad (entry) is aligned
    with the corresponding dimension of the matrix unit''s systolic array. Specifically,
    the entry size of the AM is twice that of the WM.


    1


    2


    v1


    v2


    v2


    PU


    Draft v5 – 24.03 –


    0 1 **Channel 0''s tile**


    **PIM (Channel 0) Weight matrix**


    <span id="page-4-0"></span>![](_page_4_Figure_2.jpeg)


    Bank 0


    Bank 15


    Figure 4


    1024 BF16 = 2KB


    Figure 3. (Left) Architecture of a core in NPU. (Middle) PIM architecture. (Right)
    Overall architecture of IANUS. …PU


    Weight Scratch-pad Matrix Unit Load PIM Architecture: PIM architecture for IANUS
    is based on the commercial PIM (AiM [\[26,](#page-14-7) [30\]](#page-14-5)) that
    i) exploits true all-bank parallelism, ii) is designed to accelerate end-toend
    matrix-vector multiplication and activation functions in DRAM, and iii) is based
    on commodity DRAM (GDDR6). Processing units (PUs) are implemented at each bank
    of the PIM and a global buffer is implemented at the peripheral circuit (Figure
    [3\)](#page-4-0). The global buffer is shared between all PUs and stores an input
    vector, often reused multiple times when processing matrix-vector products. In
    comparison, large data with low reusability such as weight matrix, often read
    just once during matrix-vector product, are stored at each bank. Each PU, associated
    with each bank, includes a set of multipliers, an adder tree, an accumulator for
    Multiply-Accumulate (MAC) operation, and an activation function unit.


    ## 4.2 Transformer-Aware NPU & PIM Microarchitecture


    In this subsection, we highlight the NPU microarchitecture designed to accelerate
    self-attention and vector operations in transformer-based LLMs, along with the
    data allocation scheme in PIM aimed at optimizing FC operations.


    #### 4.2.1 Data Manipulation in Self-Attention.


    Key Transpose: The transpose operation requires data transfer between on-chip
    and off-chip memory without dedicated hardware, potentially delaying PIM operations
    that also utilize off-chip memory. We avoid off-chip access by executing transpose
    within on-chip through incorporating a streaming path between DMAs (light blue
    boxes in Figure [3\)](#page-4-0) of two scratch-pads. However, moving data from
    the activation scratch-pad (AM) to the weight scratch-pad (WM) through on-chip
    DMA only performs a partial transpose operation because of the mismatch of the
    entry sizes for the two scratchpads. Thus, we introduce a streaming buffer between
    the two scratch-pads for on-chip data movement during on-chip DMA. We then implement
    weight interleaving within the matrix unit, enabling access to the WM entry with
    a specific stride.


    Splitting / Merging Attention Heads: Splitting and merging attention heads represent
    a large fraction of the self-attention latency in a GPU due to the data reordering.


    <span id="page-4-1"></span>![](_page_4_Figure_10.jpeg)


    Processing Figure 4. Data allocation and tiling scheme for a matrixvector multiplication
    in PIM.


    Unit


    BK 1 Cell BK 1 Processing Unit Cell Our compiler avoids such data reordering by
    carefully defining and generating activation scratch-pad addresses of input and
    output data in the command. For instance, when generating commands for the FC
    operation that produces , the compiler generates as many commands as the number
    of heads. The compiler then assigns a distinct output address for each command,
    guiding the matrix unit to store in the scratch-pad in a split manner. Hence,
    no data reordering overhead is required. Similarly, the compiler ensures consecutive
    output addresses of each head''s command for merging attention heads.


    #### 4.2.2 Vector Operations in Vector Unit.


    Layer Normalization: Given the limited amount of on-chip memory within the vector
    unit (VU), a two-phase approach is used where VU calculates the mean and variance
    of the tokens in the first phase while the normalization is done in the second
    phase.


    Masked Softmax: We combine masking and softmax [\[4\]](#page-13-14) within a single
    kernel. Each mask is stored as a 1-bit bitmap, reducing data movement and memory
    usage. In softmax, we subtract the max value for stability instead of the large
    value.


    GELU: For the GELU activation [\[18\]](#page-13-15), VU uses a lookup table (LUT)
    approximation, widely employed due to its accuracy and performance [\[19,](#page-13-1)
    [50\]](#page-15-1). GELU activation is also supported in PIM by reserving some
    DRAM rows inside PIM as LUT for the activation function and linearly interpolating
    data from the LUT within the processing unit of PIM.


    #### 4.2.3 Data Allocation in PIM.


    PIM is exploited for matrix-vector multiplication during the FC layers in the
    generation stages. We exploit data allocation


    and tiling scheme that maximize the performance of FC layers in PIM and demonstrate
    these strategies with the weight matrix of an FC layer in Figure [4.](#page-4-1)
    The weight matrix is divided into tiles with each tile consisting of 16 (number
    of banks per channel) × 8 (number of channels for IANUS) rows and up to 1024 columns
    (number of elements in one DRAM row). Each row in the tile is allocated to the
    same DRAM row address across each bank and each channel to maximize performance
    as PIM can perform computation across all banks and all channels in parallel.
    While the optimal tiling can vary across workloads, we assume row-major tiling.


    #### 4.3 IANUS Microarchitecture


    Command Scheduler: The command scheduler is responsible for checking dependencies
    between each command and the status of each compute, DMA, and PIM unit and sending
    commands to each unit. When a command has no dependency and the corresponding
    unit is in an idle state, the scheduler pushes the command into the "issue" queue
    of the unit, and the unit executes it. If the command cannot be issued, the command
    is pushed into the "pending" queue. Upon completion of execution, the scheduler
    resolves the dependencies between the command and the other commands.


    PIM Control Unit and PIM Memory Controller: Orchestrating multiple PIM chips is
    not trivial as it requires scheduling a large number of PIM commands and increases
    the complexity of the command scheduler. More importantly, the efficiency of PIM
    computation diminishes if a standard memory command is inserted in the middle
    of multiple PIM commands for a single "operation", such as a matrix-vector multiplication.
    Thus, we propose macro PIM command for scheduling. One macro PIM command, which
    represents a single operation, comprises multiple micro PIM commands (e.g., a
    single matrix-vector operation is executed through a macro PIM command that consists
    of multiple micro PIM commands, including providing the input vector, performing
    the MAC operation, etc.). To support macro PIM commands, a PIM control unit (PCU)
    and PIM memory controller (PIM MC) are added, as shown in Figure [3.](#page-4-0)


    When one macro PIM command reaches "ready" state, the command scheduler forwards
    the macro PIM command to the PCU. At the same time, the scheduler puts other DMA
    commands related to the off-chip memory into "wait" state to ensure the PIM execution
    is not interrupted. Once the PCU receives the macro PIM command, PCU decodes it
    into multiple micro PIM commands and forwards these to the PIM MC through the
    network-on-chip (NoC).


    The PIM MC supports both PIM commands and normal memory commands. Similar to conventional
    memory controllers, PIM MC tracks the state of each memory bank and generates
    appropriate commands following pre-defined timing constraints as well as newly
    introduced states and timing


    <span id="page-5-0"></span>


    | IANUS''s<br>address mapping | Row        | Channel             | Bank | Column                 |
    Offset |  |

    |----------------------------|------------|---------------------|------|------------------------|--------|--|

    |                            | Tile index | Row index in a tile |      | Column
    index in a tile |        |  |


    Figure 5. IANUS''s DRAM address mapping with the mapping of tile shown in Figure
    [4.](#page-4-1)


    constraints of PIM operations. When all micro PIM commands within one macro PIM
    command finish, the completion signal is forwarded to the command scheduler to
    enable DMA commands associated with the off-chip memory.


    Network-on-chip: The NoC topology in IANUS provides all-to-all connectivity between
    all of the cores and the PIM MCs. The NoC traffic for IANUS consists of both the
    memory traffic as well as the PIM traffic to support the unified memory system.
    All-to-all connectivity ensures that each core can access any memory channel when
    PIM is used as the main memory of the NPU. In addition, the NoC is also used for
    PIM commands from the PCU to the PIM MCs. The NoC supports broadcasting of PIM
    commands to all PIM MCs to reduce NoC bandwidth demand while providing parallel
    PIM operations across all PIM channels.


    DRAM Address Mapping: The DRAM address mapping of IANUS is shown in Figure [5.](#page-5-0)
    IANUS employs an address mapping of (MSB) Row-Channel-Bank-Column (LSB) and the
    main goal of the IANUS''s mapping is to maximize PIM computation performance through
    PIM-aware tile (Figure [4\)](#page-4-1) placement. By using the row address bit
    as the MSB and using those bits as the index of a tile, data within a single tile
    share the same row address that ensures row conflicts do not occur during the
    compute operations related to a single tile. In addition, each tile is assigned
    to a different row address. The column address bit is used as the LSB to ensure
    that operations on all elements of a single row within a tile are handled by one
    processing unit to execute MAC within one bank. Placing channel and bank address
    bits between the row and column address bits allows each row within a tile to
    be distributed across different channels and banks. This enables the PIM to concurrently
    compute all rows within a tile by leveraging channel and bank parallelism and
    maximize PIM computation throughput.


    # 5 PIM Access Scheduling


    The integrated NPU-PIM architecture with a unified main memory presents challenges
    as the main memory is used by both the NPU and the PIM compute logic. In this
    section, we propose PIM Access Scheduling (PAS) that enables efficient sharing
    of the physical memory between NPU and PIM. Unlike traditional memory access scheduling
    [\[40\]](#page-14-13) that involves scheduling of memory commands, PAS not only
    needs to consider scheduling normal DRAM commands and PIM commands but also needs
    to address the challenges of workload mapping across the NPU and the PIM. More
    importantly, the scheduling or mapping of the workloads impacts how


    <span id="page-6-1"></span>![](_page_6_Figure_2.jpeg)


    Figure 6. Workload mapping and execution flow, featuring intra-layer parallelism
    and attention head parallelism. For simplicity, only one attention head per core
    is shown. The mapping of operations in self-attention is detailed in Section [5.3.](#page-6-0)


    <span id="page-6-2"></span>Algorithm 1 Adaptive mapping algorithm for FC layers.


    Input/Output: CMDs (a sequence of commands) Params: n (number of input tokens),
    T (the size of MU) Define: , , , (analytical model of units)


    |     | 1: for 𝑖, 𝑐𝑚𝑑<br>in 𝐶𝑀𝐷𝑠<br>do                                |

    |-----|---------------------------------------------------------------|

    | 2:  | if 𝑐𝑚𝑑.𝑡𝑦𝑝𝑒<br>== 𝑀𝑈𝐹𝐶<br>then                                |

    | 3:  | 𝑝𝑟𝑒𝑣_𝑐𝑚𝑑<br>𝐶𝑀𝐷𝑠[𝑖<br>←<br>−<br>1]                            |

    | 4:  | // Check prefetching                                          |

    | 5:  | if 𝑝𝑟𝑒𝑣_𝑐𝑚𝑑.𝑡𝑦𝑝𝑒<br>== 𝑉𝑈<br>then                             |

    | 6:  | 𝑡𝑝𝑟𝑒 𝑓 𝑒𝑡𝑐ℎ<br>𝑉𝑈<br>(𝑛, 𝑝𝑟𝑒𝑣_𝑐𝑚𝑑.𝑑𝑖𝑚)<br>←                   |

    | 7:  | // Consider tiling and pipelining for MU                      |

    | 8:  | 𝑤𝑐 𝑓 𝑔<br>𝑐𝑚𝑑.𝑤𝑒𝑖𝑔ℎ𝑡_𝑐 𝑓 𝑔<br>←                               |

    | 9:  | 𝑤𝑙𝑜𝑎𝑑<br>𝐷𝑀𝐴𝑤𝑒𝑖𝑔ℎ𝑡<br>(𝑤𝑐 𝑓 𝑔)<br>←                           |

    | 10: | 𝑚𝑢𝐹𝐶<br>𝑀𝑈𝐹𝐶<br>(𝑛,𝑤𝑐 𝑓 𝑔)<br>←                               |

    | 11: | 𝑚𝑢𝑡𝑖𝑚𝑒<br>𝑝𝑖𝑝𝑒<br>( (𝑤𝑙𝑜𝑎𝑑,𝑚𝑢𝐹𝐶),𝑇<br>𝑡𝑝𝑟𝑒 𝑓 𝑒𝑡𝑐ℎ<br>←<br>) − |

    | 12: | // Calculate PIM time                                         |

    | 13: | 𝑝𝑖𝑚𝑡𝑖𝑚𝑒<br>𝑛<br>𝑃𝐼𝑀(𝑤𝑐 𝑓 𝑔)<br>←<br>×                         |

    | 14: | if 𝑝𝑖𝑚𝑡𝑖𝑚𝑒<br><<br>𝑚𝑢𝑡𝑖𝑚𝑒<br>then                             |

    | 15: | Replace 𝐶𝑀𝐷𝑠[𝑖].𝑡𝑦𝑝𝑒<br>with 𝑃𝐼𝑀                              |


    the DRAM/PIM commands are scheduled. In this section, we describe PAS within the
    context of IANUS, particularly on FC operations and multi-head attention in LLMs,
    and how they are mapped/scheduled on IANUS.


    ## 5.1 Overview


    We present the execution flow and workload mapping of LLMs for IANUS in Figure
    [6.](#page-6-1) To leverage parallelism across all cores in the NPU as well as
    across all PIM chips, we exploit attention head parallelism by partitioning the
    weights of the FC for , , and across PIM chips in a head-wise scheme. Through
    the head-wise partitioning, each core can access the memory in parallel to load
    the weights or the output of PIM compute for the multi-head attention.


    For other FC operations, we leverage intra-layer parallelism to minimize data
    movement of weights that are considerably larger than input or activation data
    in LLMs. To reduce synchronization overhead between each core in the NPU, we


    partition the weights of FC column-wise. Synchronization occurs four times: once
    after multi-head attention, twice after each residual addition, and once after
    GELU. Meanwhile, layer normalization and residual addition are mapped to the vector
    unit (VU) within the NPU (Figure [6\)](#page-6-1).


    ## 5.2 FC Operation


    FC can be mapped to either the matrix unit (MU) or the PIM. The summarization
    stage often has a large input token size and results in high computation requirements
    – thus, it is more appropriate to map the FC to the matrix unit. When the input
    token size is small, loading the weights from the memory can become the bottleneck
    because the arithmetic intensity of the FC operation decreases. Thus, an adaptive
    mapping algorithm is necessary to determine whether to map the FC to the PIM or
    the MU within the NPU.


    An overview of the adaptive mapping algorithm is summarized in Algorithm [1.](#page-6-2)
    To determine the appropriate unit for FC, we develop a simple analytical model
    that estimates the execution time across different execution units (e.g., MU,
    VU, DMA, PIM) based on the number of input tokens at compile time. The input for
    the adaptive mapping algorithm is a sequence of commands based on mapping of the
    FC to the matrix unit. When estimating the time of FC on MU, we assume a pipelined
    scheme for both weight loading and computation, as well as tiling configured to
    match the MU''s size (lines 8-11). We also account for weight prefetching time
    if an operation of VU precedes the FC operation (lines 5-6). We then compare the
    estimated time of FC on MU with that of PIM and assign the FC to the execution
    unit that can complete sooner (lines 13-15). If the first FC of FFN is mapped
    to the PIM, the GELU will also be allocated to the PIM since the PIM is designed
    to support GELU right after FC.


    ## <span id="page-6-0"></span>5.3 Multi-Head Attention


    As described earlier in Figure [1b,](#page-2-0) multi-head attention consists
    of a series of operations that have various computational requirements. While
    IANUS provides computing capability of both NPU and PIM, naïve scheduling that
    overlooks parallelizability and resource conflicts between operations may lead
    to the under-utilization of both units with considerable latency overhead. To
    address this challenge, we propose unified memory-aware scheduling for multi-head
    attention at both the summarization and generation stages.


    Summarization stage: As shown in Figure [7a,](#page-7-0) FC layers for , , and
    typically operate as matrix-matrix multiplications with multiple input tokens
    () – thus are computed in the matrix unit, while weight matrices ( ,, ) are loaded
    from the memory via DMA. To efficiently process multi-head attention, we utilize
    both intra-attention head parallelism and inter-attention head pipelining. We
    prioritize key generation to execute key transpose in parallel with value generation.
    As DMAs are utilized for on-chip transpose, they


    Figure 7


    <span id="page-7-0"></span>![](_page_7_Figure_1.jpeg)


    **Load Store PIM Intra-head parallel Inter-head pipeline**


    Figure 7. Unified memory-aware scheduling for multi-head attention at (a) summarization
    stage where FCs are mapped to the matrix unit and generation stage where FCs are
    mapped to the PIM: and mapping to (b) PIM or (c) matrix unit. Figures (b) and
    (c) are drwan on the same time scale to show the latency difference.


    are not used for PIM access during transpose ( 1 ). Given that the matrix unit
    supports output scaling (Section [4.1\)](#page-3-0), the key scaling operation
    is omitted. We also ensure that key and value are stored during computations (
    2 ). To hasten the start of the operation, values are moved to the weight scratch-pad
    via on-chip data transfer during the softmax ( 3 ). In addition, we utilize inter-attention
    head pipelining by prefetching the weight of the next head ( 4 ).


    Generation stage: FC layers mainly perform matrix-vector multiplications with
    one input token (), making them wellsuited for PIM computation. Similarly, since
    and operations involve matrix-vector multiplications and require loading previously
    generated keys and values, their executions can appear to be more suitable for
    PIM. As shown in Figure [7b,](#page-7-0) mapping and to PIM avoids such load operations.
    However, the overall performance benefit is limited since parallelism across both
    the PIM and the NPU cannot be exploited well as the PIM performs most of the operation.
    In addition, computing and in PIM results in poor efficiency because of the mismatch
    between the PIM DRAM row size and the data size. For example, with a head dimension
    of 64, PIM computational efficiency of is only 6.25% due to only 64 BF16 elements
    being utilized for computation out of the 1024 elements available in one DRAM
    row.


    As a result, we propose mapping and operations to the matrix unit, and accordingly,
    scheduling based on this mapping. To exploit inter-attention head parallelism,
    as


    Table 1. Simulation parameters for IANUS.


    <span id="page-7-1"></span>


    | NPU         | 4 cores, 8 PIM memory controllers<br>Composition                                                           |                                                                      |  |  |  |  |

    |-------------|------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------|--|--|--|--|

    |             | Host interface                                                                                             |
    PCIe 5.0 ×16                                                         |  |  |  |  |

    |             | Frequency                                                                                                  |
    700 MHz                                                              |  |  |  |  |

    |             | Matrix unit<br>128x64 processing elements (PEs), 4 MACs per PE,
    46 TFLOPS                                  |                                                                      |  |  |  |  |

    |             | Vector unit<br>Sixteen 4-wide VLIW processors                                                              |                                                                      |  |  |  |  |

    | Core        | Scheduler                                                                                                  |
    4 command slots per issue queue of units,                            |  |  |  |  |

    |             |                                                                                                            |
    256 command slots in pending queue                                   |  |  |  |  |

    |             | Scratch-pad                                                                                                |
    Activation 12 MB, Weight 4 MB                                        |  |  |  |  |

    |             | Memory                                                                                                     |
    GDDR6 16 Gb/s; ×16 organization; 8 channels; 256 GB/s;               |  |  |  |  |

    |             | 2 channels per chip, 16 banks per channel, row (page) size 2 KB<br>configuration<br>𝑸𝑸<br>𝑸𝑸<br>𝑸𝑸<br>𝑾𝑾𝒊𝒊
    |                                                                      |  |  |  |  |

    | PIM<br>𝑾𝑾𝒊𝒊 | 𝑾𝑾𝒊𝒊<br>𝑾𝑾𝒊𝒊<br>Timing parameters<br>𝑸𝑸<br>𝑸𝑸<br>𝑾𝑾𝒊𝒊                                                      |
    𝑡𝐶𝐾 = 0.5𝑛𝑠, 𝑡𝐶𝐶𝐷𝑆 = 𝑡𝐶𝐶𝐷𝐿 = 1𝑛𝑠, 𝑡𝑅𝐴𝑆 = 21𝑛𝑠,<br>Inter-head         |  |  |  |  |

    |             |                                                                                                            |
    𝑡𝑊 𝑅 = 36𝑛𝑠, 𝑡𝑅𝑃 = 30𝑛𝑠, 𝑡𝑅𝐶𝐷𝑅𝐷 = 36𝑛𝑠, 𝑡𝑅𝐶𝐷𝑊 𝑅 = 24𝑛𝑠<br>𝑸𝑸<br>𝑾𝑾𝒊𝒊 |  |  |  |  |

    |             | Processing unit (PU)                                                                                       |
    1 GHz; 1 PU per bank; 32 GFLOPS per PU<br>Inter-head                 |  |  |  |  |

    |             | Global buffer<br>𝑸𝑸<br>𝑸𝑸<br>𝑾𝑾𝒊𝒊<br>𝑾𝑾𝒊𝒊                                                                  |
    One 2 KB global buffer per channel<br>𝑸𝑸<br>𝑾𝑾𝒊𝒊                     |  |  |  |  |

    |             |                                                                                                            |                                                                      |  |  |  |  |


    <span id="page-7-2"></span>Table 2. Specifications of A100 GPU, DFX [\[19\]](#page-13-1),
    and IANUS.


    1


    Inter-head


    |          |             | A100 [37]<br>1    | DFX [19]    | IANUS                         |  |

    |----------|-------------|-------------------|-------------|-------------------------------|--|

    |          | Frequency   | 1155 MHz          | 200 MHz     | 700 MHz                       |  |

    | Compute  | Throughput  | 255 TFLOPS<br>1   | 1.64 TFLOPS | 184 TFLOPS                    |  |

    | On-chip  |             | RF, L1, L2: 84 MB | ∼40 MB      | Activation Scratch-pad:
    48 MB |  |

    | Memory   | Capacity    |                   |             | Weight Scratch-pad:
    16 MB     |  |

    |          | Type        | HBM2e             | HBM2        | GDDR6                         |  |

    | Off-chip | Capacity    | 80 GB             | 32 GB       | 8 GB                          |  |

    | Memory   | Bandwidth   | 2039 GB/s         | 1840 GB/s   | 256 GB/s                      |  |

    |          | Internal BW | N/A               | N/A         | 4096 GB/s                     |  |


    shown in Figure [7c,](#page-7-0) we execute key concatenation in the vector unit
    instead of storing the key ( 1 ), enabling its simultaneous execution with query
    generation in PIM. Loading the previously generated keys ( ) of th head is omitted
    in Figure [7c,](#page-7-0) as its small size compared to the FC weight allows
    for prefetching. We then transpose concatenated keys within on-chip while performing
    query generation in PIM. Furthermore, we execute and softmax respectively in parallel
    with value generation by mapping to matrix unit ( 2 ). After value generation,
    storing generated keys and values and loading concatenated values () are performed
    during softmax ( 3 ). We also employ inter-attention head pipelining by prefetching
    of the next head during ( 4 ). If the prefetching ends before the completion of
    , the key generation of the next head is performed in conjunction with . Consequently,
    our scheduling enhances performance by maximizing both intra-parallelism and inter-pipelining
    of attention head.


    # 6 Evaluations


    ## 6.1 Methodology


    To evaluate the performance of IANUS, we developed a cycle-accurate in-house simulator
    to model IANUS. The simulator integrates an NPU simulator based on a commercial
    NPU [\[1,](#page-13-16) [20,](#page-13-17) [41\]](#page-14-15) as well as a PIM
    simulator modeled after the real PIM chip, AiM [\[26,](#page-14-7) [30\]](#page-14-5).
    Both the NPU and the PIM simulator are validated against their respective real
    hardware counterparts within a 5% error margin. An overview of the key simulation
    parameters is summarized in Table [1.](#page-7-1) In addition, we modeled the
    new components added to enable IANUS, including the PIM control unit (PCU), and
    modified the memory controller to support both PIM commands and


    <span id="page-8-1"></span>![](_page_8_Figure_2.jpeg)


    Figure 8. Inference latency of various GPT-2 models on A100 GPU and IANUS.


    Table 3. Network configuration details.


    <span id="page-8-0"></span>


    |      |      | Name Embedding<br>dimension | Head<br>dimension # Heads |    |
    # Blocks | # Params | Workload                      |

    |------|------|-----------------------------|---------------------------|----|----------|----------|-------------------------------|

    | BERT | B    | 768                         | 64                        | 12 |
    12       | 110M     |                               |

    |      | L    | 1024                        | 64                        | 16 |
    24       | 340M     | Question<br>answering<br>(QA) |

    |      | 1.3B | 2048                        | 64                        | 32 |
    24       | 1.3B     |                               |

    |      | 3.9B | 2560                        | 64                        | 40 |
    48       | 3.9B     |                               |

    | GPT  | M    | 1024                        | 64                        | 16 |
    24       | 345M     | Language<br>modeling          |

    |      | L    | 1280                        | 64                        | 20 |
    36       | 762M     |                               |

    |      | XL   | 1536                        | 64                        | 24 |
    48       | 1.5B     |                               |

    |      | 2.5B | 1920                        | 96                        | 20 |
    54       | 2.5B     | (LM)                          |


    normal memory commands. To avoid latency overhead from the PCU, we designed its
    operations to be pipelined with PIM computations. Our simulator also provides
    statistics on energy consumption. It measures the dynamic energy consumed by cores
    in NPU, PIM operations, and standard DRAM operations. Based on prior analysis
    [\[26\]](#page-14-7), we assume that the power consumption of PIM computing operations
    is 3× of that for DRAM read operations.


    We compare the performance of IANUS against a GPU, state-of-the-art prior work
    (DFX [\[19\]](#page-13-1)), as well as the NPU without PIM memory. For the GPU,
    we use an NVIDIA A100- SXM GPU [\[37\]](#page-14-14) with Pytorch 2.0 and CUDA
    Toolkit 11.8 and GPU-optimized source codes from Huggingface [\[47\]](#page-14-16)
    and Megatron-LM [\[43\]](#page-14-17). The latency of models is measured using
    the torch.cuda.Event API. DFX [\[19\]](#page-13-1) is a multi-FPGA appliance specifically
    designed to accelerate the generation stage of GPT models. We assume a DFX with
    4 FPGAs that can support GPT-2 XL model. We also compare IANUS with a commercial
    NPU [\[1,](#page-13-16) [20,](#page-13-17) [41\]](#page-14-15) (the same NPU used
    in IANUS) without PIM, but with standard GDDR6 memory (NPU-MEM). It shares identical
    specifications with IANUS in Table [2](#page-7-2) except for the internal memory
    bandwidth and features a peak throughput of 184 TFLOPS. IANUS is identical to
    NPU-MEM, except that standard GDDR6 memory is replaced with PIM based on AiM [\[26,](#page-14-7)
    [30\]](#page-14-5). Each PIM chip achieves a peak throughput of 1 TFLOPS with
    32 processing units utilizing 1024 GB/s internal memory bandwidth. The specifications
    of each architecture are summarized in Table [2.](#page-7-2)


    We evaluate two notable transformer-based LLMs, BERT [\[9\]](#page-13-0) and GPT
    [\[39\]](#page-14-1) with the BF16 [\[46\]](#page-14-18) data type, which maintains
    the accuracy of the full-precision model. The configurations and tasks of each
    model are presented in Table [3.](#page-8-0) We exploit a GPT-2 XL model with
    its attention heads reduced from 25 to 24, whose accuracy was validated in [\[19\]](#page-13-1),
    to optimize parallelism. We assess the end-to-end performance of models with input
    sizes of 128, 256, and 512 tokens. For the GPT-2, we use output sizes of 1, 8,
    64, and 512 tokens. These sizes represent the typical user request ranges for
    NLP services in datacenters [\[38\]](#page-14-19). Due to the time overhead associated
    with gathering inputs from multiple users, current datacenters prefer running
    the model with non-batched input [\[12,](#page-13-18) [19\]](#page-13-1); therefore,
    we evaluate our work using a batch size of 1.


    #### 6.2 Performance Results


    End-to-end Inference Latency: Figure [8](#page-8-1) presents the endto-end latency
    of GPT-2 models on the GPU and IANUS. The result shows that IANUS achieves a 4.3×
    speedup compared to the GPU for the 2.5B model, on average. For the workload with
    significantly more output tokens than input tokens, i.e., (128,512), IANUS demonstrates
    12.0×, 8.1×, and 6.6× lower latency than the GPU for the GPT-2 M, L, and XL models,
    respectively. These substantial speedups are obtained from the high utilization
    of PIM chips'' internal bandwidth of 4096 GB/s for matrix-vector multiplication
    in the generation stage. On average, IANUS takes about 5.7 ms per token for generation
    stages of the GPT-2 2.5B model with configuration (128,64), while the GPU takes
    about 29.9 ms.


    In Figure [9,](#page-9-0) we conduct a comparison of the GPT-2 XL''s latency among
    IANUS, NPU-MEM, and DFX with four FPGAs [\[19\]](#page-13-1), which provides state-of-the-art
    performance for GPT-2. Input and output token sizes for the comparison are obtained
    from [\[19\]](#page-13-1). IANUS achieves a 49.3× speedup compared to DFX for
    the (128,1) configuration. IANUS and NPU-MEM present similar performance for this
    configuration, as the PIM in IANUS operates as a standard GDDR6 except for the
    LM head. For the generation stage, DFX achieves 6.9 ms


    <span id="page-9-0"></span>![](_page_9_Figure_1.jpeg)


    Figure 9. Inference latency of GPT-2 XL on DFX [\[19\]](#page-13-1), NPU-MEM,
    and IANUS.


    <span id="page-9-1"></span>![](_page_9_Figure_3.jpeg)


    Figure 10. Latency breakdown of GPT-2 XL and L''s generation stages for NPU-MEM
    and IANUS.


    <span id="page-9-2"></span>![](_page_9_Figure_5.jpeg)


    Figure 11. Dynamic energy of NPU-MEM and IANUS, normalized to IANUS with GPT-2
    M.


    to generate one token for the (64,256) configuration, while IANUS generates a
    token in 3.8 ms for the same configuration, achieving a speedup of 1.8×. Without
    the benefits of PIM, NPU-MEM takes 15.5 ms. To this end, IANUS achieves an average
    speedup of 3.2× compared to DFX, while NPU-MEM results in 24% slowdown.


    Latency Breakdown: To investigate the impact of using PIM, we measure the latency
    of operations in the decoder for NPU-MEM and IANUS in the generation stages of
    GPT-2 L and XL. As residual additions are executed with adjacent FC and FFN using
    a pipelining scheme, we collectively measure their latency. As shown in Figure
    [10,](#page-9-1) IANUS reduces the execution time of two FCs in multi-head attention
    from 890 ms to 215 ms for the GPT-2 XL model, achieving a speedup of 4.1×. Since
    the FFN has a four times larger weight size compared to these two FCs, it achieves
    a higher speedup of 5.1 ×. IANUS also achieves a speedup of 4.3 × for self-attention
    without offloading any operation in self-attention. This speedup is obtained from
    prefetching previously generated keys and values instead of the weight for generating
    , , and by offloading FC for , , and generation to PIM. Overall, IANUS achieves
    speedups of 4.0× and 3.6× for GPT-2 XL and L models, respectively, compared to
    NPU-MEM.


    <span id="page-9-4"></span>![](_page_9_Figure_9.jpeg)


    Figure 12. Performance evaluation of the adaptive mapping algorithm for FC across
    different GPT-2 models as the number of input tokens are varied from 4, 8, to
    16.


    Energy Efficiency: Figure [11](#page-9-2) presents dynamic energy consumption
    of IANUS and NPU-MEM for GPT-2 models where input and output token sizes are set
    to 256 and 512, respectively. The energy values are normalized to the dynamic
    energy consumed by IANUS with GPT-2 M. By offloading FC layers of the generation
    stage to PIM, IANUS achieves 10.5- 13.4× reduction in energy consumption for normal
    memory operations across all models. The energy consumption for computation of
    cores in NPU is also decreased by a factor of 6.3-10.2×. The reduction in energy
    consumption for cores'' computation and normal memory operations tends to increase
    as the model size expands. Meanwhile, the energy is consumed by PIM operations
    in IANUS. As a result, IANUS obtains 3.7×, 3.6×, 3.9×, and 4.4× improvement in
    energyefficiency compared to NPU-MEM for GPT-2 M, L, XL, and 2.5B, respectively.
    Despite its larger model size, GPT-2 L results in a smaller energy efficiency
    improvement compared to GPT-2 M due to its embedding dimension size of 1280, which
    results in twice the number of row activations, compared to GPT-2 M''s size of
    1024 for PIM computation. Note that energy efficiency in a real system can be
    further improved if static energy consumption is also considered [2](#page-9-3)
    .


    Adaptive Mapping Algorithm for FC: To evaluate the benefits of Algorithm [1,](#page-6-2)
    we evaluate the performance of GPT models when FC is mapped to PIM and matrix
    unit on various input token sizes and compared it to the result of Algorithm [1.](#page-6-2)
    As illustrated in Figure [12,](#page-9-4) Algorithm [1](#page-6-2) is effective
    with small input sizes and chooses the appropriate computation unit on various
    model sizes and input token sizes. When executing FC layers in PIM, execution
    time is proportional to the input token size as PIM sequentially repeats matrixvector
    multiplication as much as the input token size. On the other hand, the matrix
    unit shows similar performance across 4, 8, and 16 input tokens because of the
    capability of processing 128 tokens in parallel. Therefore, the matrix unit achieves
    better performance for large input token sizes. Another factor for workload mapping
    is the embedding size of the model. As the global buffer and row size of PIM is
    2KB (= 1024 BF16), models with embedding sizes that are multiples of 1024 can
    fully utilize PIM. As a result, PIM shows higher performance than the matrix unit
    at an input size of 8 for GPT-2 M (embedding size of 1024) and GPT-2 2.5B (1920,


    <span id="page-9-3"></span><sup>2</sup>Static energy consumption was not incorporated
    in the analysis because of the challenge in providing fair comparisons.


    IANUS: Integrated Accelerator based on NPU-PIM Unified Memory System ASPLOS ''24,
    April 27-May 1, 2024, La Jolla, CA, USA


    <span id="page-10-0"></span>![](_page_10_Figure_1.jpeg)


    Figure 13. Performance comparisons between unified and partitioned memory systems
    and the impact of unified memory-aware scheduling. Dashes in the bar border indicate
    the system type. Colors represent mapped units of and . The pattern indicates
    the application of scheduling.


    nearly 2×1024). With Algorithm [1,](#page-6-2) we achieve an average speedup of
    1.4× and 1.2× when compared to mapping FC to PIM and the matrix unit, respectively.


    Unified vs. Partitioned Memory System: In Figure [13,](#page-10-0) we evaluate
    the performance benefits of unified memory system (IANUS) compared to partitioned
    memory organization. Both configurations have the same total memory capacity of
    8 GB – with the unified system having 8 GB for both the PIM and the NPU, while
    for the partitioned configuration, 4 GB is dedicated for the NPU''s main memory
    and 4 GB is dedicated for PIM. While the memory capacity is the same, the unified
    memory has the benefit of additional compute provided through the extra amount
    of PIM memory available.


    We evaluate both systems using GPT-2 models with a (256,512) configuration. In
    the partitioned memory, all FC parameters shared between PIMs and NPU are duplicated
    across the both memory to avoid performance overhead caused by data movement between
    standard DRAMs and PIMs. However, for the 2.5B model, the entire FC parameters
    cannot be duplicated across both the PIM and the DRAM because of the limited memory
    capacity. Thus, to minimize transfer overhead between the two types of memories,
    the NPU''s matrix unit is mainly responsible for the FC operations on the non-duplicated
    parameters. For a fair comparison, we implement scheduling for the partitioned
    memory system that maximizes the benefits from parallel executions of NPU and
    PIM by mapping the and to the matrix unit. As shown in Figure [13,](#page-10-0)
    the concurrent execution of NPU''s DRAM accesses and PIM computations results
    in an average 1.3× speedup in the partitioned system.


    For GPT-2 M, L, and XL models, IANUS–the unified memory system–(the rightmost
    bar for each model) outperforms the scheduled partitioned memory system by 1.4-1.6×
    speedup (Figure [13\)](#page-10-0). These speedups result from the doubled PIM
    throughput that is available in the unified memory configuration. For the GPT-2
    2.5B model, IANUS shows a larger performance improvement due to the performance
    overhead in the partitioned system, stemming from the data movement of non-duplicated
    parameters from the PIM to the NPU. Similar to performance trends of other models,
    while not shown, IANUS achieves approximately 1.5× speedup in GPT-2 2.5B


    <span id="page-10-1"></span>![](_page_10_Figure_7.jpeg)


    Figure 14. Throughput and compute utilization of the BERT models on A100 GPU and
    IANUS.


    compared to the partitioned system if sufficient memory capacity is provided such
    that all FC parameters can be stored in each memory type.


    Unified Memory-Aware Scheduling for Multi-Head Attention: Figure [13](#page-10-0)
    demonstrates the performance enhancement through mapping of and operations and
    corresponding scheduling for multi-head attention in IANUS (the unified memory
    system). As in the figure, scheduling for the mapping of and to PIM results in
    an average performance boost of 7% across all models compared to naïve scheduling.
    When and operations are mapped to the matrix unit, a reduction in computation
    time for these operations leads to superior performance than the case of scheduling
    with PIM mapping for all models except GPT-2 2.5B. For the GPT-2 2.5B model, which
    has a larger head dimension size of 96 than other models, the loading time for
    the previously generated keys and values increases. This loading time is not required
    when and are mapped to PIM, thus reducing the benefits gained through matrix unit
    mapping. However, through effective scheduling, we attain a performance improvement
    of 24% for GPT-2 2.5B. Consequently, unified memory-aware scheduling yields an
    average performance improvement of 34%.


    Throughput and Compute Utilization: Figure [14](#page-10-1) presents the throughput
    and utilization of the IANUS and the GPU for BERT models. In IANUS, only the matrix
    unit and vector unit of the NPU are utilized for computation, excluding PIM, as
    BERT models do not include matrix-vector multiplication. By managing complex data
    manipulation in self-attention through on-chip data movement, IANUS attains 3.1×
    and 2.0× higher average throughput for BERT-B and L, respectively, despite having
    1.4× lower peak FLOPS than the GPU.


    As the FLOPs increase with model size, IANUS''s throughput becomes less than the
    GPU due to its limited peak FLOPS. However, IANUS achieves 5.2×, 3.3×, 1.3×, and
    1.0× higher average utilization for BERT-B, L, 1.3B, and 3.9B compared to the
    GPU. This enhanced utilization is attributed to the efficient execution of vector
    operations with the vector unit in addition to the benefits gained from self-attention.


    Sensitivity Study of Design Parameters: We conduct sensitivity studies on the
    number of cores in NPU and PIM


    <span id="page-11-0"></span>![](_page_11_Figure_1.jpeg)


    Figure 15. Sensitivity studies for summarization-only (256,1) and generation-dominant
    cases (256,512) as the numbers of cores and PIM chips are varied. Results are
    normalized to 4 cores and 4 PIMs.


    <span id="page-11-1"></span>![](_page_11_Figure_3.jpeg)


    **NPU SW model** DMA Command Scheduler Compiler **PIM Hardware Platform** PIM
    MCsFigure 16. System prototype of IANUS (PCU: PIM Control Unit).


    PCU


    **PIM Device Driver PIM FPGA** chips. To show the sensitivities of NPU and PIM
    computation capabilities, we keep memory bandwidth the same as the baseline while
    varying the number of cores and PIM chips. We present summarization-only (256,1)
    and generation-dominant (256,512) cases for comprehensive analysis with GPT-2
    L to isolate the impacts from reduced on-chip memory or PIM capacity. As shown
    in Figure [15,](#page-11-0) the fewer cores result in slowdowns for both cases
    due to the decreased intra-layer and attention-head parallelism, and summarization-only
    case suffers more as NPU executes all but one computation (LM head). On the other
    hand, PIM''s computation capability significantly affects the generation-dominant
    configuration, where a significant fraction of FC operations are executed on PIMs.


    ## 6.3 IANUS System Prototyping


    **PIM Runtime Library**


    We develop a system prototype of IANUS to validate feasibility as shown in Figure
    [16.](#page-11-1) Our prototype is based on commodity Xilinx FPGA board (VCU118)
    [\[48\]](#page-15-2) to assess the feasibility of IANUS with real PIM chips, GDDR6-AiM
    [\[26,](#page-14-7) [30\]](#page-14-5). Specifically, we use the AiM-embedded
    FPGA Mezzanine Card (FMC) and connect it to the FPGA via the FMC connector [\[28\]](#page-14-20).
    As shown, the PIM control unit (PCU) and PIM memory controllers (PIM MCs) are
    implemented on FPGA whereas we leverage our NPU simulator as the NPU of IANUS
    since its RTL design was too big to fit in a single FPGA. When macro PIM commands
    are ready to be executed in the NPU, they are dispatched to PCU through the PCIe
    interface by the PIM runtime library and device driver. These macro commands are
    then converted into corresponding micro PIM commands and are transferred to PIM
    through PCU and PIM MCs. DMA commands from the NPU simulator are also


    <span id="page-11-2"></span>Table 4. Network configurations of larger LLMs.


    <span id="page-11-3"></span>![](_page_11_Figure_9.jpeg)


    Figure 17. Inference performance scalability for larger LLMs with multiple IANUS
    devices. The results are compared to a single A100 GPU.


    similarly transferred to PIM. To validate the functionality of a system prototype
    for IANUS, we evaluate the accuracy of our system using pretrained models of GPT-2
    [\[39\]](#page-14-1) on the WikiText-2 dataset. Our system prototype achieves
    perplexity scores of 30.92 and 22.60, 19.39, and 17.48 for GPT-2 Base (117M),
    M, L, and XL, respectively, achieving similar perplexity scores as the full-precision
    models.


    # 7 Discussion


    ## 7.1 Scalability Analysis


    Given the limited memory capacity of IANUS compared to modern GPUs, the memory
    capacity of IANUS needs to be scaled to run larger LLMs. The memory (PIM) capacity
    of IANUS can be expanded in two ways: 1) increase the amount of PIM per NPU, or
    2) scale the number of IANUS devices. The first approach can be achieved by adding
    more PIM controllers or employing a clamshell configuration of GDDR6 devices [\[35\]](#page-14-21).
    However, this approach requires modifications to the IANUS architecture. In this
    work, we leverage the second approach to analyze the scalability of IANUS.


    The larger LLMs used in the scalability analysis are summarized in Table [4.](#page-11-2)
    For each model, the number of IANUS devices is selected to provide sufficient
    memory capacity to support the model – i.e., two, four, and eight IANUS devices
    are used to support the GPT 6.7B, 13B, and 30B models, respectively. The multiple
    IANUS devices are assumed to be interconnected through PCIe 5.0 ×16 host interface.
    To maximize parallelism across IANUS devices, both intra-layer parallelism and
    attention head parallelism are exploited among devices.


    As shown in Figure [17,](#page-11-3) multiple IANUS devices provide average speedups
    of 2.4×, 3.4×, and 5.3× across respective


    ![](_page_12_Figure_1.jpeg)


    Figure 18. Strong scaling of IANUS on the GPT 6.7B model.


    2 IANUSs 4 IANUSs 8 IANUSs


    0


    200


    <span id="page-12-0"></span>Tokens per


    second


    400


    models, compared to a single A100 GPU, which has sufficient memory capacity for
    the larger LLMs. Multiple IANUS devices not only provide additional memory capacity
    but also increase effective memory bandwidth with extra PIM capability. For larger
    LLMs, the key system component that impacts overall performance is the memory
    bandwidth. This is because the proportion of FC layers in LLMs, which are bottlenecked
    by memory bandwidth, increases as the size of the LLMs grows. Leveraging the PIM''s
    internal memory bandwidth, the effective memory bandwidth of IANUS reaches approximately
    2.4 TB/s, 9-10× higher than external memory bandwidth of GDDR6 memories. Thus,
    with two IANUS devices, the total effective bandwidth is 4.8 TB/s, which is approximately
    2.4× higher than the A100 memory bandwidth (2039 GB/s). This difference in memory
    bandwidth nearly matches the observed performance benefits of two IANUS devices
    over the A100 GPU. However, scaling the number of IANUS devices comes at the cost
    of communication overhead between IANUS devices compared to a single GPU. As a
    result, the performance benefits with four and eight IANUS devices do not match
    the theoretical memory bandwidth difference; however, there is still significant
    speedup compared to a single A100 GPU.


    Strong scaling of IANUS is shown in Figure [18,](#page-12-0) using the 6.7B model
    with a 256:64 token configuration. As described earlier, the additional IANUS
    devices provide higher effective memory bandwidth and result in performance gain
    – 2.5× performance improvement when the number of IANUS is increased by 4×. While
    the performance of IANUS improves with extra devices, linear speedup is not obtained
    because of the communication overhead between multiple devices. A multi-IANUS
    device system presents new opportunities for optimizing communication across the
    devices but we leave such exploration as part of future work.


    #### 7.2 Cost Analysis


    Providing a fair cost comparison between two different system architectures (e.g.,
    HBM memory with interposer vs GDDR6-based PIM memory) is a challenge since many
    factors impact cost. However, prior work has shown how thermal design power (TDP)
    can approximate total cost of ownership (TCO) in datacenters [\[22\]](#page-13-19).
    Therefore, we use TDP for the cost comparison. The TDP of A100 GPU is estimated
    at 400 W [\[37\]](#page-14-14) while the TDP of IANUS is conservatively assumed
    to be 120 W, based on estimates from the NPU [\[1,](#page-13-16) [20,](#page-13-17)
    [41\]](#page-14-15) and


    PIM [\[26,](#page-14-7) [30\]](#page-14-5) components. Using the performance/TDP
    metric for the cost-efficiency evaluation, configurations of two, four, and eight
    IANUS devices yield improvements in costefficiency of 3.9×, 2.7×, and 2.1× over
    the single A100 GPU for the 6.7B, 13B, and 30B models, respectively. For each
    comparison, the performances of IANUS devices and the GPU for the performance/TDP
    metric are measured with a 256:64 input-to-output token ratio. While the cost-efficiency
    benefits of IANUS devices are evident, they diminish as the


    number of IANUS devices increases. The cost efficiency of IANUS can potentially
    be enhanced by leveraging PIM chips with higher memory capacity and/or more PIM
    chips connected to a single NPU.


    # 8 Related Works


    Domain-specific Accelerators: Various hardware accelerators have been proposed
    to accelerate transformer models. TurboTransformer [\[10\]](#page-13-20) executes
    BERT variants effectively by operation fusion and pipelining. Unfortunately, this
    approach suffers from severe under-utilization in text generation workloads. Several
    accelerators [\[13,](#page-13-2) [14,](#page-13-3) [34,](#page-14-2) [45,](#page-14-3)
    [49\]](#page-15-3) focus on multi-head-attention mechanisms only, requiring additional
    hardware to handle other operations such as layer normalization. Meanwhile, IANUS
    utilizes integrated both NPU and PIM to accelerate end-to-end inference of LLMs.


    PIM Accelerators: Utilizing large in-memory bandwidth, PIM architectures reduce
    massive data movement between DRAMs and a host [\[16,](#page-13-21) [24,](#page-14-6)
    [29–](#page-14-4)[32\]](#page-14-22). McDRAM [\[42\]](#page-14-23) presents near-memory
    structures and a horizontal arrangement of data within memory banks. TransPIM
    [\[51\]](#page-15-4) introduces the first memory-based accelerator for end-to-end
    inference of transformers. However, it only achieves an average throughput of
    734 GOPS due to area and power constraints. Chopim [\[7\]](#page-13-22) employs
    a unified memory system between the near-data accelerator and the host. Unlike
    Chopim, which focuses on homogeneous kernels, IANUS targets heterogeneous kernels
    that require scheduling to optimize performance.


    # 9 Conclusion


    We propose IANUS, an integrated accelerator based on NPU-PIM unified memory system,
    that fully exploits the benefits of NPU and PIM to accelerate end-to-end inference
    in transformer-based LLMs. To overcome the challenges posed by a unified memory
    system with PIM, we propose PIM Access Scheduling that schedules PIM operations
    and normal memory accesses through the workload mapping and scheduling. IANUS
    results in 6.2× and 3.2× speedup in comparison to GPU and DFX-based solutions
    for the GPT-2 model, highlighting the potential of such hybrid architectures.
    To demonstrate the feasibility of IANUS, we constructed an FPGA prototype system
    based on a commercial NPU and real PIM chips.


    # Acknowledgments


    We thank the shepherd and all reviewers for their valuable comments. This work
    was supported in part by the IITP grant funded by the MSIT (No.RS 2023-00228255,
    PIM-NPU Based Processing System Software Developments for Hyperscale Artificial
    Neural Network Processing) and in part by the IITP grant funded by the MSIT (No.
    2021-0-00106, AI accelerator-optimized neural network automatic generation technology
    and open service platform development).


    # References


    - <span id="page-13-16"></span>[1] Minwook Ahn, Seok Joong Hwang, Wonsub Kim,
    Seungrok Jung, Yeonbok Lee, Mookyoung Chung, Woohyung Lim, and Youngjoon Kim.
    Aix: A high performance and energy efficient inference accelerator on fpga for
    a dnn-based commercial speech recognition. In 2019 Design, Automation & Test in
    Europe Conference & Exhibition (DATE), pages 1495–1500. IEEE, 2019.

    - <span id="page-13-10"></span>[2] Jorge Albericio, Patrick Judd, Tayler Hetherington,
    Tor Aamodt, Natalie Enright Jerger, and Andreas Moshovos. Cnvlutin: Ineffectualneuron-free
    deep neural network computing. ACM SIGARCH Computer Architecture News, 44(3):1–13,
    2016.

    - <span id="page-13-8"></span>[3] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey
    E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.

    - <span id="page-13-14"></span>[4] John Bridle. Training stochastic model recognition
    algorithms as networks can lead to maximum mutual information estimation of parameters.
    Advances in neural information processing systems, 2, 1989.

    - <span id="page-13-11"></span>[5] Tianshi Chen, Zidong Du, Ninghui Sun, Jia Wang,
    Chengyong Wu, Yunji Chen, and Olivier Temam. Diannao: A small-footprint highthroughput
    accelerator for ubiquitous machine-learning. ACM SIGARCH Computer Architecture
    News, 42(1):269–284, 2014.

    - <span id="page-13-4"></span>[6] Yu-Hsin Chen, Tushar Krishna, Joel S Emer, and
    Vivienne Sze. Eyeriss: An energy-efficient reconfigurable accelerator for deep
    convolutional neural networks. IEEE journal of solid-state circuits, 52(1):127–138,
    2016.

    - <span id="page-13-22"></span>[7] Benjamin Y. Cho, Yongkee Kwon, Sangkug Lym,
    and Mattan Erez. Near data acceleration with concurrent host access. In 2020 ACM/IEEE
    47th Annual International Symposium on Computer Architecture (ISCA), pages 818–831,
    2020.

    - <span id="page-13-7"></span>[8] Fabrice Devaux. The true processing in memory
    accelerator. In 2019 IEEE Hot Chips 31 Symposium (HCS), pages 1–24. IEEE Computer
    Society, 2019.

    - <span id="page-13-0"></span>[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
    Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for
    language understanding. arXiv preprint arXiv:1810.04805, 2018.

    - <span id="page-13-20"></span>[10] Jiarui Fang, Yang Yu, Chengduo Zhao, and Jie
    Zhou. Turbotransformers: an efficient gpu serving system for transformer models.
    In Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of
    Parallel Programming, pages 389–402, 2021.

    - <span id="page-13-13"></span>[11] Joseph A Fisher. Very long instruction word
    architectures and the eli-512. In Proceedings of the 10th annual international
    symposium on Computer architecture, pages 140–150, 1983.

    - <span id="page-13-18"></span>[12] Jeremy Fowers, Kalin Ovtcharov, Michael Papamichael,
    Todd Massengill, Ming Liu, Daniel Lo, Shlomi Alkalay, Michael Haselman, Logan
    Adams, Mahdi Ghandi, Stephen Heil, Prerak Patel, Adam Sapek, Gabriel Weisz, Lisa
    Woods, Sitaram Lanka, Steven K. Reinhardt, Adrian M. Caulfield, Eric S. Chung,
    and Doug Burger. A configurable cloud-scale dnn processor for real-time ai. In
    2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA),
    pages 1–14, 2018.

    - <span id="page-13-2"></span>[13] Tae Jun Ham, Sung Jun Jung, Seonghak Kim, Young
    H. Oh, Yeonhong Park, Yoonho Song, Jung-Hun Park, Sanghee Lee, Kyoung Park, Jae
    W. Lee, and Deog-Kyoon Jeong. 3 : Accelerating attention mechanisms


    in neural networks with approximation. In 2020 IEEE International Symposium on
    High Performance Computer Architecture (HPCA), pages 328–341, 2020.


    - <span id="page-13-3"></span>[14] Tae Jun Ham, Yejin Lee, Seong Hoon Seo, Soosung
    Kim, Hyunji Choi, Sung Jun Jung, and Jae W Lee. Elsa: Hardware-software co-design
    for efficient, lightweight self-attention mechanism in neural networks. In 2021
    ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA),
    pages 692–705. IEEE, 2021.

    - <span id="page-13-9"></span>[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and
    Jian Sun. Deep residual learning for image recognition. In Proceedings of the
    IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.

    - <span id="page-13-21"></span>[16] Mingxuan He, Choungki Song, Ilkon Kim, Chunseok
    Jeong, Seho Kim, Il Park, Mithuna Thottethodi, and TN Vijaykumar. Newton: A dram-maker''s
    accelerator-in-memory (aim) architecture for machine learning. In 2020 53rd Annual
    IEEE/ACM International Symposium on Microarchitecture (MICRO), pages 372–385.
    IEEE, 2020.

    - <span id="page-13-12"></span>[17] Kartik Hegde, Jiyong Yu, Rohit Agrawal, Mengjia
    Yan, Michael Pellauer, and Christopher Fletcher. Ucnn: Exploiting computational
    reuse in deep neural networks via weight repetition. In 2018 ACM/IEEE 45th Annual
    International Symposium on Computer Architecture (ISCA), pages 674–687. IEEE,
    2018.

    - <span id="page-13-15"></span>[18] Dan Hendrycks and Kevin Gimpel. Gaussian error
    linear units (gelus). arXiv preprint arXiv:1606.08415, 2016.

    - <span id="page-13-1"></span>[19] Seongmin Hong, Seungjae Moon, Junsoo Kim, Sungjae
    Lee, Minsub Kim, Dongsoo Lee, and Joo-Young Kim. Dfx: A low-latency multi-fpga
    appliance for accelerating transformer-based text generation. In 2022 55th IEEE/ACM
    International Symposium on Microarchitecture (MICRO), pages 616–630. IEEE, 2022.

    - <span id="page-13-17"></span>[20] Seok Joong Hwang, Jeongho Han, Minwook Ahn,
    Seungrok Jung, Wonsub Kim, Yongshik Moon, Sangjun Yang, Moo-Kyoung Chung, Jaehyeok
    Jang, Youngjae Jin, Yongsang Park, Namseob Lee, Daewoo Kim, Euiseok Kim, Choong
    Hwan Choi, and Heeyul Lee. Aix v2: Flexible high performance ai inference accelerator
    for datacenters. In 2019 IEEE Hot Chips 31 Symposium (HCS), 2019.

    - <span id="page-13-5"></span>[21] Norm Jouppi, George Kurian, Sheng Li, Peter
    Ma, Rahul Nagarajan, Lifeng Nai, Nishant Patil, Suvinay Subramanian, Andy Swing,
    Brian Towles, Clifford Young, Xiang Zhou, Zongwei Zhou, and David A Patterson.
    Tpu v4: An optically reconfigurable supercomputer for machine learning with hardware
    support for embeddings. In Proceedings of the 50th Annual International Symposium
    on Computer Architecture, ISCA ''23, New York, NY, USA, 2023. Association for
    Computing Machinery.

    - <span id="page-13-19"></span>[22] Norman P. Jouppi, Doe Hyun Yoon, Matthew Ashcraft,
    Mark Gottscho, Thomas B. Jablin, George Kurian, James Laudon, Sheng Li, Peter
    Ma, Xiaoyu Ma, Thomas Norrie, Nishant Patil, Sushma Prasad, Cliff Young, Zongwei
    Zhou, and David Patterson. Ten lessons from three generations shaped google''s
    tpuv4i : Industrial product. In 2021 ACM/IEEE 48th Annual International Symposium
    on Computer Architecture (ISCA), pages 1–14, 2021.

    - <span id="page-13-6"></span>[23] Norman P. Jouppi, Cliff Young, Nishant Patil,
    David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan
    Boden, Al Borchers, Rick Boyle, Pierre-luc Cantin, Clifford Chao, Chris Clark,
    Jeremy Coriell, Mike Daley, Matt Dau, Jeffrey Dean, Ben Gelb, Tara Vazir Ghaemmaghami,
    Rajendra Gottipati, William Gulland, Robert Hagmann, C. Richard Ho, Doug Hogberg,
    John Hu, Robert Hundt, Dan Hurt, Julian Ibarz, Aaron Jaffey, Alek Jaworski, Alexander
    Kaplan, Harshit Khaitan, Daniel Killebrew, Andy Koch, Naveen Kumar, Steve Lacy,
    James Laudon, James Law, Diemthu Le, Chris Leary, Zhuyuan Liu, Kyle Lucke, Alan
    Lundin, Gordon MacKean, Adriana Maggiore, Maire Mahony, Kieran Miller, Rahul Nagarajan,
    Ravi Narayanaswami, Ray Ni, Kathy Nix, Thomas Norrie, Mark Omernick, Narayana
    Penukonda, Andy Phelps, Jonathan Ross, Matt Ross, Amir


    Salek, Emad Samadiani, Chris Severn, Gregory Sizikov, Matthew Snelham, Jed Souter,
    Dan Steinberg, Andy Swing, Mercedes Tan, Gregory Thorson, Bo Tian, Horia Toma,
    Erick Tuttle, Vijay Vasudevan, Richard Walter, Walter Wang, Eric Wilcox, and Doe
    Hyun Yoon. In-datacenter performance analysis of a tensor processing unit. In
    Proceedings of the 44th Annual International Symposium on Computer Architecture,
    ISCA ''17, page 1–12, New York, NY, USA, 2017. Association for Computing Machinery.


    - <span id="page-14-6"></span>[24] Jin Hyun Kim, Shin-Haeng Kang, Sukhan Lee,
    Hyeonsu Kim, Yuhwan Ro, Seungwon Lee, David Wang, Jihyun Choi, Jinin So, YeonGon
    Cho, JoonHo Song, Jeonghyeon Cho, Kyomin Sohn, and Nam Sung Kim. Aquabolt-xl hbm2-pim,
    lpddr5-pim with in-memory processing, and axdimm with acceleration buffer. IEEE
    Micro, 42(3):20–30, 2022.

    - <span id="page-14-12"></span>[25] Hsiang Tsung Kung and Charles E Leiserson.
    Systolic arrays (for vlsi). In Sparse Matrix Proceedings 1978, volume 1, pages
    256–282. Society for industrial and applied mathematics Philadelphia, PA, USA,
    1979.

    - <span id="page-14-7"></span>[26] Daehan Kwon, Seongju Lee, Kyuyoung Kim, Sanghoon
    Oh, Joonhong Park, Gi-Moon Hong, Dongyoon Ka, Kyudong Hwang, Jeongje Park, Kyeongpil
    Kang, Jungyeon Kim, Junyeol Jeon, Nahsung Kim, Yongkee Kwon, Vladimir Kornijcuk,
    Woojae Shin, Jongsoon Won, Minkyu Lee, Hyunha Joo, Haerang Choi, Guhyun Kim, Byeongju
    An, Jaewook Lee, Donguc Ko, Younggun Jun, Ilwoong Kim, Choungki Song, Ilkon Kim,
    Chanwook Park, Seho Kim, Chunseok Jeong, Euicheol Lim, Dongkyun Kim, Jieun Jang,
    Il Park, Junhyun Chun, and Joohwan Cho. A 1ynm 1.25 v 8gb 16gb/s/pin gddr6-based
    accelerator-in-memory supporting 1tflops mac operation and various activation
    functions for deep learning application. IEEE Journal of Solid-State Circuits,
    58(1):291–302, 2022.

    - <span id="page-14-9"></span>[27] Hyoukjun Kwon, Ananda Samajdar, and Tushar
    Krishna. Maeri: Enabling flexible dataflow mapping over dnn accelerators via reconfigurable
    interconnects. ACM SIGPLAN Notices, 53(2):461–475, 2018.

    - <span id="page-14-20"></span>[28] Yongkee Kwon, Kornijcuk Vladimir, Nahsung
    Kim, Woojae Shin, Jongsoon Won, Minkyu Lee, Hyunha Joo, Haerang Choi, Guhyun Kim,
    Byeongju An, Jeongbin Kim, Jaewook Lee, Ilkon Kim, Jaehan Park, Chanwook Park,
    Yosub Song, Byeongsu Yang, Hyungdeok Lee, Seho Kim, Daehan Kwon, Seongju Lee,
    Kyuyoung Kim, Sanghoon Oh, Joonhong Park, Gimoon Hong, Dongyoon Ka, Kyudong Hwang,
    Jeongje Park, Kyeongpil Kang, Jungyeon Kim, Junyeol Jeon, Myeongjun Lee, Minyoung
    Shin, Minhwan Shin, Jaekyung Cha, Changson Jung, Kijoon Chang, Chunseok Jeong,
    Euicheol Lim, Il Park, Junhyun Chun, and Sk Hynix. System architecture and software
    stack for gddr6-aim. In 2022 IEEE Hot Chips 34 Symposium (HCS), pages 1–25. IEEE,
    2022.

    - <span id="page-14-4"></span>[29] Young-Cheon Kwon, Suk Han Lee, Jaehoon Lee,
    Sang-Hyuk Kwon, Je Min Ryu, Jong-Pil Son, O Seongil, Hak-Soo Yu, Haesuk Lee, Soo
    Young Kim, Youngmin Cho, Jin Guk Kim, Jongyoon Choi, Hyun-Sung Shin, Jin Kim,
    BengSeng Phuah, HyoungMin Kim, Myeong Jun Song, Ahn Choi, Daeho Kim, SooYoung
    Kim, Eun-Bong Kim, David Wang, Shinhaeng Kang, Yuhwan Ro, Seungwoo Seo, JoonHo
    Song, Jaeyoun Youn, Kyomin Sohn, and Nam Sung Kim. 25.4 a 20nm 6gb function-in-memory
    dram, based on hbm2 with a 1.2 tflops programmable computing unit using bank-level
    parallelism, for machine learning applications. In 2021 IEEE International Solid-State
    Circuits Conference (ISSCC), volume 64, pages 350–352. IEEE, 2021.

    - <span id="page-14-5"></span>[30] Seongju Lee, Kyuyoung Kim, Sanghoon Oh, Joonhong
    Park, Gimoon Hong, Dongyoon Ka, Kyudong Hwang, Jeongje Park, Kyeongpil Kang, Jungyeon
    Kim, Junyeol Jeon, Nahsung Kim, Yongkee Kwon, Kornijcuk Vladimir, Woojae Shin,
    Jongsoon Won, Minkyu Lee, Hyunha Joo, Haerang Choi, Jaewook Lee, Donguc Ko, Younggun
    Jun, Keewon Cho, Ilwoong Kim, Choungki Song, Chunseok Jeong, Daehan Kwon, Jieun
    Jang, Il Park, Junhyun Chun, and Joohwan Cho. A 1ynm 1.25 v 8gb, 16gb/s/pin gddr6-based
    accelerator-in-memory supporting 1tflops mac operation and various activation
    functions for deep-learning applications. In 2022 IEEE International Solid-State
    Circuits Conference (ISSCC), volume 65, pages 1–3. IEEE, 2022.

    - <span id="page-14-8"></span>[31] Sukhan Lee, Shin-haeng Kang, Jaehoon Lee, Hyeonsu
    Kim, Eojin Lee, Seungwoo Seo, Hosang Yoon, Seungwon Lee, Kyounghwan Lim, Hyunsung
    Shin, Jinhyun Kim, O Seongil, Anand Iyer, David Wang, Kyomin Sohn, and Nam Sung
    Kim. Hardware architecture and software stack for pim based on commercial dram
    technology : Industrial product. In 2021 ACM/IEEE 48th Annual International Symposium
    on Computer Architecture (ISCA), pages 43–56, 2021.

    - <span id="page-14-22"></span>[32] Shuangchen Li, Dimin Niu, Krishna T Malladi,
    Hongzhong Zheng, Bob Brennan, and Yuan Xie. Drisa: A dram-based reconfigurable
    in-situ accelerator. In Proceedings of the 50th Annual IEEE/ACM International
    Symposium on Microarchitecture, pages 288–301, 2017.

    - <span id="page-14-10"></span>[33] Daofu Liu, Tianshi Chen, Shaoli Liu, Jinhong
    Zhou, Shengyuan Zhou, Olivier Teman, Xiaobing Feng, Xuehai Zhou, and Yunji Chen.
    Pudiannao: A polyvalent machine learning accelerator. ACM SIGARCH Computer Architecture
    News, 43(1):369–381, 2015.

    - <span id="page-14-2"></span>[34] Liqiang Lu, Yicheng Jin, Hangrui Bi, Zizhang
    Luo, Peng Li, Tao Wang, and Yun Liang. Sanger: A co-design framework for enabling
    sparse attention using reconfigurable architecture. In MICRO-54: 54th Annual IEEE/ACM
    International Symposium on Microarchitecture, pages 977– 991, 2021.

    - <span id="page-14-21"></span>[35] Micron. Gddr6 datasheet. [Online]. Available:
    [https://media](https://media-www.micron.com/-/media/client/global/documents/products/data-sheet/dram/gddr/gddr6/gddr6_sgram_8gb_brief.pdf)[www.micron.com/-/media/client/global/documents/products/data](https://media-www.micron.com/-/media/client/global/documents/products/data-sheet/dram/gddr/gddr6/gddr6_sgram_8gb_brief.pdf)[sheet/dram/gddr/gddr6/gddr6\\_sgram\\_8gb\\_brief.pdf](https://media-www.micron.com/-/media/client/global/documents/products/data-sheet/dram/gddr/gddr6/gddr6_sgram_8gb_brief.pdf).

    - <span id="page-14-11"></span>[36] Thomas Norrie, Nishant Patil, Doe Hyun Yoon,
    George Kurian, Sheng Li, James Laudon, Cliff Young, Norman Jouppi, and David Patterson.
    The design process for google''s training chips: Tpuv2 and tpuv3. IEEE Micro,
    41(2):56–63, 2021.

    - <span id="page-14-14"></span>[37] NVIDIA. Nvidia a100 tensor core gpu. [Online].
    Available: [https:](https://www.nvidia.com/en-us/data-center/a100/) [//www.nvidia.com/en-us/data-center/a100/](https://www.nvidia.com/en-us/data-center/a100/).

    - <span id="page-14-19"></span>[38] OpenAI. Input:output token ratio. [Online].
    Available: [https://beta.](https://beta.openai.com/docs/usage-guidelines/use-case-guidelines)
    [openai.com/docs/usage-guidelines/use-case-guidelines](https://beta.openai.com/docs/usage-guidelines/use-case-guidelines).

    - <span id="page-14-1"></span>[39] Alec Radford, Jeffrey Wu, Rewon Child, David
    Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask
    learners. OpenAI blog, 1(8):9, 2019.

    - <span id="page-14-13"></span>[40] Scott Rixner, William J Dally, Ujval J Kapasi,
    Peter Mattson, and John D Owens. Memory access scheduling. ACM SIGARCH Computer
    Architecture News, 28(2):128–138, 2000.

    - <span id="page-14-15"></span>[41] SAPEON. Product of SAPEON - X330. [Online].
    Available: [https:](https://www.sapeon.com/products/sapeon-x330) [//www.sapeon.com/products/sapeon-x330](https://www.sapeon.com/products/sapeon-x330).

    - <span id="page-14-23"></span>[42] Hyunsung Shin, Dongyoung Kim, Eunhyeok Park,
    Sungho Park, Yongsik Park, and Sungjoo Yoo. Mcdram: Low latency and energyefficient
    matrix computations in dram. IEEE Transactions on Computer-Aided Design of Integrated
    Circuits and Systems, 37(11):2613–2622, 2018.

    - <span id="page-14-17"></span>[43] Mohammad Shoeybi, Mostofa Patwary, Raul Puri,
    Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multibillion
    parameter language models using model parallelism. arXiv preprint arXiv:1909.08053,
    2019.

    - <span id="page-14-0"></span>[44] Ashish Vaswani, Noam Shazeer, Niki Parmar,
    Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin.
    Attention is all you need. Advances in neural information processing systems,
    30, 2017.

    - <span id="page-14-3"></span>[45] Hanrui Wang, Zhekai Zhang, and Song Han. Spatten:
    Efficient sparse attention architecture with cascade token and head pruning. In
    2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),
    pages 97–110. IEEE, 2021.

    - <span id="page-14-18"></span>[46] Shibo Wang and Pankaj Kanwar. Bfloat16: The
    secret to high performance on cloud tpus. Google Cloud Blog, 4, 2019.

    - <span id="page-14-16"></span>[47] Thomas Wolf, Lysandre Debut, Victor Sanh,
    Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi
    Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma,
    Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama
    Drame, Quentin Lhoest, and Alexander M. Rush. Huggingface''s transformers: State-of-the-art
    natural language processing. arXiv preprint arXiv:1910.03771, 2019.

    - <span id="page-15-2"></span><span id="page-15-0"></span>[48] Xilinx. Xilinx
    VCU118 Evaluation Kit. [Online]. Available: [https:](https://www.xilinx.com/products/boards-and-kits/vcu118.html)
    [//www.xilinx.com/products/boards-and-kits/vcu118.html](https://www.xilinx.com/products/boards-and-kits/vcu118.html).

    - <span id="page-15-3"></span>[49] Amir Yazdanbakhsh, Ashkan Moradifirouzabadi,
    Zheng Li, and Mingu Kang. Sparse attention acceleration with synergistic in-memory
    pruning and on-chip recomputation. In 55th IEEE/ACM International Symposium on
    Microarchitecture, MICRO 2022, Chicago, IL, USA, October 1-5, 2022, pages 744–762.
    IEEE, 2022.

    - <span id="page-15-1"></span>[50] Joonsang Yu, Junki Park, Seongmin Park, Minsoo
    Kim, Sihwa Lee, Dong Hyun Lee, and Jungwook Choi. Nn-lut: neural approximation


    of non-linear operations for efficient transformer inference. In Proceedings of
    the 59th ACM/IEEE Design Automation Conference, pages 577–582, 2022.


    <span id="page-15-4"></span>[51] Minxuan Zhou, Weihong Xu, Jaeyoung Kang, and
    Tajana Rosing. Transpim: A memory-based acceleration via software-hardware codesign
    for transformer. In 2022 IEEE International Symposium on High-Performance Computer
    Architecture (HPCA), pages 1071–1085. IEEE, 2022.'
- title: LLC Intra-set Write Balancing
  abstract: 'The increasing use of Non-Volatile Memory (NVM) in computer architecture
    has

    brought about new challenges, one of which is the write endurance problem.

    Frequent writes to a particular cache cell in NVM can lead to degradation of

    the memory cell and reduce its lifespan. To solve this problem, we propose a

    sample-based blocking technique for the Last Level Cache (LLC). Our approach

    involves defining a threshold value and sampling a subset of cache sets. If the

    number of writes to a way in a sampled set exceeds the threshold, the way is

    blocked, and writes are redirected to other ways. We also maintain a history

    structure to record the number of writes in a set and a PC-Table to use for

    blocking in unsampled sets. Based on blocking on sampled sets, variance of

    values stored in history is used to determine whether blocking had a positive

    impact or not, and on this basis, value corresponding to instruction pointer is

    incremented or decremented. This value is later used for blocking in unsampled

    sets. Our results show that our approach significantly balances write traffic

    to the cache and improves the overall lifespan of the memory cells while having

    better performance to the base-line system. Our approach can also be applied to

    other cache hierarchies and NVM technologies to mitigate the problem of write

    endurance.'
  url: http://arxiv.org/abs/2410.15344v1
  keywords: LLC · NVM · Write Endurance · Write Balancing · Interset · Variance ·
    PC
  document: '# LLC INTRA-SET WRITE BALANCING


    WRITE BALANCING


    Keshav Krishna Department of Computer Science IIT Ropar Rupnagar, Punjab, 140001
    keshaviitropar@gmail.com


    Ayush Verma Department of Computer Science IIT Ropar Rupnagar, Punjab, 140001


    October 22, 2024


    #### ABSTRACT


    The increasing use of Non-Volatile Memory (NVM) in computer architecture has brought
    about new challenges, one of which is the write endurance problem. Frequent writes
    to a particular cache cell in NVM can lead to degradation of the memory cell and
    reduce its lifespan. To solve this problem, we propose a sample-based blocking
    technique for the Last Level Cache (LLC). Our approach involves defining a threshold
    value and sampling a subset of cache sets. If the number of writes to a way in
    a sampled set exceeds the threshold, the way is blocked, and writes are redirected
    to other ways. We also maintain a history structure to record the number of writes
    in a set and a PC-Table to use for blocking in unsampled sets. Based on blocking
    on sampled sets, variance of values stored in history is used to determine whether
    blocking had a positive impact or not, and on this basis, value corresponding
    to instruction pointer is incremented or decremented. This value is later used
    for blocking in unsampled sets. Our results show that our approach significantly
    balances write traffic to the cache and improves the overall lifespan of the memory
    cells while having better performance to the base-line system. Our approach can
    also be applied to other cache hierarchies and NVM technologies to mitigate the
    problem of write endurance.


    *Keywords* LLC · NVM · Write Endurance · Write Balancing · Interset · Variance
    · PC


    #### 1 Introduction


    A type of computer memory known as NVMs (Non-Volatile Memories) can continue to
    store data even after the power is turned off. NVMs use a variety of technologies,
    including Phase Change Memory (PCM), Resistive Random Access Memory (RRAM) Fe-FET
    and MRAM, to store data persistently in contrast to conventional volatile memory
    such as DRAM, which requires constant power to maintain data retention [\[1\]](#page-9-0).


    Phase Change Memory (PCM) is a type of NVM that uses phase change materials to
    store information in their amorphous and crystalline phases, which can be reversibly
    switched by the application of an external voltage [\[2\]](#page-9-1). Resistive
    Random Access Memory (RRAM) is another type of NVM that has shown promise due
    to its high speed, low cost, enhanced storage density, potential applications
    in various fields, and excellent scalability [\[3\]](#page-9-2).


    This is in contrast to conventional SRAM technology used in caches. SRAM (static
    random access memory) provides high performance and write endurance, but SRAM
    has low density and high leakage energy which leads to increased energy consumption
    and temperature of the chips [\[4\]](#page-9-3).


    Because they can store data even when there is no power, NVMs are a highly relevant
    component for caches. This prevents the cache from needing lengthy warm-up times
    or pricey copying operations to restore its state following a power outage or
    system reboot.


    NVMs provide a number of benefits, but they also have some drawbacks. Their low
    write endurance, or the number of times a memory cell may be programmed or wiped
    before it loses reliability, is a serious drawback [\[5\]](#page-9-4), [\[6\]](#page-9-5).
    Applying wear-leveling techniques, which uniformly spread write operations over
    the memory, can solve this problem but complicates the system.[\[5\]](#page-9-4)
    Another drawback is that NVM write speeds are slower than those of conventional
    SRAM caches, which might impact system performance.


    NVMs also experience intra-set and inter-set write variance. While intra-set variation
    refers to fluctuation inside a set, inter-set variation refers to the change in
    write latency between various cache sets. These variances make it difficult to
    build effective caching techniques and can cause performance reduction. To address
    this issue of intra-set write variation, we propose a new system that balances
    the write variability among the ways of the set while maintaining better performance
    than earlier approaches. We propose a variance based feed- back mechanism on some
    sampled sets to decide whether blocking had a positive impact or not, and using
    this feedback, do blocking on other sets.


    We ran our system against previous approaches and received better performance,
    while still tackling the write endurance issue.


    Many intra-set cache wear leveling techniques have been proposed in the existing
    literature. This section briefly discusses all such techniques.


    [\[7\]](#page-9-6) described i2wap, a wear levelling technology that reduces both
    inter-set and intra-set wear levelling. Probabilistic Set Line Flush (PoLF) is
    the first technique for reducing intra-set write variance. Swap Shift, which adjusts
    the mapping of two sets after a defined number of writes called Swap Threshold,
    is the second strategy for reducing inter-set write variation.


    [\[8\]](#page-9-7) reports on another wear levelling approach named Sequoia for
    reducing writing variance. The inter-set write variance is decreased in the first
    strategy, G-OAS, by partitioning the cache into numerous groups of sets and modifying
    the set mapping of heavily written and weakly written sets in each group while
    taking the counter associated with each set into account. WAD decreases intra-set
    write variation by shifting data inside the set once the corresponding set counter
    (RSC) saturates.


    The EqualChance algorithm described by [\[9\]](#page-9-8) decreases intra-set
    write variation by performing a transfer/swap operation within the cache set.
    [\[10\]](#page-9-9) proposes a technique called Start-Gap that uses the two registers
    Start and Gap to move the line location to the neighbour place after a specific
    number of writes. [\[11\]](#page-9-10) reports on a ReRAM NUCA that handles the
    lifetime problem in a performance aware manner.


    LastingNVCache, as proposed by [\[12\]](#page-9-11), decreases intra-set write
    variation by including write counters with each cache block. When the counter
    hits a certain threshold, the write process is aborted by invalidating the block.
    A word level strategy presented by [\[13\]](#page-10-0) decreases writing variance
    by exploring data with a small width. A hybrid cache architecture is as follows:
    Ayush, as presented by [\[14\]](#page-10-1), decreases write variation by moving
    data blocks between cache zones.


    EqualWrites, as proposed by [\[6\]](#page-9-5), decreases write variation by shifting
    the block from a write-intensive point inside the set to a chilly location. Several
    write throttling techniques at various levels of memory hierarchy have already
    been presented. A Lady technique described by [\[15\]](#page-10-2) extends the
    life of flash-based solid-state devices by regulating writes dynamically according
    on workload parameters. Mellow writes, as described by[\[16\]](#page-10-3) enhances
    main memory longevity by introducing a wear limitation approach. To improve endurance,
    slow writes are performed with lesser power dissipation.


    In this work, we compare our proposed approach with [\[6\]](#page-9-5).


    The paper is organized as follows: Introduction and Related works are discussed
    in Section 1. Methodology is reported in Section 2. Description of our algorithm
    is presented in Section 3. Section 4 lists the results and analysis. Finally,
    we conclude this paper in Section 4, followed by acknowledgement in Section 5.


    # 2 Methodology


    ## 2.1 Motivation


    The motivation behind this is that there is a large variation of writes among
    ways of a set. This variance can be attributable to a number of variables, including
    production differences, transistor ageing, and environmental fluctuations, which
    can have an impact on the performance and dependability of the cache in different
    ways.


    The write endurance of the cache—the number of write operations that can be made
    on the cache before it fails—can be significantly impacted by this write variance.
    Some cache ways may receive much more writes than others in a cache with high
    write variation, which could cause those ways to break early and reduce the cache''s
    total write endurance. This may lead to higher maintenance and replacement expenses
    as well as decreased system reliability and performance. The impacts of write
    variance in NVM-based caches have been addressed using a variety of strategies,
    including dynamic write balancing and wear levelling. These strategies try to
    evenly distribute write operations among the cache paths, lowering the risk of
    early failure and boosting the cache''s write endurance.


    These strategies proposed so far only consider most recent writes among the sets
    and then use thresholds to block certain ways. But This may lead to excessive
    blocking and creation of some undesired effects like oscillatory nature of writes,
    which is explained in detail in later section.


    So we propose a feedback based algorithm that considers the effects of previous
    blocking before making a decision.


    #### 2.2 Key Idea


    Large write variation between ways of a set can occur in the final level cache,
    which reduces the cache''s write endurance. An algorithm has been created to address
    this issue, and it is based on the notion that by sampling a small number of sets,
    we can also make accurate blocking judgements on larger sets. This means that
    we can predict the behaviour of other sets in the cache by studying the behaviour
    of a few selected sets.


    The method takes use of the fact that programmes and data structures with identical
    access patterns are often comparable. The programme can identify which approaches
    are most and least frequently utilised by analysing the behaviour of a small number
    of sets.


    The programme also takes advantage of another key idea: instruction pointer bias
    by noting that only a small number of instruction pointers are frequently processed
    in the context of blocking methods in the last level cache to address the write
    endurance problem. Similar sets and methods of accessing these instruction pointers,
    as well as similar writing across different executions, characterise them.


    This observation is used by the algorithm to reinforce certain instruction pointers
    positively or negatively. The approach grants a specific instruction pointer positive
    reinforcement if blocking by that pointer has a positive impact on the cache in
    sampled sets. In contrast, the approach reinforces a given instruction pointer
    negatively if blocking by that port has a negative impact on the cache in sampled
    sets.


    The decision to block a path in other sets is then made using the value assigned
    to each instruction pointer based on whether it received positive or negative
    reinforcement. In other words, the algorithm uses the correlation between the
    cache access patterns and the instruction pointer bias to determine the appropriate
    blocking decisions for other sets. The approach ensures that the last level cache
    has a balanced write endurance across all ways by only blocking those ways that
    cause substantial write variations.


    ## 3 Algorithm Description


    #### 3.1 Counters


    A structure is made to store the count of write operations done in that particular
    block of cache and the instruction pointer for that write. This structure is a
    2D array, the array dimension being the number of sets and ways of the cache to
    be simulated. Interval size was divided based on number of cycles executed by
    the processor(s). It can correctly measure time elapsed irrespective of other
    factors such as CPU core count. In the counter 2D array, count of write operations(counter
    values) performed in that i-th interval are stored. After the start of every cycle,
    these counter values and instruction pointer values are set to be zero. The value
    of the counter is incremented when handlefill() operation i.e. request for write
    in that empty block of cache or data from main memory is brought into cache is
    called for that particular set and way of the cache, and the instruction pointer
    that caused that write is updated as the instruction pointer value.


    ## 3.2 History storing structure


    To extend upon the counter values, which were stored for all round operation of
    the cache, and did not give insight on how writes were happening after every ''I''
    cycles, we made a history keeping structure. The structure is a map of key and
    vector of vector of values, which stored for each set, what are the counter values
    for previous k ''I'' cycles(the number of writes and instruction pointer). After
    every I cycles, the counter values were added for each set into the map, and if
    size of map for that set is greater than k(set to be 8), most ancient counter
    values were removed, so that the size of the structure for each set index is limited
    to k vectors. The history structure is only maintained for LLC(last level cache).


    ![](_page_3_Figure_2.jpeg)


    Figure 1: Structure of counter used to record number of writes in I cycles and
    the instruction pointer that caused the write


    ![](_page_3_Figure_4.jpeg)


    Figure 2: Structure of history which stores past ''k'' counter values for each
    set. For implementation of algorithm, k = 8 is taken.


    #### 3.3 Sampled sets and Unsampled sets


    Two types of sets are implemented in LLC: Sampled sets and Non-sampled sets. Sampled
    sets are a subset of all the sets in the cache, and they are decided based on
    mathematical function using bitmasking. Out of 2048 total sets, 32 sets are taken
    to be sampled sets, and the rest are Non-Sampled sets. Counters and previous ''k''
    history storing structure is made only for Sampled sets. This is because storing
    counters and history structure for every sets will increase the hardware overhead
    largely and deployment of this algorithm on an actual LLC would not be feasible.
    Having only 32 sampled sets to store the counter and history will be very hardware
    friendly. On the other hand, non-sampled sets are the remaining sets in the cache
    that are not part of the sampled sets. These sets do not have counters and history
    structures.


    The sampled sets are selected based on the helper function, which compares the
    first 6 bits of the set index with the last 6 bits of the set index shifted by
    the logarithm of the cache set size. This ensures that the sampled sets are distributed
    evenly across the cache while also reducing the chances of collisions between
    different cores accessing the same sets.


    ## 3.4 PC Table


    An important observation which is ''Instruction Pointer bias'' is utilized in
    the algorithm. It was observed that only a limited number of instruction pointers
    were frequently processed in the context of blocking methods. These frequently
    accessed instruction pointers exhibited similar access patterns and methods, as
    well as similar writing behavior across different program executions. As a result,
    by taking advantage of this bias, it is feasible to optimize write operations
    and reduce total cache wear, resulting in balanced write endurance.


    PC Table is maintained for every Instruction pointer. This table is a map for
    ''uint64'' to ''int'' that maps the instruction pointer to its respective value.
    This value is set to be zero initially and is incremented or decremented based
    on whether blocking a particular cache cell of the sampled set had a positive
    impact or negative impact on the remaining cache cells in the next iteration.
    This PC-Table is accessed to decide whether blocking is to be done or not in Non-Sampled
    sets depending on the value corresponding to that particular instruction pointer.


    ### 3.5 Variance based feedback


    After blocking a particular cache cell of a sampled set, based on comparison of
    the counter values with the threshold value, its impact on remaining cache cells(ways)
    of the same set is observed in the next iteration. The variance among the counter
    values of sets are calculated in the next iteration. The Instruction Pointer that
    blocked the particular cell is trained based on the variance calculated in the
    next iteration. The variances due to blocking by this particular instruction pointer
    in previous ''k'' cycles is also taken into consideration. Weighted mean of these
    variances is then calculated, giving more weights to recent variances. Based on
    the value of this weighted mean of variances, feedback to increase or decrease
    the value corresponding to that instruction pointer is done in the PC-Table.


    ## 3.6 Blocking


    Blocking of a particular cache cell is handled differently on the basis whether
    that cell belongs to one of the Sampled sets or Non-Sampled sets. If the set to
    which cache belongs to Sampled set, at the start of current cycle, its previous
    cycle counter value is compared with threshold. If the respective counter value
    is less than threshold value, no blocking is done and the working would proceed
    normally. On the other hand, if counter value exceed theshold, that particular
    cache cell is blocked. The way returned by chechhit() function is set to be -1(analogous
    to write-miss), implying that some other unblocked cell needs to be returned by
    the replacement function. The replacement algorithm (srrip) also takes care of
    the fact that the victim cell returned by it should not be a blocked cell. In
    the next iteration, the effect of this blocking is observed by calculating variance.
    The Instruction Pointer that blocked the particular cell is trained based on the
    variance calculated in the next iteration. Weighted mean of the variance in previous
    ''k'' cycles due to blocking by instruction pointer is calculated. Based on the
    value of this weighted mean of variances, feedback to increase or decrease the
    value corresponding to that instruction pointer is done in the PC-Table.


    If the cache cell belongs to Non-Sampled sets, the instruction pointer that brought
    this particular cell is noted. The value corresponding to this instruction pointer
    is fetched from the PC-Table. If this value is negative, cache cell is blocked
    in current iteration, else it is unblocked. Instruction pointer biasness described
    in section 3.3.4 is the basis for this decision.


    ![](_page_5_Figure_2.jpeg)


    Figure 3: Schematic diagram of flow of the algorithm. Some sets are sampled as
    sample sets, and for them counter values are calculated and stored in history
    after every ''I'' cycles. At the end of each of the interval of ''I'' cycles,
    PC-table is trained based on whether blocking due to an instruction pointer had
    positive effect or not. For unsampled sets, whenever a write occurs, the instruction
    pointer is used to access the PC-table and get the value. If value < 0, then that
    way is blocked for write operations, else normal write can occur.


    | Table 1: Simulation parameters used for Last Level Cache |  |  |  |  |

    |----------------------------------------------------------|--|--|--|--|

    |----------------------------------------------------------|--|--|--|--|


    | LLC Parameter           | Values     |

    |-------------------------|------------|

    | No. of sets             | 2048       |

    | No. of sampled sets     | 32         |

    | No. of non-sampled sets | up to 2016 |

    | No. of ways             | 16         |

    | Threshold               | 29         |


    ## 4 Results


    #### 4.1 Problems with existing approaches


    The graph [4](#page-6-0) shows the variation of counter values in different cycles
    for a particular set and its corresponding ways if only threshold based blocking
    mechanism is used. The green line is the threshold value and blue, red and yellow
    lines depict cycle-1, cycle-2 and cycle-3 respectively. Oscillatory nature of
    counter values can be very well observed from the graph i.e. in the previous cycle,
    the way for which the threshold was exceeded is blocked for the current cycle.
    But, the write counter values for those unblocked ways have now exceeded the threshold
    and so they will get block in the next cycle.


    ### 4.2 IP Bias


    The graph [5](#page-6-1) depicts the Instruction Pointer Bias which has been utilised
    in our algorithm. The blue line depicts number of sampled set accesses and red
    line depicts number of unsampled set accesses by an Instruction Pointer. It is
    worth noting here that only 32 sampled sets are present out of total 2048 total
    sets. Only a limited number of instruction pointers were frequently processed
    in the context of blocking methods. These frequently accessed instruction pointers
    exhibited similar access patterns and methods, as well as similar writing behavior
    across different program executions.


    ![](_page_6_Figure_2.jpeg)


    <span id="page-6-0"></span>Figure 4: Graph of counter values vs ways. It depicts
    how blocking using equal writes or other similar approaches can lead to an oscillatory
    nature.


    ![](_page_6_Figure_4.jpeg)


    <span id="page-6-1"></span>Figure 5: Graph of number of accesses vs instruction
    pointers for sampled sets and not sampled sets. We can observe that some sets
    instruction pointers have large accesses and others very less.


    ![](_page_7_Figure_2.jpeg)


    <span id="page-7-0"></span>Figure 6: Graph of variance vs number of cycles elapsed
    for existing algorithm and proposed algorithm. We can see variances are almost
    always less for our proposed algorithm.


    | Table 2: IPC values of existing approaches and proposed algorithm for different
    traces. |  |

    |-----------------------------------------------------------------------------------------|--|

    |-----------------------------------------------------------------------------------------|--|


    <span id="page-7-1"></span>


    | Traces                   | IPC (Existing Approach) | IPC (Our Approach) |

    |--------------------------|-------------------------|--------------------|

    | gcc_13B.trace.xz         | 0.123613                | 0.123836           |

    | bwaves_1861B.trace.xz    | 0.437062                | 0.437127           |

    | mcf_158B.trace.xz        | 0.0376413               | 0.0376554          |

    | libquantum_964B.trace.xz | 0.277188                | 0.277591           |


    As a result, by taking advantage of this bias, it is feasible to optimize write
    operations and reduce total cache wear, resulting in balanced write endurance


    #### 4.3 Variance


    The grap[h6](#page-7-0) compares the variances of ways of a particular set in
    difference cycles obtained by the existing approach(blue line graph) to those
    obtained by our algorithm(red line graph). For lower number of cycles, both the
    methods have similar variances. But for larger number of cycles, our method gives
    lower variance value among the ways of the set implying that appropriate levelling
    of write operation is handled by our algorithm.


    ## 4.4 IPC


    The grap[h7](#page-8-0) shows a comparison of the IPC(Instructions per Cycle)
    value of existing approach ([\[6\]](#page-9-5)) and our approach. When number
    of cycles is low, both the approaches have similar IPC values. As number of cycles
    increases, our approach gives better IPC value as compared to existing approach.
    IPC value is a great metric to compare performance of different algorithms applied
    to cache-architecture. Note that the above graph is drawn for *libquantum* trace.


    ![](_page_8_Figure_2.jpeg)


    <span id="page-8-0"></span>Figure 7: Graph of IPC vs number of cycles. We can
    see that our proposed algorithm has better IPC than existing approaches.


    Table 3: Miss ratio (misses/total accesses) of existing approaches like equal
    writes against our proposed algorithm for different traces.


    <span id="page-8-1"></span>


    | Traces                   | Miss Ratio(Existing Approaches) | Miss Ratio(Our
    Approach) |

    |--------------------------|---------------------------------|--------------------------|

    | gcc_13B.trace.xz         | 0.930459                        | 0.92914                  |

    | bwaves_1861B.trace.xz    | 0.983                           | 0.98299                  |

    | mcf_158B.trace.xz        | 0.98379                         | 0.98347                  |

    | libquantum_964B.trace.xz | 0.89084                         | 0.89084                  |


    #### 4.5 IPC trends over different traces


    IPC values obtained by existing approach and our approach over different traces
    has been given in the tabl[e2.](#page-7-1) For all the four different traces,
    IPC value of our approach is higher than the existing method. This means more
    number of instructions are executed in same number of cycles meaning that our
    approach has better performance than existing method ([\[6\]](#page-9-5)).


    ## 4.6 Miss ratio over different traces


    Miss Ratio which is the number of misses divided by total cache accesses is another
    performance metric in cache architecture. Less the miss ratio, better is the performance.
    LLC miss ratio value for different traces has been given in the tabl[e3](#page-8-1)
    each for existing approach and our approach. For *libquantum trace*, both approach
    have the same miss ratio. While for other three traces, our approach has less
    miss ratio than the existing approach.


    # 5 Conclusion


    NVM(Non-Volatile Memory) has major advantages over SRAM, like its non-volatile,
    low power consumption, and high density. But to make it usable in caches, a major
    challenge of NVM, namely its write endurance issue, needs to be overcome. The
    main reason for this write endurance issue is that different ways of a set as
    well as different sets, do not have the same number of writes. This inter-set
    and intra-set write variability causes some blocks to fail, hampering the working
    of the cache. To overcome this issue, many existing approaches were proposed.
    A common feature of these approaches is that they block, that is, make a way not
    accessible for writes. This then directs writes to a cold way, balancing the writes.
    But a common problem with these approaches is that as they only use previous writes
    to make the decision to block or not, they suffer from problems associated with
    excessive blocking, like an oscillatory nature of the number of writes. In this
    paper, we have proposed a feedback-based blocking method that uses previous ''k''
    histories to make the decision to block a way or not. We compared our proposed
    system with an existing approach([\[6\]](#page-9-5)) and found our algorithm has
    better IPC(Instructions per cycle). Also, the proposed system''s variance was
    better, implying better balancing of writes among the ways. Thus if we can solve
    the write endurance problem, we can better utilize NVMs.


    # 6 Acknowledgement


    I would want to express my sincere thanks and gratitude to everyone who helped
    bring this project to a conclusion.


    First and foremost, I''d want to express my gratitude to my project supervisors,
    Dr. Shirshendu Das and Dr. Sudeepta Mishra, for their excellent guidance and support
    during the project. Their knowledge and suggestions have been very much valuable
    in influencing the project''s direction and ensuring that it fulfills the requisite
    requirements.


    I would also want to thank Mr. Prathamesh, the project''s teaching assistant,
    for his assistance. His advice and support have been vital in the project''s successful
    conclusion.


    ## References


    - <span id="page-9-0"></span>[1] Mengwei Si, Huai-Yu Cheng, Takashi Ando, Guohan
    Hu, and Peide D Ye. Overview and outlook of emerging non-volatile memories. *Mrs
    Bulletin*, 46(10):946–958, 2021.

    - <span id="page-9-1"></span>[2] Simone Raoux, Feng Xiong, Matthias Wuttig, and
    Eric Pop. Phase change materials and phase change memory. *MRS bulletin*, 39(8):703–710,
    2014.

    - <span id="page-9-2"></span>[3] Furqan Zahoor, Tun Zainal Azni Zulkifli, and
    Farooq Ahmad Khanday. Resistive random access memory (rram): an overview of materials,
    switching mechanism, performance, multilevel cell (mlc) storage, modeling, and
    applications. *Nanoscale research letters*, 15:1–26, 2020.

    - <span id="page-9-3"></span>[4] Ahmad Hassan, Hans Vandierendonck, and Dimitrios
    S. Nikolopoulos. Energy-efficient hybrid dram/nvm main memory. In *2015 International
    Conference on Parallel Architecture and Compilation (PACT)*, pages 492–493, 2015.
    doi[:10.1109/PACT.2015.58.](https://doi.org/10.1109/PACT.2015.58)

    - <span id="page-9-4"></span>[5] Saeed Kargar and Faisal Nawab. Challenges and
    future directions for energy, latency, and lifetime improvements in nvms. *Distributed
    and Parallel Databases*, 41:1–27, 09 2022. doi[:10.1007/s10619-022-07421-x.](https://doi.org/10.1007/s10619-022-07421-x)

    - <span id="page-9-5"></span>[6] Sparsh Mittal and Jeffrey S Vetter. Equalwrites:
    Reducing intra-set write variations for enhancing lifetime of non-volatile caches.
    *IEEE Transactions on Very Large Scale Integration (VLSI) Systems*, 24(1):103–114,
    2015.

    - <span id="page-9-6"></span>[7] Jue Wang, Xiangyu Dong, Yuan Xie, and Norman
    P Jouppi. i 2 wap: Improving non-volatile cache lifetime by reducing inter-and
    intra-set write variations. In *2013 IEEE 19th International Symposium on High
    Performance Computer Architecture (HPCA)*, pages 234–245. IEEE, 2013.

    - <span id="page-9-7"></span>[8] Mohammad Reza Jokar, Mohammad Arjomand, and Hamid
    Sarbazi-Azad. Sequoia: A high-endurance nvm-based cache architecture. *IEEE Transactions
    on Very Large Scale Integration (VLSI) Systems*, 24(3):954–967, 2015.

    - <span id="page-9-8"></span>[9] Sparsh Mittal and Jeffrey S Vetter. Equalchance:
    Addressing intra-set write variation to increase lifetime of non-volatile caches.
    In *2nd Workshop on Interactions of NVM/Flash with Operating Systems and Workloads
    (*{*INFLOW*} *14)*, 2014.

    - <span id="page-9-9"></span>[10] Moinuddin K Qureshi, John Karidis, Michele Franceschini,
    Vijayalakshmi Srinivasan, Luis Lastras, and Bulent Abali. Enhancing lifetime and
    security of pcm-based main memory with start-gap wear leveling. In *Proceedings
    of the 42nd annual IEEE/ACM international symposium on microarchitecture*, pages
    14–23, 2009.

    - <span id="page-9-10"></span>[11] Jagadish B Kotra, Mohammad Arjomand, Diana
    Guttman, Mahmut T Kandemir, and Chita R Das. Re-nuca: A practical nuca architecture
    for reram based last-level caches. In *2016 IEEE International Parallel and Distributed
    Processing Symposium (IPDPS)*, pages 576–585. IEEE, 2016.

    - <span id="page-9-11"></span>[12] Sparsh Mittal, Jeffrey S Vetter, and Dong Li.
    Lastingnvcache: A technique for improving the lifetime of non-volatile caches.
    In *2014 IEEE Computer Society Annual Symposium on VLSI*, pages 534–540. IEEE,
    2014.

    - <span id="page-10-0"></span>[13] Shuai Wang, Guangshan Duan, Yupeng Li, and
    Qianhao Dong. Word-and partition-level write variation reduction for improving
    non-volatile cache lifetime. *ACM Transactions on Design Automation of Electronic
    Systems (TODAES)*, 23(1):1–18, 2017.

    - <span id="page-10-1"></span>[14] Sparsh Mittal and Jeffrey S Vetter. Ayush:
    Extending lifetime of sram-nvm way-based hybrid caches using wear-leveling. In
    *2015 IEEE 23rd International Symposium on Modeling, Analysis, and Simulation
    of Computer and Telecommunication Systems*, pages 112–121. IEEE, 2015.

    - <span id="page-10-2"></span>[15] Sungjin Lee and Jihong Kim. Effective lifetime-aware
    dynamic throttling for nand flash-based ssds. *IEEE Transactions on Computers*,
    65(4):1075–1089, 2014.

    - <span id="page-10-3"></span>[16] Lunkai Zhang, Brian Neely, Diana Franklin,
    Dmitri Strukov, Yuan Xie, and Frederic T Chong. Mellow writes: Extending lifetime
    in resistive memories through selective slow write backs. *ACM SIGARCH Computer
    Architecture News*, 44(3):519–531, 2016.'
- title: "ControlPULPlet: A Flexible Real-time Multi-core RISC-V Controller for\n\
    \  2.5D Systems-in-package"
  abstract: 'The growing complexity of real-time control algorithms with increasing

    performance demands, along with the shift to 2.5D technology, drive the need

    for scalable controllers to manage chiplets'' coupled operation in 2.5D

    systems-in-package. These controllers must offer real-time computing

    capabilities, as well as System-in-package (SiP) compatible IO interfaces for

    communicating with the controlled dies. Due to real-time constraints, a key

    challenge is minimizing the performance penalty of die-to-die communication

    with respect to native on-chip control interfaces. We address this challenge

    with ControlPULPlet, an open-source, real-time multi-core RISC-V controller

    designed specifically for SiP integration. ControlPULPlet features a 32-bit

    CV32RT core for fast interrupt handling and a specialized direct memory access

    engine to automate periodic sensor readout. A tightly-coupled programmable

    multi-core cluster for acceleration of advanced control algorithms is

    integrated through a dedicated AXI4 port. A flexible AXI4-compatible die-to-die

    (D2D) link enables efficient communication in 2.5D SiPs. We implemented and

    fabricated ControlPULPlet as a silicon demonstrator called Kairos in TSMC''s

    65nm CMOS. Kairos runs model predictive control algorithms at up to 290 MHz in

    a 30 mW power envelope. The D2D link attains a peak duplex transfer rate of 51

    Gbit/s at 200 MHz, at the minimal costs of just 7.6 kGE in PHY area per

    channel, adding just 2.9% to the total system area.'
  url: http://arxiv.org/abs/2410.15985v2
  keywords: Real-time, autonomous control, 2.5D, chiplet, RISC-V
  document: '# ControlPULPlet: A Flexible Real-time Multi-core RISC-V Controller for
    2.5D Systems-in-package


    Alessandro Ottaviano [,](https://orcid.org/0009-0000-9924-3536) *Student Member,
    IEEE*, Robert Balas [,](https://orcid.org/0000-0002-7231-9315) *Student Member,
    IEEE*, Tim Fischer [,](https://orcid.org/0009-0007-9700-1286) *Student Member,
    IEEE*, Thomas Benz [,](https://orcid.org/0000-0002-0326-9676) *Student Member,
    IEEE*, Andrea Bartolini [,](https://orcid.org/0000-0002-1148-2450) *Member, IEEE*,
    Luca Benini [,](https://orcid.org/0000-0001-8068-3806) *Fellow, IEEE*


    *Abstract*—The growing complexity of real-time control algorithms with increasing
    performance demands, along with the shift to 2.5D technology, drive the need for
    scalable controllers to manage chiplets'' coupled operation in 2.5D systems-in-package.
    These controllers must offer real-time computing capabilities, as well as System-in-package
    (SiP) compatible IO interfaces for communicating with the controlled dies. Due
    to real-time constraints, a key challenge is minimizing the performance penalty
    of die-to-die communication with respect to native on-chip control interfaces.
    We address this challenge with ControlPULPlet, an open-source, real-time multi-core
    RISC-V controller designed specifically for SiP integration. ControlPULPlet features
    a 32-bit CV32RT core for fast interrupt handling and a specialized direct memory
    access engine to automate periodic sensor readout. A tightly-coupled programmable
    multi-core cluster for acceleration of advanced control algorithms is integrated
    through a dedicated AXI4 port. A flexible AXI4-compatible die-to-die (D2D) link
    enables efficient communication in 2.5D SiPs. We implemented and fabricated ControlPULPlet
    as a silicon demonstrator called Kairos in TSMC''s 65nm CMOS. Kairos runs model
    predictive control algorithms at up to 290 MHz in a 30 mW power envelope. The
    D2D link attains a peak duplex transfer rate of 51 Gb/s at 200 MHz, at the minimal
    costs of just 7.6 kGE in PHY area per channel, adding just 2.9 % to the total
    system area.


    *Index Terms*—Real-time, autonomous control, 2.5D, chiplet, RISC-V


    ## I. INTRODUCTION


    An increasing number of integrated systems rely on closedloop control to meet
    their mission objectives in terms of performance, power consumption, and thermal
    stability. The control loop consists of three main components, depicted in Fig.
    [1.](#page-0-0) First, sensory circuits on the left side probe the current state
    of the system. Next, a control policy minimizes tracking errors while maintaining
    stability by closely following the setpoint of an objective function. Finally,
    the resulting control actions are applied to physical actuators on the right side,
    leading to changes in the controlled system''s behavior and, therefore, perturbations
    within its environment. Since the environment state evolves independently of the
    control pipeline, it is periodically


    E-mail: {aottaviano,balasr,tbenz,fischeti,lbenini}@iis.ee.ethz.ch


    <span id="page-0-0"></span>![](_page_0_Figure_12.jpeg)


    Fig. 1: Main elements involved in the control loop pipeline of a controller agent.


    probed by the sensory circuitry. Smaller sampling periods lead to finer granularity
    of control actions. Such control systems are used across various application domains,
    including automotive systems and robotics [\[1\]](#page-12-0), power conversion
    [\[2\]](#page-12-1), and CPU power management [\[3\]](#page-12-2), [\[4\]](#page-12-3).
    For instance, in a smart power converter, the objective function may be conversion
    efficiency, while in CPU power management, it could correspond to a power budget
    based on runtime workload and temperature.


    The system component executing the control schemes is the controller agent, or
    controller. To maximize flexibility, controllers are implemented as embedded,
    Multiple-Input Multiple-Output (MIMO) digital programmable units (see Section
    [IV\)](#page-10-0), with power envelopes from tens to hundreds of mW and on-chip
    memory footprint under a few MiB [\[3\]](#page-12-2), [\[5\]](#page-12-4).


    As shown in Fig. [1,](#page-0-0) a controller performs two main tasks: data communication
    during sensor readout and actuator configuration, and processing for control policy
    computation. These tasks are constrained by three key factors: predictability
    of execution, real-time performance, and reliability. This work focuses on the
    first two properties. Predictable and real-time execution means the communication
    and processing pipelines must complete deterministically before the next control
    loop period begins, known as the deadline. To achieve this goal, we need to meet
    three design objectives: (1) designing controllers with predictable hardware features,
    such as cache-less memory hierarchies using scratchpad memorys (SPMs) or simple,
    inorder, non-speculative processor cores; (2) ensuring physical isolation between
    the controller and its environment to reduce interference on shared resources,
    such as the interconnect (bus) hierarchy. Isolation includes runtime freedom from
    interference, fabrication-level independence, and standalone certification for
    safety integrity [\[6\]](#page-12-5); (3) minimizing communication and processing
    latency. Communication latency can


    A. Ottaviano, and R. Balas contributed equally to this work.


    A. Ottaviano, R. Balas, T. Fischer, T. Benz, and L. Benini are with the Integrated
    Systems Laboratory (IIS), ETH Zurich, Switzerland.


    L. Benini is also with the Department of Electrical, Electronic and Information
    Engineering (DEI), University of Bologna, Bologna, Italy.


    A. Bartolini is with the Department of Electrical, Electronic and Information
    Engineering (DEI), University of Bologna, Bologna, Italy. E-mail: a.bartolini@unibo.it


    <span id="page-1-0"></span>![](_page_1_Figure_1.jpeg)


    Fig. 2: Controller topologies, categorized by packaging technology.


    be reduced through high-bandwidth interfaces between the controller and the controlled
    system, and through fast interrupt response times (< 10 clock cycles). Processing
    latency can be improved by scaling the controller''s computing resources (e.g.,
    domain-specific accelerators (DSAs) or multi-core architectures) tailored to the
    complexity of the control policy.


    Another impacting factor on the isolation and communication latency is the integration
    strategy of the controller and the controlled system, which we depict in Fig.
    [2.](#page-1-0) Inter-package integration (Fig. [2a](#page-1-0)) offers excellent
    isolation but higher communication latency. Conversely, monolithic integration
    on the same silicon die (Fig. [2b](#page-1-0)) minimizes latency but hinders isolation,
    as components share resources during operation and are logically indivisible as
    part of the same fabrication process.


    Systems in package (SiPs) offer a tradeoff between isolation and latency by enclosing
    multiple chips in a single carrier. In this setup, die-to-die (D2D) connections
    between the controller and the system can be implemented using wire bonding (Fig.
    [2d](#page-1-0)), as seen in commercial Si-GaN power switches [\[7\]](#page-12-6).
    The latest development in system in package (SiP) technology is the "disintegration"
    of a monolithic silicon die in many chiplets, or dielets. This post-Moore approach
    reduces design and production costs, improves production yield thanks to smaller
    chip sizes, and facilitates the integration of heterogeneous components fabricated
    independently [\[8\]](#page-12-7). Passive or active interposers hosting D2D connectivity
    fabrics [\[9\]](#page-12-8) offer up to 44× the bandwidth of off-chip links [\[10\]](#page-12-9),
    though still lower than native on-die communication. Figure [2e](#page-1-0)-f
    show an example of controller integration with passive and active interposers,
    respectively. Beyond interposer technology, bleedingedge technologies like wafer-scale
    integration enable dense, low-latency chiplet communication via redistribution
    layers or silicon bridges on wafer, bypassing traditional packaging, as shown
    in Fig. [2c](#page-1-0).


    Early adopters of chiplet technology in commercial products, such as AMD, have
    leveraged it in high performance computing (HPC) systems and data-intensive machine
    learning (ML) workloads to address the longer development cycles and increased
    expenses of transitioning to advanced manufacturing nodes in conventional monolithic
    integration [\[11\]](#page-12-10)–[\[13\]](#page-12-11). Recently, chiplets have
    been explored and prototyped in other domains, e.g., Si-GaN switches, to achieve
    tighter integration between controllers and controlled systems [\[2\]](#page-12-1).


    Most embedded controllers with real-time and predictable execution requirements
    continue to rely on monolithic integration. However, as advanced autonomous systems
    — such as humanoid robots and self-driving cars — demand growing computational
    performance and integration, while maintaining real-time constraints, they are
    beginning to face similar challenges to high-performance systems, making SiP approaches
    for control integration a compelling alternative.


    In light of these opportunities and challenges, we present ControlPULPlet, a platform
    that can be configured for monolithic or SiP integration. The platform builds
    on the RISC-V ControlPULP controller architecture, which follows the programmable
    multi-core accelerator (PMCA) paradigm [\[3\]](#page-12-2). This paper introduces
    several hardware enhancements compared to the ControlPULP architecture towards
    a flexible, real-time embedded controller with 2.5D interface compatibility: (i)
    it introduces a D2D control interface that can be used as a dropin alternative
    to the native AXI4 interface for connecting to the controlled system, facilitating
    integration in chiplet designs; (ii) it extends the general-purpose core functionality
    with fast interrupt handling through a lightweight ISA extension, essential in
    real-time systems, and a specialized, autonomous direct memory access (DMA) engine
    for periodic data acquisition through the D2D link without any periodic intervention
    of the processor; (iii) finally, it evaluates these additional features on a silicon
    demonstrator chip, called Kairos. The synthesizable hardware description is open-source
    under a liberal license[1](#page-1-1) .


    This paper makes the following contributions:


    - An open-source, embedded RISC-V controller with comprehensive real-time and
    computing capabilities compatible with the monolithic and chiplet design paradigms.
    The design enhances the open-source ControlPULP [\[3\]](#page-12-2), tuned for
    on-chip control applications (Section [II\)](#page-2-0).

    - A scalable, AXI4-compatible source-synchronous digital D2D link. With eight
    channels and a flow control buffer depth of 128, the link achieves an average
    peak bus utilization of 85 %, compared to 95 % for its on-chip equivalent (Section
    [III-A\)](#page-6-0). This setup incurs a minimal PHY area overhead of 2.9 % and
    results in a negligible performance impact to the latency of an exemplary control
    loop application (Section [III-B\)](#page-6-1). Increasing the buffer depth enhances
    utilization and matches on-chip control performance, albeit at additional area
    cost.

    - Integration of hardware enhancements for real-time execution: fast interrupt
    handling and context switching [\[14\]](#page-12-12), and direct memory access
    (DMA) with periodic midend [\[15\]](#page-12-13) (Section [II-B\)](#page-3-0).

    - A standalone, single-core demonstrator chip called Kairos, fabricated in TSMC''s
    65 nm node. At 1.2 V, Kairos achieves a peak clock frequency of 290 MHz with a
    power envelope not exceeding 30 mW during data-


    <span id="page-1-1"></span><sup>1</sup><https://github.com/pulp-platform/control-pulp/>


    <span id="page-2-2"></span>![](_page_2_Figure_1.jpeg)


    Fig. 3: ControlPULPlet architecture, building upon the work in [\[3\]](#page-12-2).
    Extensions introduced in this work are highlighted in color, with pre-existing
    architectural blocks shown in grayscale for distinction. ■ represents a bus adapter
    that convert between open bus interface (OBI) and AXI4 protocols, and between
    32 b and 64 b bus widths.


    intensive control workloads (Section [III\)](#page-5-0). On the chip, the D2D
    link attains a duplex peak transfer rate of 51.2 Gb/s at the nominal 200 MHz (Section
    [III\)](#page-5-0).


    ## II. ARCHITECTURE


    <span id="page-2-0"></span>In this section, we first describe the ControlPULPlet
    platform, highlighting the main enhancements compared to the ControlPULP architecture
    (Section [II-A\)](#page-2-1). Then, we dive into the integration of the real-time
    features (Section [II-B\)](#page-3-0) and the architecture of the D2D link (Section
    [II-C\)](#page-3-1).


    ## <span id="page-2-1"></span>*A. ControlPULPlet Platform*


    The high-level block diagram of the ControlPULPlet controller is illustrated in
    Fig. [3.](#page-2-2) Pre-existing architectural blocks are represented in grayscale,
    while architectural additions introduced in this work are highlighted with a distinct
    color scheme for clarity and differentiation.


    *Manager and programmable multi-core accelerator (PMCA) Domains:* The design is
    centered around a 32 bit, RISC-V manager domain integrating a CV32RT in-order
    core [\[14\]](#page-12-12). To accelerate compute-intensive control workloads,
    such as model predictive control (MPC), an 8-core, DMAcapable PMCA with per-core
    32 b floating point units (FPUs) connects to the manager domain; the PMCA has
    been extensively validated in [\[3\]](#page-12-2). Communication between these
    domains occurs via two 64 b AXI4 DSA ports, shown in Fig. [3.](#page-2-2) These
    ports can be disabled at design time for simpler applications where a single core
    suffices.


    *Interconnect System:* The interconnect fabric consists of 32 b data and address-wide,
    2-clock-cycle access latency, OBIcompliant network elements for low and predictable
    access latency to the memory system. To bridge with the on-chip and inter-chiplet
    IO interface, detailed below, the 32 b OBIcompliant data interface is converted
    to a 64 b AXI4-compliant interface, and arbitrated by an AXI4 crossbar with 3-clockcycles
    latency. Several bus adapters convert between the two protocols, as shown in Fig.
    [3](#page-2-2) by the small green rectangles.


    *On-chip and Inter-chiplet IO Interface:* The subordinate interface of the 64
    b AXI4 bus — indicated as sAXI4 in Fig. [3](#page-2-2) — is responsible for booting
    the controller. This can be performed by the controlled system, or a separate,
    secure hardware root-of-trust.


    The manager interface — indicated as mAXI4 in Fig. [3](#page-2-2) — realizes the
    sensing/actuation communication path shown in Fig. [1.](#page-0-0) It is responsible
    for sensor readout and dispatch of the quantities produced by the control policy.
    As illustrated in Fig. [3,](#page-2-2) a bypass network utilizing AXI4-compatible
    network switches enables selection between the on-chip AXI4 interface and the
    D2D protocol for I/O connectivity. This selection occurs at the AXI4 bus level,
    which serves as the frontend network layer of the D2D link. The detailed microarchitecture
    of the latter is provided in Section [II-C.](#page-3-1) The two interfaces are
    mutually exclusive, catering to distinct use cases: native AXI4 facilitates on-chip
    integration with controlled systems, while the D2D protocol supports chiplet-based
    integration. Consequently, the bypass configuration is static and must be determined
    at design time prior to physical implementation.


    A 64 b, AXI-compliant system DMA engine connects directly with the outgoing D2D
    interface to decouple off-chiplet transfers from other tasks running on the manager
    and PMCA domains. This separation also ensures that high-contention dataplane
    transfers do not interfere with time-critical controlplane operations [\[16\]](#page-12-14),
    [\[17\]](#page-12-15), which are the primary focus of this work. The AXI4-compliant
    interface enables highperformance transfers through support for burst transactions
    and multiple outstanding requests. The DMA engine integrates a hardware extension
    for autonomous, core-independent data movement, which we detail in Section [II-B.](#page-3-0)
    To allow also the manager core to interact with the controlled system, an additional
    AXI4 manager port is arbitrated with the system DMA port (green rectangle in Fig.
    [3\)](#page-2-2).


    *Memory System:* An on-chip, 512 KiB shared L2 SPM in the manager domain serves
    as the main exchange point with the PMCAs, which owns a private, bank-interleaved
    128 KiB L1 SPM. In both domains, the OBI interconnect ensures constant access
    times to the memory endpoints. To minimize access conflicts between large burst
    transfers from the system or DSA DMA engines and narrow transfers from the manager
    core, the L2 SPM banks use an interleaved addressing scheme. The L1 SPM employs
    a similar scheme to reduce interference among the 8 PMCA cores.


    *Doorbell-based Mailboxes Messaging:* ControlPULPlet features a configurable doorbell-based
    mailbox unit with 64 default mailboxes, enabling real-time communication for dispatching
    requests, setpoint updates, or control policy constraints. The mailboxes are accessible
    through the AXI4 subordinate external port from the controlled system. A RISC-V
    core-local interrupt controller (CLIC) handles incoming interrupt lines from peripherals
    and the mailbox unit; in the latter case, one interrupt line per mailbox is used.


    *Off-chip IO Interface:* To handle off-chip communication with the controlled
    system, as in the case of discrete physical actuators like motors, brakes, robotic
    arms, and voltage regulator modules, ControlPULPlet supports a broad set of state-of-the-art
    (SoA) peripherals. Each IO peripheral is managed by a private, IO DMA, featuring
    a configurable number of peripherals, defaulting to 32 GPIOs, 1 UART, 12 I2C,
    and 8 SPI. The DMA core architecture mirrors that used for on-chip or D2D communication,
    since the unit supports multiple interface protocols and thus enables the same
    hardware to handle diverse scenarios [\[15\]](#page-12-13). The autonomous data
    transfer extension is also supported for off-chip peripherals. Finally, two 32
    b system timers and a PWM timer enable diverse periodic control scenarios, such
    as motor control.


    *Programming Model:* ControlPULPlet employs a lightweight real-time OS (RTOS)-based
    programming model, supporting multiple RTOSs. An RTOS manages periodic tasks in
    firmware via platform timers, essential for control applications. This approach
    underscores two key aspects. First, it emphasizes fast interrupt handling to optimize
    context switching on single-core processors (Section [II-B\)](#page-3-0). Second,
    it highlights the benefits of a programmable RISC-V-based controller that balances
    flexibility and specialization. Such a controller supports standardized RTOSs,
    making it suitable for diverse safety- and time-critical control applications.


    ## <span id="page-3-0"></span>*B. Real-time features*


    We detail the real-time extensions integrated into the ControlPULPlet platform,
    focusing on the interrupt subsystem and autonomous sensor readout. The final paragraph
    details their system-level integration.


    *CV32RT:* The CV32RT core achieves exceptional performance for real-time tasks
    through several architectural innovations tailored for minimizing interrupt latency
    and context switching time. It integrates a fast interrupt controller based on
    the Core Local Interrupt Controller (CLIC) RISC-V standard. The CLIC introduces
    support for vectored interrupts, levelbased prioritization, and selective hardware
    vectoring (SHV), which accelerates both nested and non-nested interrupt scenarios.
    Additionally, the fastirq lightweight ISA extension of the core reduces the interrupt
    latency and context switching to as low as 6 and 100 fixed clock cycles, respectively,
    on-par to SoA at low area cost. Therefore, the RTOS overhead is bound to 107 clock
    cycles for context switch and timer tick setup (1 cycle). fastirq''s architecture
    comprises a dual-banked register file with an integrated FSM that triggers a background,
    fully hardware-managed context-saving mechanism upon interrupt arrival. A bank
    switch allows immediate execution of the handler while the prior context is asynchronously
    stored to memory via a dedicated datapath. The design includes stack pointer adjustment
    logic and memory access coordination to avoid conflicts, enabling consistent execution
    even during partial saves. When equal-priority, back-to-back interrupts are fired,
    *tail-chaining* minimizes redundant context save/restore sequences, preserving
    only the first save and the final restore.


    *DMA With Real-time Mid-end:* For real-time data acquisition tasks, such as reading
    sensor arrays with complex and irregular address maps, the system integrates a
    modular DMA [\[15\]](#page-12-13). A dedicated real-time (RT) mid-end manages
    Ndimensional data transfers with support for highly configurable parameters such
    as stride, shape, and period, enabling precise control of streaming patterns via
    software. The mid-end architecture includes a microcoded transfer controller and
    a programmable sequencer, which orchestrate address generation and handshake logic
    independently from the data path. This decoupling of control and data planes allows
    data movement to proceed in parallel with computation, minimizing contention


    <span id="page-3-2"></span>![](_page_3_Figure_7.jpeg)


    Fig. 4: D2D link architecture overview.


    <span id="page-3-3"></span>![](_page_3_Figure_9.jpeg)


    Fig. 5: D2D link network layer.


    on shared interconnects and offloading the general-purpose cores. A loop flattening
    unit and nested loop counter enable complex multidimensional access patterns,
    while internal FI-FOs and backpressure-aware arbitration logic ensure timingaccurate
    execution in real-time workloads. The autonomous DMA is crucial for maintaining
    tight timing constraints and preserving computational slack for control algorithms,
    as discussed in Section III-B.


    *Integration and Configuration:* The CLIC is configured with 128 interrupt lines
    by default, accommodating 64 interrupts from mailboxes and additional interrupts
    from other peripherals. To support automatic context saving via the fastirq extension,
    a shadow port is injected through the system interconnect from the dual-banked
    register file. As illustrated in Fig. [3,](#page-2-2) this port (highlighted in
    red) handles the saved context, pushed to memory where the stack resides after
    register banks are swapped upon interrupt reception.


    The system DMA engine used for D2D communication is configured with 64 b data
    width. As shown in Fig. [3,](#page-2-2) the DMA feeds into the bypass network
    before selecting the IO protocol of the interface (native AXI4 or D2D link), bypassing
    the traffic on the main system interconnect to reduce unpredictable delays in
    the sensor readout path. For off-chip transfers, the IO DMA is configured with
    32 b data width, matching the sizing of the APB bus.


    ## <span id="page-3-1"></span>*C. D2D link*


    The D2D link is a digital, source-synchronous link employing double data rate
    (DDR) signaling, designed to translate AXI4 transactions into an off-chiplet interface
    with a configurable number of channels (CH). The channels operate independently,
    ensuring isolation and flexibility. A channel is composed of a configurable number
    of data lanes (LN), with a source-synchronous clock in each direction. The channels
    are synchronized at the receiving end of the link. The microarchi-


    <span id="page-4-0"></span>![](_page_4_Figure_1.jpeg)


    Fig. 6: D2D link PHY for one channel. Highlighted in purple and green are the
    TX and RX modules, respectively, and their main data and clock signals.


    tecture of a D2D link is shown in Fig. [4.](#page-3-2) It implements three layers:
    network, data, and physical (PHY) layer.


    *Network Layer:* The network layer provides a duplex (manager/subordinate) AXI4
    frontend interface. For embedded control chiplets operating on a dedicated control
    plane, as in our design, AXI4 offers an effective trade-off between simplicity,
    robustness, and performance, compared to full protocol stacks like PCIe. The network
    layer serializes duplex AXI4 requests and responses into an AXI stream (AXIS)
    interface, employing a credit-based flow control mechanism to generate backpressure
    on the AXI4 frontend.


    Figure [5](#page-3-3) illustrates the network layer circuitry. The ten channels
    of the m AXI4 and s AXI4 ports, as labeled in the figure, are arbitrated based
    on their transfer direction, thus grouping input/output requests and responses.
    Flow control is managed through credit counters, which decrement when transactions
    are issued by the D2D (e.g., address write/read or data write transmissions) and
    increment when transactions are received by the D2D (e.g., write responses or
    read requests). This credit-based flow control mechanism generates backpressure
    on the AXI4 frontend, preventing buffer overflow on the receiver side or guaranteeing
    no data loss.


    For each transfer direction, the network layer converts AXI4 transactions into
    AXIS packets, multiplexing the five AXI4 channels (AW, W, B, AR, R) into a unified
    stream (Fig. 5). Each AXIS packet includes: (i) one AXI beat from the selected
    channel, with a payload width equal to the maximum of all channel widths; (ii)
    an optional B channel response (used to complete write transactions); (iii) a
    4-bit header tagging the AXI channel type; and (iv) a credit count field to support
    backpressure. The receiver decodes each packet and dispatches the payloads to
    the appropriate AXI channel. The dataflow of header, payload, and credits in a
    packet is shown in Fig. 6.


    The arbitration policy prioritizes control channels (AR, AW) and follows a round-robin
    policy to prevent starvation and violation of AXI4 ordering rules [\[18\]](#page-12-16).
    The design does not support transactions issued but not yet completed (outstanding),
    allowing only one AW/AR transfer with the same transaction ID at a time.


    *Data Link Layer and Flow Control:* The data link layer (Fig. [4\)](#page-3-2)
    segments the AXIS payload of an AXIS packet into channel-sized flits. When DDR
    is enabled, each channel has width 2 · LN, as shown in Fig. 4 on the right of
    the Data link layer module, for data transmission and reception, which thereby
    happens in one clock cycle. This does not account


    <span id="page-4-1"></span>![](_page_4_Figure_9.jpeg)


    Fig. 7: D2D chronogram of an exemplary 32 B-burst DMA write transfer through the
    D2D link. We show the network, link, and PHY TX signal interfaces of the transmitting
    D2D link instance and the PHY RX signal interface of the receiving D2D link instance.
    Only one channel is shown for readability.


    for the transmitting and receiving clocks. In this setting, the number of bits
    transferable per clock cycle, or the theoretical bandwidth, is expressed as θ
    = 2 · CH · LN b.


    The data link layer includes a flow control credit buffer to store incoming data.
    This buffer operates as a FIFO (First-In, First-Out) queue. In a given D2D instance,
    data is pushed into the FIFO during reception and popped out during transmission.
    A similar process occurs symmetrically in the FIFO queue on the other D2D instance.
    The network layer''s flow control mechanism ensures that the FIFO does not overflow
    by applying backpressure as needed. To prevent deadlocks during long write/read
    bursts, e.g. from the system DMA, the network layer can send empty payload packets
    with credits to match the capacity of the burst transfer.


    Each FIFO element has a size of θ, meaning it contains all the data flits across
    all channels. The FIFO depth equals the number of flow control credits CRD times
    the number of splits (or chunks) required to divide a packet into smaller segments:
    CRD · (size(packet) / θ). One should aim at configuring the link to preserve the
    original packet size, minimizing the number of chunks, else increasing the FIFO
    depth. The CRD parameter plays a critical role in determining the link throughput,
    particularly for large write/read bursts where the FIFO dimensions constrain the
    backpressure efficiency. This impact is analyzed in detail in Section [III-A.](#page-6-0)


    A channel router, shown in Fig. [4,](#page-3-2) combinationally splits the stream
    in flits and distributes them across the channels. When CH = 1, the router module
    is bypassed in hardware to save area. Faulty channel support could be incorporated
    into the D2D design by extending the data link layer with hardwarelevel detection
    and suppression during link calibration. We leave this enhancement to future work.


    *PHY Layer:* The PHY layer of a single channel provides access to the data lanes
    and the transmitting/receiving


    clock signals across all channels. Each channel includes both a transmission (TX)
    module and a receiver (RX) module, highlighted in Fig. 6 in purple and green,
    respectively. When DDR signaling is enabled, the channel flit tx\_dat\_in, of
    size 2 · LN, is transmitted in one clock cycle. This means that the data interface
    exposed by the link, of size LN on the transmission and reception paths, sends
    or receives data every half clock cycle to keep up with the bandwidth supplied
    by the data link layer flits. At the circuit level, on the TX path, we use a clock
    multiplexer to alternate the transmission of LN data bits depending on the clock
    signal''s high or low phase, thereby achieving DDR signaling. Alternatively, single
    data rate (SDR) signaling can be used bypassing the clock multiplexing logic,
    albeit with a potential reduction in performance.


    The TX module forwards a source-synchronous clock, tx clk out, off-chiplet alongside
    the channel flit. This forwarded clock incorporates a three-quadrature (270°)
    phase shift relative to the PHY input clock, clk in, to maximize the data sampling
    window. The phase shift is achieved using a fully digital, configurable delay
    line implemented as a binary tree of multiplexers. This structure shifts the clock
    by 90°, and a cascade inverter is used to achieve the final three-quadrature phase
    shift, illustrated in Fig. [6.](#page-4-0) This three-quadrature shifting optimizes
    the data sampling eye window, enabling robust data transmission. To ensure data
    is sampled only when valid, the TX module employs a clock gate (CG) cell. The
    CG is controlled by the tx data valid i signal and modulates the logic level of
    the PHY''s source clock prior to phase shifting. The data-valid/ready handshake
    signals are generated at the data link layer and are derived from their corresponding
    signals in the AXI4 and AXIS frontends injected at the network layer. On the RX
    side, data sampling is performed using the received clock, rx clk in. A FIFO-based
    clock domain crossing (CDC) with 2-stage flip-flop synchronizers is responsible
    for synchronizing the incoming data stream, rx data in, with the PHY source clock
    clk in. At the circuit level, when DDR is enabled, the rx\_data\_in input of size
    LN is sampled every cycle on the falling edge of the receiving clock (thus, every
    half-clock cycle), as illustrated by the flip-flop in the bottomright corner of
    Fig. 6. This value is then coalesced with the sample captured on the rising edge
    of ControlPULP''s clock to form a 2 · LN-bit data flit over a full clock cycle,
    fed to the CDC described above. The total number of wires exposed by the PHY is
    given by Nwrs = CH · (2 · (LN + 1)), which accounts for the data lanes and the
    forwarded clock in both input and output directions (duplex).


    *Latency:* Figure [7](#page-4-1) illustrates the chronogram of a 32 B burst transfer
    initiated by the system DMA via a D2D link configured with LN = 8 b and CH = 8.
    The diagram showcases the network, link, and TX PHY interfaces of the transmitting
    D2D instance, along with the RX PHY interface of the receiving instance. Only
    the first channel of the data link layer and PHY is depicted. The transmitting
    and receiving instances are interconnected to form the inter-chiplet bridge.


    At the network layer (yellow), a 4 B address is first transmitted on the AW channel,
    followed by the 32 B data on the W channel. Since the system DMA has a 64 b data
    width, 8 B are dispatched per clock cycle. The handshake signals associated with
    the transfer are also shown. The number of clock cycles between the address and
    data transmissions depends on the implementation of the DMA and the controller
    interconnect. For simplicity, this delay is depicted as 1 clock cycle, denoted
    by the symbol ≈. The AXI4 interface is then converted into an AXIS interface,
    represented by the tuple tx axis out req o.t and tx axis out req o.t valid. The
    latency for this conversion, TAXI4-AXIS, is at most 1 clock cycle. The D2D link
    configuration ensures that the AXIS data payload size fully encapsulates the upstream
    AXI4 transactions.


    At the data link layer (red), the tx data out payload for a single channel is
    shown, with each flit being 2·LN = 2 B. The handshake interface for the data link
    layer is also included. The channel flit splitting of the AXIS stream and its
    subsequent routing incur no additional latency.


    The PHY layer illustrates the TX interface of the transmitting D2D link (purple)
    and the RX interface of the receiving D2D link (green) for one channel. At the
    PHY TX layer, the tx data out signal is transmitted on both edges of the transmitted
    clock (tx clk out), resulting in a data rate of 1 B per half clock cycle in this
    example. The PHY TX layer incurs a latency, TRX,CDC, of 1 clock cycle due to the
    clock gating cell, which latches the enable signal at the rising edge of clk in
    and applies it at the subsequent clock cycle.


    Once the data and clock signals leave the PHY TX of the transmitting D2D instance,
    they are synchronized by the RX module of the receiving D2D instance. The interchiplet
    latency between the two links is determined by the IO, interposer, and packaging
    solution. This delay is depicted as T<sup>δ</sup> in the chronogram and is set
    arbitrarily to 1 clock cycle for readability. On the receiving PHY (RX PHY), up
    to three additional clock cycles are required to synchronize the rx data in signal
    with rx data out between rx clk in and clk in via the CDC. This delay is due to
    the synchronization and buffer logic within the CDC.


    After the PHY RX layer in the receiving D2D instance, the data path (data link
    and network layer) mirrors that of the transmitter and thus is omitted from Fig.
    [7.](#page-4-1)


    While the D2D link is not fully latency-insensitive in the theoretical sense,
    since it relies on finite buffering and timely credit returns, it achieves a high
    degree of latency tolerance through handshake-based elastic buffering and credit-based
    flow control. This design absorbs link-level jitter and moderate inter-chiplet
    delays, as we show in Section [III-A.](#page-6-0)


    # III. EXPERIMENTAL RESULTS


    <span id="page-5-0"></span>We first present a functional evaluation of the D2D
    throughput (Section [III-A\)](#page-6-0). Then, we assess and discuss the impact
    of the D2D interface and real-time features on an exemplary dynamic voltage and
    frequency scaling (DVFS) control loop case study (Section [III-B\)](#page-6-1).
    Figure [8](#page-6-2) illustrates the testbench setup for cycle-accurate RTL simulations,
    which comprises ControlPULPlet and its D2D link instance, and a destination D2D
    link instance. The vertical dashed green line shows the chiplet boundary. The
    destination D2D link connects to a duplex downstream memory controller [2](#page-5-1)
    . This choice is


    <span id="page-5-1"></span><sup>2</sup>Available open-source at<https://github.com/pulp-platform/axi>


    <span id="page-6-2"></span>![](_page_6_Figure_1.jpeg)


    Fig. 8: Simulation setup. The D2D interface of a ControlPULPlet instance is connected
    to another D2D instance and then to an AXI4 duplex memory controller. The testbench
    environment uses the register transfer level (RTL) description of the actual hardware;
    it is, therefore, cycle-accurate.


    <span id="page-6-3"></span>![](_page_6_Figure_3.jpeg)


    Fig. 9: Throughput comparison between D2D link and AXI4 with CH = 8 and LN = 8
    b at varying CRD and system DMA burst size in bytes.


    preferred over a loopback configuration, which would connect ControlPULPlet''s
    D2D link''s input and output, as it allows modeling the storage elements of sensors/actuators
    for the controller policy. The latency of the interconnect and memory system after
    the receiving D2D instance is represented as Tmem. In this simulation setup, both
    T<sup>δ</sup> and Tmem are configurable. Figure [8](#page-6-2) also highlights
    ControlPULPlet''s offchip interface, connected to verification IPs (VIPs)to simulate
    communication along this path.


    ## <span id="page-6-0"></span>*A. D2D Throughput*


    *Sizing the D2D Link:* Finding an optimal D2D link configuration requires determining
    the minimum values of LN and CH that can sustain the bandwidth of the native AXI4
    interface after serialization into the AXIS protocol, namely θ ≥ AXIS packet size.
    As described in Section [II-C,](#page-3-1) the size of an AXIS packet includes
    a constant and variable component. The constant component is determined by the
    system-level configuration of the five AXI channels. It is approximately 100 b
    for a configuration with 64 b data and 32 b address widths, as for ControlPULPlet''s
    system DMA. The variable component depends on the depth of the control flow FIFO,
    as credits are sent/received over the D2D interface. When using the native AXI4
    interface, the DMA can transfer up to 64 b per clock cycle, i.e. θ = 64 b. To
    ensure that the D2D link matches this theoretical bandwidth, a channel width of
    CH ≥ 7 and LN ≥ 8 b guarantees that θ ≥ 112 b can be transferred at each clock
    cycle. For this evaluation, we select an upper bound CH = 8 and LN = 8 b. This
    makes the flow control FIFO size dependent on the choice of CRD only. Additionally,
    we configure T<sup>δ</sup> = 0 and Tmem = 1 clock cycles to minimize external
    overhead in the throughput evaluation.


    *Bus Throughput and Utilization:* Once the theoretical bandwidth θ is determined,
    the maximum attainable duplex throughput of the D2D link reads Θ = 2 · α · f ·
    θ b/s, where α ∈ [0, 1] is the relative bus utilization, and f is the operating
    frequency. At f = 200 MHz, this configuration achieves a theoretical bandwidth
    of Θ = α· 51.2 Gb/s. It represents the bus inefficiencies, i.e., clock cycles
    wasted without transferring data. In the context of our D2D design, α is influenced
    by factors including the additional conversion latencies introduced by the AXI4
    frontend (Fig. [7\)](#page-4-1) and the limited storage capacity of the flow control
    FIFO. Consequently, even when θ is carefully selected to match the theoretical
    bandwidth of the native on-chip AXI4 interface, α inherently reflects the impact
    of the architectural design choices made for the D2D link, affecting the overall
    throughput Θ.


    *Results and Discussion:* We measure the D2D link''s read and write paths throughput
    Θ at varying CRD. We compare it with the throughput achieved by the native AXI4
    on-chip interface without D2D link. For this purpose, the system DMA is programmed
    to issue repeated write and read transfers at increasing burst sizes, starting
    from 8 B.


    Results are shown in Fig. [9.](#page-6-3) The native on-chip interface, labeled
    as AXI4 and represented in purple, approaches peak utilization (α = 1) starting
    from burst sizes of 2 KiB, the maximum permitted by the AXI4 specifications. The
    D2D interface achieves approximately 85 % bus saturation for large bursts on the
    write and read paths with CRD = 128, compared to 95 % of the native AXI4 on-chip
    performance. The performance gap can be narrowed by increasing further the FIFO
    depth CRD, but results in greater area overhead for the D2D link and may necessitate
    a different implementation strategy for the flow control buffer, e.g., an SRAM-based
    design. This area cost is evaluated in-system in Section [III-D.](#page-9-0)


    ## <span id="page-6-1"></span>*B. Case study: Online DVFS Control Loop of HPC
    Processors*


    We consider a control scenario involving online DVFS of HPC processors. In this
    application, the controlled system includes N<sup>c</sup> application-class processing
    elements (PEs). The sensory layer consists of on-chip Process, Voltage, Temperature
    (PVT) sensors. The actuation layer includes setting on-chip phase locked loops
    (PLLs) for per-core frequency scaling and off-chip voltage regulators for voltage
    scaling. We consider 500 PVT sensor registers of 8 B each. Similarly, we assume
    one PLL per-core. Setpoints, such as the reference DVFS operating point , are
    asynchronously sent to the controller via the mailbox unit. The online control
    policy solves a floating-point MPC problem periodically, which outputs the controlled
    DVFS points. The MPC uses a prediction horizon H<sup>p</sup> = 2. For a detailed
    discussion of the MPC power and thermal model, we refer the reader to [\[19\]](#page-13-0).
    We define the DVFS control problem for each PE as PN<sup>c</sup> × N<sup>c</sup>
    Hp.


    <span id="page-7-0"></span>![](_page_7_Figure_1.jpeg)


    Fig. 10: Impact of D2D communication over native on-chip communication (AXI4)
    on a DVFS CPU control scenario at a varying number of CRD. CH = 8 and LN = 8 b
    for the D2D link.


    *Evaluation Settings:* We configure the RTL testbench setup in Fig. [8](#page-6-2)
    with T<sup>δ</sup> = 50 and Tmem = 100 clock cycles. We consider this a reasonable
    upper bound for NoCbased interconnect systems with single-cycle access to on-chip
    memory [\[3\]](#page-12-2). Two synchronous, periodic tasks are initiated by FreeRTOS
    using the platform''s timers: a task for voltage sensing and actuation through
    the off-chip interface and a task for temperature sensing, control policy computation,
    and frequency actuation through the D2D interface. The two tasks have different
    periods: Tshort = 250 µs and Tlong = 3 ms, respectively. ControlPULPlet runs at
    500 MHz. The control policy computation is handled by the manager core and terminated
    after 5 iterations; the PMCA is not used, as MPC acceleration is outside this
    work''s scope. Message dispatching is simulated by issuing messages to ControlPULPlet''s
    mailbox at the beginning of each period with the target DVFS setpoint.


    Overall, meeting the stringent real-time constraints of this application requires
    close HW/SW co-design. We define slack as the free time available to the controller
    before the task with the longest period ends. Denoting the computation and communication
    times required by the controller as Tctrl = Tcomp + Tcomm, the slack is calculated
    as Tslack = Tlong − Tctrl. To meet the deadline, we must always ensure Tslack
    > 0.


    *Results and Discussion:* We first highlight the benefits of fast interrupt handling
    and autonomous DMA before focusing on D2D link communication. The time to decode
    a message dispatched to the mailbox unit is given by:


    $$T\_{\rm MSG} = T\_{\rm CLIC\text{-to-ISR}} + T\_{\rm ISR} + T\_{\rm ISR\text{-to-dec}}$$


    where TCLIC-to-ISR is the time from CLIC acceptance to the first interrupt service
    routine (ISR) instruction, TISR is the ISR execution time, and TISR-to-dec is
    the deferred decoding time, kept outside the ISR to reduce its duration, a common
    real-time programming practice. TCLIC-to-ISR achieves a 72 % speedup (46 to 13
    clock cycles) thanks to automatic hardware-based context saving and restore and,
    reduces the decoding time by 13 %. This acceleration is critical when multiple
    concurrent message requests from various PEs must be processed by the manager
    domain. As described in Sec. II-B, this situation benefits from tail-chaining
    optimizations in the CLIC, dramatically reducing interrupt handling time to 8
    clock cycles.


    For the system DMA, programming incurs a fixed cost of Tsingle,DMAprog = 100 clock
    cycles, as it happens once during Tlong. During long inter-chiplet transfers,
    the automation of this phase provides significant performance gains. For the IO
    DMA, the benefit is even greater. Programming must occur periodically (Tshort)
    and requires a context switch by the running RTOS. :


    $$T\_{\text{tot}, \text{DMAprog}} = \left(T\_{\text{long}} / T\_{\text{short}}\right)
    \cdot \left(T\_{\text{single}, \text{DMAprog}} + T\_{\text{ctx\\_switch}}\right)$$


    With RT-DMA, context switches are eliminated, reducing the overhead to Tsingle,DMAprog.
    For Tshort = 250 µs and Tlong = 3 ms, this results in a 98 % speedup. These optimizations
    mitigate Tcomm and enhance deadline completion guarantees.


    Finally, Fig. [10](#page-7-0) illustrates the impact of the D2D link on control
    loop performance for two control problems (P3 × 3 H2 and P6 × 6 H2), where fastirq
    and autonomous DMA are always active. The execution time is normalized to the
    native AXI4 baseline. For both problems, the baseline already meets the deadline,
    with Tslack = 86 % and Tslack = 21 % for the smaller and larger problems, respectively.
    The D2D link is configured with CH = 8, LN = 8, and varying flow control FIFO
    sizes. We report on message decoding, sensor/actuator communication, and control
    policy computation. Compared to the baseline, the D2D interface has predictably
    a higher Tcomm. For CRD = 8, latency increases by 3.1 × and 3.2 × for P3 × 3 H2
    and P6 × 6 H2, respectively. A more efficient D2D configuration. i.e., CRD = 128,
    reduces this gap to 1.1 × in both applications, hereby approaching the native
    AXI4 performance of on-chip control. As shown in Fig. [10,](#page-7-0) in the
    analyzed application the compute phase dominates Tctrl, accounting for over 99
    % of execution time. Consequently, the D2D link penalty on the overall Tctrl is
    always negligible, i.e., <0.9 % for CRD = 8 and <0.05 % for CRD = 128. Certainly,
    if the MPC computation required fetching some of the data from an off-chip memory
    , the memory traffic over the D2D link would increase, as well as its penalty.


    This section shows that the RT-DMA and an optimally designed D2D link enable high-throughput
    communication without processor intervention, on par with the native onchip performance.
    Fast interrupt handling similarly reduces the processor''s response latency and
    context-switching overhead, allowing it to focus on computation and PMCA offload,
    if enabled. In the next section, we show that the D2D-based control also comes
    at a minimal area and energy cost.


    # *C. Silicon demonstrator*


    We evaluate ControlPULPlet''s area and energy footprint through a silicon demonstrator
    named Kairos, which is designed, implemented, and fabricated using TSMC''s 65
    nm node, targeting a 200 MHz system clock in the SS corner at 125 ◦C. Figure [11](#page-8-0)
    shows Kairos''s annotated die shot and implementation characteristics. The chip
    area is 7.2 mm<sup>2</sup> , housed in a QFN64 package with nominal core and IO
    voltages Vdd,core=1.2 V and Vdd,IO=2.5 V, respectively. All of Kairos'' external
    pads are implemented as regular IOs.


    We configure Kairos without DSA ports due to area constraints, as the PMCA is
    not the primary focus of this work


    <span id="page-8-0"></span>![](_page_8_Figure_1.jpeg)


    Fig. 11: Annotated die shot of Kairos with key characteristics. We highlight the
    D2D logic and IOs on the bottom left of the die. The IO pads are on the sides
    of the chip since the package is quad-flat no-leads (QFN).


    <span id="page-8-1"></span>![](_page_8_Picture_3.jpeg)


    Fig. 12: Kairos evaluation board. Two Kairos chip instances (each in one of the
    blue sockets) are connected to each other on the PCB through the D2D link. D2D
    probes are visible on the top and bottom of the figure.


    and has already been thoroughly verified in [\[3\]](#page-12-2). The manager domain
    features 448 KiB of SPM to accommodate control firmware components and libraries.
    The system interconnect configuration defaults to 32 b data and 32 b addresses,
    while the DMA is configured with a data width of 64 b. The chip integrates one
    system and PWM timers, one UART, four I2C, four SPI hosts, and 64 mailboxes of
    32 B each. We include two on-chip frequency locked loops (FLLs) for the manager
    and peripheral domain clocks. The D2D link is configured with CH = 1 and LN =
    8, totaling Nwrs = 18, and CRD = 8.


    *Limitations:* We are constrained to a low-performance configuration due to the
    64-pin capacity of the surfacemounted, wire-bonded QFN64 package, reflecting practical
    limitations in accessing advanced 2.5D integration technologies. This limitation
    constrained the design of the standalone evaluation board for D2D link assessment,
    which is shown in Fig. [12.](#page-8-1) The board integrates two Kairos chips,
    interconnected to enable communication via their respective D2D links. One chip
    serves as the controller, the other as a subordinate memory endpoint simulating
    the controlled system domain. The packaging limitations primarily affect T<sup>δ</sup>
    (see Fig. [7\)](#page-4-1) due to the inter-chip track length, on average 70.5
    mm long and 0.2 mm wide in our design compared to the ≈ 1 mm of a native interposer-based
    design [\[20\]](#page-13-1). Tmem depends instead on the endpoint of the receiving
    D2D link, typically the controlled system, and is not affected by the packaging
    choice. In this implementation, both chips share the same SPM-based memory system
    with single-cycle access interconnect, bounding Tmem to 1–2 clock cycles. An additional
    limitation arises from the use of a mature technology node, which restricts the
    minimum operating voltage and keeps the energy-per-bit higher than what advanced
    nodes could achieve. This partially obscures the performance benefits of our design,
    particularly the near-lossless interface compared to the monolithic AXI4 counterpart.
    Despite these limitations, the main D2D link metrics within the chip boundaries
    (e.g., power, interface energy, and area) remain valid under our simpler off-chip
    packaging. We detail this evaluation in Section [III-D.](#page-9-0)


    *D2D Link Scalability and Multi-chiplet Support:* This work evaluates the D2D
    interface using a two-chiplet setup, sufficient to characterize the source-synchronous
    D2D module in latency, energy, and throughput. Nonetheless, broader scalability
    and multi-chiplet integration are also relevant, especially for control-centric
    applications. Two integration models arise in such systems. In a centralized setup,
    a manager controller chiplet connects to multiple controlled chiplets with distributed
    sensors/actuators, forming a hub-and-spoke topology. The main controller instantiates
    multiple AXI4 manager ports and D2D links. ControlPULPlet''s RT-DMA and AXI pipelining
    allow issuing multiple outstanding transactions in parallel. Since this topology
    increases area overhead, it underscores the importance of a compact D2D bridge.
    In the distributed model, each chiplet combines control and sensing/actuation
    logic, coordinating via D2D links and shared memory (e.g., mailbox or local SPM).
    The number of D2D ports per chiplet defines the topology, from ring (two) to mesh
    (four). For control use cases, ring-based topologies are favored due to minimal
    controller interference.


    *Adaptability to Advanced Packaging Technologies:* Our D2D interface is agnostic
    to the underlying packaging technology and requires only a routed signal path
    with controlled impedance and bounded clock skew, constraints that are met by
    several advanced packaging platforms. In particular, EMIB and Si-IF are well-matched
    to our D2D design: EMIB provides short silicon bridges with low trace length and
    skew, ideal for moderate-bandwidth, source-synchronous links; Si-IF offers ultra-dense,
    low-latency connections, making it especially suitable for tightly coupled chiplets.
    CoWoS, while offering higher bandwidth and full passive interposers, may exceed
    the form factor and cost requirements typical of embedded control systems. InFO
    technologies support compact, low-profile integration for consumer or IoT devices,
    and our D2D link can be conservatively configured (e.g., fewer channels or reduced
    frequency) to remain within its timing margins. Overall, the


    <span id="page-9-1"></span>![](_page_9_Figure_0.jpeg)


    Fig. 13: Impact of D2D link at varying channels and flow control FIFO depth on
    ControlPULPlet area. LN is fixed at 8 b.


    <span id="page-9-2"></span>![](_page_9_Figure_2.jpeg)


    Fig. 14: Kairos area breakdown. Rest denotes the interconnect system, its adapters
    and other logic.


    <span id="page-9-3"></span>![](_page_9_Figure_4.jpeg)


    Fig. 15: D2D link area breakdown in ControlPULPlet (CH = 8, CRD = 128), and the
    Kairos chip demonstrator (CH = 1, CRD = 8).


    <span id="page-9-4"></span>![](_page_9_Figure_6.jpeg)


    Fig. 16: Kairos power at maximum frequency and varying Vdd,core for some relevant
    benchmarks.


    electrical assumptions of our link are compatible with the wiring models of these
    platforms, enabling straightforward integration across a range of packaging options.


    ## <span id="page-9-0"></span>*D. Silicon Performance*


    We first explore the absolute and relative area impact of the D2D in ControlPULPlet,
    including the manager and PMCA domains, under various parametrizations using post-synthesis
    data. Next, we examine the fabricated chip demonstrator configuration (manager
    domain only), detailing the overall area and energy figures of its key components.


    *Area breakdown:* Figure [13](#page-9-1) illustrates the impact of progressively
    increasing the D2D link''s CH (from 1 to 8) and CRD (from 8 to 128) parameters
    on ControlPULPlet''s area in GE. A GE represents the area of a two-input, minimumstrength
    NAND gate. The manager domain SPMs and PMCA dominate the area cost, accounting
    for nearly 60 % of the system. The link area overhead at fixed flow control credits
    — dashed red lines referring to the right y-axis — increases approximately linearly,
    which is expected due to a linear increase in the number of channels. We observe
    that the most performant configuration in terms of attainable bus utilization
    (i.e., CH = 8 and CRD = 128, Section [III-A\)](#page-6-0) is lightweight, incurring
    just 2.9 % area overhead in-system.


    Fig. [11](#page-8-0) and Fig. [14](#page-9-2) highlight the approximate location
    of Kairos main components and their relative size in the fabricated chip, respectively.
    The IO pads are on the sides of the chip due to the QFN package. The memory system
    still dominates the overall area, while the minimal configuration of the link
    (CH = 1, LN = 8) incurs 1 % area overhead. The RT mid-end integrated into both
    system and IO DMA engines adds a minimal area overhead of 2 kGE compared to a
    vanilla DMA, primarily due to the additional configuration registers and period
    counters. Similarly, the fast interrupt handling features impact just 10 % on
    the area of theCV32RT core.


    Figure [15](#page-9-3) presents a detailed area breakdown of the D2D link components
    in ControlPULPlet (top) and the Kairos demonstrator (bottom). In the former, the
    control flow FIFO occupies more than 60 % of the D2D link area. As the number
    of channels increases, the PHY''s area contribution starts dominating the D2D
    link. The size of a single PHY is independent of the flow control FIFO and is
    particularly lightweight at only 7.6 kGE per channel.


    ## *Power Consumption:*


    *a) System Level:* To assess Kairos'' power consumption, we select two representative
    scenarios with varying computational and memory demands (Fig. [16\)](#page-9-4).
    The conv\_128 benchmark performs a 32 b integer convolution on a 128×128 matrix
    using a 5×5 filter. We evaluate a vanilla version (conv\_128V) and an optimized
    one with SIMD, postincrement load/store, and loop unrolling (conv\_128O). The
    mpc\_P3×3\_H2 is that described in Section [III-B.](#page-6-1) Figure [16](#page-9-4)
    shows the power consumption (left axis, in black) of the chip at various core
    voltages Vdd,core around the nominal Vdd,core=1.2 V. For each operating voltage,
    we find and report the maximum operating frequency (right axis in red). At the
    nominal 1.2 V, Kairos can achieve frequencies up to 290 MHz, while maintaining
    a power envelope under 30 mW. A crosscomparison with SoA is provided in the following
    Section [IV.](#page-10-0) The D2D link accounts for only 1.2 % of total power
    in siliconcalibrated post-layout simulations using memory-bound synthetic tests.


    *b) D2D Link:* The energy consumption per transferred bit of the D2D interface
    is affected by two components: one on-chip, due to the D2D microarchitecture,
    and one off-chip, due to PCB trace length and IO voltage. With D2D interface,
    we intend the whole network, data link, and PHY modules.


    <span id="page-10-1"></span>![](_page_10_Figure_1.jpeg)


    Fig. 17: D2D energy efficiency in Kairos at different Vdd,IO and DMA burst lengths,
    measured using the standalone board.


    We evaluate the first component with the testbench framework shown in Fig. [8,](#page-6-2)
    which provides fine-grained internal energy estimates within the chip, isolating
    the D2D link from the rest of the SoC through silicon-calibrated post-layout simulations
    of ControlPULPlet. The second component is assessed with the manufactured board,
    as this setup accounts for the impact of I/O voltage and PCB track length between
    the two chip instances. In both cases, we use DMA-operated transfers to maximize
    bus utilization, as in Section [III-A.](#page-6-0) Siliconcalibrated post-layout
    simulations of the chip yield a total internal energy-per-bit of 1.3 pJ/b at nominal
    Vdd,core=1.2 V, including network, data link, and physical layers. The PHY accounts
    for roughly half of this (0.62 pJ/b), with the remainder spent on AXI4-to-AXIS
    conversion and flow control logic. Figure [17](#page-10-1) reports the energy-per-bit
    across three Vdd,IO values, from the minimum functional 1.5 V to the nominal 2.5
    V, at varying DMA burst sizes. The energy is minimized at 16 B, matching the data
    transferred per cycle in DDR mode (CH = 1, LN = 8). Transfers of 4 B are inefficient,
    as they use only half of the DMA bus width (8 B). For burst sizes from 32 B to
    2 KiB, corresponding to 256 beats and the maximum supported by AXI4, energy increases
    due to burst fragmentation and flow control FIFO limitations. At Vdd,core=1.5
    V and recalling the average reach of 70.5 mm, the minimum normalized energy is
    18.23 pJ/b, highlighting the penalty introduced by I/O voltage and PCB traces
    compared to internal D2D energy. A 2.5D integration would mitigate this penalty,
    as interposer tracks are almost two orders of magnitude shorter and offer better
    electrical properties than standard FR4 PCB traces.


    ## IV. RELATED WORK


    <span id="page-10-0"></span>In the following, Section [IV-A](#page-10-2) discusses
    controllers with different acceleration and real-time features, with a focus on
    their D2D interface, when available. Section [IV-B](#page-11-0) then examines
    standalone D2D interfaces proposed by prior works.


    ## <span id="page-10-2"></span>*A. Programmable Hardware Controllers*


    We focus on *programmable* controllers, which address the limited flexibility
    of fixed-function controllers [\[2\]](#page-12-1), [\[31\]](#page-13-2). Programmable
    controllers are classified as (i) single-core programmable, (ii) multi-core programmable,
    and (iii) heterogeneous multi-core controllers with DSAs. Table [I](#page-11-1)
    highlights the relevant SoA in this category. The table groups controllers by
    D2D interface support.


    *a) Single-core Programmable Controllers:* Singlecore programmable controllers
    like STMicroelectronics'' 32 b STM32 series (Fig. [2a](#page-1-0)) support real-time
    applications with accelerated context switching, consuming less than 150 mW at
    500 MHz [\[21\]](#page-13-3). Similarly, AURIX TriCore TC2xx series provides hardware-assisted
    context switchingwith 10-16 clock cycles interrupt response time and 156-162 clock
    cycles context switching [\[22\]](#page-13-4), consuming 300 mW at just 130 MHz.Our
    design outperforms TriCore in interrupt latency and context switching (both 1.6×
    faster) with similar power consumption.


    On-chip single-core controllers for CPU DVFS and SoC management include Intel''s
    power control unit (PCU) [\[23\]](#page-13-5), IBM''s on-chip controller (OCC)
    [\[24\]](#page-13-6), Arm''s system control processor (SCP) [\[5\]](#page-12-4),
    and AMD''s system management unit (SMU) [\[17\]](#page-12-15), all based on 32
    b commercial processors. These controllers implement simple PID control policies
    with control periods of 0.25 ms to 1 ms, aligning favorably with ControlPULP''s
    capabilities. The SMU uniquely integrates a chiplet-based D2D interface via AMD''s
    Infinity Fabric on Package (IFOP). IFOP comprises a scalable data and control
    fabric (SDF and SCF, respectively). Online CPU DVFSis managed through the SCP
    with a 1 ms-period PID policy, facilitating communication from the SMU, located
    within the IO die (IOD) chiplet, to the compute chiplets housing highperformance
    Zen cores. In AMD''s Rome [\[28\]](#page-13-7), IFOP achieves 2 pJ/b and 440 GB/s
    at 1.2 V on a 12 nm passive interposer, though control and data planes likely
    have different bandwidth needs. A similar chiplet-compatible D2D demonstrator
    using an OpenRISC microcontroller in 16 nm achieves 2 Gb/s per pin with 1.5 pJ/b
    at 1.2 V [\[29\]](#page-13-8). While these controllers prioritize deterministic
    execution, fast context switching, and efficient D2D control interfaces, they
    lack the computational power required by advanced control algorithms.


    *b) Multi-core Controllers:* Multi-core controllers address this computational
    gap. The six-core AURIX TC29x [\[25\]](#page-13-9) and ASPEED''s AST2600, a common
    Baseboard Management Controller [\[26\]](#page-13-10), demonstrate increased processing
    capability, reflected in their high power consumption, between 1.5W to 2W. However,
    general-purpose cores result in high silicon cost and lower energy efficiency
    for specialized workloads.


    *c) Heterogeneous Multi-core Controllers:* To improve efficiency, heterogeneous
    multi-core controllers integrate DSAs. ControlPULP combines a 32 b embedded core
    with an 8-core tightly-coupled PMCA, balancing flexibility, performance, and efficiency
    for real-time control applications [\[3\]](#page-12-2). Similarly, AURIX TC4x
    series integrates a 6-core high-end controller with diverse accelerators for AI-based
    model predictive and model adaptive control [\[27\]](#page-13-11). Register banking
    enables fast interrupt response clock cycles and context switching, key properties
    retained from its predecessors. Increased acceleration and performance incurs
    additional power consumption costs than the low-end TC2x series, about 2.5W. Most
    multicore and heterogeneous controllers are available as standalone chips or soft
    intellectual property (IP) blocks for on-chip integration, but few support chiplet-compatible
    D2D interfaces. A notable exception is the 96-core demonstrator by Vivet et al.,
    which integrates a power management controller including switched-capacitor voltage
    regulators (SCVRs), one for each


    <span id="page-11-1"></span>


    | TABLE I: Comparison of programmable controller classes. We highlight several
    metrics of the controller. The autonomous DMA is not highlighted, as it is |  |

    |---------------------------------------------------------------------------------------------------------------------------------------------------------|--|

    | unique to our design.                                                                                                                                   |  |


    |                           |                 | Applic.            |                                  |   |                                 |        |                               |                                                                           |
    D2D           |                 |      |                                      |               |                            |

    |---------------------------|-----------------|--------------------|----------------------------------|---|---------------------------------|--------|-------------------------------|---------------------------------------------------------------------------|---------------|-----------------|------|--------------------------------------|---------------|----------------------------|

    | Controller                | Class           |                    | DSA<br>D2D<br>Integr.<br>Interf.
    |   | Fast<br>Tech<br>Interr.<br>[nm] |        | Max. Freq.<br>[MHz]           |
    Peak Power<br>[mW]                                                        | Prot.         |                 |      |
    #Links #Wires/Link Peak BW<br>[Gb/s] | Reach<br>[mm] | En. eff.<br>[pJ/b]         |

    |                           |                 |                    |                                  |   |                                 |        |                               |
    Controllers for 2D integration with simple on-chip or off-chip interfaces |               |                 |      |                                      |               |                            |

    | STM32 [21]                | 1-core          | IoT                | ✓                                |
    ✗ | ✗                               | 28     | 24-480                        |
    66-132 @3.3 V                                                             | -             |
    -               | -    | -                                    | -             |
    -                          |

    | TC21XL [22]               |                 | 1-core automotive  | ✓                                |
    ✗ | ✗                               | 65     | 133                           |
    290 @3.3 V                                                                | -             |
    -               | -    | -                                    | -             |
    -                          |

    | PCU [23]                  | 1-core,<br>FSMs | CPU<br>DVFS        | ✗                                |
    ✗ | n.a.                            | a<br>- | n.a.                          |
    n.a.                                                                      | -             |
    -               | -    | -                                    | -             |
    -                          |

    | OCC [24]                  | 1-core          | CPU<br>DVFS        | ✗                                |
    ✗ | ✗                               | a<br>- | <266-658                      |
    n.a.                                                                      | -             |
    -               | -    | -                                    | -             |
    -                          |

    | SCP [5]                   | 1-core          | CPU<br>DVFS        | ✗                                |
    ✗ | ✓                               | a<br>- | <400                          |
    n.a.                                                                      | -             |
    -               | -    | -                                    | -             |
    -                          |

    | TC29x [25]                |                 | 3-cores automotive | ✓                                |
    ✗ | ✓                               | -      | <400                          |
    1697 @3.3 V                                                               | -             |
    -               | -    | -                                    | -             |
    -                          |

    | AST2600 [26]              | 2-cores         | server<br>mngmnt   | ✗                                |
    ✗ | ✗                               | 28     | 1000, Cort. A                 |
    200, Cort. M > 2000 @1.8-3.3 V                                            | -             |
    -               | -    | -                                    | -             |
    -                          |

    | TC4x [27]                 | 6-cores         | DSAs automotive    | ✓                                |
    ✗ | ✓                               | 28     | 500                           |
    2240-2501 @3.3 V                                                          | -             |
    -               | -    | -                                    | -             |
    -                          |

    | ControlPULP [3]           | 1-core          | PMCA automotive    | ✓                                |
    ✗ | ✓                               | d22    | d500                          |
    n.a.                                                                      | -             |
    -               | -    | -                                    | -             |
    -                          |

    |                           |                 |                    |                                  |   |                                 |        |                               |
    Controllers for 2.5D integration with die-to-die interfaces               |               |                 |      |                                      |               |                            |

    | AMD SMU [17], [28] 1-core |                 | CPU<br>DVFS        | ✗                                |
    ✓ | ✗                               | b12    | n.a.                          |
    n.a.                                                                      | IFOP          |
    c8              | n.a. | c3520<br>@2 GHz                      | n.a.          |
    c2<br>@1.2 V               |

    | Liu et al. [29]           | 1-core          | n.a.               | ✗                                |
    ✓ | ✗                               |        | 16 500, OpenRISC<br>1000, D2D |
    n.a.                                                                      | AIB-like      |
    3               | 96   | 2 (per pin)<br>@1 GHz                | <2            |
    1.48<br>@1.2 V             |

    | Vivet et al. [30]         | n.a.            | CPU<br>DVFS        | n.a.                             |
    ✓ | n.a.                            | n.a.   | 1200                          |
    e1160                                                                     | 3D
    Plug 6+8+1 |                 | 12   | 3152<br>@1.1 GHz 1.5-1.8             |               |
    0.75<br>@1.2 V             |

    | ControlPULPlet<br>(ours)  | 1-core<br>PMCA  | n.a.               | ✓                                |
    ✓ | ✓                               | 65     | f290                          |
    f 30 / g210                                                               |               |
    custom f 1 – h8 | 16+2 | h>51<br>@200 MHz <70.5               |               |
    i 1.3 – f 11.7<br>l @1.2 V |


    <sup>a</sup> soft IP not tied to standalone chip <sup>b</sup> IO die (IOD) <sup>c</sup>
    from AMD Rome; we report Infinity Fabric numbers, exact SCF values are unavailable
    <sup>d</sup> soft IP not tied to standalone chip, reported post-synthesis estimates
    <sup>e</sup> one SCVR <sup>f</sup> Kairos demonstrator @Vdd,core=1.2 V, Vdd,IO=1.5
    V <sup>g</sup> considering both manager and PMCA domains <sup>h</sup> peak with
    CH=8, CRD=128, 90 % of the on-chip control interface requirements; optimizable
    at additional area cost <sup>i</sup> internal energy estimated with silicon-calibrated
    post-layout simulation <sup>l</sup> scaled from Vdd,IO=1.5 V (measured on Kairos
    demonstrator)


    <span id="page-11-2"></span>TABLE II: Comparison of standalone D2D Interfaces.
    We highlight several metrics of each interface. A and D refer to analog and digital,
    respectively.


    | D2D Interface              | Tech<br>[nm]  | On-chip<br>Protocol    |     |
    Design Reach<br>[mm] | Peak BW<br>[Gb/s]  | Area<br>[kGE]                 | Energy<br>[pJ/b]          |
    #Wires<br>/Link |

    |----------------------------|---------------|------------------------|-----|----------------------|--------------------|-------------------------------|---------------------------|-----------------|

    | Kuttappa et al. [32]       | 16            | aAXI (36 b)            | A/D |
    1.2                  | b18 @1 GHz         | n.a.                          | c0.85                     |
    18+8            |

    | Melek et al. (UCIe) [33]   | 3             | n.a.                   | A/D |
    1.4                  | 2048 @8 GHz        | n.a.                          | 0.29
    @0.45 V              | 64+4            |

    | BoW [20]                   | 65-5          | PCIe, MAC<br>PIPE/LPIF | D   |
    10                   | d5-8 @2.5-4<br>GHz | n.a.                          | <1
    @0.7-0.9 V             | 16+2            |

    | Lin et al. (LIPINCON) [34] | 7             | Mesh (1968 b)          | A/D |
    0.5                  | 2560 @4GHz         | e18420                        | 0.56
    @0.3 V               | 40+6            |

    | GRS [35]                   | 16            | n.a.                   | A/D |
    80                   | d25 @12.5GHz       | 589.9                         | 1.17
    @0.3 V               | 8+1             |

    | Li et al. [36]             | Kintek-7 FPGA | n.a. (64 b)            | D   |
    n.a.                 | 8.3 @250 MHz       | n.a.                          | n.a.                      |
    32+1+1          |

    | Ours                       | 65            | AXI4 (64b)             | D   |
    f70.5                | 51 @200 MHz        | g28.2/h270.2<br>(7.6 one PHY) | 11.7
    (1.3 int.)<br>@1.2 V | 16+2            |


    <sup>a</sup> The exact AXI protocol (AXI3 or AXI4) not specified <sup>b</sup>
    Aggregate BW of one cluster only <sup>c</sup> Operational voltage is not specified


    <sup>d</sup> Per pin <sup>e</sup> one channel (implemented 2 channels per chiplet)
    <sup>f</sup> Reach based on the average trace length of the PCB shown in Fig.
    [12;](#page-8-1) other works refer to interposer-based reach <sup>g</sup> Network,
    data, and PHY for the Kairos config. CH = 1, LN = 8 <sup>h</sup> Network, data,
    and PHY for the peak BW config. CH = 8, LN = 8


    compute chiplet, on an active silicon interposer [\[30\]](#page-13-12).


    # <span id="page-11-0"></span>*B. D2D Interfaces*


    Table [II](#page-11-2) compares industrial and academic D2D protocols. While energy-per-bit
    is difficult to compare due to differing technology nodes and voltages, peak bandwidth
    can be scaled assuming higher frequencies, though still bounded by the frontend
    bus width. Area is reported in gate equivalents, normalized to a minimum-strength
    NAND gate, making it technology-agnostic. UCIe [\[33\]](#page-13-14), BoW [\[20\]](#page-13-1),
    and LIPINCON over CoWoS [\[34\]](#page-13-15) support widely used frontend standards,
    such as PCIe and CXL. The standalone D2D interfaces and protocols surveyed in
    Table II are specifically optimized for high-throughput communication across the
    data plane of the chiplet, driving the design toward high-frequency operation
    (in the multi-GHz range) and the scaling out of individual link instances. Table
    [II](#page-11-2) presents the number of wires per link as a normalized figure
    of merit for this purpose. For example, GRS [\[35\]](#page-13-16) achieves 25
    Gb/s per pin over an 80 mm reach on a silicon interposer, operating at a high
    frequency of 12.5 GHz. Similarly, LIPINCON [\[34\]](#page-13-15) integrates 1968
    parallel bits, while in [\[33\]](#page-13-14), Cadence proposes a UCIe-compatible
    D2D interface with 64 parallel data wires across 8 links delivering 2048 Gb/s
    at 8 GHz. These implementations contrast with the 64 b AXI4 frontend used in this
    work for the control plane interface, where high bandwidth and speed are not primary
    requirements. However, because of its flexibility, our D2D design can be readily
    scaled to meet the demands of a wider data bus for data-intensive applications.
    Similar to our work, Intel''s design in [\[32\]](#page-13-13) features a D2D interface
    with an AXI frontend, and a 36 b data bus. The mixed analog/digital PHY is integrated
    into a 14-cluster system on chip (SoC) for media and ML applications, using 9
    data wires per cluster to deliver 18 Gb/s at 1 GHz per cluster. Though currently
    dataplane-focused, these high-bandwidth links enable future D2D communication
    in complex distributed control systems, making them increasingly relevant for
    control-plane use. In academia, Li et al. [\[36\]](#page-13-17) propose a custom,
    parallel D2D interface achieving 8.3 Gb/s at 250 MHz. The microarchitecture only
    supports a fixed 32 parallel data lanes with a 64b buffer FIFO used for protocol
    conversion. Moreover, the D2D physical implementation assessment is limited to
    FPGA mapping without silicon prototyping, which prevents accurate power and area
    estimates.


    Our D2D design stands out for its lightweight area overhead, even when configured
    for peak throughput (CH = 8, LN = 8 for a data bus of 64 b). As shown in Table
    II, it achieves a smaller die size than PHY-only reported implementations in more
    advanced technology nodes, such as [\[35\]](#page-13-16) (2.2×) and [\[34\]](#page-13-15)
    (136×), while also incorporating the data link and network layers. Furthermore,
    the link achieves higher throughput compared to designs with similar microarchitecture
    and operating frequency, e.g., [\[36\]](#page-13-17), which reports 6× less bandwidth
    while doubling the number of wires per link. At iso-frequency, our interface achieves
    superior peak bandwidth, e.g., outperforming the recent [\[32\]](#page-13-13)
    by 14×, thanks to the high bus utilization enabled by the D2D design, despite
    other works using more advanced technology nodes. The energy efficiency of our
    silicon prototype is negatively affected by the flat QFN package and the lack
    of an interposer as substrate, leading to PCB-induced overhead (PCB track parasitic).
    However, cycle-accurate simulations provide high observability of the inner D2D
    circuitry, enabling silicon-calibrated measurement of its internal energy (1.3
    pJ/b), a lower bound independent from the length and parasitics of the off-die
    wires. The inner D2D energy is comparable with leading academic and industry designs
    at the same voltage, e.g., [\[29\]](#page-13-8) (1.48 pJ/b) and the AMD SMU [\[17\]](#page-12-15)
    (2 pJ/b). Moreover, since energy-per-bit scales with the square of the operating
    voltage, a more advanced technology node enabling lower voltages could reduce
    our energy-per-bit below 1 pJ/b, proving it competitive with the surveyed works.


    ## V. CONCLUSION


    To meet the demands of real-time control algorithms with increasing performance
    requirements and the trend toward SiP and chiplet integration technology, we introduce
    ControlPULPlet, an open-source , real-time, heterogeneous multicore RISC-V controller.
    It features a 32 b CV32RT core for fast and deterministic interrupt management,
    a specialized DMA for real-time data transfer, and a tightly-coupled PMCA for
    compute-intensive control algorithm acceleration. A flexible AXI4-compatible D2D
    link supports both inter-chiplet and on-chip communication. Operating at 1.2 V,
    Kairos, ControlPULPlet''s silicon demonstrator in 65nm CMOS, peaks at 290 MHz
    within 30 mW power envelope during intensive control workloads. The D2D link enables
    off-die access at 11.7 pJ/b at 1.2 V IO voltage, with only 1.3 pJ/b internal energy,
    attaining duplex peak transfer rates of 51 Gb/s at 200 MHz, 2.9 % and 1.2 % area
    and power penalties, and minimal performance degradation on periodic control policies
    compared to its on-chip counterpart.


    ## REFERENCES


    - <span id="page-12-0"></span>[1] F. Rehm, J. Seitter, J.-P. Larsson, S. Saidi,
    G. Stea, R. Zippo, D. Ziegenbein, M. Andreozzi, and A. Hamann, "The road towards
    predictable automotive high - performance platforms," in 2021 Des., Automat. &
    Test in Eur. Conf. & Exhib. (DATE), 2021, pp. 1915–1924.

    - <span id="page-12-1"></span>[2] H. Ren, K. Sahoo, Z. Guo, R. Pugazhendhi, Z.
    Wong, T. Xiang, T. S. Fisher, and S. S. Iyer, "Heterogeneous power delivery for
    large chipletbased systems using integrated gan/si-interconnect fabric with sub-10
    um bond pitch," in 2023 Int. Electron Devices Meeting (IEDM), 2023, pp. 1–4.

    - <span id="page-12-2"></span>[3] A. Ottaviano, R. Balas, G. Bambini, A. D. Vecchio,
    M. Ciani, D. Rossi, L. Benini, and A. Bartolini, "Controlpulp: A risc-v on-chip
    parallel power controller for many-core hpc processors with fpga-based hardware-in-the-loop
    power and thermal emulation," Int. J. of Parallel Program., vol. 52, no. 1, pp.
    93–123, apr 2024.

    - <span id="page-12-3"></span>[4] X. Li, L. Chen, S. Chen, F. Jiang, C. Li, W.
    Zhang, and J. Xu, "Deep reinforcement learning-based power management for chipletbased
    multicore systems," IEEE Trans. on Very Large Scale Integration (VLSI) Systems,
    vol. 32, no. 9, pp. 1726–1739, 2024.

    - <span id="page-12-4"></span>[5] Arm, "Power control system architecture," [https://developer.arm.com/](https://developer.arm.com/documentation/den0050/d/?lang=en)
    [documentation/den0050/d/?lang=en,](https://developer.arm.com/documentation/den0050/d/?lang=en)
    2023, accessed: 2025-02-05.

    - <span id="page-12-5"></span>[6] A. Esper, G. Nelissen, V. Nelis, and E. Tovar,
    "An industrial view on the common academic understanding of mixed-criticality
    systems," Real-Time Syst., vol. 54, 07 2018.

    - <span id="page-12-6"></span>[7] D. Mishra, V. Arora, L. Nguyen, S. Iriguchi,
    H. Sada, L. Clemente, S. Lim, H. Lin, A. Lohia, S. Gurrum, J. Sauser, and S. Spencer,
    "Packaging innovations for high voltage (hv) gan technology," in 2017 IEEE 67th
    Electron. Compon. and Technol. Conf. (ECTC), 2017, pp. 1480–1484.

    - <span id="page-12-7"></span>[8] S. Mirabbasi, L. C. Fujino, and K. C. Smith,
    "Through the looking glass—the 2023 edition: Trends in solid-state circuits from
    isscc," IEEE Solid-State Circ. Mag., vol. 15, no. 1, pp. 45–62, 2023.

    - <span id="page-12-8"></span>[9] D. D. Sharma, G. Pasdast, S. Tiagaraj, and K.
    Aygun, "High- ¨ performance, power-efficient three-dimensional system-in-package
    designs with universal chiplet interconnect express," Nature Electronics, vol.
    7, no. 3, pp. 244–254, 2024.

    - <span id="page-12-9"></span>[10] P. Iff, M. Besta, M. Cavalcante, T. Fischer,
    L. Benini, and T. Hoefler, "Hexamesh: Scaling to hundreds of chiplets with an
    optimized chiplet arrangement," in 2023 60th ACM/IEEE Des. Automat. Conf. (DAC),
    2023, pp. 1–6.

    - <span id="page-12-10"></span>[11] S. Naffziger, N. Beck, T. Burd, K. Lepak,
    G. H. Loh, M. Subramony, and S. White, "Pioneering chiplet technology and design
    for the amd epyc™ and ryzen™ processor families : Industrial product," in 2021
    ACM/IEEE 48th Annu. Int. Symp. on Comput. Architecture (ISCA), 2021, pp. 57–70.

    - [12] G. H. Loh, S. Naffziger, and K. Lepak, "Understanding chiplets today to
    anticipate future integration opportunities and limits," in 2021 Des., Automat.
    & Test in Europe Conf. & Exhib. (DATE), 2021, pp. 142–145.

    - <span id="page-12-11"></span>[13] A. Kannan, N. E. Jerger, and G. H. Loh, "Enabling
    interposer-based disintegration of multi-core processors," in 2015 48th Annu.
    IEEE/ACM Int. Symp. on Microarchitecture (MICRO), 2015, pp. 546–558.

    - <span id="page-12-12"></span>[14] R. Balas, A. Ottaviano, and L. Benini, "Cv32rt:
    Enabling fast interrupt and context switching for risc-v microcontrollers," IEEE
    Trans. on Very Large Scale Integration (VLSI) Systems, vol. 32, no. 6, pp. 1032–1044,
    2024.

    - <span id="page-12-13"></span>[15] T. Benz, M. Rogenmoser, P. Scheffler, S. Riedel,
    A. Ottaviano, A. Kurth, T. Hoefler, and L. Benini, "A high-performance, energy-efficient
    modular dma engine architecture," IEEE Trans. on Comput., vol. 73, no. 1, pp.
    263–277, 2024.

    - <span id="page-12-14"></span>[16] Intel, "Configuring power management functionalities
    in a processor," U.S. Patent US8984313B2, March 2015.

    - <span id="page-12-15"></span>[17] T. Burd, N. Beck, S. White, M. Paraschou,
    N. Kalyanasundharam, G. Donley, A. Smith, L. Hewitt, and S. Naffziger, ""Zeppelin":
    An SoC for Multichip Architectures," IEEE J. of Solid-State Circuits, vol. 54,
    no. 1, pp. 133–143, 2019.

    - <span id="page-12-16"></span>[18] Arm, "AMBA AXI and ACE Protocol Specification
    AXI3, AXI4, and AXI4-Lite ACE and ACE-Lite," [https://developer.arm.com/](https://developer.arm.com/documentation/ihi0022/latest/)
    [documentation/ihi0022/latest/,](https://developer.arm.com/documentation/ihi0022/latest/)
    2023, version J.

    - <span id="page-13-0"></span>[19] G. Bambini, A. Ottaviano, C. Conficoni, A.
    Tilli, L. Benini, and A. Bartolini, "Modeling and controlling many-core hpc processors:
    an alternative to pid and moving average algorithms," ACM Trans. Auton. Adapt.
    Syst., Sep. 2024. [Online]. Available: [https:](https://doi.org/10.1145/3694687)
    [//doi.org/10.1145/3694687](https://doi.org/10.1145/3694687)

    - <span id="page-13-1"></span>[20] S. Ardalan, H. Cirit, R. Farjad, M. Kuemerle,
    K. Poulton, S. Subramanian, and B. Vinnakota, "Bunch of wires: An open die-to-die
    interface," in 2020 IEEE Symp. on High-Perform. Interconnects (HOTI), 2020, pp.
    9–16.

    - <span id="page-13-3"></span>[21] STMicroelectronics, STM32F042F6 Microcontroller,
    STMicroelectronics, 2024, accessed: 2025-02-05. [Online]. Available: [https:](https://www.st.com/en/microcontrollers-microprocessors/stm32f042f6.html)
    [//www.st.com/en/microcontrollers-microprocessors/stm32f042f6.html](https://www.st.com/en/microcontrollers-microprocessors/stm32f042f6.html)

    - <span id="page-13-4"></span>[22] Infineon, AURIX Family - TC21XL, Infineon,
    2024, accessed: 2025-02-05. [Online]. Available: [https://www.infineon.com/cms/en/product/microcontroller/32-bit](https://www.infineon.com/cms/en/product/microcontroller/32-bit-tricore-microcontroller/32-bit-tricore-aurix-tc2xx/)[tricore-microcontroller/32-bit-tricore-aurix-tc2xx/](https://www.infineon.com/cms/en/product/microcontroller/32-bit-tricore-microcontroller/32-bit-tricore-aurix-tc2xx/)

    - <span id="page-13-5"></span>[23] R. Schone, T. Ilsche, M. Bielert, A. Gocht,
    and D. Hackenberg, "Energy ¨ efficiency features of the intel skylake-sp processor
    and their impact on performance," in 2019 Int. Conf. on High Perform. Comput.
    & Simul. (HPCS), 2019, pp. 399–406.

    - <span id="page-13-6"></span>[24] T. Rosedahl, M. Broyles, C. Lefurgy, B. Christensen,
    and W. Feng, "Power/Performance Controlling Techniques in OpenPOWER," in High
    Perform. Comput., J. M. Kunkel, R. Yokota, M. Taufer, and J. Shalf, Eds. Springer
    Int. Publishing, 2017, pp. 275–289.

    - <span id="page-13-9"></span>[25] Infineon, AURIX Family - TC29x, Infineon, 2024,
    accessed: 2025-02-05. [Online]. Available: [https://www.infineon.com/cms/en/product/microcontroller/32-bit](https://www.infineon.com/cms/en/product/microcontroller/32-bit-tricore-microcontroller/32-bit-tricore-aurix-tc2xx/)[tricore-microcontroller/32-bit-tricore-aurix-tc2xx/](https://www.infineon.com/cms/en/product/microcontroller/32-bit-tricore-microcontroller/32-bit-tricore-aurix-tc2xx/)

    - <span id="page-13-10"></span>[26] ASPEED, AST2600, ASPEED, 2024, accessed: 2025-02-05.
    [Online]. Available: [https://www.aspeedtech.com/server](https://www.aspeedtech.com/server_ast2600/)
    ast2600/

    - <span id="page-13-11"></span>[27] Infineon, AURIX Family - TC4x, Infineon, 2024,
    accessed: 2025- 02-05. [Online]. Available: [https://www.infineon.com/cms/en/product/](https://www.infineon.com/cms/en/product/microcontroller/32-bit-tricore-microcontroller/32-bit-tricore-aurix-tc4x/)
    [microcontroller/32-bit-tricore-microcontroller/32-bit-tricore-aurix-tc4x/](https://www.infineon.com/cms/en/product/microcontroller/32-bit-tricore-microcontroller/32-bit-tricore-aurix-tc4x/)

    - <span id="page-13-7"></span>[28] S. Naffziger, K. Lepak, M. Paraschou, and M.
    Subramony, "2.2 amd chiplet architecture for high-performance server and desktop
    products," in 2020 IEEE Int. Solid-State Circuits Conf. - (ISSCC), 2020, pp. 44–45.

    - <span id="page-13-8"></span>[29] C. Liu, J. Botimer, and Z. Zhang, "A 256gb/s/mm-shoreline
    aibcompatible 16nm finfet cmos chiplet for 2.5d integration with stratix 10 fpga
    on emib and tiling on silicon interposer," in 2021 IEEE Custom Integr. Circuits
    Conf. (CICC), 2021, pp. 1–2.

    - <span id="page-13-12"></span>[30] P. Vivet, E. Guthmuller, Y. Thonnart, G. Pillonnet,
    C. Fuguet, I. Miro-Panades, G. Moritz, J. Durupt, C. Bernard, D. Varreau, J. Pontes,
    S. Thuries, D. Coriat, M. Harrand, D. Dutoit, D. Lattard, L. Arnaud, J. Charbonnier,
    P. Coudrain, A. Garnier, F. Berger, A. Gueugnot, A. Greiner, Q. L. Meunier, A.
    Farcy, A. Arriordaz, S. Cheramy, and ´ F. Clermidy, "Intact: A 96-core processor
    with six chiplets 3d-stacked on an active interposer with distributed interconnects
    and integrated power management," IEEE J. of Solid-State Circ., vol. 56, no. 1,
    pp. 79–97, 2021.

    - <span id="page-13-2"></span>[31] Y. K. Cherivirala and D. D. Wentzloff, "A capacitor-less
    digital ldo regulator with synthesizable pid controller achieving 99.75% efficiency
    and 93.3-ps response time in 65 nm," IEEE Trans. on Circ. and Sys. II: Express
    Briefs, vol. 70, no. 5, pp. 1769–1773, 2023.

    - <span id="page-13-13"></span>[32] R. Kuttappa, J. Sundaram, S. R. Srinivasa,
    P. Aseron, G. Murali, V. Honkote, P. Budhkar, D. Kurian, R. Kalim, T. P. Thomas,
    A. Srinivasan, and T. Karnik, "A high-performance passive base system for distributed
    ai/media acceleration," in 2025 IEEE Custom Integr. Circuits Conf. (CICC), 2025,
    pp. 1–6.

    - <span id="page-13-14"></span>[33] D. T. Melek, R. Navinkumar, J. Vandersand,
    P. Sarkar, B. Prakash, A. Leuciuc, K. Geary, S. Ma, C. M. Mehta, S. Jain, B. Bothra,
    P. Sabharwal, R. Vaish, K. Bhanushali, Y. Ding, C. Frost, J. Annunziata, K. Sadhu,
    D. Kyritsis, J. Bostak, M. Li, S. Williams, and K. Chang, "A 0.29pj/b 5.27tb/s/mm
    ucie advanced package link in 3nm finfet with 2.5d cowos packaging," in 2025 IEEE
    Int. Solid-State Circuits Conf. - (ISSCC), vol. 68, 2025, pp. 590–592.

    - <span id="page-13-15"></span>[34] M.-S. Lin, T.-C. Huang, C.-C. Tsai, K.-H.
    Tam, K. C.-H. Hsieh, C.- F. Chen, W.-H. Huang, C.-W. Hu, Y.-C. Chen, S. K. Goel,
    C.-M. Fu, S. Rusu, C.-C. Li, S.-Y. Yang, M. Wong, S.-C. Yang, and F. Lee, "A 7
    nm 4-ghz arm¹-core-based cowos¹ chiplet design for high-performance computing,"
    IEEE J. of Solid-State Circuits, vol. 55, no. 4, pp. 956–966, 2020.

    - <span id="page-13-16"></span>[35] J. W. Poulton, J. M. Wilson, W. J. Turner,
    B. Zimmer, X. Chen, S. S. Kudva, S. Song, S. G. Tell, N. Nedovic, W. Zhao, S.
    R. Sudhakaran, C. T. Gray, and W. J. Dally, "A 1.17-pj/b, 25-gb/s/pin ground-referenced
    single-ended serial link for off- and on-package communication using a process-
    and temperature-adaptive voltage regulator," IEEE J. of Solid-State Circuits,
    vol. 54, no. 1, pp. 43–54, 2019.


    <span id="page-13-17"></span>[36] S. Li, Y. Wang, C. Feng, and H. Wang, "Hardware
    architecture of the universal high-speed interconnection chiplet for chiplet integration,"
    in 2024 9th Int. Conf. on Comput. and Commun. Syst. (ICCCS), 2024, pp. 1083–1087.


    ![](_page_13_Picture_18.jpeg)


    Alessandro Ottaviano (Graduate Student Member, IEEE) received the B.Sc. in Physical
    Engineering from Politecnico di Torino, Italy, and the M.Sc. in Electrical Engineering
    as a joint degree between Politecnico di Torino, Grenoble INP-Phelma and EPFL
    Lausanne, in 2018 and 2020 respectively. He is currently pursuing a Ph.D. degree
    in the Digital Circuits and Systems group of Prof. Benini. His research interests
    include real-time and predictable computing systems and energy-efficient processor
    architecture.


    ![](_page_13_Picture_20.jpeg)


    Robert Balas (Graduate Student Member, IEEE) received his B.Sc. and M.Sc. degrees
    in electrical engineering and information technology from ETH Zurich in 2015 and
    2017, respectively. He is currently pursuing a Ph.D. degree in the Digital Circuits
    and Systems group of Prof. Benini. His research interests include real-time computing,
    compilers, and operating-systems.


    ![](_page_13_Picture_22.jpeg)


    Tim Fischer received his BSc and MSc in "Electrical Engineering and Information
    Technology" from the Swiss Federal Institute of Technology Zurich (ETHZ), Switzerland,
    in 2018 and 2021, respectively. He is currently pursuing a Ph.D. degree at ETH
    Zurich in the Digital Circuits and Systems group led by Prof. Luca Benini. His
    research interests include scalable and energy-efficient interconnects for both
    on-chip and off-chip communication.


    ![](_page_13_Picture_24.jpeg)


    Thomas Benz (Graduate Student Member, IEEE) received his B.Sc. and M.Sc. degrees
    in electrical engineering and information technology from ETH Zurich in 2018 and
    2020, respectively. He is currently pursuing a Ph.D. degree in the Digital Circuits
    and Systems group of Prof. Benini. His research interests include energy-efficient
    high-performance computer architectures, memory interconnects, data movement,
    and the design of ASICs.


    ![](_page_13_Picture_26.jpeg)


    Andrea Bartolini (Member, IEEE) received the Ph.D. degree from the University
    of Bologna, Bologna, Italy, in 2013. He is an Associate Professor with the Department
    of Electrical, Electronic and Information Engineering Guglielmo Marconi, University
    of Bologna, Bologna, Italy. He has published more than 120 papers in peer-reviewed
    international journals and conferences and several book chapters with focus on
    dynamic resource management, ranging from embedded to large-scale HPC systems.


    ![](_page_13_Picture_28.jpeg)


    Luca Benini (Fellow, IEEE) holds the chair of digital Circuits and systems at
    ETHZ and is Full Professor at the Universita di Bologna. He received ` a PhD from
    Stanford University. His research interests are energy-efficient parallel computing
    systems, smart sensing micro-systems, and machine learning hardware. He is a Fellow
    of the IEEE, of the ACM, a member of the Academia Europaea, and of the Italian
    Academy of Engineering and Technology.'
- title: "Pointer: An Energy-Efficient ReRAM-based Point Cloud Recognition\n  Accelerator\
    \ with Inter-layer and Intra-layer Optimizations"
  abstract: "Point cloud is an important data structure for a wide range of applications,\n\
    including robotics, AR/VR, and autonomous driving. To process the point cloud,\n\
    many deep-learning-based point cloud recognition algorithms have been proposed.\n\
    However, to meet the requirement of applications like autonomous driving, the\n\
    algorithm must be fast enough, rendering accelerators necessary at the\ninference\
    \ stage. But existing point cloud accelerators are still inefficient\ndue to two\
    \ challenges. First, the multi-layer perceptron (MLP) during feature\ncomputation\
    \ is the performance bottleneck. Second, the feature vector fetching\noperation\
    \ incurs heavy DRAM access.\n  In this paper, we propose Pointer, an efficient\
    \ Resistive Random Access\nMemory (ReRAM)-based point cloud recognition accelerator\
    \ with inter- and\nintra-layer optimizations. It proposes three techniques for\
    \ point cloud\nacceleration. First, Pointer adopts ReRAM-based architecture to\
    \ significantly\naccelerate the MLP in feature computation. Second, to reduce\
    \ DRAM access,\nPointer proposes inter-layer coordination. It schedules the next\
    \ layer to fetch\nthe results of the previous layer as soon as they are available,\
    \ which allows\non-chip fetching thus reduces DRAM access. Third, Pointer proposes\n\
    topology-aware intra-layer reordering, which improves the execution order for\n\
    better data locality. Pointer proves to achieve 40x to 393x speedup and 22x to\n\
    163x energy efficiency over prior accelerators without any accuracy loss."
  url: http://arxiv.org/abs/2410.17782v1
  keywords: ''
  document: "# Pointer: An Energy-Efficient ReRAM-based Point Cloud Recognition Accelerator\
    \ with Inter-layer and Intra-layer Optimizations\n\nQijun Zhang, Zhiyao Xie\\\
    * Hong Kong University of Science and Technology qzhangcs@connect.ust.hk, eezhiyao@ust.hk\n\
    \n## ABSTRACT\n\nPoint cloud is an important data structure for a wide range of\
    \ applications, including robotics, AR/VR, and autonomous driving. To process\
    \ the point cloud, many deep-learning-based point cloud recognition algorithms\
    \ have been proposed. However, to meet the requirement of applications like autonomous\
    \ driving, the algorithm must be fast enough, rendering accelerators necessary\
    \ at the inference stage. But existing point cloud accelerators are still inefficient\
    \ due to two challenges. First, the multi-layer perceptron (MLP) during feature\
    \ computation is the performance bottleneck. Second, the feature vector fetching\
    \ operation incurs heavy DRAM access.\n\nIn this paper, we propose Pointer, an\
    \ efficient Resistive Random Access Memory (ReRAM)-based point cloud recognition\
    \ accelerator with inter- and intra-layer optimizations. It proposes three techniques\
    \ for point cloud acceleration. First, Pointer adopts ReRAMbased architecture\
    \ to significantly accelerate the MLP in feature computation. Second, to reduce\
    \ DRAM access, Pointer proposes inter-layer coordination. It schedules the next\
    \ layer to fetch the results of the previous layer as soon as they are available,\
    \ which allows on-chip fetching thus reduces DRAM access. Third, Pointer proposes\
    \ topology-aware intra-layer reordering, which improves the execution order for\
    \ better data locality. Pointer proves to achieve 40× to 393× speedup and 22×\
    \ to 163× energy efficiency over prior accelerators without any accuracy loss.\n\
    \n# CCS CONCEPTS\n\n• Computer systems organization → Architectures.\n\n#### KEYWORDS\n\
    \nPoint cloud, AI accelerator\n\n#### 1 INTRODUCTION\n\nPoint cloud is a data\
    \ structure dedicated to describing three dimensional (3D) objects using coordinates\
    \ and other auxiliary features [\\[12\\]](#page-7-0). It is increasingly popular\
    \ in many applications, including virtual reality, augmented reality, autonomous\
    \ driving, and robotics. To process the point cloud data, multiple point cloud\
    \ recognition algorithms have been proposed [\\[10,](#page-7-1) [11\\]](#page-7-2).\
    \ PointNet++ [\\[11\\]](#page-7-2) and its\n\nASPDAC '25, January 20–23, 2025,\
    \ Tokyo, Japan\n\n<https://doi.org/10.1145/3658617.3697658>\n\nvariants are one\
    \ of the most widely used point cloud recognition algorithms. As Fig. [1](#page-1-0)\
    \ shows, PointNet++ consists of multiple setabstraction layers. Each set-abstraction\
    \ layer includes two stages: 1) Point mapping stage, which performs the farthest\
    \ point sample (FPS) and neighbor search to compute the mapping between output\
    \ point cloud and input point cloud; 2) Feature processing stage, which first\
    \ aggregates feature vectors according to the mapping, then computes features\
    \ with multi-layer perceptron (MLP), finally reduces intermediate results to output\
    \ feature vector.\n\nHowever, most point-cloud applications require the point\
    \ cloud recognition to be performed in real-time to enable interactions with humans\
    \ or environments, bringing a long-lasting efficiency challenge in deployment.\
    \ To solve this challenge, an energy-efficient high-performance point cloud accelerator\
    \ is crucial. Existing accelerator designs include Mesorasi [\\[3\\]](#page-7-3),\
    \ Crescent [\\[2\\]](#page-7-4), Point-X [\\[19\\]](#page-7-5), PointAcc [\\[4\\\
    ]](#page-7-6), PRADA [\\[14\\]](#page-7-7), FLNA [\\[8\\]](#page-7-8), TiPU [\\\
    [20\\]](#page-7-9), MARS [\\[17\\]](#page-7-10), Sava [\\[7\\]](#page-7-11), and\
    \ FusionArch [\\[6\\]](#page-7-12). However, despite these explorations [\\[2–](#page-7-4)[4,](#page-7-6)\
    \ [6–](#page-7-12) [8,](#page-7-8) [14,](#page-7-7) [17,](#page-7-10) [19,](#page-7-5)\
    \ [20\\]](#page-7-9), as summarized in Fig. [1,](#page-1-0) there are still two\
    \ unsolved challenges in existing point cloud accelerators:\n\n- (1) The first\
    \ challenge is slow MLP execution in feature computation. MLP takes up about 70%\
    \ execution time when executed on GPUs [\\[3\\]](#page-7-3). Despite existing\
    \ efforts in optimizing [\\[3,](#page-7-3) [14\\]](#page-7-7) or optimizing [\\\
    [4\\]](#page-7-6) MLP executions, it is still the bottleneck.\n- (2) The second\
    \ challenge is the heavy DRAM access in the aggregation step, as depicted in Fig\
    \ [1.](#page-1-0) For each point, it needs to fetch the feature vectors of all\
    \ input points from DRAM. This operation incurs heavy DRAM access.\n\nTo solve\
    \ these two challenges, we propose Pointer, a ReRAMbased point cloud recognition\
    \ accelerator with both inter- and intralayer optimizations. As Fig [1](#page-1-0)\
    \ shows, it brings three essential improvements, denoted as 1 2 3 . For challenge\
    \ 1, Pointer proposes 1 a ReRAM-based architecture to accelerate the MLP execution\
    \ bottleneck [\\[1\\]](#page-7-13)[\\[13\\]](#page-7-14) in PointNet++. Such in-memory\
    \ processing accelerates MLP by reducing costly data movement of weight fetching\
    \ in this MLP operation. For challenge 2, Pointer reduces the heavy DRAM access\
    \ by optimizing the dataflow across set-abstraction layers. Existing solutions\
    \ first complete all computations in the previous layer and save results to DRAM,\
    \ then fetch these results from DRAM as the inputs for the next layer. To reduce\
    \ such DRAM access, Pointer proposes 2 inter-layer coordination to enable onchip\
    \ fetching. It starts the calculation of the point in next layer as soon as all\
    \ its required inputs from the previous layer are available. Such immediate data\
    \ reuse allows on-chip storage and fetching of previous layer's results. To further\
    \ improve data locality, Pointer proposes 3 topology-aware intra-layer reordering\
    \ to reorder execution. The new execution order maximizes the reuse of the common\
    \ inputs of processed points, further reducing DRAM accesses.\n\nThe contributions\
    \ in Pointer can be summarized as below.\n\n<sup>\\*</sup>Corresponding Author\n\
    \nPermission to make digital or hard copies of all or part of this work for personal\
    \ or classroom use is granted without fee provided that copies are not made or\
    \ distributed for profit or commercial advantage and that copies bear this notice\
    \ and the full citation on the first page. Copyrights for components of this work\
    \ owned by others than the author(s) must be honored. Abstracting with credit\
    \ is permitted. To copy otherwise, or republish, to post on servers or to redistribute\
    \ to lists, requires prior specific permission and/or a fee. Request permissions\
    \ from permissions@acm.org.\n\n<sup>©</sup> 2025 Copyright held by the owner/author(s).\
    \ Publication rights licensed to ACM. ACM ISBN 979-8-4007-0635-6/25/01\n\n<span\
    \ id=\"page-1-0\"></span>![](_page_1_Figure_0.jpeg)\n\nFigure 1: The workflow\
    \ of PointNet++, which consists of two major stages named point mapping and feature\
    \ processing. The point mapping stage includes farthest point sample (FPS) and\
    \ neighbor search. The feature processing stage includes aggregation, feature\
    \ computation, and reduction. In the aggregation step, for each sampled point\
    \ with feature vector , its neighboring points 's feature vectors are also fetched.\
    \ Then their difference D ( , ) is computed. Then an MLP M performs feature computation,\
    \ generating M (D ( , )) for each . Finally all results are reduced by computing\
    \ the maximum of each column.\n\n- To the best of our knowledge, Pointer is the\
    \ first ReRAMbased accelerator for PointNet++-based point cloud recognition. ReRAM\
    \ array greatly speeds up the slow and energyhungry MLP execution during feature\
    \ computation step.\n- To reduce DRAM access, Pointer proposes inter-layer coordination.\
    \ It schedules the next layer to fetch the results of the previous layer as soon\
    \ as they are available, which allows on-chip fetching thus reduces DRAM access.\n\
    - To further reduce DRAM access, Pointer proposes intra-layer reordering. It reschedules\
    \ the execution order with topology awareness, further allowing data reuse of\
    \ common inputs.\n- Pointer achieves 40× to 393× speedup and 22× to 163× energy\
    \ efficiency over the state-of-the-art no-accuracy-loss accelerator with a similar\
    \ hardware cost.\n\n# 2 BACKGROUND AND RELATED WORK\n\n### 2.1 PointNet++: Deep\
    \ Learning on Point Clouds\n\nPointNet++ [\\[11\\]](#page-7-2) is one of the most\
    \ widely-adopted point cloud recognition algorithms. It consists of multiple set-abstraction\
    \ layers. As Fig. [1](#page-1-0) shows, each set-abstraction layer will transform\
    \ an input point cloud to an output point cloud with fewer points, which belong\
    \ to a subset of the original input. Then this output point cloud will be the\
    \ input of the next set-abstraction layer. Each setabstraction layer consists\
    \ of two major stages, named point mapping and feature processing.\n\nThe point\
    \ mapping stage consists of two steps, named farthest point sampling (FPS) and\
    \ neighbor search. The FPS will determine which input points should remain in\
    \ the output point cloud of this layer, as shown in Fig. [1.](#page-1-0) As mentioned,\
    \ for the output point cloud of each layer, its points are the subset of the input\
    \ point cloud. The neighbor search determines the neighbors of each output point\
    \ by searching for the top-k nearest points of it.\n\nThe feature processing stage\
    \ consists of three steps, named aggregation, feature computation, and reduction.\
    \ We use a point as an example, assuming it is selected to remain in the output\
    \ cloud point. We denote its feature vector as and its neighboring points as with\
    \ feature vector . The aggregation stage first calculates the \"difference\" D\
    \ ( , ) between feature vector of and each of its neighbors. Then at the feature\
    \ computation stage, an multi-layer perceptron (MLP) M is applied on D ( , ) to\
    \ generate M (D ( , )). Then all of the M (D ( , )) corresponding to all neighbors\
    \ are reduced by maximum pooling, generating the output feature vector for point\
    \ .\n\n# 2.2 Process-In-Memory with ReRAM\n\nReRAM is an emerging non-volatile\
    \ memory technology. Besides storing data, the ReRAM crossbar architecture is\
    \ widely adopted to accelerate the vector-matrix multiplication [\\[1,](#page-7-13)\
    \ [13\\]](#page-7-14). The mechanism\n\n<span id=\"page-1-1\"></span>![](_page_1_Figure_14.jpeg)\n\
    \nFigure 2: (a) Multiply-accumulate operation with ReRAM. (b) The ReRAM array\
    \ used as vector-matrix multiplier.\n\nis shown in Fig. [2\\(](#page-1-1)a), with\
    \ the vertical line denoting bitline, and horizontal line denoting wordline. The\
    \ resistances of each memristor cell can be programmed for computation. Assuming\
    \ the resistances of the two cells in Fig. [2\\(](#page-1-1)a) are programmed\
    \ as <sup>1</sup> and 2, thus the conductance values are <sup>1</sup> = 1/<sup>1</sup>\
    \ and <sup>2</sup> = 1/2. When the voltages of these wordlines are<sup>1</sup>\
    \ and2, thus the current on the bitline is = <sup>1</sup> + <sup>2</sup> = <sup>1</sup>\
    \ ∗ <sup>1</sup> + <sup>2</sup> ∗ 2. Therefore, if we set conductance values {1,2,\
    \ ...,} equal to elements in a vector and voltages {1,2, ...,} equal to the other\
    \ vector, then the current on the bitline is the dot product result of two vectors.\n\
    \nTo extend the vector product engine to a vector-matrix multiplier, which is\
    \ the fundamental operation in MLP, the structure in Fig. [2\\(](#page-1-1)a)\
    \ can be horizontally extended to a crossbar architecture in Fig. [2\\(](#page-1-1)b).\
    \ Assume the vector length is still and the matrix shape is ×. In this case, the\
    \ current values on the bitlines {1, 2, ..., } can represent the output vector.\n\
    \n#### 2.3 Related Work\n\nExisting accelerators for PointNet++-based point cloud\
    \ recognition algorithms can be categorized into two types, depending on whether\
    \ they incur accuracy variation. Accelerators with accuracy variation include\
    \ Mesorasi [\\[3\\]](#page-7-3), Crescent [\\[2\\]](#page-7-4), PRADA [\\[14\\\
    ]](#page-7-7), FLNA [\\[8\\]](#page-7-8), TiPU [\\[20\\]](#page-7-9), Sava [\\\
    [7\\]](#page-7-11), and FusionArch [\\[6\\]](#page-7-12). But accuracy variation\
    \ is often unacceptable, especially in accuracy-critical scenarios where the accuracy\
    \ variation can lead to catastrophic consequences. The accelerators without accuracy\
    \ variation include Point-X [\\[19\\]](#page-7-5), PointAcc [\\[4\\]](#page-7-6),\
    \ MARS [\\[17\\]](#page-7-10). The Point-X only focuses on DGCNN [\\[15\\]](#page-7-15),\
    \ which cannot be directly applied to widely-adopted PointNet++ and its other\
    \ variants. In other words, it does not generalize well. Only PointAcc and MARS\
    \ can accelerate general point cloud recognition algorithms without accuracy variation,\
    \ and MARS mainly enhances the mapping unit of PointAcc. Our Pointer is also a\
    \ general point cloud accelerator without accuracy variation. So in this paper,\
    \ the baseline is MARS-like accelerator, which is a state-of-the-art no-accuracy-variation\
    \ point cloud accelerator.\n\nAlthough HSPA [\\[5\\]](#page-7-16) proposed a ReRAM-based\
    \ point cloud recognition accelerator, it focuses on the DeepSets [\\[18\\]](#page-7-17)\
    \ algorithms. It can not fully support the PointNet++-based algorithms, where\
    \ irregular DRAM access is critical even with the ReRAM engine. Moreover, although\
    \ Point-X [\\[19\\]](#page-7-5), TiPU [\\[20\\]](#page-7-9), and FusionArch [\\\
    [6\\]](#page-7-12) discussed the \"spatial-locality\" and packed adjacent point\
    \ processing together spatially or temporally, they did not consider the global\
    \ schedule space where the inter-layer coordination is applied, which is hard\
    \ to support directly by their architecture design. In comparison, a simple reordering\
    \ with light hardware overhead proposed in our work can be natural when integrating\
    \ with the inter-layer coordination which is also an order-related technique.\
    \ These two techniques can be achieved in a scheduler uniformly.\n\n#### 3 METHODOLOGY\n\
    \nIn this Section, we will present Pointer in detail. We first cover the basic\
    \ ReRAM-based PointNet++ accelerator design, which greatly accelerates the MLP\
    \ operation. Then we introduce our proposed inter-layer coordination and topology-aware\
    \ intra-layer reordering techniques, which further effectively reduce the DRAM\
    \ access. Finally, we describe the detailed hardware implementation of Pointer.\n\
    \n#### <span id=\"page-2-1\"></span>3.1 Basic ReRAM-based Point Cloud Accelerator\n\
    \nConventional PointNet++ accelerators commonly use Multiplyand-Accumulate (MAC)\
    \ array to compute MLP. Because of the limited on-chip buffer, it requires repeatedly\
    \ loading the weight from DRAM, leading to the slow and energy-hungry execution\
    \ of MLP. In comparison, ReRAM is a promising solution for vectormatrix multiplication.\
    \ By performing in-memory computing, the data movement overhead in existing MLP\
    \ is eliminated, which improves performance and reduces energy consumption. This\
    \ leads to the basic ReRAM-based PointNet++ accelerator. Similar to the existing\
    \ dataflow in Fig. [1,](#page-1-0) the feature vectors are fetched and differences\
    \ are computed, then fed into ReRAM-based MLP engine, finally the feature vector\
    \ of output point is written into the DRAM.\n\nReliability is a main concern when\
    \ adopting ReRAM-based accelerators. For better reliability, we propose to adopt\
    \ the ReRAM array with a relatively small number of bits per cell (i.e. 2 bits\
    \ per cell). In addition, since the ReRAM computation is not the speed bottleneck,\
    \ we can trade off the ReRAM computation speed for lower overhead. For example,\
    \ we can accept more sequential operations by adopting fewer ReRAM array replications\
    \ [\\[13\\]](#page-7-14).\n\nBecause different set-abstraction layers are mapped\
    \ to different ReRAM arrays, different layers can be theoretically executed in\
    \ parallel. But in fact, because of complex data dependency between layers, for\
    \ basic ReRAM-based design, each set-abstraction layer is still executed sequentially,\
    \ as shown in Fig. [3\\(](#page-3-0)a). In other words, the next set-abstraction\
    \ layer still starts after all computation in the previous layer is completed.\
    \ And for each set-abstraction layer, following existing solution, output points\
    \ are still calculated directly by their index orders. For example, if output\
    \ points are {1, 3, 5, 7, 9}, then at this layer, execution order is = [<sup>1</sup>\
    \ − <sup>3</sup> − <sup>5</sup> − <sup>7</sup> − 9]. This naive scheduling will\
    \ be improved in subsequent subsections.\n\n# 3.2 Enable On-Chip Fetching with\
    \ Inter-layer Coordination\n\nAfter adopting ReRAM, the MLP execution is no longer\
    \ the runtime bottleneck. The new bottleneck becomes the heavy DRAM access during\
    \ feature vector fetching in aggregation, which also incurs high energy consumption\
    \ for DRAM.\n\nDRAM access of feature vector fetching can be reduced by buffering\
    \ the data and fetching them on-chip, which can not only improve performance but\
    \ also reduce energy consumed by DRAM. However, since the existing design requires\
    \ previous set-abstraction layer to be fully completed before starting the next\
    \ layer, it will require all feature vectors to be held on-chip till the fetching,\
    \ requiring a prohibitively large buffer. Otherwise, feature vectors generated\
    \ by previous layer will be evicted into DRAM before they are requested by the\
    \ next layer. We illustrate this scenario in Fig. [3\\(](#page-3-0)a).\n\nTo address\
    \ this DRAM access bottleneck, we propose the interlayer coordination technique\
    \ to improve on-chip fetching. The key idea is to start the computation of the\
    \ point in the next layer earlier, immediately after the calculations of all its\
    \ required input points from the previous layer have finished. It requires coordinating\
    \ the execution of the previous layer based on the execution order of\n\n<span\
    \ id=\"page-2-0\"></span><sup>1</sup>Here we assume there is a simple buffer in\
    \ the basic ReRAM-based accelerator, in order to compare with designs with inter-layer\
    \ coordination and intra-layer reordering.\n\n<span id=\"page-3-0\"></span>![](_page_3_Figure_0.jpeg)\n\
    \nFigure 3: Inter-layer coordination and intra-layer reordering, using the example\
    \ in Fig. [1.](#page-1-0) The upper sub-figures (i)(ii)(iii) illustrate the point\
    \ execution order in each layer. The number within circles is the index of points\
    \ , and the outside the circles is the process order within each layer. The bottom\
    \ sub-figures (a)(b)(c) illustrate how inter-layer coordination and intra-layer\
    \ reordering improve the on-chip buffer hit rate thus performance. The buffer\
    \ content shows the available content at the start point of each time step. (a)\
    \ Basic ReRAM-based acceleartor[1](#page-2-0) . It simply schedules the execution\
    \ by index order. (b) Accelerator with inter-layer coordination. It schedules\
    \ the execution order in layer 1 based on the receptive field of points in layer\
    \ 2. (c) Accelerator with both inter-layer coordination and intra-layer reordering\
    \ (i.e., Pointer). The intra-layer reordering determines the execution order of\
    \ layer 2. The inter-layer coordination still determines the execution order of\
    \ layer 1.\n\nthe next layer. For such optimization, the dependency between the\
    \ points in different layers is critical.\n\nAs Fig. [4](#page-3-1) shows, the\
    \ dependency across multiple consecutive layers leads to a pyramid-shaped receptive\
    \ field for each last-layer output point, which can be viewed as the top of the\
    \ \"pyramid\". In this example, two set-abstraction layers will lead to a 3-level\
    \ pyramid-shaped receptive field.\n\nThe algorithm of inter-layer coordination\
    \ is shown as the lines [9-](#page-4-0) [13](#page-4-0) of Algorithm [1.](#page-4-0)\
    \ The example of inter-layer coordination is illustrated in Fig. [3\\(](#page-3-0)ii)(b)\
    \ where the number of layer is 2. We denote the computation of point in the layer\
    \ as . For the example with two set-abstraction layers, we represent a pyramid-shaped\
    \ receptive field as E 2 i − { 1 }, where E 2 i represents the executions of in\
    \ layer 2 and { 1 } represents the execution of all points in 's receptive field\
    \ in layer 1. To distinguish two different layers in our example, we use bold\
    \ type to emphasize executions of points in layer 2. In our example, corresponding\
    \ to the three output points, there are three pyramid-shaped receptive fields:\
    \ (1)E 2 1 − { 1 1 , 1 4 , 1 7 }, (2)E 2 3 − { 1 2 , 1 3 , 1 6 }, (3)E 2 5 − {\
    \ 1 4 , 1 5 , 1 7 }. Such receptive field information determines the data dependency.\n\
    \nWe schedule the computation to maximize data reuse according to the data dependency.\
    \ As the example shown in Fig. [3\\(](#page-3-0)ii)(b), the\n\n<span id=\"page-3-1\"\
    ></span>![](_page_3_Figure_6.jpeg)\n\nFigure 4: An example of the pyramid-shaped\
    \ receptive field. This example is consistent with Fig. [1](#page-1-0) and Fig.\
    \ [3.](#page-3-0)\n\ncomputation is now receptive-field by receptive-field, instead\
    \ of layer by layer. Because the computation order of the last layer (layer 2)\
    \ is the index order by default, which means that<sup>2</sup> = [1−3−5], equivalent\
    \ to <sup>2</sup> = [E 2 1 − E 2 3 − E 2 5 ]. So such receptive-field by receptive-field\
    \ scheduling first calculates 1 1 − 1 4 − 1 7 − E 2 1 , then 1 2 − 1 3 − 1 6 −\
    \ E 2 3 , then 1 4 − 1 7 − 1 5 − E 2 5 . But actually 1 4 and 1 7 appear in two\
    \ receptive fields and they only need to be calculated\n\nonce. When we schedule\
    \ these three receptive fields directly using the index order in the second layer\
    \ E 2 1 −E 2 3 −E 2 5 , the final execution order with both layers is shown below:\n\
    \n<span id=\"page-4-1\"></span>\n$$E\\_1^1 - E\\_4^1 - E\\_7^1 - E\\_1^2 - E\\\
    _2^1 - E\\_3^1 - E\\_6^1 - E\\_3^2 - E\\_5^1 - E\\_5^2 \\tag{1}$$\n\nIt can also\
    \ be expressed as<sup>1</sup> = [ 1 1 − 1 4 − 1 7 − 1 2 − 1 3 − 1 6 − 1 5 ],<sup>2</sup>\
    \ = [E 2 1 − E 2 3 − E 2 5 ]. Since the executions in layer 1 and layer 2 can\
    \ be executed in parallel in the ReRAM-based accelerator, this final order in\
    \ Equation [1](#page-4-1) is the same as the schedule of both layers in Fig. [3\\\
    (](#page-3-0)b).\n\nIn summary, the inter-layer coordination reduces DRAM access\
    \ by holding feature vectors on-chip for immediate subsequent fetching. But as\
    \ shown in Fig. [3\\(](#page-3-0)b), there are still some on-chip buffer misses\
    \ when the 1 4 and 1 7 are fetched at the second time. It indicates that inter-layer\
    \ coordination alone is still insufficient.\n\n#### 3.3 Topology-aware Intra-layer\
    \ Reordering\n\nThe limitation of only using inter-layer coordination is that\
    \ current scheduling is unaware of point cloud topology. It simply schedules the\
    \ execution of the whole receptive field based on the index order of points in\
    \ the last layer (i.e., <sup>2</sup> = [E 2 1 − E 2 3 − E 2 5 ]), without considering\
    \ the actual topology of the point cloud. As a result, consecutive executed receptive\
    \ fields have small overlap thus poor data locality. Using the example in Fig.\
    \ [3\\(](#page-3-0)ii), there is no overlap between receptive fields of E 2 1\
    \ and E 2 3 , and also that of E 2 3 and E 2 5 .\n\nTo tackle the problem, we\
    \ propose topology-aware intra-layer reordering to schedule the execution order\
    \ of the last layer (i.e.,\n\n#### <span id=\"page-4-0\"></span>Algorithm 1 Scheduling\
    \ Order Generation\n\nInput: The number of layers, . All output points of the\
    \ last layer, . The distance function between two points, ( , ). Receptive fields\
    \ of each point E k i − { −1 }\n\n```\nOutput: The execution order of each layer\
    \ {1, 2, ... }\n```\n\n```\n/* line 1-8 are algorithm of 3 Intra-layer reordering\
    \ */\n```\n\n```\n/* Select points one by one based on the topology */\n```\n\n\
    ```\n1: Initiate an empty order list for the last layer  = [ ]\n```\n\n```\n/*\
    \ Start from a random point */\n```\n- 2: Select a point from randomly\n- 3: Take\
    \ out from , append it to\n\n```\n4: Set  =\n```\n5: while is not empty do\n\n\
    ```\n/* Select point nearest to the last selected point*/\n6:  = arg min\n   \
    \                 (,  )\n```\n ∈ /\\* denotes the current selected point\\*/\n\
    \n```\n/*  denotes the last selected point*/\n```\n\n```\n7: Take  out from ,\
    \ append it to\n```\n\n```\n8: Set  =\n```\n\n```\n/* line 9-13 are algorithm\
    \ of 2 Inter-layer coordination */\n   /* The execution order of prior layers\
    \ depend on later layers, thus it iterates\n   from later layer to prior layers\
    \ */\n9: for  :    − 1  1 () do\n      /* Generate the order for layer */\n10:\
    \ Initiate an empty order list for layer ,  = [ ]\n11: for \n            +1\n\n\
    \                ∈ +1 (by order) do\n                                   k+1\n\
    ```\n![](_page_4_Figure_24.jpeg)\n\n13: .append({ })\n\n<span id=\"page-4-2\"\
    ></span>![](_page_4_Figure_26.jpeg)\n\nFigure 5: An obvious overlap between the\
    \ receptive fields of two neighboring points in the last layer. (a) Original point\
    \ cloud. (b) Green points are the output points in the last setabstraction layer,\
    \ the red and blue points are two neighboring points. (c) Green points are original\
    \ point cloud, the red and blue points are the receptive fields of the two points\
    \ in (b) respectively. There is a large overlap between these two fields.\n\n\
    <span id=\"page-4-3\"></span>![](_page_4_Figure_28.jpeg)\n\n#### Figure 6: The\
    \ architecture of Pointer. The blue part is the support for the ReRAM-based accelerator,\
    \ the green part is for inter-layer coordination, and the orange part is for intralayer\
    \ reordering.\n\nlayer 2 in this example). The topology-aware intra-layer reordering\
    \ is shown as the lines [1-8](#page-4-0) of Algorithm [1.](#page-4-0) The topology-aware\
    \ order means that rather than scheduling them by index order, we generate a new\
    \ order that tries to schedule consecutive points to be neighboring in the physical\
    \ space. In this example, it reorders the scheduling of points in the last layer\
    \ from the original <sup>2</sup> = [E 2 1 − E 2 3 − E 2 5 ] to the ′ 2 = [E 2\
    \ 1 − E 2 5 − E 2 3 ]. Since inter-layer coordination makes a receptive-field\
    \ by receptive-field scheduling, executions in previous layers in the receptive\
    \ field will follow the order in the last layer. The new overall order will now\
    \ become:\n\n$$E\\_1^1 - E\\_4^1 - E\\_7^1 - E\\_1^2 - E\\_5^1 - E\\_5^2 - E\\\
    _2^1 - E\\_3^1 - E\\_6^1 - E\\_3^2 \\tag{2}$$\n\nas illustrated in Fig. [3\\(](#page-3-0)c).\
    \ It can also be expressed as <sup>1</sup> = [ 1 1 − 1 4 − 1 7 − 1 5 − 1 2 − 1\
    \ 3 − 1 6 ],<sup>2</sup> = [E 2 1 − E 2 5 − E 2 3 ]. It now successfully removes\
    \ all on-chip buffer misses by further exploiting data locality and boosting data\
    \ reuse. The improved data reuse can further reduce DRAM access, thus improving\
    \ performance and energy efficiency.\n\nHowever, a challenge in this intra-layer\
    \ reordering is how to find out which points in the last layer have the largest\
    \ common part in their receptive fields. The most intuitive solution is to search\
    \ all of the possible orders and analyze the data reuse of each order, which is\
    \ unrealistic. The accelerator should explore an efficient way to generate the\
    \ execution order.\n\nIn Pointer, we adopt a highly lightweight yet effective\
    \ technique to generate topology-aware execution order of points in the last layer.\
    \ For all unexecuted points in the last layer, the (i+1)th point in the order\
    \ should be the nearest to the ith point. Since these points are close in the\
    \ last layer, points in their receptive field in previous layers are expected\
    \ to also be close and overlap well. This is validated by an example from our\
    \ experiment in Fig. [5,](#page-4-2) which randomly selects two consecutive points\
    \ in our generated topologyaware order, as shown in Fig. [5\\(](#page-4-2)b).\
    \ We find that their receptive fields in blue and red overlap very well, as Fig.\
    \ [5\\(](#page-4-2)c) shows. It indicates that this approximation can still generate\
    \ a high-quality execution order. In addition, this reordering technique introduces\
    \ negligible overhead, since it only requires the distance between selected pair\
    \ of points, which fortunately has already been calculated during the existing\
    \ FPS and neighbor search steps.\n\n#### 3.4 Hardware Implementation of Pointer\n\
    \nFig. [6](#page-4-3) shows the overview of the Pointer architecture design. The\
    \ basic ReRAM-based PointNet++ accelerator introduced in Section [3.1](#page-2-1)\
    \ is the part without color. It consists of front-end and back-end. Similar to\
    \ the prior work [\\[14\\]](#page-7-7), the front-end is for the point mapping\
    \ stage, and the back-end is for the feature processing stage. The backend mainly\
    \ consists of four parts: ReRAM tile, reconfigurable data path, digital computation\
    \ unit, and main controller. 1) The ReRAM tile consists of many ReRAM arrays,\
    \ which serve the computation and storage for weight in MLP. 2) The reconfigurable\
    \ data path will control and transfer data between different ReRAM arrays.3) The\
    \ digital computation unit executes some operations such as ADD, MAX, and non-linear\
    \ function. 4) The main controller coordinates different parts of the accelerator.\n\
    \nThe architecture support for inter-layer coordination is shown as the green\
    \ part in Fig. [6.](#page-4-3) The support for topology-aware intra-layer reordering\
    \ is shown as the orange part of Fig. [6.](#page-4-3) It is a small order generator\
    \ added to front-end with negligible hardware overhead.\n\n#### 4 EXPERIMENT\n\
    \n#### 4.1 Experiment Setup\n\n4.1.1 Benchmarks. We evaluate the representative\
    \ point cloud recognition model PointNet++ [\\[11\\]](#page-7-2) on our accelerator.\
    \ Three different configurations of evaluated PointNet++ are summarized in Table.\
    \ [1.](#page-5-0) Same as the original PointNet++, all three models consist of\
    \ two set-abstraction layers. The input point cloud size is 1024 points in our\
    \ models. For dataset, we adopt ModelNet40 [\\[16\\]](#page-7-18), which is widely\
    \ adopted in many point cloud recognition research and point cloud accelerator\
    \ evaluations, consisting of 12311 point clouds.\n\n4.1.2 Modeling Accelerator\
    \ Architecture. To evaluate our design, we develop a simulator to model the behavior\
    \ of our design assuming 8GB/s DDR3 bandwidth. Specifically, we mainly simulate\
    \ the back-end (i.e., feature processing stage) of the Pointer and our baseline.\
    \ It is because when deployed in the application, the point mapping and feature\
    \ processing stages can be pipelined and the feature processing is slower than\
    \ point mapping.\n\nOur design is evaluated under 40nm technology and the frequency\
    \ is set to 1GHz. As for area, we use CACTI [\\[9\\]](#page-7-19) to model\n\n\
    <span id=\"page-5-0\"></span>\n\n|         | Model ID                        |\
    \ Model 0 | Model 1 | Model 2  |\n|---------|---------------------------------|---------|---------|----------|\n\
    | Layer 1 | Input Feature Vector Length     | 4       | 8       | 16       |\n\
    |         | Output Feature Vector Length    | 128     | 256     | 512      |\n\
    |         | The Shape of MLP (three layers) | 4*64    | 8*128   | 16*256   |\n\
    |         |                                 | 64*64   | 128*128 | 256*256  |\n\
    |         |                                 | 64*128  | 128*256 | 256*512  |\n\
    |         | The Number of Neighbors         | 16      | 16      | 16       |\n\
    |         | The Number of Central Point2    | 512     | 512     | 512      |\n\
    | Layer 2 | Input Feature Vector Length     | 129     | 256     | 512      |\n\
    |         | Output Feature Vector Length    | 256     | 512     | 1024     |\n\
    |         | The Shape of MLP (three layers) | 128*128 | 256*256 | 512*512  |\n\
    |         |                                 | 128*128 | 256*256 | 512*512  |\n\
    |         |                                 | 128*256 | 256*512 | 512*1024 |\n\
    |         | The Number of Neighbors         | 16      | 16      | 16       |\n\
    |         | The Number of Central Point     | 128     | 128     | 128      |\n\
    \nTable 1: Three PointNet++ models evaluated in experiment.\n\nthe SRAM and published\
    \ data from the prior work [\\[13\\]](#page-7-14) to model ReRAM. The evaluated\
    \ area of the back-end and the order generator of our design[3](#page-5-2) is\
    \ 1.25<sup>2</sup> . For our design configuration, the ReRAM tile consists of\
    \ 96 IMAs with each IMA consisting of 8 128\\*128 ReRAM arrays, and the buffer\
    \ size is 9KB. We estimate energy efficiency with reference energy data collected\
    \ from [\\[9,](#page-7-19) [13\\]](#page-7-14).\n\nWe select a baseline accelerator\
    \ similar to MARS [\\[17\\]](#page-7-10). The baseline's MAC array consists of\
    \ 32\\*32 MAC. For a fair comparison, we always keep the SRAM size used in our\
    \ design and baseline the same, which is a 9KB on-chip buffer. The area of the\
    \ back-end of the MARS-like baseline accelerator is 1.56<sup>2</sup> , which indicates\
    \ that the hardware overhead of our design is similar to the baseline.\n\nTo further\
    \ clearly evaluate each of our proposed methods, we also introduce two variants\
    \ of Pointer for ablation study. The first one is the Pointer without inter-layer\
    \ coordination and intra-layer reordering, which is called Pointer-1 (i.e., only\
    \ with contribution 1 ). The second one is the Pointer without intra-layer reordering,\
    \ which is called Pointer-12 (i.e., only with contribution 1 2 ).\n\n# 4.2 Experimental\
    \ Result\n\n4.2.1 Performance Speedup and Energy Efficiency. Fig. [7](#page-6-0)\
    \ shows the speedup of Pointer over the MARS-like baseline. For three evaluated\
    \ PointNet++ models, Pointer speedups by 40×, 135×, and 393×. This speedup is\
    \ more obvious for larger models, demonstrating the great scalability of Pointer.\
    \ When the model scales up, the weight matrix becomes larger, so the data movement\
    \ overhead in the baseline is heavier. But for Pointer, because the data movement\
    \ overhead has been eliminated by using ReRAM, the performance slowdown is much\
    \ slower than baseline, so the speedup is becoming significant.\n\nThe result\
    \ also shows that the performance of Pointer-12 always outperforms Pointer-1.\
    \ It validates the benefit of inter-layer coordination. The Pointer also always\
    \ outperforms the Pointer-12. It validates the benefit of intra-layer topology-aware\
    \ reordering.\n\nFig. [8](#page-6-1) shows energy consumption of Pointer compared\
    \ with the MARS-like baseline. The energy consumption is normalized with the baseline.\
    \ For the three evaluated models, Pointer improves the energy efficiency by 22×,\
    \ 62×, and 163×, which demonstrates the energy efficiency improvement of Pointer.\
    \ Because the energy consumption mainly comes from the DRAM access, the reduction\
    \ of DRAM access in Pointer can significantly reduce the energy.\n\n<span id=\"\
    page-5-1\"></span><sup>2</sup>The number of central points is the number of selected\
    \ output points in FPS.\n\n<span id=\"page-5-2\"></span><sup>3</sup>The front-end\
    \ of our design is similar to the baseline except for the order generator.\n\n\
    <span id=\"page-6-0\"></span>![](_page_6_Figure_0.jpeg)\n\nFigure 7: The speedup\
    \ of Pointer and its two variants (i.e., Pointer-1, Pointer-12), compared with\
    \ the MARS-like baseline [\\[17\\]](#page-7-10). It measures the performance of\
    \ three different Point++ models, whose configurations are shown in Table [1.](#page-5-0)\n\
    \n<span id=\"page-6-1\"></span>![](_page_6_Figure_2.jpeg)\n\nFigure 8: The energy\
    \ consumption of Pointer and its two variants normalized with MARS-like baseline.\n\
    \n4.2.2 Source of Performance Gain. To further analyze Pointer's superior performance,\
    \ we present a breakdown of the overall DRAM access traffic into three parts:\
    \ feature vector fetching, feature vector writing, and weight fetching for MLP,\
    \ as shown in Fig. [9\\(a\\).](#page-6-2)\n\nIn Fig. [9\\(a\\),](#page-6-2) the\
    \ comparison between Pointer-1 and baseline shows that the ReRAM-based accelerator\
    \ eliminates weight fetching. Compared with Pointer-1, the average DRAM traffic\
    \ of feature vector fetching in Pointer-12 is reduced from 627KB to 396KB, cutting\
    \ down 37% of traffic by enabling on-chip fetching. Please notice that the feature\
    \ vector writing remains unchanged, because all of the computed feature vectors\
    \ will be saved back into the DRAM once. Compared with Pointer-12, the average\
    \ DRAM traffic of feature vector fetching in Pointer is further reduced to 121KB,\
    \ further\n\n<span id=\"page-6-2\"></span>![](_page_6_Figure_6.jpeg)\n\n<span\
    \ id=\"page-6-3\"></span>Figure 9: (a) The breakdown of DRAM traffic for feature\
    \ vector fetching, feature vector writing, and weight fetching of Pointer, baseline,\
    \ and two variants of Pointer for ablation study. (b) The comparison of Pointer-12\
    \ and Pointer for how the performance changes with the buffer size. There is no\
    \ buffer for Pointer-1 so it is not shown.\n\n<span id=\"page-6-4\"></span>![](_page_6_Figure_8.jpeg)\n\
    \n<span id=\"page-6-5\"></span>Figure 10: On-chip buffer hit rate with different\
    \ buffer sizes. No buffer for Pointer-1 so it is not shown.\n\ncutting down 69%\
    \ of the traffic, also 81% compared with Pointer-1, because the intra-layer reordering\
    \ improves the data reuse.\n\nTo further inspect how the intra-layer reordering\
    \ reduces the feature vector fetching, we analyze the on-chip buffer hit rate\
    \ with and without intra-layer reordering to demonstrate the data locality improvement.\
    \ It shows that after equipping the accelerator with intra-layer reordering, the\
    \ on-chip buffer hit rate of set-abstraction layer 1 is improved from 68% to 71%,\
    \ and that of layer 2 is improved from 33% to 82%, which shows that data locality\
    \ is improved.\n\n4.2.3 Impact of Buffer Size. The buffer size has a large impact\
    \ on the performance. Fig. [9\\(b\\)](#page-6-3) shows how the speedup values\
    \ change with respect to the buffer size in Pointer-12 and Pointer. The buffer\
    \ size has such performance impact since it directly affects the onchip buffer\
    \ hit rate. Here we further explore the correlation between on-chip buffer hit\
    \ rate and buffer size in Fig. [10\\(a\\)](#page-6-4) and Fig. [10\\(b\\).](#page-6-5)\n\
    \nFor the first layer, as shown in Fig. [10\\(a\\),](#page-6-4) because there\
    \ are a larger number of input points to process in this first layer, the hit\
    \ rate is relatively low when the buffer is small, lower than 50% for both Pointer-12\
    \ and Pointer. With the increase of the buffer size, the hit rate also increases,\
    \ but the hit rate of Pointer increases more significantly than Pointer-12 due\
    \ to its better data locality.\n\nFor the second layer, as shown in Fig. [10\\\
    (b\\),](#page-6-5) the on-chip buffer hit rate of Pointer is always higher than\
    \ Pointer-12 when the buffer size is smaller than 512. As the buffer size increases,\
    \ the gap between Pointer and Pointer-12 becomes smaller. It is because the effect\
    \ of the poor data locality is less obvious when given a larger buffer. When the\
    \ buffer size reaches 512, the hit rate is 100% because there are only 512 points\
    \ in the input point cloud of layer 2.\n\n#### 5 CONCLUSION\n\nIn this paper,\
    \ we propose Pointer, a ReRAM-based point cloud accelerator with inter- and intra-layer\
    \ optimizations. We first design a basic ReRAM-based accelerator to accelerate\
    \ feature computation. Then we propose inter-layer coordination and intra-layer\
    \ reordering to reduce DRAM access. Experiments show that Pointer outperforms\
    \ MARS under different model sizes and buffer sizes. Our proposed techniques may\
    \ be transferred to other applications with irregular feature vector fetching\
    \ such as graph neural network.\n\n#### ACKNOWLEDGEMENT\n\nThis work is partially\
    \ supported by National Natural Science Foundation of China 92364102, and ACCESS\
    \ – AI Chip Center for Emerging Smart Systems, sponsored by InnoHK funding, Hong\
    \ Kong SAR.\n\n#### REFERENCES\n\n- <span id=\"page-7-13\"></span>[1] Ping Chi,\
    \ Shuangchen Li, Cong Xu, Tao Zhang, Jishen Zhao, Yongpan Liu, Yu Wang, and Yuan\
    \ Xie. 2016. Prime: A novel processing-in-memory architecture for neural network\
    \ computation in reram-based main memory. ACM SIGARCH Computer Architecture News\
    \ 44, 3 (2016), 27–39.\n- <span id=\"page-7-4\"></span>[2] Yu Feng, Gunnar Hammonds,\
    \ Yiming Gan, and Yuhao Zhu. 2022. Crescent: taming memory irregularities for\
    \ accelerating deep point cloud analytics. In Proceedings of the 49th Annual International\
    \ Symposium on Computer Architecture. 962–977.\n- <span id=\"page-7-3\"></span>[3]\
    \ Yu Feng, Boyuan Tian, Tiancheng Xu, Paul Whatmough, and Yuhao Zhu. 2020. Mesorasi:\
    \ Architecture support for point cloud analytics via delayed-aggregation. In 2020\
    \ 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO). IEEE,\
    \ 1037–1050.\n- <span id=\"page-7-6\"></span>[4] Yujun Lin, Zhekai Zhang, Haotian\
    \ Tang, Hanrui Wang, and Song Han. 2021. Pointacc: Efficient point cloud accelerator.\
    \ In MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture.\
    \ 449–461.\n- <span id=\"page-7-16\"></span>[5] Yaotian Ling, Zongwei Wang, Lindong\
    \ Wu, Yimao Cai, and Ru Huang. 2024. An RRAM-Based Hierarchical Computing-in-Memory\
    \ Architecture With Synchronous Parallelism for 3D Point Cloud Recognition. IEEE\
    \ Transactions on Circuits and Systems II: Express Briefs (2024).\n- <span id=\"\
    page-7-12\"></span>[6] Xueyuan Liu, Zhuoran Song, Guohao Dai, Gang Li, Can Xiao,\
    \ Yan Xiang, Dehui Kong, Ke Xu, and Xiaoyao Liang. 2024. FusionArch: A Fusion-Based\
    \ Accelerator for Point-Based Point Cloud Neural Networks. In 2024 Design, Automation\
    \ & Test in Europe Conference & Exhibition (DATE). IEEE, 1–6.\n- <span id=\"page-7-11\"\
    ></span>[7] Xueyuan Liu, Zhuoran Song, Xiang Liao, Xing Li, Tao Yang, Fangxin\
    \ Liu, and Xiaoyao Liang. 2024. Sava: A Spatial-and Value-Aware Accelerator for\
    \ Point Cloud Transformer. In 2024 Design, Automation & Test in Europe Conference\
    \ & Exhibition (DATE). IEEE, 1–6.\n- <span id=\"page-7-8\"></span>[8] Dongxu Lyu,\
    \ Zhenyu Li, Yuzhou Chen, Ningyi Xu, and Guanghui He. 2023. FLNA: An energy-efficient\
    \ point cloud feature learning accelerator with dataflow decoupling. In 2023 60th\
    \ ACM/IEEE Design Automation Conference (DAC). IEEE, 1–6.\n- <span id=\"page-7-19\"\
    ></span>[9] Naveen Muralimanohar, Rajeev Balasubramonian, and Norman P Jouppi.\
    \ 2009. CACTI 6.0: A tool to model large caches. HP laboratories 27 (2009), 28.\n\
    - <span id=\"page-7-1\"></span>[10] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas\
    \ J Guibas. 2017. Pointnet: Deep learning on point sets for 3d classification\
    \ and segmentation. In Proceedings of\n\nthe IEEE conference on computer vision\
    \ and pattern recognition. 652–660.\n\n- <span id=\"page-7-2\"></span>[11] Charles\
    \ Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. 2017. Pointnet++: Deep\
    \ hierarchical feature learning on point sets in a metric space. Advances in neural\
    \ information processing systems 30 (2017).\n- <span id=\"page-7-0\"></span>[12]\
    \ Radu Bogdan Rusu and Steve Cousins. 2011. 3d is here: Point cloud library (pcl).\
    \ In 2011 IEEE international conference on robotics and automation. IEEE, 1–4.\n\
    - <span id=\"page-7-14\"></span>[13] Ali Shafiee, Anirban Nag, Naveen Muralimanohar,\
    \ Rajeev Balasubramonian, John Paul Strachan, Miao Hu, R Stanley Williams, and\
    \ Vivek Srikumar. 2016. ISAAC: A convolutional neural network accelerator with\
    \ in-situ analog arithmetic in crossbars. ACM SIGARCH Computer Architecture News\
    \ 44, 3 (2016), 14–26.\n- <span id=\"page-7-7\"></span>[14] Zhuoran Song, Heng\
    \ Lu, Gang Li, Li Jiang, Naifeng Jing, and Xiaoyao Liang. 2023. PRADA: Point Cloud\
    \ Recognition Acceleration via Dynamic Approximation. In 2023 Design, Automation\
    \ & Test in Europe Conference & Exhibition (DATE). IEEE, 1–6.\n- <span id=\"page-7-15\"\
    ></span>[15] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M Bronstein,\
    \ and Justin M Solomon. 2019. Dynamic graph cnn for learning on point clouds.\
    \ ACM Transactions on Graphics (tog) 38, 5 (2019), 1–12.\n- <span id=\"page-7-18\"\
    ></span>[16] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang,\
    \ Xiaoou Tang, and Jianxiong Xiao. 2015. 3d shapenets: A deep representation for\
    \ volumetric shapes. In Proceedings of the IEEE conference on computer vision\
    \ and pattern recognition. 1912–1920.\n- <span id=\"page-7-10\"></span>[17] Xinhao\
    \ Yang, Tianyu Fu, Guohao Dai, Shulin Zeng, Kai Zhong, Ke Hong, and Yu Wang. 2023.\
    \ An efficient accelerator for point-based and voxel-based point cloud neural\
    \ networks. In 2023 60th ACM/IEEE Design Automation Conference (DAC). IEEE, 1–6.\n\
    - <span id=\"page-7-17\"></span>[18] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh,\
    \ Barnabas Poczos, Russ R Salakhutdinov, and Alexander J Smola. 2017. Deep sets.\
    \ Advances in neural information processing systems 30 (2017).\n- <span id=\"\
    page-7-5\"></span>[19] Jie-Fang Zhang and Zhengya Zhang. 2021. Point-x: A spatial-locality-aware\
    \ architecture for energy-efficient graph-based point-cloud deep learning. In\
    \ MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture.\
    \ 1078–1090.\n- <span id=\"page-7-9\"></span>[20] Jiapei Zheng, Hao Jiang, Xinkai\
    \ Nie, Zhangcheng Huang, Chixiao Chen, and Qi Liu. 2023. TiPU: A Spatial-Locality-Aware\
    \ Near-Memory Tile Processing Unit for 3D Point Cloud Neural Network. In 2023\
    \ 60th ACM/IEEE Design Automation Conference (DAC). IEEE, 1–6."
- title: "FirePower: Towards a Foundation with Generalizable Knowledge for\n  Architecture-Level\
    \ Power Modeling"
  abstract: "Power efficiency is a critical design objective in modern processor design.\
    \ A\nhigh-fidelity architecture-level power modeling method is greatly needed\
    \ by CPU\narchitects for guiding early optimizations. However, traditional\narchitecture-level\
    \ power models can not meet the accuracy requirement, largely\ndue to the discrepancy\
    \ between the power model and actual design\nimplementation. While some machine\
    \ learning (ML)-based architecture-level power\nmodeling methods have been proposed\
    \ in recent years, the data-hungry ML model\ntraining process requires sufficient\
    \ similar known designs, which are\nunrealistic in many development scenarios.\n\
    \  This work proposes a new power modeling solution FirePower that targets\nfew-shot\
    \ learning scenario for new target architectures. FirePower proposes\nmultiple\
    \ new policies to utilize cross-architecture knowledge. First, it\ndevelops power\
    \ models at component level, and components are defined in a\npower-friendly manner.\
    \ Second, it supports different generalization strategies\nfor models of different\
    \ components. Third, it formulates generalizable and\narchitecture-specific design\
    \ knowledge into two separate models. FirePower also\nsupports the evaluation\
    \ of the generalization quality. In our experiments,\nFirePower can achieve a\
    \ low error percentage of 5.8% and a high correlation R\nof 0.98 on average only\
    \ using two configurations of target architecture. This\nis 8.8% lower in error\
    \ percentage and 0.03 higher in R compared with directly\ntraining McPAT-Calib\
    \ baseline on configurations of target architecture."
  url: http://arxiv.org/abs/2410.17789v1
  keywords: ''
  document: '# FirePower: Towards a Foundation with Generalizable Knowledge for Architecture-Level
    Power Modeling


    Qijun Zhang, Mengming Li, Yao Lu, Zhiyao Xie\* Hong Kong University of Science
    and Technology {qzhangcs, mengming.li, yludf}@connect.ust.hk, eezhiyao@ust.hk


    ## ABSTRACT


    Power efficiency is a critical design objective in modern processor design. A
    high-fidelity architecture-level power modeling method is greatly needed by CPU
    architects for guiding early optimizations. However, traditional architecture-level
    power models can not meet the accuracy requirement, largely due to the discrepancy
    between the power model and actual design implementation. While some machine learning
    (ML)-based architecture-level power modeling methods have been proposed in recent
    years, the data-hungry ML model training process requires sufficient similar known
    designs, which are unrealistic in many development scenarios.


    This work proposes a new power modeling solution FirePower that targets few-shot
    learning scenario for new target architectures. FirePower proposes multiple new
    policies to utilize crossarchitecture knowledge. First, it develops power models
    at component level, and components are defined in a power-friendly manner. Second,
    it supports different generalization strategies for models of different components.
    Third, it formulates generalizable and architecture-specific design knowledge
    into two separate models. FirePower also supports the evaluation of the generalization
    quality. In our experiments, FirePower can achieve a low error percentage of 5.8%
    and a high correlation of 0.98 on average only using two configurations of target
    architecture. This is 8.8% lower in error percentage and 0.03 higher in compared
    with directly training McPAT-Calib baseline on configurations of target architecture.


    ## CCS CONCEPTS


    • Hardware → Power estimation and optimization.


    ## KEYWORDS


    Power model, machine learning


    ## 1 INTRODUCTION


    Power efficiency is a critical design objective for processor design. With the
    increasing design complexity, it takes significant time and effort in power optimization.
    As a result, a fast and highly accurate architecture-level power modeling method
    is greatly needed for


    ASPDAC ''25, January 20–23, 2025, Tokyo, Japan


    <https://doi.org/10.1145/3658617.3697554>


    <span id="page-0-0"></span>![](_page_0_Figure_16.jpeg)


    ![](_page_0_Figure_17.jpeg)


    Figure 1: Proposed power modeling paradigm FirePower vs. existing architecture-specific
    paradigm. FirePower targets few-shot learning for new target architectures. It
    extracts general knowledge from an already known architecture, providing a "foundation"
    to support modeling new architectures.


    early power evaluation prior to RTL implementation. Traditional analytical architecture-level
    power models such as McPAT [\[19\]](#page-7-0) and Wattch [\[9\]](#page-7-1) are
    often inaccurate, largely due to the discrepancy among architecture-level simulator,
    power model, and real target CPU. This has been discussed in many prior works
    [\[26,](#page-7-2) [30\]](#page-7-3). Despite some works [\[13,](#page-7-4) [25\]](#page-7-5)
    updating internal design of analytical power models, they require significant
    human efforts and are primarily developed in-house to cater to proprietary designs.


    In recent years, ML-based architecture-level power modeling methods [\[18,](#page-7-6)
    [30–](#page-7-3)[32\]](#page-7-7) have been explored and demonstrated better accuracy
    by calibrating analytical models with ML models. However, as Fig. [1\(](#page-0-0)a)
    shows, most ML-based power models are developed for a specific architecture, and
    are only applicable to new configurations under exactly the same architecture.
    For example, most prior works train and test power models on BOOM CPUs [\[33\]](#page-7-8)
    only. Training and testing are performed on different BOOM configurations, sharing
    obvious similarities. Building an ML-based power model requires sufficient ground-truth
    power labels of known configurations of target architecture [\[18,](#page-7-6)
    [30,](#page-7-3) [31\]](#page-7-9). For a new project developing a slightly different
    architecture, the whole power model must be retrained from scratch based on a
    brand-new training dataset.


    In practice, for an ongoing project targeting a specific architecture, there are
    not many already accomplished designs available to provide training labels. If
    collecting labels from scratch, the process can be highly expensive: For each
    design configuration, its label collection requires register-transfer level (RTL)
    implementation, synthesis, and simulation with workloads. RTL implementation can
    be especially tedious. A more practical scenario is, only a few design configurations
    under the target architecture are available to provide training labels. This is
    a typical few-shot learning scenario. The existing architecture-specific power
    modeling paradigm suffers from limited accuracy when only a few labels are available.


    <sup>\*</sup>Corresponding Author


    Permission to make digital or hard copies of all or part of this work for personal
    or classroom use is granted without fee provided that copies are not made or distributed
    for profit or commercial advantage and that copies bear this notice and the full
    citation on the first page. Copyrights for components of this work owned by others
    than the author(s) must be honored. Abstracting with credit is permitted. To copy
    otherwise, or republish, to post on servers or to redistribute to lists, requires
    prior specific permission and/or a fee. Request permissions from permissions@acm.org.


    <sup>©</sup> 2025 Copyright held by the owner/author(s). Publication rights licensed
    to ACM. ACM ISBN 979-8-4007-0635-6/25/01


    Motivated by the limitation of architecture-specific methods, we propose to extract
    general knowledge that can be applied across architectures. This is possible since
    different architectures still have similarities. Take out-of-order CPUs as an
    example, despite different design decisions, general design principles are similar,
    leading to partially similar power characteristics. However, separating general
    and specific knowledge at the architecture level is challenging since there is
    no clear standard of general or architecture-specific part of power. To the best
    of our knowledge, no prior ML power models have explicitly explored this topic
    at the architecture level.


    In this work, we propose a two-phase power modeling paradigm named FirePower,
    as shown in Fig. [1\(](#page-0-0)b). (1) The first phase is named knowledge extraction.
    This first phase can be time-consuming, but it is performed only once and by us,
    FirePower developers. Developers will first collect data on one known architecture
    (e.g., BOOM CPU series). Sufficient power labels will be collected for multiple
    configurations with good coverage for the design space. The framework will extract
    generalizable knowledge from this architecture. This general knowledge will provide
    a foundation for the few-shot power modeling tasks on other architectures. (2)
    The second phase will utilize the foundation with general knowledge to build the
    power model for each new target architecture. With only a few available configurations
    of the target architecture (i.e., few-shot), the power model significantly outperforms
    existing architecturespecific models, which have to be trained from scratch.


    The contributions of this work can be summarized below.


    - We analyze the limitation of the architecture-specific power modeling paradigm
    when applied to different target architectures. Then we propose a new paradigm
    FirePower[1](#page-1-0) that targets the few-shot learning scenario for new target
    architectures by different users.

    - To the best of our knowledge, FirePower is the first datadriven architecture-level
    power model that explores crossarchitecture design knowledge. This exploration
    contributes to the understanding of the gap among architectures. This framework
    is fully automated, without requiring additional designer knowledge about any
    target architecture.

    - FirePower proposes multiple new policies to use cross-architecture knowledge.
    1) It develops models at component level, and components are defined in power-friendly
    manner. 2) It supports different generalization strategies for different components.
    3) It formulates generalizable and architecturespecific design knowledge into
    two separate models.

    - To detect the risk of huge differences between known and target architectures,
    FirePower supports the evaluation of the generalization quality for any target
    architecture. This helps provide the applicable scope of the method.

    - We evaluate FirePower using two widely adopted opensource RISC-V CPU designs:
    BOOM and XiangShan. It demonstrates that FirePower can achieve a low MAPE of 5.8%
    and a high correlation of 0.98 on average only using two configurations of the
    target architecture. It achieves 8.8% lower MAPE and 0.03 higher compared with
    directly training McPAT-Calib on configurations of the target architecture.


    ## 2 RELATED WORK


    Standard power estimation flow includes RTL implementation, logic synthesis, RTL
    simulation, and power simulation [\[23,](#page-7-10) [24\]](#page-7-11). In recent
    years, design-specific ML power models have been proposed [\[11,](#page-7-12)
    [12,](#page-7-13) [14](#page-7-14)[–16,](#page-7-15) [20,](#page-7-16) [22,](#page-7-17)
    [27,](#page-7-18) [28,](#page-7-19) [34\]](#page-7-20) for RTL stage using RTL
    signals as input. However, they still require RTL implementation, and a new model
    needs to be developed from scratch for each design. For pre-RTL stage, an accurate
    architecture-level power model is greatly needed.


    Architecture-level power modeling takes architecture-level hardware parameters
    (denoted as ) and event statistics (denoted as ) as input to calculate power.
    Hardware parameters are the parameters used to describe the CPU configuration,
    such as ℎ ℎ and ℎ . Event statistics are the information generated by running
    the workloads on the architecture-level performance simulator, such as the number
    of cache hits and branch instructions.


    Traditional analytical architecture-level power models like Mc-PAT [\[19\]](#page-7-0)
    calculate energy consumption for each event based on hardware parameters, then
    divide the accumulation of them by the execution time to calculate power. Wattch
    [\[9\]](#page-7-1) also adopts a similar method. It calculates the power of each
    cycle by accumulating the energy consumption for each event in this cycle and
    then dividing it by the time of a cycle. However, because of the discrepancy among
    the architecture-level performance simulator, the power model, and the actual
    CPU design, these analytical models are often inaccurate.


    To deal with the inaccuracy of the analytical power model, datadriven ML power
    models have been proposed in recent years [\[30–](#page-7-3) [32\]](#page-7-7).
    One of the representative data-driven architecture-level power models is McPAT-Calib
    [\[30\]](#page-7-3). McPAT-Calib trains an ML model to calibrate McPAT output
    towards power labels. Denoting the output of McPAT as , the McPAT-Calib can be
    formulated below.


    $$P = F\_{\mathfrak{m}I}(H, E, M)$$


    This ML model tries to capture the mapping from these features to the power, such
    mapping is obviously different for different architectures. In this case, it needs
    to be retrained from scratch when applied to a new architecture. The work of [\[31\]](#page-7-9)
    uses the same formulation but performs transfer learning to a new domain of the
    same architecture design, where a domain means configurations with the same ℎ.
    Therefore, it [\[31\]](#page-7-9) still doesn''t support cross-architecture power
    modeling. The PANDA [\[32\]](#page-7-7) proposes human-crafted resource functions
    to achieve few-shot learning where the known configuration is limited. However,
    designing the resource function requires significant engineering expertise for
    each target architecture. This is not an automated solution.


    ## 3 PROBLEM FORMULATION


    Here we introduce problem formulation. FirePower developer starts with a known
    architecture. The design space of the known architecture is already well-explored
    by the developer so there are sufficient known configurations. The power simulation
    of these configurations has also been performed with multiple workloads. The collection
    of the dataset is denoted as D.


    When applying FirePower, there are multiple in-progress projects that need to
    build power models for their own target architectures at a low cost. For these
    ongoing design projects, there are only a few available configurations of target
    architectures. The dataset with a


    <span id="page-1-0"></span><sup>1</sup> It is open-sourced at https://github.com/hkust-zhiyao/FirePower


    few configurations of target architecture and corresponding power labels is denoted
    as D. In experiments, we test three scenarios when there are 2, 3, or 4 available
    configurations in D.


    Our goal is to facilitate power modeling of these in-progress projects based on
    limited data of their target architectures D. FirePower achieves this in two phases:
    knowledge extraction and application. In the knowledge extraction phase, the developer
    extracts generalizable knowledge based on known architecture D. In the application
    phase, the knowledge is generalized to help develop power models for target architectures
    based on D.


    ## 4 METHODOLOGY


    ## 4.1 Power Model Overview


    The general FirePower solution is based on two basic insights.


    Insight 1. Instead of directly modeling total power, architecturelevel power should
    be modeled for each power-friendly component. Specifically, the whole design will
    be partitioned into multiple common components for power modeling purposes. Individual
    power models will be developed for each component, and total power is a summation
    of all component power. This brings multiple benefits: 1) compared with total
    power, individual components altogether provide more power labels; 2) designers
    have the flexibility to define components in a power-friendly manner, which is
    introduced in Section [4.2;](#page-2-0) and 3) smaller common components are affected
    by very few hardware parameters, thus component power model tends to be simple
    thus more general. It is further discussed in Insight 2.


    Insight 2. When developing data-driven architecture-level power models, we observe
    that some knowledge tends to be more general, while others are more architecture-specific.
    Here more general knowledge refers to correlation patterns between basic component
    hardware parameters and component hardware scale, which describes the overall
    amount of logic (e.g., number of logic gates) in the component. One important
    reason is, the number of hardware parameters of each component is very limited,
    ranging from 1 to 4 in our experiment. Patterns based on fewer hardware parameters
    tend to be simpler and thus less "overfit" to a specific architecture. In contrast,
    knowledge related to event statistics involves not only complex event activities
    but also the interaction between events and hardware scale. This makes patterns
    related to event statistics relatively complex. We thus set this part architecture-specific.


    Inspired by two aforementioned insights, FirePower chooses com-ponent-level power
    modeling, with each component''s power decoupled into generalizable and architecture-specific
    parts. With hardware parameters of the -th component as and event statistics as
    , the FirePower power model can be formulated below.


    $$P^{\bar{l}} = F\_{\bar{h}\text{sw}}{}^{\bar{l}}(H\_{\bar{l}}) \ast F\_{\text{overent}}{}^{\bar{l}}(H\_{\bar{l}},
    E\_{\bar{l}})$$


    The denotes the hardware model of component, which learns the basic correlation
    between hardware scale and hardware parameters. The denotes the event model, an
    ML model to capture more complex correlations related to event statistics. As
    for why multiplication is used to associate these two models, power consumption
    is roughly proportional to hardware scale (e.g., number of logic gates) in , assuming
    a constant average toggle rate. The event-related knowledge captured by partially
    reflects


    ![](_page_2_Figure_10.jpeg)


    <span id="page-2-1"></span>Figure 2: The illustration of our power-friendly component
    definition for the out-of-order CPU core.


    | Component 𝑖     | Hardware Parameters of                     | Important        |  |  |  |  |  |

    |-----------------|--------------------------------------------|------------------|--|--|--|--|--|

    |                 | Each Component 𝐻𝑖                          | Parameter        |  |  |  |  |  |

    | BPTAGE          | FetchWidth, BranchCount                    | FetchWidth       |  |  |  |  |  |

    | BPBTB           | FetchWidth, BranchCount                    | FetchWidth       |  |  |  |  |  |

    | BPOthers        | FetchWidth, BranchCount                    | FetchWidth       |  |  |  |  |  |

    | IFU             | FetchWidth, DecodeWidth,                   |                  |  |  |  |  |  |

    |                 | FetchBufferEntry, ICacheFetchBytes         | –                |  |  |  |  |  |

    | I-TLB           | ICacheTLBEntry                             | –                |  |  |  |  |  |

    | ICacheTagArray  | ICacheWay, ICacheFetchBytes                | DCache/ICacheWay
    |  |  |  |  |  |

    | ICacheDataArray | ICacheWay, ICacheFetchBytes                | FetchWidth       |  |  |  |  |  |

    | ICacheOthers    | ICacheWay, ICacheFetchBytes                | –                |  |  |  |  |  |

    | RNU             | DecodeWidth                                | DecodeWidth      |  |  |  |  |  |

    | ROB             | DecodeWidth, RobEntry                      | –                |  |  |  |  |  |

    | FP ISU          | DecodeWidth, FpIssueWidth                  | –                |  |  |  |  |  |

    | Int ISU         | DecodeWidth, IntIssueWidth,                | DecodeWidth      |  |  |  |  |  |

    | Mem ISU         | DecodeWidth, MemIssueWidth                 | –                |  |  |  |  |  |

    | Regfile         | DecodeWidth, IntPhyRegister, FpPhyRegister | –                |  |  |  |  |  |

    | FU Pool         | Mem/FpIssueWidth, IntIssueWidth            | Mem/FpIssueWidth
    |  |  |  |  |  |

    | LSU             | LDQEntry, STQEntry, MemIssueWidth          | –                |  |  |  |  |  |

    | D-TLB           | DCacheTLBEntry                             | DTLBEntry        |  |  |  |  |  |

    | DCacheTagArray  | DCacheWay, DCacheTLBEntry,                 |                  |  |  |  |  |  |

    |                 | MemIssueWidth                              | –                |  |  |  |  |  |

    |                 | DCacheWay, DCacheTLBEntry,                 | –                |  |  |  |  |  |

    | DCacheDataArray | MemIssueWidth                              |                  |  |  |  |  |  |

    | DCacheMSHR      | MSHREntry                                  | MSHREntry        |  |  |  |  |  |

    | DCacheOthers    | DCacheWay, DCacheTLBEntry,                 | –                |  |  |  |  |  |

    |                 | MSHREntry, MemIssueWidth                   |                  |  |  |  |  |  |

    | Other Logic     | All                                        | –                |  |  |  |  |  |


    Table 1: Our identified architecture-level hardware parameters and the important
    parameter detected for Retraining.


    the toggle rate of the real workload. The toggle rate is also proportional to
    power. Multiplication is the simplest operator to capture the linear relationship
    between these two models and power.


    The remainder of this section will introduce the FirePower methodology in detail.
    Section [4.2](#page-2-0) describes our proposed power-friendly component definition
    for power modeling at the component level. Section [4.3](#page-3-0) and Section
    [4.4](#page-3-1) will introduce the knowledge extraction (Phase 1) and application
    (Phase 2) of FirePower, respectively.


    ## <span id="page-2-0"></span>4.2 Power-Friendly Component Definition


    To facilitate component-level data-driven power modeling, we propose a power-friendly
    component definition for out-of-order CPU. The component definition has two targets:
    being common and finegrained. 1) To generalize power model to different microarchitecture
    designs, the component definition should be common, where each component can be
    found in different out-of-order CPUs. 2) To facilitate per-component power modeling,
    component definition should be fine-grained so that the circuit in the same component
    should correlate with similar hardware parameters and event statistics.


    To meet the two targets above, we propose a power-friendly component definition.
    The components are in three main parts: Frontend, Execution, and Mem Access. Details
    are introduced below.


    • The Frontend includes 8 components: TAGE in branch predictor (BPTAGE), BTB in
    branch predictor (BPBTB), others in branch predictor (BPOthers), instruction fetch
    unit (IFU), instruction translation lookup buffer (I-TLB), instruction


    <span id="page-3-2"></span>![](_page_3_Figure_0.jpeg)


    Figure 3: Correlation between power and the most related hardware parameter. The
    two components correlate with different hardware parameters. DCacheDataArray correlates
    with ℎ , DCacheMSHR correlates with .


    cache tag array (ICacheTagArray), instruction cache data array (ICacheDataArray),
    and others in instruction cache (ICacheOthers).


    - The Execution consists of 7 components: renaming unit (RNU), reorder buffer
    (ROB), issue unit of float point instruction (Fp ISU), issue unit of integer instruction
    (Int ISU), issue unit of memory access instruction (Mem ISU), register file (Regfile),
    and function unit pool (FU Pool).

    - The Mem Access has 6 components: load-store unit (LSU), data translation lookup
    buffer (D-TLB), miss status handle register (DCacheMSHR), data cache tag array
    (DCacheTagArray), data cache data array (DCacheDataArray), and others in data
    cache (DCacheOthers).

    - Other circuits not covered by the above components are referred to as a new
    component named Other Logic.


    Table [1](#page-2-1) shows the associated hardware parameters of each component,
    where event statistics are not listed because of page limitation.


    No existing architecture-level ML power modeling works built component-level models
    except PANDA [\[32\]](#page-7-7). The component power model in PANDA [\[32\]](#page-7-7)
    is based on default component partitioning without considering power. For example,
    in [\[32\]](#page-7-7), the DCache is a whole component, but within this DCache,
    we can find that the DCacheDataArray highly correlates with a hardware parameter
    ℎ , while DCacheMSHR correlates with another hardware parameter . The correlation
    is illustrated in Fig. [3](#page-3-2) using the XiangShan CPU [\[29\]](#page-7-21),
    showing their correlations with different hardware parameters. They are thus separated
    and processed with different power models in FirePower to capture clearer patterns.
    It conforms to the aforementioned fine-granularity target.


    ## <span id="page-3-0"></span>4.3 Phase 1: Knowledge Extraction


    In phase 1 of FirePower, developers perform knowledge extraction, extracting the
    generalizable knowledge based on sufficient data of an already known architecture.
    This process only needs to be performed once by solution developers, as shown
    in the left of Fig. [4.](#page-3-3) Based on the known architecture, generalizable
    knowledge is extracted for each component, including two types of information:
    (1) hardware model ℎ built on the known architecture, (2) the importance of the
    hardware parameters of this component.


    4.3.1 Hardware Model. The hardware model learns the relationship between hardware
    scale and hardware parameters, as introduced in the overview. To represent the
    hardware scale as labels,


    <span id="page-3-3"></span>![](_page_3_Figure_11.jpeg)


    Figure 4: The FirePower framework with two phases. Knowledge extraction in phase
    1 extracts hardware model and parameter importance from a known architecture as
    general knowledge. Application in phase 2 adopts two knowledge generalization
    strategies, Retraining and No Retraining, depending on the parameter importance
    distribution.


    we calculate the average power across all workloads. This average power label
    reflects the general power characteristics across workloads. The input features,
    as summarized in Table [1,](#page-2-1) are hardware parameters of each component.
    The ML model we use is the XGBoost [\[10\]](#page-7-22), which is one of the most
    widely adopted regressors. It is a decision-tree-based ensemble learning algorithm
    using an ensemble of weak prediction models for regression.


    4.3.2 Parameter Importance. In addition to the hardware model, we will further
    evaluate the importance of hardware parameters for each component. Such importance
    reflects the impact of each hardware parameter on the component power. The distribution
    of parameter importance will help us analyze the power correlation pattern for
    each component: The key idea is to evaluate whether there is a dominating hardware
    parameter for each component. It will affect how to generalize the knowledge in
    phase 2. More details about the usage of parameter importance are discussed in
    Sec. [4.4.1.](#page-4-0)


    The hardware parameter importance is calculated based on the hardware model. Such
    a tree-model-based evaluation calculates feature importance based on impurity
    decreases contributed by each feature (e.g., parameter) [\[8\]](#page-7-23). Importance
    evaluation is not limited to tree models, there are also some methods, such as
    SHAP [\[21\]](#page-7-24), to evaluate parameter importance for arbitrary ML models.


    ## <span id="page-3-1"></span>4.4 Phase 2: Application


    The application as phase 2 of FirePower is shown in the right of Fig. [4.](#page-3-3)
    Compared to knowledge extraction, which is performed only once in total by the
    developer, the application phase can be applied many times to different projects
    and target architectures. In each project, phase 2 utilizes the extracted generalizable
    knowledge and the limited data of the target architecture to build the power model
    for the target architecture. Specifically, the application phase performs three
    steps: (1) apply the generalizable knowledge to build the hardware model , (2)
    train the event model , and (3) evaluate the generalization to estimate the generalization''s
    quality.


    <span id="page-4-0"></span>4.4.1 Hardware Model. For each component, we support
    two knowledge generalization strategies, named Retraining and No Retraining, as
    illustrated in Fig. [4.](#page-3-3)


    Strategy 1: No Retraining is straightforward, it directly adopts hardware model
    trained on known architecture dataset D for target one. Directly applying hardware
    model may not accurately estimate average power due to differences in hardware
    scale of different architectures. The rationale behind No Retraining is hardware
    model primarily captures the correlation rather than the absolute average power
    value. The ratio between known and target architecture will be captured by additional
    event model .


    Strategy 2: For Retraining, it trains a brand new hardware model using limited
    available configurations D of the target architecture. Such a retraining faces
    a trade-off. On the one hand, available configurations directly from target architecture
    naturally help transfer. On the other hand, since available configurations in
    D are limited, the model can easily overfit. To avoid overfitting, the key idea
    behind retraining is to maximally simplify new hardware model . This retrained
    hardware model will be a linear model based on one most important parameter from
    . Retraining policy is more suitable for components with relatively simple power
    characteristics with one dominating important parameter.


    Strategy selection: For each component, we select the most appropriate strategy
    (Retraining vs. No Retraining) based on parameter importance distribution. Such
    parameter importance is also the knowledge extracted from known architecture in
    phase 1. When only one hardware parameter has a dominating importance, it indicates
    the overall power correlation is simple. On the contrary, if the distribution
    is more uniform, it means this component is relatively complex. Therefore, if
    maximum parameter importance exceeds a threshold[2](#page-4-1) , Retraining will
    be adopted. Otherwise, No Retraining is selected. The strategy selection result
    based on BOOM CPU architecture is listed in Table [1,](#page-2-1) where the important
    parameter used for Retraining is listed, and the "–" means No Retraining.


    4.4.2 Event Model. The event model will mainly capture the more complex correlation
    related to event statistics, which is highly architecture-specific. To train the
    event model for target architectures, for each component, we take both hardware
    parameters and event statistics of each component as features. The event model''s
    training label is / , which is the ratio between the component power label in
    D and the hardware model . The adopted ML model is also XGBoost [\[10\]](#page-7-22).


    # <span id="page-4-2"></span>4.5 Generalization Quality Evaluation


    The effectiveness of knowledge generalization can be compromised when there is
    a significant difference between the target architecture and the known architecture.
    Hence, it is crucial to evaluate the generalization''s quality to help determine
    whether to accept generalized power model or resort to time-consuming traditional
    power modeling paradigm. Such quality evaluation helps indicate applicable scope
    of FirePower based on the known architecture.


    For such generalization evaluation on each component, we compare average power
    labels of each target configuration from D with predictions of hardware model
    from phase 1, without considering retraining. Such reflects known architecture
    <sup>D</sup>, and thus the comparison indicates the difference between known and
    target architecture. When comparing, we adjust prediction of hardware model by
    multiplying it with an ideal scaling factor, since hardware model only captures
    the trend and detailed ratios are left for event models , as discussed in Sec.
    [4.4.1.](#page-4-0)


    ## 5 EXPERIMENT SETUP


    # 5.1 RISC-V CPU Cores for Experiment


    In our experiment, we adopt two different RISC-V CPU cores as our experimented
    architectures. RISC-V [\[5\]](#page-7-25) is one of the most widely adopted open-source
    instruction set architecture. Nowadays, most open-source CPU design projects are
    based on RISC-V, the most representative two projects of which are BOOM [\[33\]](#page-7-8)
    and Xiang-Shan [\[29\]](#page-7-21). BOOM and XiangShan are both highly configurable,
    enabling us to generate different configurations for each architecture. They are
    both out-of-order CPU cores with similar major CPU components but there are also
    many differences. Although they both use RISC-V, the version is not the same,
    with RV64GC for BOOM and RV64GCBK for XiangShan. Besides, architectural designs
    for components are different. Taking the second-level branch predictor as an example,
    BOOM adopts traditional BTB for target prediction, while XiangShan adopts Fetch
    Target Buffer to replace BTB. Considering the reasonable similarities and differences
    between BOOM and XiangShan, we evaluate our paradigm on them.


    When evaluating each method, we conduct multiple experiments with different known/target
    architecture settings and different numbers of configurations of target architecture.
    Because of limited accessible open-source RISC-V CPU architectures, there is only
    one target architecture. Known/target architecture settings include BOOM as known
    architecture and XiangShan as target architecture denoted as BOOM→XS and vice
    versa denoted as XS→BOOM. For different numbers of configurations of target architecture,
    in knowledge extraction (Phase 1), all known architecture configurations are used.
    For application (Phase 2), configurations of target architecture are used for
    knowledge generalization, and remaining ones are used for testing. We evaluate
    the accuracy with mean absolute percentage error (MAPE) and correlation coefficient
    .


    Configurations adopted for BOOM and XiangShan are listed in Table [2,](#page-5-0)
    covering different scales. There are 15 configurations for BOOM named B1 to B15
    and 10 for XiangShan named X1 to X10. To reflect the scenario where only a few
    configurations of target architecture are accessible, the number of labeled configurations
    of target architecture is set as 4, 3, and 2 for three sets of experiments.


    ## 5.2 Design Implementation Flow


    In our experiment, to collect the dataset, RTL code generation and RTL simulation
    of BOOM CPU [\[33\]](#page-7-8) is performed with Chipyard [\[6\]](#page-7-26)
    v1.8.1, and that of XiangShan CPU [\[29\]](#page-7-21) is performed with OpenXiangShan
    [\[4\]](#page-7-27). For workload-driven power simulation to generate


    <span id="page-4-1"></span><sup>2</sup>We set the threshold to 0.95 in the experiment,
    with the sum of all hardware parameters in each component normalized to 1.


    <span id="page-5-0"></span>


    | Hardware Parameter | B1 | B2 | B3 | B4 | B5 | B6 | B7 | B8  | B9  | B10 | B11
    | B12 | B13 | B14 | B15 | X1 | X2 | X3 | X4 | X5 | X6 | X7 | X8  | X9  | X10 |

    |--------------------|----|----|----|----|----|----|----|-----|-----|-----|-----|-----|-----|-----|-----|----|----|----|----|----|----|----|-----|-----|-----|

    | FetchWidth         | 4  | 4  | 4  | 4  | 4  | 8  | 8  | 8   | 8   | 8   | 8   |
    8   | 8   | 8   | 8   | 4  | 4  | 4  | 4  | 4  | 8  | 8  | 8   | 8   | 8   |

    | DecodeWidth        | 1  | 1  | 1  | 2  | 2  | 2  | 3  | 3   | 3   | 4   | 4   |
    4   | 5   | 5   | 5   | 2  | 2  | 2  | 3  | 3  | 3  | 4  | 4   | 4   | 5   |

    | FetchBufferEntry   | 5  | 8  | 16 | 8  | 16 | 24 | 18 | 24  | 30  | 24  | 32  |
    40  | 30  | 35  | 40  | 8  | 16 | 24 | 16 | 24 | 24 | 24 | 32  | 32  | 24  |

    | RobEntry           | 16 | 32 | 48 | 64 | 64 | 80 | 81 | 96  | 114 | 112 | 128
    | 136 | 125 | 130 | 140 | 16 | 32 | 48 | 64 | 64 | 80 | 81 | 96  | 114 | 112 |

    | IntPhyRegister     | 36 | 53 | 68 | 64 | 80 | 88 | 88 | 110 | 112 | 108 | 128
    | 136 | 108 | 128 | 140 | 36 | 53 | 68 | 64 | 80 | 88 | 88 | 110 | 112 | 108 |

    | FpPhyRegister      | 36 | 48 | 56 | 56 | 64 | 72 | 88 | 96  | 112 | 108 | 128
    | 136 | 108 | 128 | 140 | 36 | 53 | 68 | 64 | 80 | 88 | 88 | 110 | 112 | 108 |

    | LDQ/STQEntry       | 4  | 8  | 16 | 12 | 16 | 20 | 16 | 24  | 32  | 24  | 32  |
    36  | 24  | 32  | 36  | 16 | 20 | 24 | 20 | 24 | 28 | 24 | 32  | 40  | 32  |

    | BranchCount        | 6  | 8  | 10 | 10 | 12 | 14 | 14 | 16  | 16  | 18  | 20  |
    20  | 18  | 20  | 20  | 7  | 7  | 7  | 7  | 7  | 7  | 7  | 7   | 7   | 7   |

    | Mem/FpIssueWidth   | 1  | 1  | 1  | 1  | 1  | 1  | 1  | 1   | 2   | 1   | 2   |
    2   | 2   | 2   | 2   | 2  | 2  | 2  | 2  | 2  | 2  | 2  | 2   | 2   | 2   |

    | IntIssueWidth      | 1  | 1  | 1  | 1  | 2  | 2  | 2  | 3   | 3   | 4   | 4   |
    4   | 5   | 5   | 5   | 2  | 2  | 2  | 2  | 4  | 4  | 4  | 6   | 6   | 6   |

    | DCache/ICacheWay   | 2  | 4  | 8  | 4  | 4  | 8  | 8  | 8   | 8   | 8   | 8   |
    8   | 8   | 8   | 8   | 4  | 4  | 8  | 4  | 4  | 8  | 8  | 8   | 8   | 8   |

    | DTLBEntry          | 8  | 8  | 16 | 8  | 8  | 16 | 16 | 16  | 32  | 32  | 32  |
    32  | 32  | 32  | 32  | 8  | 8  | 16 | 8  | 8  | 16 | 16 | 16  | 32  | 32  |

    | MSHREntry          | 2  | 2  | 4  | 2  | 2  | 4  | 4  | 4   | 4   | 4   | 4   |
    8   | 8   | 8   | 8   | 2  | 2  | 4  | 2  | 2  | 4  | 4  | 4   | 4   | 4   |

    | ICacheFetchBytes   | 2  | 2  | 2  | 2  | 2  | 4  | 4  | 4   | 4   | 4   | 4   |
    4   | 4   | 4   | 4   | 2  | 2  | 2  | 2  | 2  | 2  | 2  | 2   | 2   | 2   |

    |                    |    |    |    |    |    |    |    |     |     |     |     |     |     |     |     |    |    |    |    |    |    |    |     |     |     |


    Table 2: The CPU configurations used in our experiment. The B1-B15 denote the
    15 configurations of BOOM, and the X1-X10 denote the 10 configurations of XiangShan.


    ground truth power, we used eight workloads in riscv-tests [\[3\]](#page-7-28)
    including dhrystone, median, multiply, qsort, rsort, towers, spmv, and vvadd.
    Minor modifications are made for adaptation on XiangShan.


    The RTL simulation is performed with Synopsys VCS® [\[2\]](#page-7-29). We performed
    logic synthesis and power simulation with Synopsis Design Compiler® [\[1\]](#page-7-30)
    and PrimePower [\[24\]](#page-7-11) respectively. Our VLSI flow is based on TSMC
    40nm standard cell library and associated Memory Compiler for SRAM generation.
    For the microarchitecture simulation, we use gem5 [\[7\]](#page-7-31) as performance
    simulator to generate event statistics. We also use McPAT [\[19\]](#page-7-0)
    as power model to generate power estimation as some of features for McPAT-Calib.


    ## 5.3 Summary of Baseline Methods


    We compare FirePower with the state-of-the-art architecture-level power model
    McPAT-Calib [\[30\]](#page-7-3) as our baseline. The other work [\[32\]](#page-7-7)
    is not included since it is not an automated method, requiring engineer-defined
    functions. Besides McPAT-Calib [\[30\]](#page-7-3), we further include four more
    ablation studies based on part of FirePower''s policies. (1) The McPAT-Calib +
    Component. It builds ML models for each component, using the associated hardware
    parameters and event statistics as features and per-component power as labels.
    (2) The McPAT-Calib + Transfer Learning. It adopts the McPAT-Calib as the power
    modeling method, builds a model on known architecture as the source model, and
    then adopts one of the most widely adopted transfer learning algorithms, pseudo
    label [\[17\]](#page-7-32), for knowledge generalization. In detail, for each
    testing data of the target architecture, we search for the nearest labeled data
    of the target architecture using the distance in feature space, where the label
    is . We use the source model to make predictions for the testing data and its
    nearest sample, denoted as and . The prediction on test data is = . (3) The McPAT-Calib
    + Component + Transfer Learning. It combines (1) and (2), performing transfer
    learning for each component respectively. (4) FirePower without Retraining. It
    only adopts the No Retraining as the knowledge generalization strategy, without
    taking the parameter importance as the generalizable knowledge. For a fair comparison,
    for all baselines and our FirePower solution, we adopt the same XGBoost [\[10\]](#page-7-22),
    which is the best ML model reported in McPAT-Calib [\[30\]](#page-7-3), with default
    hyperparameters, i.e. n\_estimator=100 and depth=3.


    <span id="page-5-1"></span>![](_page_5_Figure_6.jpeg)


    ![](_page_5_Figure_7.jpeg)


    # 6 EXPERIMENTAL RESULTS


    # 6.1 Power Modeling Accuracy


    Fig. [5](#page-5-1) summarizes comparisons between FirePower with our baseline,
    McPAT-Calib, and four ablation studies, under different numbers of available configurations
    of target architecture, i.e. 4, 3, and 2 configurations. Fig. [6](#page-6-0) further
    visualize detailed results for Fire-Power and McPAT-Calib with only 2 available
    configurations, where samples of the same configuration are in the same color.
    The comparison with McPAT-Calib shows that FirePower can consistently achieve
    superior accuracy over McPAT-Calib regardless of scenarios. FirePower achieves
    the lowest MAPE and the highest correlation coefficient , with at most (on average)
    11.5% (7%) lower MAPE and 0.04 (0.03) higher correlation compared with McPAT-Calib.
    With only two configurations of target architecture, FirePower can still achieve
    a low MAPE of 5.8% and a high correlation of 0.98 on average, which is 8.8% lower
    in error percentage and 0.03 higher in compared with McPAT-Calib. The superiority
    of FirePower over McPAT-Calib is contributed by its ability to generalize knowledge


    <span id="page-6-0"></span>![](_page_6_Figure_0.jpeg)


    Figure 6: Accuracy comparison between FirePower and McPAT-Calib (Available Config
    of Target Arch = 2).


    acquired from known architecture. In contrast, architecture-specific McPAT-Calib
    trains model from scratch.


    Fig. [5](#page-5-1) also shows FirePower can constantly achieve the best accuracy
    compared with four ablation studies for both MAPE and correlation . McPAT-Calib
    + Component is an enhanced version of McPAT-Calib by building models for each
    component. It has an advantage over McPAT-Calib, validating the effect of powerfriendly
    component definition. However, it can still not achieve a high accuracy compared
    with FirePower. It verifies knowledge generalization is critical to enable few-shot
    power modeling.


    McPAT-Calib + Transfer Learning and McPAT-Calib + Component + Transfer Learning
    are two knowledge generalization methods based on transfer learning. They directly
    transfer the power model as a whole, regardless of generality. Results in Fig.
    [5](#page-5-1) show that they outperform McPAT-Calib and McPAT-Calib + Component
    in many scenarios. It demonstrates that information from other known architecture
    can improve accuracy of few-shot modeling. But FirePower still has an obvious
    advantage over them, which is because these two methods do not decouple general
    and architecture-specific knowledge. It verifies the necessity of decoupling generalizable
    and architecture-specific knowledge, where generalizing architecture-specific
    knowledge has negative impact.


    Fig. [5](#page-5-1) also shows that "FirePower without Retraining" is more accurate
    than McPAT-Calib + Component, also outperforming two transfer-learning-based methods
    in some scenarios. This validates that even generalization with No Retraining
    can also work. Comparison between FirePower and FirePower without Retraining verifies
    that the Retraining and strategy selection are crucial. The parameter importance
    is essential generalizable knowledge for FirePower.


    ## 6.2 Generalization Evaluation


    Fig. [7](#page-6-1) illustrates the generalization evaluation for components with
    high similarity between known and target architecture. Fig. [8](#page-6-2) shows
    the lower-similarity one. Higher similarity is supposed to result in a high generalization
    quality, and vice versa.


    <span id="page-6-1"></span>![](_page_6_Figure_8.jpeg)


    (c) OtherLogic (Accessible Config of Target) (d) OtherLogic (All Config of Target)


    Figure 7: Generalization quality evaluation for components with high similarity
    across architectures. (a)(c) The generalization qualities observed with the accessible
    configuration of the target architecture. (b)(d) The generalization qualities
    evaluated with all configurations of the target architecture.


    <span id="page-6-2"></span>![](_page_6_Figure_11.jpeg)


    (a) LSU (Accessible Config of Target) (b) LSU (All Config of Target) Figure 8:
    Generalization quality evaluation for components with low similarity across architectures.
    (a) The generalization quality observed with the accessible configuration of the
    target architecture. (b) The generalization quality evaluated with all configurations
    of the target architecture.


    In Fig. [7](#page-6-1) and [8,](#page-6-2) each point represents a configuration
    of the target architecture. The x-axis is the golden average power across workloads.
    The y-axis is the adjusted prediction of the hardware model trained on known architecture,
    which is discussed in Sec. [4.5.](#page-4-2) The similarity between these two
    values indicates generalization quality, which can be measured with MAPE. Fig.
    [7\(](#page-6-1)b)(d) and Fig. [8\(](#page-6-2)b) show actual generalization quality
    with all configurations of the target architecture, which is not accessible when
    building the model. Fig. [7\(](#page-6-1)a)(c) and Fig. [8\(](#page-6-2)a) show
    generalization quality that we can observe with limited accessible configurations
    of target architecture.


    We can find the actual generalization quality evaluated with all configurations
    in Fig. [7\(](#page-6-1)b)(d) and Fig. [8\(](#page-6-2)b) correlates with generalization
    quality measured with the accessible ones in Fig. [7\(](#page-6-1)a)(c) and Fig.
    [8\(](#page-6-2)a). This means architects can roughly estimate the quality with
    the generalization quality observed with the accessible configurations. Generally,
    as illustrated in these figures, if the generalization quality observed with the
    accessible configurations has


    a MAPE lower than 10%, it suggests a relatively high actual generalization quality
    indicating that it can result in a high-quality generalized model. Conversely,
    if MAPE exceeds this threshold, the generalization may result in a low-quality
    model. In experiments, we always adopt generalized hardware model, because power
    percentage of evaluated low-similarity components is relatively small.


    ## 7 CONCLUSION


    We propose FirePower which targets few-shot learning scenario for new target architectures
    by different users. Developer extracts the generalizable knowledge from a well-developed
    architecture, and then multiple projects can use this knowledge for few-shot power
    modeling, with limited available configurations of target architectures. The foundation-based
    paradigm reduces data requirement significantly, which is a compelling addition
    to architects'' toolbox.


    ## ACKNOWLEDGEMENT


    This work is partially supported by National Natural Science Foundation of China
    62304192, and ACCESS – AI Chip Center for Emerging Smart Systems, sponsored by
    InnoHK funding, Hong Kong SAR. We acknowledge the suggestions from Dr. Andrea
    Mondelli.


    ## REFERENCES


    - <span id="page-7-30"></span>[1] 2021. Design Compiler® RTL Synthesis. https://www.synopsys.com/
    implementation-and-signoff/rtl-synthesis-test/design-compiler-nxt.html.

    - <span id="page-7-29"></span>[2] 2021. VCS® functional verification solution.
    https://www.synopsys.com/ verification/simulation/vcs.html.

    - <span id="page-7-28"></span>[3] 2022. RISC-V Tests. https://github.com/riscv-software-src/riscv-tests.

    - <span id="page-7-27"></span><span id="page-7-25"></span>[4] 2023. OpenXiangShan.
    https://github.com/OpenXiangShan.

    - <span id="page-7-26"></span>[5] 2023. RISC-V. https://riscv.org.

    - [6] Alon Amid, David Biancolin, Abraham Gonzalez, Daniel Grubb, Sagar Karandikar,
    Harrison Liew, Albert Magyar, Howard Mao, Albert Ou, Nathan Pemberton, et al.
    2020. Chipyard: Integrated design, simulation, and implementation framework for
    custom socs. IEEE Micro 40, 4 (2020), 10–21.

    - <span id="page-7-31"></span>[7] Nathan Binkert, Bradford Beckmann, Gabriel Black,
    Steven K Reinhardt, Ali Saidi, Arkaprava Basu, Joel Hestness, Derek R Hower, Tushar
    Krishna, Somayeh Sardashti, et al. 2011. The gem5 simulator. ACM SIGARCH computer
    architecture news 39, 2 (2011), 1–7.

    - <span id="page-7-23"></span>[8] Leo Breiman. 2001. Random forests. Machine learning
    (2001).

    - <span id="page-7-1"></span>[9] David Brooks, Vivek Tiwari, and Margaret Martonosi.
    2000. Wattch: A framework for architectural-level power analysis and optimizations.
    ACM SIGARCH Computer Architecture News 28, 2 (2000), 83–94.

    - <span id="page-7-22"></span>[10] Tianqi Chen and Carlos Guestrin. 2016. Xgboost:
    A scalable tree boosting system. In Proceedings of the 22nd acm sigkdd international
    conference on knowledge discovery and data mining. 785–794.

    - <span id="page-7-12"></span>[11] Wenji Fang, Yao Lu, Shang Liu, Qijun Zhang,
    Ceyu Xu, Lisa Wu Wills, Hongce Zhang, and Zhiyao Xie. 2023. MasterRTL: A Pre-Synthesis
    PPA Estimation Framework for Any RTL Design. In 2023 IEEE/ACM International Conference
    on Computer Aided Design (ICCAD). IEEE, 1–9.

    - <span id="page-7-13"></span>[12] Wenji Fang, Yao Lu, Shang Liu, Qijun Zhang,
    Ceyu Xu, Lisa Wu Wills, Hongce Zhang, and Zhiyao Xie. 2024. Transferable Pre-Synthesis
    PPA Estimation for RTL Designs With Data Augmentation Techniques. IEEE Transactions
    on Computer-Aided Design of Integrated Circuits and Systems (2024).

    - <span id="page-7-4"></span>[13] Abdullah Guler and Niraj K Jha. 2020. McPAT-Monolithic:
    An area/power/timing architecture modeling framework for 3-D hybrid monolithic
    multicore systems. IEEE Transactions on Very Large Scale Integration (VLSI) Systems
    28, 10 (2020), 2146–2156.

    - <span id="page-7-14"></span>[14] Donggyu Kim, Jerry Zhao, Jonathan Bachrach,
    and Krste Asanović. 2019. Simmani: Runtime power modeling for arbitrary RTL with
    automatic signal selection. In Proceedings of the 52nd Annual IEEE/ACM International
    Symposium on Microarchitecture. 1050–1062.

    - [15] Ajay Krishna Ananda Kumar, Sami Al-Salamin, Hussam Amrouch, and Andreas
    Gerstlauer. 2022. Machine learning-based microarchitecture-level power modeling
    of CPUs. IEEE Trans. Comput. 72, 4 (2022), 941–956.

    - <span id="page-7-15"></span>[16] Ajay Krishna Ananda Kumar and Andreas Gerstlauer.
    2019. Learning-based CPU power modeling. In 2019 ACM/IEEE 1st Workshop on Machine
    Learning for CAD (MLCAD). IEEE, 1–6.

    - <span id="page-7-32"></span>[17] Dong-Hyun Lee et al. 2013. Pseudo-label: The
    simple and efficient semisupervised learning method for deep neural networks.
    In Workshop on challenges in representation learning, ICML, Vol. 3. Atlanta, 896.

    - <span id="page-7-6"></span>[18] Wooseok Lee, Youngchun Kim, Jee Ho Ryoo, Dam
    Sunwoo, Andreas Gerstlauer, and Lizy K John. 2015. PowerTrain: A learning-based
    calibration of McPAT power models. In 2015 IEEE/ACM International Symposium on
    Low Power Electronics and Design (ISLPED). IEEE, 189–194.

    - <span id="page-7-0"></span>[19] Sheng Li, Jung Ho Ahn, Richard D Strong, Jay
    B Brockman, Dean M Tullsen, and Norman P Jouppi. 2009. McPAT: An integrated power,
    area, and timing modeling framework for multicore and manycore architectures.
    In Proceedings of the 42nd annual ieee/acm international symposium on microarchitecture
    (MICRO). 469–480.

    - <span id="page-7-16"></span>[20] Yao Lu, Qijun Zhang, and Zhiyao Xie. 2024.
    Unleashing Flexibility of ML-based Power Estimators Through Efficient Development
    Strategies. In Proceedings of the 29th ACM/IEEE International Symposium on Low
    Power Electronics and Design. 1–6.

    - <span id="page-7-24"></span>[21] Scott M Lundberg and Su-In Lee. 2017. A unified
    approach to interpreting model predictions. Advances in neural information processing
    systems 30 (2017).

    - <span id="page-7-17"></span>[22] Jian Peng, Tingyuan Liang, Zhiyao Xie, and
    Wei Zhang. 2023. PROPHET: Predictive On-Chip Power Meter in Hardware Accelerator
    for DNN. In 2023 60th ACM/IEEE Design Automation Conference (DAC). IEEE, 1–6.

    - <span id="page-7-10"></span>[23] Siemens. 2023. PowerPro RTL Low-Power. [https://www.mentor.com/hls](https://www.mentor.com/hls-lp/powerpro-rtl-low-power/)[lp/powerpro-rtl-low-power/](https://www.mentor.com/hls-lp/powerpro-rtl-low-power/)

    - <span id="page-7-11"></span>[24] Synopsys. 2023. PrimePower: RTL to Signoff
    Power Analysis. [https://www.](https://www.synopsys.com/implementation-and-signoff/signoff/primepower.html)
    [synopsys.com/implementation-and-signoff/signoff/primepower.html](https://www.synopsys.com/implementation-and-signoff/signoff/primepower.html)

    - <span id="page-7-5"></span>[25] Aoxiang Tang, Yang Yang, Chun-Yi Lee, and Niraj
    K Jha. 2014. McPAT-PVT: Delay and power modeling framework for FinFET processor
    architectures under PVT variations. IEEE Transactions on Very Large Scale Integration
    (VLSI) Systems 23, 9 (2014), 1616–1627.

    - <span id="page-7-2"></span>[26] Sam Likun Xi, Hans Jacobson, Pradip Bose, Gu-Yeon
    Wei, and David Brooks. 2015. Quantifying sources of error in McPAT and potential
    impacts on architectural studies. In 2015 IEEE 21st International symposium on
    high performance computer architecture (HPCA). IEEE, 577–589.

    - <span id="page-7-18"></span>[27] Zhiyao Xie, Shiyu Li, Mingyuan Ma, Chen-Chia
    Chang, Jingyu Pan, Yiran Chen, and Jiang Hu. 2022. DEEP: Developing extremely
    efficient runtime on-chip power meters. In Proceedings of the 41st IEEE/ACM International
    Conference on Computer-Aided Design. 1–9.

    - <span id="page-7-19"></span>[28] Zhiyao Xie, Xiaoqing Xu, Matt Walker, Joshua
    Knebel, Kumaraguru Palaniswamy, Nicolas Hebert, Jiang Hu, Huanrui Yang, Yiran
    Chen, and Shidhartha Das. 2021. APOLLO: An automated power modeling framework
    for runtime power introspection in high-volume commercial microprocessors. In
    MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture. 1–14.

    - <span id="page-7-21"></span>[29] Yinan Xu, Zihao Yu, Dan Tang, Guokai Chen,
    Lu Chen, Lingrui Gou, Yue Jin, Qianruo Li, Xin Li, Zuojun Li, et al. 2022. Towards
    developing high performance RISC-V processors using agile methodology. In 2022
    55th IEEE/ACM International Symposium on Microarchitecture (MICRO). IEEE, 1178–1199.

    - <span id="page-7-3"></span>[30] Jianwang Zhai, Chen Bai, Binwu Zhu, Yici Cai,
    Qiang Zhou, and Bei Yu. 2022. McPAT-Calib: A RISC-V BOOM microarchitecture power
    modeling framework. IEEE Transactions on Computer-Aided Design of Integrated Circuits
    and Systems (TCAD) 42, 1 (2022), 243–256.

    - <span id="page-7-9"></span>[31] Jianwang Zhai, Yici Cai, and Bei Yu. 2023. Microarchitecture
    Power Modeling via Artificial Neural Network and Transfer Learning. In 2023 28th
    Asia and South Pacific Design Automation Conference (ASPDAC).

    - <span id="page-7-7"></span>[32] Qijun Zhang, Shiyu Li, Guanglei Zhou, Jingyu
    Pan, Chen-Chia Chang, Yiran Chen, and Zhiyao Xie. 2023. PANDA: Architecture-level
    power evaluation by unifying analytical and machine learning solutions. In 2023
    IEEE/ACM International Conference on Computer Aided Design (ICCAD). IEEE, 01–09.

    - <span id="page-7-8"></span>[33] Jerry Zhao, Ben Korpan, Abraham Gonzalez, and
    Krste Asanovic. 2020. Sonicboom: The 3rd generation berkeley out-of-order machine.
    In Fourth Workshop on Computer Architecture Research with RISC-V, Vol. 5.

    - <span id="page-7-20"></span>[34] Yuan Zhou, Haoxing Ren, Yanqing Zhang, Ben
    Keller, Brucek Khailany, and Zhiru Zhang. 2019. PRIMAL: Power inference using
    machine learning. In Proceedings of the 56th Annual Design Automation Conference
    2019. 1–6.'
- title: 'ARAS: An Adaptive Low-Cost ReRAM-Based Accelerator for DNNs'
  abstract: "Processing Using Memory (PUM) accelerators have the potential to perform\
    \ Deep\nNeural Network (DNN) inference by using arrays of memory cells as computation\n\
    engines. Among various memory technologies, ReRAM crossbars show promising\nperformance\
    \ in computing dot-product operations in the analog domain.\nNevertheless, the\
    \ expensive writing procedure of ReRAM cells has led\nresearchers to design accelerators\
    \ whose crossbars have enough capacity to\nstore the full DNN. Given the tremendous\
    \ and continuous increase in DNN model\nsizes, this approach is unfeasible for\
    \ some networks, or inefficient due to the\nhuge hardware requirements. Those\
    \ accelerators lack the flexibility to adapt to\nany given DNN model, facing an\
    \ challenge.\n  To address this issue we introduce ARAS, a cost-effective ReRAM-based\n\
    accelerator that employs a smart scheduler to adapt different DNNs to the\nresource-limited\
    \ hardware. ARAS also overlaps the computation of a layer with\nthe weight writing\
    \ of several layers to mitigate the high writing latency of\nReRAM. Furthermore,\
    \ ARAS introduces three optimizations aimed at reducing the\nenergy overheads\
    \ of writing in ReRAM. Our key optimization capitalizes on the\nobservation that\
    \ DNN weights can be re-encoded to augment their similarity\nbetween layers, increasing\
    \ the amount of bitwise values that are equal or\nsimilar when overwriting ReRAM\
    \ cells and, hence, reducing the amount of energy\nrequired to update the cells.\
    \ Overall, ARAS greatly reduces the ReRAM writing\nactivity. We evaluate ARAS\
    \ on a popular set of DNNs. ARAS provides up to 2.2x\nspeedup and 45% energy savings\
    \ over a baseline PUM accelerator without any\noptimization. Compared to a TPU-like\
    \ accelerator, ARAS provides up to 1.5x\nspeedup and 61% energy savings."
  url: http://arxiv.org/abs/2410.17931v1
  keywords: ''
  document: '# ARAS: An Adaptive Low-Cost ReRAM-Based Accelerator for DNNs


    Mohammad Sabri, Marc Riera, Antonio Gonzalez ´ Universitat Politecnica de Catalunya
    (UPC) ` mohammad.sabri@upc.edu, marc.riera.villanueva@upc.edu, antonio@ac.upc.edu


    *Abstract*—Processing Using Memory (PUM) accelerators have the potential to perform
    Deep Neural Network (DNN) inference by using arrays of memory cells as computation
    engines. Among various memory technologies, ReRAM crossbars show promising performance
    in computing dot-product operations in the analog domain. Nevertheless, the expensive
    writing procedure of ReRAM cells has led researchers to design accelerators whose
    crossbars have enough capacity to store the full DNN. Given the tremendous and
    continuous increase in DNN model sizes, this approach is unfeasible for some networks,
    or inefficient due to the huge hardware requirements. Those accelerators lack
    the flexibility to adapt to any given DNN model, facing an *adaptability* challenge.


    To address this issue we introduce ARAS, a cost-effective ReRAM-based accelerator
    that employs a smart scheduler to adapt different DNNs to the resource-limited
    hardware. ARAS also overlaps the computation of a layer with the weight writing
    of several layers to mitigate the high writing latency of ReRAM. Furthermore,
    ARAS introduces three optimizations aimed at reducing the energy overheads of
    writing in ReRAM. Our key optimization capitalizes on the observation that DNN
    weights can be re-encoded to augment their similarity between layers, increasing
    the amount of bitwise values that are equal or similar when overwriting ReRAM
    cells and, hence, reducing the amount of energy required to update the cells.
    Overall, ARAS greatly reduces the ReRAM writing activity. We evaluate ARAS on
    a popular set of DNNs. ARAS provides up to 2.2× speedup and 45% energy savings
    over a baseline PUM accelerator without any optimization. Compared to a TPU-like
    accelerator, ARAS provides up to 1.5× speedup and 61% energy savings.


    #### I. INTRODUCTION


    <span id="page-0-0"></span>Processing Using Memory (PUM) is a promising approach
    to mitigate the high computational and energy costs associated with the inference
    of Deep Neural Network (DNN) models. Various Non-Volatile Memory (NVM) technologies,
    including Phase-Change Memory (PCM) [\[24\]](#page-12-0), Spin-Transfer Torque
    Magnetic RAM (STT-MRAM) [\[13\]](#page-11-0), and Resistive-RAM (ReRAM) [\[38\]](#page-12-1),
    can efficiently implement the pivotal dot-product operations required for DNNs
    in the analog domain. Among these, ReRAM technology stands out with its lower
    read latency and higher density, making it a suitable candidate for designing
    DNN accelerators. Therefore, a growing body of work [\[5\]](#page-11-1), [\[6\]](#page-11-2),
    [\[51\]](#page-12-2) is exploring ReRAM crossbars due to their dense and efficient
    analog computation capabilities.


    Despite the promising features of ReRAM, its high write latency and write energy
    consumption pose severe limitations in designing efficient DNN accelerators. As
    a result, prior works have proposed to process multiple DNN inferences on a pipelined
    mode to mitigate the writing overheads of ReRAM [\[1\]](#page-11-3), [\[2\]](#page-11-4),
    [\[17\]](#page-11-5), [\[29\]](#page-12-3), [\[34\]](#page-12-4), [\[36\]](#page-12-5),
    [\[37\]](#page-12-6), [\[44\]](#page-12-7), [\[53\]](#page-12-8). Besides, these
    accelerators are designed under the assumption of having enough ReRAM crossbars
    to accommodate all network weights, that is, the writing of DNN weights is only
    done once at the beginning of the first inference. While this approach addresses
    the inefficiency of ReRAM writing, it also introduces a significant issue that
    impedes the future use of ReRAM-based accelerators as a prominent compute platform.


    The rapid growth in the complexity of DNNs has led to a dramatic increase in network
    model sizes every year [\[19\]](#page-11-6), [\[20\]](#page-12-9). To illustrate
    it, AlexNet [\[22\]](#page-12-10) had 144MB of parameters in 2012, while the BERT-Large
    model [\[7\]](#page-11-7), introduced in 2018, comprises a substantial 1.3GB of
    parameters, and Chat-GPT 3.5 [\[3\]](#page-11-8), released in 2023, pushed the
    boundaries to 800GB. Current ReRAM-based accelerators face an *adaptability* issue
    because the crossbars they employ today may not be sufficient for handling larger
    networks in the future. In addition, scaling the resources of an accelerator to
    the biggest DNN is not the most effective solution in terms of area and power.
    Similar to other systems, such as TPUs and GPUs, ReRAM-based accelerators should
    be flexible and capable of efficiently executing any type of DNN, regardless of
    its size, using limited resources. Most accelerators are typically initialized
    for a specific DNN model and lack mechanisms for efficiently updating weights
    to accommodate different models at runtime. This issue hinders the widespread
    use of ReRAMbased accelerators, especially in server environments where there
    are continuous requests to perform inference for different DNN models. Hence,
    a practical ReRAM-based accelerator must address this *adaptability* challenge
    to ease its adoption.


    In this paper, we present ARAS, an Adaptive low-cost ReRAM-based Accelerator for
    DNN inference. ARAS main novelty is an offline scheduler and its corresponding
    hardware support that includes efficient mechanisms for programming DNN weights
    into a limited-size accelerator at runtime. The execution of a DNN model involves
    various tasks, such as data transfers, weight updating, and dot-product computations,
    among others, which must be properly scheduled in the accelerator to efficiently
    manage its resources. To the best of our knowledge, this is the first work aimed
    at devising a scheduler capable of optimizing the utilization of a ReRAM compute
    engine. ARAS performs one DNN inference at a time and employs a new execution
    approach, supporting a broad range of different layers and DNN models. ARAS prioritizes
    the allocation of available ReRAM crossbars for the concurrent weight updating
    of upcoming layer(s). It then conducts the associated computations sequentially,
    layer-by-layer, due to data dependencies across layers, iterating until all required
    computations for the network are completed. This approach enables the accelerator
    to methodically process large networks even with constrained resources, maximizing
    efficiency and resource utilization.


    The weight updating task has a significant impact on both performance and energy
    consumption due to the high writing latency and write energy costs of ReRAM. Consequently,
    ARAS incorporates a set of optimizations to counter the inherent inefficiencies
    of ReRAM writes. To mitigate these overheads, ARAS employs a scheduling strategy
    that overlaps the latency of computing a layer with the latency of writing weights
    for the next layer(s). In addition, the accelerator includes a multi-banked global
    buffer with banks of different sizes whose purpose is to reduce static energy
    consumption. Furthermore, ARAS employs an adaptive weight replication scheme to
    improve the execution of long compute latency layers such as some convolutions.
    This scheme is designed to reduce latency in critical layers by performing computations
    for multiple convolution steps simultaneously. Our approach requires to search
    for a replication factor that finds the best trade-off between the latency reduction
    and the cost of replicating the weights. Finally, ARAS leverages the concept of
    partial weight reuse when updating ReRAM cells to write new layer weights. Given
    that each weight is represented by multiple ReRAM cells, weight reuse refers to
    the equality or similarity of the current value of a cell and the new value to
    be overwritten in it. ARAS aims to increase partial reuse across weights between
    consecutive layers, avoiding ReRAM cell updates for certain bits of weights, and
    saving the corresponding energy required for writing.


    To summarize, this work focuses on designing ReRAM accelerators that are adaptable
    to efficiently execute any present and future DNN, regardless of its model size.
    The main contributions are:


    - We have designed an adaptive accelerator that enables concurrent neural computation
    and network weight updates. ARAS incorporates an offline scheduler that orchestrates
    the execution of layers, prioritizing the writing of new weights as soon as ReRAM
    crossbars finish their assigned computations.

    - We employ a heterogeneous multi-banking scheme that partitions the on-chip memory
    responsible for storing the DNN''s input and output activations. This scheme aims
    to select the smallest possible banks based on the capacity required to store
    the activations of each layer, reducing static energy by 3% on average.

    - We introduce an adaptive weight replication scheme to enhance the accelerator''s
    throughput by concurrently computing multiple convolution steps. ARAS determines
    the best replication factor taking into account the overheads of additional weight
    writes and the speedup that can be achieved in each layer, allocating ReRAM crossbars
    based on the computation latency and available crossbars.

    - We develop a novel partial weight reuse scheme with the goal of maximizing the
    partial bit-level similarity among weights of different layers. This scheme allows
    us to skip


    ![](_page_1_Figure_7.jpeg)


    <span id="page-1-0"></span>Fig. 1. Crossbar architecture (a) and analog operation
    with ReRAM cells (b).


    the updating of cells when the bits to be overwritten are identical to the new
    ones, thus reducing the number of ReRAM cells that are updated. Our key observation
    reveals that aligning the mean of layers'' weights to a common value enables efficient
    reuse of a substantial portion of the most significant bits, reducing ReRAM writing
    activity by 17% on average.


    • We evaluate ARAS on five representative DNNs. On Average, ARAS''s novel scheduler
    and its associated optimizations improve performance and reduce energy by 1.5×
    and 28%, respectively. Compared to a TPU-like accelerator, ARAS achieves, on average,
    1.2× speedup and 33% reduction in energy consumption.


    # II. BACKGROUND AND RELATED WORK


    # <span id="page-1-2"></span><span id="page-1-1"></span>*A. ReRAM Cell and Crossbar
    Architecture*


    ReRAM cells are two-terminal devices (see Figure [1\(](#page-1-0)b)) able to represent
    values through their analog multilevel conductance states. In DNN accelerators,
    ReRAM cells are used to store the network weights. The process of encoding or
    writing these weights (weight updates) relies on transitions between various conductance
    states in the ReRAM cells, typically initiated by electrical inputs. The physical
    mechanism governing these conductance transitions may vary for different types
    of ReRAM insulators [\[26\]](#page-12-11), [\[30\]](#page-12-12), [\[33\]](#page-12-13),
    [\[50\]](#page-12-14). Generally, the conductance of ReRAM cells can be augmented
    or reduced using positive and negative programming voltage pulses, denoted as
    weight increase and weight decrease, respectively.


    The most compact and simple structure for creating a DNN weight matrix with ReRAM
    cells is the crossbar array configuration, in which each 1T1R cell is positioned
    at every intersection point. In addition, crossbar structures offer the advantage
    of achieving high integration density [\[4\]](#page-11-9). ReRAM crossbars serve
    a dual purpose by storing the weights and executing the dot-product operations
    between inputs and weights. As depicted in Figure [1,](#page-1-0) when the input
    vector is encoded using read voltage signals, the weighted sum operation (equivalent
    to matrix-vector multiplication) can be executed in parallel using the weights
    stored in the crossbar. The results of this weighted sum are obtained at the end
    of each column, represented as analog currents, and post-processed by additional
    peripherals.


    ![](_page_2_Figure_0.jpeg)


    <span id="page-2-0"></span>Fig. 2. Example of a Row-by-Row writing scheme. The
    gray cell keeps its value in both steps, while the red and green cell values are
    modified.


    #### <span id="page-2-2"></span>*B. ReRAM Writing*


    Gao et al. [\[9\]](#page-11-10) introduced a fully parallel writing scheme to
    update weights in ReRAM crossbars. However, programming all the cells in the array
    simultaneously might demand more peak power than the peripheral circuits can supply.
    As a result, a row-by-row writing scheme [\[4\]](#page-11-9) is typically employed
    for the weight update process, as shown in Figure [2.](#page-2-0) The weight increase
    and decrease operations require distinct programming voltage polarities, so the
    weight update process is divided into two steps, each with its specific voltage
    polarity for every row. In each step, the SLs (Source Lines) provide voltage pulses
    or constant voltages (if no update) to modify each selected cell, while the BL
    (Bit Line) provides the required polarity for that particular step. Ideally, selecting
    the entire row at once is the most effective approach to ensure maximum parallelism.
    However, it is possible that only a portion of a row is selected at a time if
    the write circuitry cannot handle the substantial write current required for the
    entire row. It is important to note that in ReRAM cells, the update of a cell''s
    value is achieved by applying specific pulses through the associated SL, and the
    number of pulses required is determined by the current cell value and the desired
    target value. Consequently, in a crossbar array, each SL (crossbar column) is
    equipped with its independent driver to deliver distinct pulses.


    Current commercial ReRAM storage chips are primarily designed for single or few-time
    writes, resulting in relatively low endurance compared to other memory technologies.
    This underscores a notable gap in practical ReRAM cell development that is well-suited
    for PUM applications. However, there is a growing body of research that developed
    different insulators to enhance endurance cycles to levels around 10<sup>11</sup>
    and 10<sup>12</sup> [\[14\]](#page-11-11), [\[21\]](#page-12-15), [\[25\]](#page-12-16),
    [\[49\]](#page-12-17), [\[52\]](#page-12-18). Additionally, some research [\[15\]](#page-11-12)
    indicates the potential for ReRAM endurance of up to 10<sup>15</sup>, although
    achieving it may be a long-term goal.


    #### *C. DNN Mapping*


    DNN mapping [\[10\]](#page-11-13), [\[34\]](#page-12-4), [\[55\]](#page-12-19)
    has a significant impact on the performance and energy consumption of ReRAM-based
    accelerators. ARAS employs a conventional scheme, shown in Figure [3,](#page-2-1)
    where each kernel of a CONV layer is unrolled into a single column of the ReRAM
    crossbar, which may be further partitioned into multiple arrays depending on the
    number of weights and the number of bits per weight. Then, a window of activations
    is also unrolled and applied to all the kernels in a single step. In the next
    step, the window slides over, and the


    ![](_page_2_Figure_7.jpeg)


    <span id="page-2-1"></span>Fig. 3. Conventional mapping of a CONV layer in ReRAM
    crossbars.


    new activations are streamed bit-serially to the crossbar again. Similarly, a
    FC layer is a special case of CONV layer.


    #### <span id="page-2-3"></span>*D. Weight Replication*


    Weight replication is a key technique used in prior ReRAMbased accelerators [\[29\]](#page-12-3),
    [\[46\]](#page-12-20). This technique is mainly used in Convolutional Neural Networks
    (CNNs) and aims to boost throughput by replicating the weights of convolution
    layers in ReRAM crossbars, computing multiple activation windows simultaneously.
    The advantages of replication are particularly pronounced in the initial layers
    of CNNs, which have relatively small weight matrices but a vast number of activation
    windows. Therefore, for these layers, given that the kernel sets are small, the
    replication costs are modest, and the improvement in throughput is substantial.
    In previous works [\[1\]](#page-11-3), [\[34\]](#page-12-4), replication follows
    a greedy scheme: as long as there are available resources, the layer with the
    lowest throughput is chosen for replication. However, it is crucial to balance
    the replication factor for each layer. An inappropriate replication factor could
    adversely impact performance and energy efficiency. This is because it may lead
    to additional writes in accelerators where resources are limited, including both
    ReRAM crossbars for weight storage and on-chip buffers for partial results.


    #### III. ARAS ACCELERATOR


    #### *A. Architecture*


    In this section, we present the ARAS architecture. Previous ReRAM-based accelerators
    [\[34\]](#page-12-4), [\[42\]](#page-12-21), [\[44\]](#page-12-7) assume that
    all DNN weights are pre-stored in the ReRAM crossbars. These designs often employ
    a deep pipeline capable of executing multiple DNN inferences concurrently. However,
    such an approach demands substantial amount of resources, including both ReRAM
    crossbars and on-chip SRAM buffers, which is not the most effective solution in
    the long term. In contrast, ARAS operates with limited resources, adopting a layer-bylayer
    execution that overlaps the computation of a given layer with the procedure of
    writing weights for subsequent layers. Figure [4](#page-3-0) shows a block diagram
    top-down view of the architecture of the ARAS accelerator.


    Figure [4\(](#page-3-0)a) depicts a high-level schematic of the chip architecture.
    A single chip comprises an External IO Interface (Ext-IO), multiple PEs, a Special
    Function Unit (SFU), Accumulation Units (ACC), and a Global Buffer (Gbuffer).
    The Ext-IO is used to communicate with Main Memory (MM) to load the network weights
    and inputs and store its final outputs. The ACC units are responsible for aggregating
    all the partial results of the neural operations for a given layer.


    ![](_page_3_Figure_0.jpeg)


    <span id="page-3-0"></span>Fig. 4. Architecture of the ARAS accelerator including
    the organization of: (a) Chip, (b) Processing Element (PE), and (c) Analog Processing
    Unit (APU).


    These partial results may be generated by various PEs when a layer occupies multiple
    PEs due to its size. SFU performs transitional functions like pooling, non-linear
    activations (i.e. sigmoid or ReLU), and normalization, to support the full range
    of computations required for state-of-the-art DNNs. These components work in concert
    to enable efficient end-to-end DNN inference execution and support a broad spectrum
    of neural network operations.


    The Global Buffer (Gbuffer) serves as a storage unit for intermediate activations
    generated during the execution of a layer. To optimize on-chip memory utilization,
    the Gbuffer is structured with banks of varying sizes. This design allows for
    a variable allocation of memory banks based on the size of the layer being processed.
    Consequently, inefficiency in memory allocation is mitigated. This adaptive approach
    contributes to the overall energy efficiency of ARAS since those banks that do
    not include essential information are power-gated.


    Figure [4\(](#page-3-0)b) presents the structure of a PE, which contains m×n APUs,
    m buffers to store either input activations to process or weights to write, an
    output buffer to store partial sums, n accumulation modules to add the partial
    sums from different APUs, a set of shift registers to serialize the activations,
    and a multiplexer to switch between writing and computing phases. Finally, Figure
    [4\(](#page-3-0)c) shows the main components of an APU including a crossbar array
    to store synaptic weights that is built based on ReRAM cells (1T1R). Each APU
    also includes an input register to store activations, a writing register to store
    weight deltas, a WL/BL switch matrix to drive the wordlines and bitlines, an SL
    switch matrix to drive the sourcelines, an analog multiplexer, a shared pool of
    ADCs, and functional units to accumulate and shift the partial results from different
    BLs and iterations. Section [II](#page-1-1) provides more details on the analog
    dot-product operation with ReRAM crossbars and the writing of ReRAM cells.


    #### *B. Execution Dataflow*


    In this section, we explain the dataflow followed by the ARAS accelerator. Figure
    [5](#page-3-1) illustrates the procedures for writing weights and performing computations.
    Each level of the top-down architecture is represented by a different color.


    *Writing Procedure Dataflow:* The top of Figure [5](#page-3-1) shows the weight
    writing flowchart for a given ReRAM crossbar row. Initially, the deltas of the
    weights of a row are fetched


    ![](_page_3_Figure_8.jpeg)


    <span id="page-3-1"></span>Fig. 5. ARAS execution dataflow. The top flowchart
    shows the weight writing procedure while the bottom corresponds to the computation.


    from main memory. As described in Section [II-B,](#page-2-2) the number of writing
    pulses required to update the value of multi-level ReRAM cells depends on the
    current value of the cell and its next coming value. The difference between current
    and new values is referred to as weight deltas, calculated offline, and directly
    proportional to the energy of writing weights. Then, these deltas are transferred
    through a NoC to the corresponding PE, where the shift registers are bypassed
    to store the deltas directly in the destination buffer. Next, the target APU reads
    deltas from the corresponding buffer and stores them in its Writing Registers.
    Finally, the APU drivers are configured to perform weight updating by decreasing
    and increasing the cell values in two phases (Figure [2\)](#page-2-0). Note that
    the concurrent writing of all cells in a crossbar row entails that the row''s
    writing latency is determined by the cells that exhibit the highest latency in
    each phase, that is, the slowest cells in the row are the bottleneck of the writing
    operation. Besides, ARAS considers the main memory bandwidth as a potential bottleneck
    when many APUs are concurrently updating the weights, limiting the simultaneous
    writing of APUs. In this work, we focus on reducing the magnitude of the weight
    deltas to improve energy consumption, so ReRAM writing latency remains an open
    challenge for future work.


    *Computation Procedure Dataflow:* The bottom of Figure [5](#page-3-1) summarizes
    the compute dataflow for a given convolutional window. First, activations are
    fetched and distributed to the PEs based on the corresponding location of the
    written weights, as shown in Figure [3,](#page-2-1) where all the weights stored
    in a given row of APUs belong to a single layer. Then, activations undergo a serialization
    process within the PEs, the PE controller activates the associated buffer, and
    activations are streamed and stored bit-serially into the buffer. Unlike the work
    in [\[53\]](#page-12-8), the shift register set, which is in charge of serializing
    activations, is shared for different rows of APUs in a PE to mitigate area overhead.
    Therefore, to avoid harming latency, the number of rows of APUs should be limited
    to


    ![](_page_4_Figure_0.jpeg)


    <span id="page-4-0"></span>the throughput of the shift register set. Similar to
    previous works [\[5\]](#page-11-1), [\[44\]](#page-12-7), ARAS iterates the activations
    bit by bit, then performs dot-product operations for each activation bit, and
    finally shifts and accumulates with the previous iterations. Therefore, the total
    number of iterations depends on the activation''s precision. In the last iteration,
    APUs return their part of the neural computation, and the final results related
    to each kernel/filter is computed by accumulating all APUs'' partial results.
    According to the kernels'' dimension and layer mapping, partial results accumulation
    can be performed inside PEs or in the chip accumulator. Finally, the outputs of
    a window passing through activation and pooling functions before stored back to
    the Global Buffer, and the accelerator is configured to compute the next window.


    Note that ARAS takes an efficient approach by allowing PEs to write weights from
    different layers, but computing one layer at a time. This strategy, although more
    complex in terms of control and scheduling, optimizes resource usage, resulting
    in improved throughput and energy efficiency.


    ## IV. ARAS SCHEDULER


    In this section, we present the proposed offline scheduler, which plays a key
    role in managing both computation and weight writing tasks, by efficiently assigning
    the resources needed for each task. Our scheduler follows a systematic layer-by-layer
    computation approach, that is, as soon as one DNN layer''s computation is completed,
    the next layer can start processing. This approach allows ARAS to overlap the
    computations of a layer and the writing of weights of the following layers. Furthermore,
    it leverages the concurrent updating of multiple ReRAM crossbars, aiming to maximize
    the utilization of the resources and minimize the latency associated with the
    writing process.


    # *A. Baseline Scheduler*


    Figure [6](#page-4-0) shows an overview of the offline flow to configure ARAS.
    Initially, the Data Flow Graph is extracted from the PyTorch model, which includes
    the network architecture along with the execution order of the layers and their
    respective weights. In the subsequent stage, the scheduler analyzes the network
    architecture, receives the configuration of the accelerator, and calculates the
    number of PEs and APUs that are required to execute each layer. Next, it runs
    two different procedures to perform the scheduling and binding of tasks simultaneously.
    Finally, the ARAS offline flow provides a sequence of instructions for both updating
    weights, including the deltas, and computing the operations of each layer.


    Figure [7](#page-4-1) introduces a simple naive scheduler that follows a sequential
    layer-by-layer execution pattern. Initially, all


    ![](_page_4_Figure_8.jpeg)


    <span id="page-4-2"></span><span id="page-4-1"></span>Fig. 8. ARAS Baseline Scheduler.


    weights of a particular layer are written into the ReRAM crossbars and, then,
    dot-product operations are carried out for that layer. This sequence continues
    until the last layer is processed. In the event that a layer exceeds the capacity
    of the accelerator, it is divided into smaller segments, each of which is executed
    sequentially. However, this naive scheduler has several limitations as it cannot
    efficiently utilize all the available resources, particularly when dealing with
    small layers that only occupy a fraction of the resources. Furthermore, this approach
    is unable to hide the write latency, since writing a layer''s weights is done
    in series with its computations.


    Conversely, we propose a more sophisticated offline scheduler that has the capability
    to overlap the latency of writing weights with the computations. Our scheduler
    statically fixes the accelerator resource allocation along with the layers'' execution
    order due to the deterministic behavior of DNN inference, avoiding the high-cost
    of doing it online. Figure [8](#page-4-2) shows an example of the ARAS baseline
    scheduler. The ARAS accelerator still conducts computations of one layer at a
    time due to data dependencies between layers, but our scheduler intelligently
    determines when to initiate the writing of layers'' weights based on the availability
    of free resources. Upon completion of a layer''s computation, the accelerator
    releases its resources, and the scheduler assigns those resources for the writing
    of the next layers. Based on the amount of crossbars, the accelerator can write
    the weights of a portion of a layer, an entire layer, or several layers, simultaneously.
    We can observe that the execution latency of the baseline scheduler is reduced
    in comparison to the naive scheduler. The main reason is due to the efficient
    overlap of the writing and computation phases.


    The proposed scheduler takes decisions on how to manage and execute two distinct
    procedures that operate concurrently. The first procedure is responsible for delivering
    activations and configuring the components needed for neural computation. We refer
    to this as the "Computing Scheduling Procedure". In parallel, there is the "Weight
    Writing Scheduling Procedure", which handles the retrieval of weights, delta calculation,
    and mapping into ReRAM crossbars. Both procedures are coordinated and synchronized
    under certain conditions. The scheduler output comprises a list of instructions
    with a specific order, outlining the configuration of ARAS hardware compo-


    ![](_page_5_Figure_0.jpeg)


    <span id="page-5-0"></span>Fig. 9. ARAS Computing (a) and Weight Writing (b) Scheduling
    Procedures.


    nents for doing dot-product operations and writing weights on the ReRAM crossbars.
    This order adheres to a layer-bylayer computation that starts the writing of new
    weights as soon as APUs become available. Next sections elaborate on the description
    of each procedure.


    #### *B. Computing Scheduling Procedure*


    Figure [9\(](#page-5-0)a) outlines the computing procedure of the ARAS scheduler,
    which comprises five distinct states. The procedure starts with an initialization
    state triggered by a user request on a new DNN model or accelerator configuration.
    In this state, the scheduler produces instructions to transfer all network initial
    inputs from main memory to the Gbuffer. In the next state, the scheduler employs
    a heuristic to determine the ideal allocation of banks from the Gbuffer for each
    layer, an optimization discussed later in section [V-A,](#page-5-1) modifying
    the configuration of the ARAS controller to store the activations in the corresponding
    banks. In the subsequent state, ARAS schedules the computations generating the
    instructions for dotproduct operations layer-by-layer, waiting for the binding
    of new weights if necessary. Afterward, in the Release state, APUs that finished
    their assigned computations become free and ARAS starts binding new weights to
    the released APUs. Note that the Computation and Release states for a given layer
    are done back-to-back. Finally, after executing all layers the procedure ends,
    and the produced instructions can be used for multiple executions of the requested
    DNN model, restarting the procedure every time the model or hardware changes.


    #### *C. Weight Writing Scheduling Procedure*


    Concurrently, the Weight Writing Scheduling Procedure, shown in Figure [9\(](#page-5-0)b),
    starts along the computing procedure. First, the scheduler calculates the number
    of PEs and APUs needed for each layer by considering the layer''s topology. In
    previous approaches [\[44\]](#page-12-7), [\[54\]](#page-12-22), it was common
    to assign only the weights of one layer to each Tile/PE due to the complexity
    of managing multiple layers in a single Tile/PE. In contrast, our scheduler can
    efficiently accommodate different layers of weights within a single PE, yet each
    row of APUs in a PE belongs to one layer, since the same activations are broadcast
    to all APUs in the row. Next, ARAS employs an optimization, introduced in section
    [V-C,](#page-7-0) to increase partial weight reuse and reduce energy consumption
    of ReRAM writing.


    Then, the scheduler evaluates the availability of resources. In the following
    state, ARAS benefits from an optimization, described in section [V-B,](#page-6-0)
    that determines a set of candidate layers to write, together with their replication
    factors, according to the available resources. In the binding state, the scheduler
    produces instructions for writing the candidate layers and assigns the corresponding
    resources by binding the APUs. ARAS optimally seizes the opportunity to write
    as many weights as possible whenever there are free resources, which may lead
    to writing only part of a layer when resources are insufficient to accommodate
    all of its weights. The procedure concludes when there are no more weights for
    which to plan their writing. Otherwise, the procedure waits to have available
    resources to continue. Similar to the computing procedure, the writing procedure
    starts again when a new request arrives.


    #### V. ARAS OPTIMIZATIONS


    #### <span id="page-5-1"></span>*A. Adaptive Bank Selection*


    The Global Buffer (Gbuffer) in Figure [4](#page-3-0) is in charge of storing the
    network''s input and intermediate activations. Our Gbuffer is sized to accommodate
    one layer at a time, the one computing, fitting all input and output activations
    from any layer in our set of DNNs. However, if there is a layer that is larger,
    ARAS can partition the activations to process them in multiple iterations, similar
    to when not all the weights of a layer fit in the accelerator crossbars. Typically,
    the Gbuffer is able to accommodate the biggest layer, so only a fraction is actively
    used for most of the small layers. This underutilized storage results in high
    static power consumption.


    To address this issue, we employ a multi-bank storage technique, a strategy commonly
    leveraged in advanced systems. Unlike previous approaches that employed a group
    of homogeneous banks of the same size, our Gbuffer includes banks of varying sizes.
    For each layer, the scheduler identifies the optimal set of banks to accommodate
    the input and output activations. Consequently, during the computation of a layer,
    only the banks required to store its activations are enabled, while the unused
    banks are power-gated. This approach ensures that the smallest set of banks is
    selected for each layer, minimizing static power consumption.


    We formulate the process of identifying the optimal combination of banks for each
    layer as an Integer Linear Programming (ILP) problem with the objective of minimizing
    the Gbuffer static power during the execution of each layer. Equation [1](#page-6-1)
    shows how to compute the static power of the Gbuffer for the execution of a layer
    l, where a<sup>i</sup> is a boolean variable indicating if the ith bank is selected
    to be active or not, and LeakageP ower<sup>i</sup> corresponds to the static power
    of the ith bank. The main goal is to minimize StaticP owergbuf f er(l). In addition,
    Equation [2](#page-6-2) shows the on-chip memory required for storing the activations
    of a layer, where Mem<sup>i</sup> is the storage capacity of the ith bank. Note
    that the banks being used for storing the input activations can not be selected
    for storing the output activations of any given layer, hence, by introducing specific
    constraints, the ILP solver is guided to select the optimal banks.


    <span id="page-6-1"></span>

    $$StaticPower\_{gbuffer}(l) = \sum\_{i=1}^{K} (a\_i \times LeakagePower\_i) \tag{1}$$


    <span id="page-6-2"></span>

    $$RequireedMeM(l) \le \sum\_{i=1}^{K} (a\_i \times Mem\_i) \tag{2}$$


    This optimization takes place after the initial state of the computing scheduling
    procedure. Therefore, once this optimization is completed, the scheduler determines
    the smallest set of banks for each layer. As a result, at runtime, the ARAS controller
    enables the selected banks for each layer, efficiently power-gating the unused
    banks and reducing static power.


    ## <span id="page-6-0"></span>*B. Adaptive Replication Scheme*


    In Section [II-D](#page-2-3) we described how the replication of weights improves
    the throughput of a convolution layer by simultaneously processing multiple activation
    windows. Earlier studies [\[1\]](#page-11-3), [\[44\]](#page-12-7) employed a
    ''greedy'' replication approach aiming to make the best use of all PEs while balancing
    the execution flow. These studies assumed that all weights were pre-loaded onto
    the accelerator, overlooking the latency and energy costs of writing into additional
    ReRAM crossbars at runtime.


    In contrast, ARAS presents an innovative approach introducing a resource-constrained
    accelerator that updates the ReRAM crossbars for every layer, and includes an
    adaptive replication scheme. The ARAS scheduler decides when/how to replicate
    a layer''s weights based on its criticality, in terms of compute latency, and
    the availability of APUs, ensuring that any added latency and energy from replicating
    weights does not penalize the overall improvements.


    Shallow layers in Convolutional Neural Networks (CNNs) often have large input
    feature maps (IFMs) and multiple activation windows, which leads to long computation
    time. However, these layers tend to have a small set of weights, making them prime
    candidates for weight replication to significantly reduce computation time. On
    the other hand, replication improves computation speed at the expense of a greater
    number of writes and occupied APUs. Consequently, a high replication factor may
    delay the writing of weights for subsequent layers, decreasing concurrent writings
    due to the limited availability of APUs, and harming total execution time and
    energy.


    The proposed scheduler of ARAS determines the replication factor for each layer
    during the third state of the Weight Writing Scheduling Procedure. In this state,
    ARAS plans the writing order of the weights of each layer taking into account
    the free rows of APUs in any PE at any given time. The scheduler is also responsible
    for deciding which layers can be written with their respective replication factors.
    Algorithm [1](#page-6-3) presents the replication scheme employed in ARAS. It
    is important to note that in our accelerator all APUs within a PE''s row are exclusively
    allocated to store weights of a specific layer, and they collectively process
    the same input segments but weights of different kernels. If the available resources
    fall short of meeting the requirements for writing the weights of the subsequent
    layer L, only a partial portion of that layer is


    # <span id="page-6-3"></span>Algorithm 1 Weight Replication Scheme


    |     | 1: procedure REPLICATION(#F reeResources, L)                        |  |  |  |  |  |

    |-----|---------------------------------------------------------------------|--|--|--|--|--|

    |     | // ComL(i): Return computation latency of the ith layer.            |  |  |  |  |  |

    |     | // Resources(i): Return number of required resources for ith layer. |  |  |  |  |  |

    | 2:  | if #F reeResources < Resources(L) then                              |  |  |  |  |  |

    | 3:  | Skip Replication                                                    |  |  |  |  |  |

    | 4:  | else if #F reeResources < Resources(L) + Resources(L + 1) then      |  |  |  |  |  |

    | 5:  | Replication(L, #F reeResources)                                     |  |  |  |  |  |

    | 6:  | else                                                                |  |  |  |  |  |

    |     | // Accelerator has enough space for 2 layers or more.               |  |  |  |  |  |

    |     | // Check how many layers can be fitted without replication.         |  |  |  |  |  |

    | 7:  | K = GetNumberOfLayers(L, #F reeResources)                           |  |  |  |  |  |

    | 8:  | while !f inish do                                                   |  |  |  |  |  |

    | 9:  | #F reeResources + = ReleaseResources(L + K − 1)                     |  |  |  |  |  |

    | 10: | ReplicateLongestLayers(#F reeResources)                             |  |  |  |  |  |

    | 11: | if ComL(L + 1) +  + ComL(L + K − 2) ≤ W L then                      |  |  |  |  |  |

    | 12: | f inish = T rue                                                     |  |  |  |  |  |

    | 13: | else                                                                |  |  |  |  |  |

    | 14: | K ← K − 1                                                           |  |  |  |  |  |

    | 15: | end if                                                              |  |  |  |  |  |

    | 16: | end while                                                           |  |  |  |  |  |

    | 17: | end if                                                              |  |  |  |  |  |

    |     | 18: end procedure                                                   |  |  |  |  |  |

    |     |                                                                     |  |  |  |  |  |


    written without any replication (lines 2-3). Conversely, when the resources are
    insufficient to accommodate the concurrent writing of two subsequent layers L
    and L + 1, the scheme tries to replicate layer L (lines 4-5), since only the weights
    of layer L can be entirely written and maybe replicated.


    On the other hand, the proposed scheme adopts an iterative approach (lines 6-17)
    when plenty of rows of APUs within each PE are free. First, the scheduler aims
    to identify the number of consecutive layers K that can be effectively written
    into the accelerator without replication, and then loops to find the best replication
    factors. The objective is to maximize both the amount of concurrently written
    layers and the overall improvement in total execution time. Within each iteration,
    the algorithm identifies the slowest layers and endeavors to enhance its performance
    by introducing replication. This is achieved by delaying the execution of the
    last layer L+K −1 and reallocating its resources to bolster the slowest layers''
    processing. The freed resources are allocated by a function (line 10) that iteratively
    allocates a portion of the available resources to the slowest layer in each iteration
    and continues until all available resources are exhausted. This procedure is repeated
    until the computation latency of all the candidate layers, except layer L, is
    reduced to a point where further replication no longer contributes to higher performance.
    This inflection point is determined by ensuring that the computation latency does
    not exceed the writing latency W L of the following layers, which start to write
    after finishing the computations of layer L and releasing its resources, as exceeding
    this threshold offers no further gains but penalties. Note that the writing latency
    remains relatively constant, whether it involves writing to a single APU or to
    multiple APUs simultaneously, due to the concurrent nature of the ReRAM writing
    process.


    Figure [10](#page-7-1) illustrates the ARAS replication scheme in action. In particular,
    this example shows the replication strategy for VGG16. At the beginning, the accelerator
    has the capacity to allocate resources for writing the weights of the first 9
    layers without any replication. In the first iteration, the algorithm opts to
    delay the execution of L<sup>9</sup> and redistributes its resources to


    ![](_page_7_Figure_0.jpeg)


    <span id="page-7-1"></span>Fig. 10. An example of the replication scheme for the
    VGG16 network.


    enable replication for other layers. This process is repeated for two additional
    iterations, after which a key observation arises: as the computation latency from
    L<sup>2</sup> to L<sup>6</sup> falls below the writing latency of the next layers
    W L, further replication would not yield performance improvements. This is because
    the accelerator must await the completion of weight writing for L<sup>7</sup>
    before processing more layers due to the layer-bylayer computation. Thus, at this
    point ARAS achieves the optimal resource utilization and performance. As illustrated
    in Figure [9,](#page-5-0) the weight replication scheme is triggered by the scheduler
    whenever there are available resources, determining the number of concurrent layers
    that can write next and their respective replication factors. In this particular
    example, upon completing the computations of L1, the resources allocated to it
    are freed, prompting the replication scheme to initiate its operation for the
    following layers.


    #### <span id="page-7-0"></span>*C. Adaptive Partial Weight Reuse*


    As discussed in Section [I,](#page-0-0) ReRAM cells exhibit high energy consumption
    for writing, an inherent characteristic that may offset the advantages of employing
    ReRAM crossbars as a computational engine. Consequently, the creation of a scheme
    focused on alleviating the substantial energy costs associated with weight writing
    to ReRAM cells has the potential to make ReRAM-based DNN accelerators more appealing
    in comparison to established commercial platforms like TPU.


    Expanding upon the details outlined in Section [II-A,](#page-1-2) the process
    of updating a multi-level ReRAM cell, rather than writing an absolute value, involves
    incrementing or decrementing its current value to reach the desired final value.
    Consequently, the energy consumption associated with updating ReRAM cells is strongly
    correlated with the magnitude of the delta between the cell''s current value and
    its next value. The ideal scenario occurs when the current value of the cell matches
    its subsequent value, skipping the need for cell updates and


    ![](_page_7_Figure_6.jpeg)


    Fig. 11. Histogram of quantized weights for the last 5 layers of ResNet-50.


    <span id="page-7-2"></span>![](_page_7_Figure_8.jpeg)


    <span id="page-7-3"></span>saving the corresponding energy. In order to further
    reduce the energy consumption during the writing process, ARAS adopts a strategy
    that emphasizes increasing the equality or similarity of weights across layers
    that overwrite each other into the same crossbars cells. This approach aims to
    minimize the deltas between the values being written in the same ReRAM cells,
    thereby mitigating the energy overhead of updating these cells.


    Due to the inherent characteristics of ReRAM [\[35\]](#page-12-23) cells, most
    ReRAM-based accelerators store each weight using multiple cells. In this work,
    activations and weights are both quantized into 8-bit integers (INT8), while ARAS
    uses a precision of 2 bits per ReRAM cell. Consequently, four cells are required
    to store a single weight. In Figure [11,](#page-7-2) we present the histogram
    of weights of a set of consecutive layers from ResNet-50 after applying uniform
    INT8 quantization. We can observe that the weight distribution across these layers
    exhibits distinct patterns. This variance diminishes the likelihood of achieving
    similarity when overwriting the weights of the preceding layer(s), particularly
    within the cells responsible for storing the Most Significant Bits (MSB) as described
    below.


    Figure [12](#page-7-3) depicts the distribution of the original quantized weights
    (a) for two given layers and an example of shifting (b) those weights to the right.
    After executing layer LX, the associated ReRAM cells are overwritten by the weights
    of layer L<sup>Y</sup> . The horizontal axis of this figure is divided into four
    sections with different colors, each representing a different bitwise value in
    the 4th cell, which contains the two most significant bits of each weight. Furthermore,
    P iX(k) (P i<sup>Y</sup> (k)) shows the probability that the ith cell for each
    weight of L<sup>X</sup> (L<sup>Y</sup> ) has a value of k, where k can be one
    of 00, 01, 10, 11.


    Equation [3](#page-7-4) quantifies the likelihood that the ith ReRAM cell retains
    the same value across layers L<sup>X</sup> and L<sup>Y</sup> . This probability
    directly correlates with the ratio of cell updates that can be skipped during
    the weight writing process, resulting in energy savings for the ReRAM cells. Note
    that the accumulation of P iX(k) for the four possible values is one.


    <span id="page-7-4"></span>

    $$\begin{split} Sim(X,Y,i) &= Pi\_X(00) \times Pi\_Y(00) + Pi\_X(01) \times Pi\_Y(01)
    \\ &+ Pi\_X(10) \times Pi\_Y(10) + Pi\_X(11) \times Pi\_Y(11) \end{split} \quad
    (3)$$


    In modern DNNs, it is common to observe that the four least significant bits of
    layers'' weights, which correspond to the 1st and 2nd ReRAM cells, exhibit a nearly
    uniform distribution [\[1\]](#page-11-3). As a result, according to Equation [3,](#page-7-4)
    the skipping ratio for these two cells is approximately 0.25. In contrast, the
    other two ReRAM cells, responsible for storing the most significant bits, present
    a probability distribution more concentrated in a particular value of k, influencing
    the skipping ratio of the cell. Based on Equation [3,](#page-7-4) the ideal skipping
    ratio for the 3rd and 4th cells is attained when P iX(k) and P i<sup>Y</sup> (k)
    both reach their maximum probability at the same value of k. Consequently, adjusting
    the weights of both layers to align the peaks of P i<sup>X</sup> and P i<sup>Y</sup>
    to the same value k can lead to an increased skipping ratio for these cells.


    Given the bell-shaped distribution of weights in DNNs, the majority of weights
    in a layer tend to cluster around the mean. In order to attain a high skipping
    ratio for the 4th cell, we shift the mean values of weights in both L<sup>X</sup>
    and L<sup>Y</sup> to the midpoint of a specific section, as depicted in red in
    Figure [12\(](#page-7-3)b). This approach ensures that the highest possible number
    of weights in both layers share the same value k in the 4th cell, thereby maximizing
    both P4<sup>X</sup> and P4<sup>Y</sup> , and consequently Sim(X, Y, 4). To optimize
    the skipping ratio for the 3rd cell, we can adjust the mean point of both layers
    in a manner that maximizes both P3<sup>X</sup> and P3<sup>Y</sup> for a value
    of k. Similar to the strategy used for the 4th cell, if each section of Figure
    [12](#page-7-3) is further subdivided into four equal parts, shifting the mean
    of both layers to the middle point of one of these parts maximizes P3<sup>X</sup>
    and P3<sup>Y</sup> , leading to an increased skipping ratio for the 3rd cell.
    However, the positions that maximize the skipping ratio for the 3rd and 4th ReRAM
    cells are distinct, making it difficult to maximize both simultaneously.


    Equations [4](#page-8-0) and [5](#page-8-1) present the shifting formula, where
    W eights(l) is the weight matrix of a given layer l, Offset is the value by which
    all weights within that layer are adjusted, and Center denotes the common mean
    value determined by the scheduler. Since the shifted weights are subject to the
    same numerical precision constraints, weights that after the shifting process
    fall outside the defined numerical precision ranges are clipped to the respective
    boundaries of these ranges. The clipping operation may result in accuracy loss,
    hence, the main objective of ARAS is to identify a specific Center where the weights
    exhibit maximum similarity while incurring in negligible accuracy loss.


    <span id="page-8-0"></span>

    $$NewWeights(l) = clip(Weights(l) +Offset\text{, }0\text{, }2^n - 1) \quad (4)$$


    <span id="page-8-1"></span>

    $$Offset = round(Center - mean(Weight(l)))\tag{5}$$


    In total, there are 20 different centers with the potential to maximize the combined
    skipping ratio of the 3rd and 4th cells. However, many of these centers (e.g.
    32, 224) may lead to a significant accuracy loss due to a high clipping rate.
    To address this, ARAS disregards those centers near the limits of the range, and
    retains only six potential centers, namely (88, 104, 96, 160, 152, 168), where
    there is a chance of maximizing the combined skipping ratio for the 3rd and 4th
    cells with negligible accuracy loss. ARAS systematically evaluates all six centers,
    selecting the one that incurs the lowest accuracy loss and achieves the highest
    skipping ratio. Typically, introducing inaccuracies in the first layer of a DNN
    can lead to a substantial loss in model accuracy [\[11\]](#page-11-14). Therefore,
    ARAS avoids shifting the weights in the first layer and instead relies on the
    use of the original weights.


    The dot-product results must be recomputed before transferring them to the next
    layer in order to account for the effect of the weight-shifting procedure. Equation
    [6](#page-8-2) shows the quantization function, where W<sup>q</sup> and W<sup>f</sup>
    are the quantized and floating-point representations of a given weight respectively,
    and q<sup>w</sup> and zp<sup>w</sup> denote the scaling factor and zero point,
    which are two common parameters of uniform quantization. In addition, Equation
    [7](#page-8-3) illustrates the process of the dot-product operation, where y<sup>f</sup>
    is the FP representation of the dot-product result and i iterates on the number
    of vector elements. In order to remove the impact of adding the Offset to the
    quantized weights, the same Offset value is subtracted from zpw. In most DNN accelerators,
    the output feature maps (results of dot-product operations) are de-quantized before
    being transferred to the next layer. ARAS effectively compensates the shifting
    process by adjusting zp<sup>w</sup> in the de-quantization procedure without introducing
    any additional overhead for recomputing the original dot-product.


    <span id="page-8-2"></span>

    $$W\_q = round(q\_w \times W\_f - zp\_w) \tag{6}$$


    <span id="page-8-3"></span>

    $$y\_f = \sum\_i \frac{x\_q + zp\_x}{q\_x} \times \frac{(w\_q + OOffset) + \overbrace{(zp\_w
    - OOffset)}^{Adjusted \; zp\_w}}{q\_w} + b\_f \quad \text{(7)}$$


    $$\text{VI. } \text{METHOOLOGY}$$


    # VI. METHODOLOGY


    <span id="page-8-4"></span>We have developed an event-driven simulator for ReRAMbased
    DNN accelerators that accurately models ARAS. Moreover, our ARAS scheduler assigns
    the resources and plans the execution order of a given DNN statically. Regarding
    area, latency, and energy consumption evaluation, the ReRAM crossbars are modeled
    using NeuroSim [\[48\]](#page-12-24), while on-chip buffers are characterized
    using CACTI-P [\[28\]](#page-12-25). On the other hand, the logic components are
    implemented in Verilog, and synthesized with Design Compiler [\[47\]](#page-12-26)
    employing the 28/32nm technology library from Synopsys. Regarding main memory,
    we model an LPDDR4 of 8 GB with a bandwidth of 19.2 GB/s (single channel) using
    DRAMSim3 [\[27\]](#page-12-27).


    To compare with a popular computational platform, we select Google''s TPU, which,
    like ARAS, is an adaptive and efficient platform highly optimized for DNN execution.
    We have extended ScaleSim [\[43\]](#page-12-28) to model a TPU-like accelerator
    with the same tools as the ones used to model ARAS. ScaleSim is a simulator for
    modeling systolic array-based accelerators that employ a dataflow model and hardware
    components similar to those found in [\[18\]](#page-11-15), [\[31\]](#page-12-29),
    [\[41\]](#page-12-30). Table [I](#page-9-0) shows the relevant parameters of the
    TPU-like accelerator that we use in this work. Total Data Buffer Size includes
    Activation Buffer and Weight Buffer together. To have a fair comparison, we scale
    the number of MAC units and the data buffer in the TPU-like accelerator to match
    the same area as ARAS (26 mm<sup>2</sup> ). Furthermore, both ARAS and TPU operate
    at the same frequency.


    <span id="page-9-0"></span>TABLE I PARAMETERS OF THE TPU-LIKE ACCELERATOR.


    | Technology             | 32 nm  |

    |------------------------|--------|

    | Frequency              | 1 GHz  |

    | Number of MAC Units    | 64x64  |

    | Total Data Buffer Size | 4.5 MB |

    | Data Precision         | 8-bits |

    |                        |        |


    <span id="page-9-1"></span>TABLE II PARAMETERS FOR ALL THE ARAS ACCELERATOR CONFIGURATIONS.


    | Technology                   | 32 nm         |

    |------------------------------|---------------|

    | Frequency                    | 1 GHz         |

    | Number of ADCs per APUs      | 16            |

    | ADCs Sampling Precision      | 6-bits        |

    | PE Buffers Size              | 1.5 KB        |

    | Crossbar Computation Latency | 96 Cycles     |

    | Crossbar Writing Latency     | 768000 Cycles |


    This configuration ensures a reasonable baseline for evaluating ARAS''s performance
    and energy efficiency.


    We model four different configurations of ARAS: a baseline ARAS without any optimization,
    ARAS<sup>B</sup> with the Adaptive Bank Selection, ARASBR with both the first
    optimization and the Adaptive Replication Scheme, and finally ARASBRW with all
    the optimizations combined including the Partial Weight Reuse. In all cases, activations
    and weights are quantized to 8-bit integers without incurring any accuracy loss
    for our DNNs. Table [II](#page-9-1) shows the parameters for all the configurations
    of ARAS. We set the number of PEs to 96, where each PE consists of 6x4 APUs, the
    crossbar size in each APU is 128x128 ReRAM cells, and each cell has a 2-bit resolution.
    Thus, 8-bit weights are represented with 4 consecutive cells. The Global Buffer
    is sized based on the worst-case scenario, which corresponds to the layer with
    the largest amount of input and output activations. In the baseline there are
    15 banks, each with a size of 256kB, whereas the other configurations employ a
    heterogeneous bank scheme incorporating banks with the following sizes: 2x1KB,
    2KB, 4KB, 64KB, 128KB, 256KB, 512KB, 1MB, and 2MB. All configurations have power-gating
    for the unused banks. Our optimizations do not require any additional hardware
    but slightly more on-chip memory with minor area overhead.


    We evaluate our scheme on five state-of-the neural networks for image classification
    and Natural Language Processing (NLP) including VGG-16 [\[45\]](#page-12-31),
    ResNet-50 [\[12\]](#page-11-16), DenseNet-161 [\[16\]](#page-11-17), BERT-Base,
    and BERT-Large [\[7\]](#page-11-7). The ImageNet [\[23\]](#page-12-32) dataset
    is used to evaluate the CNNs for image classification, while both BERT models
    are implemented by NVIDIA [\[32\]](#page-12-33), trained on the Wikipedia dataset
    [\[8\]](#page-11-18), and finetuned with SQuAD v1.1 [\[39\]](#page-12-34) for
    the question-answering task.


    Among all the implemented optimizations, only the Partial Weight Reuse described
    in Section [V-C](#page-7-0) may impact accuracy, but this impact is negligible.
    Table [III](#page-9-2) presents the accuracy loss, in absolute terms, after applying
    all optimizations. For CNNs, the accuracy is measured in terms of top-1 accuracy,
    while for BERT, it is measured using F1 accuracy. On average, ARAS has less than
    0.12% accuracy loss.


    # VII. EXPERIMENTAL RESULTS


    This section evaluates the performance, energy efficiency, and ReRAM writing activity
    of our proposal. First, we conduct


    TABLE III ACCURACY LOSS OF ARAS FOR EACH DNN.


    <span id="page-9-2"></span>![](_page_9_Figure_11.jpeg)


    <span id="page-9-3"></span>an analysis of the total number of voltage pulses required
    to write into ReRAM crossbars after applying the weight reuse optimization. Second,
    we present the speedups achieved by ARAS due to the replication scheme. Then,
    we provide a breakdown of energy consumption and discuss the impact of each optimization.
    Next, we report the lifespan of ARAS. Finally, we compare the performance and
    energy of ARAS to that of a TPU-like accelerator.


    #### <span id="page-9-4"></span>*A. ReRAM Writing Activity Analysis*


    Figure [13](#page-9-3) reports the normalized ReRAM writing activity, in terms
    of total voltage pulses required to update the cells, after the Partial Weight
    Reuse optimization. On average for our set of DNNs, ARASBRW reduces the total
    pulses by 17% over the ARAS baseline. The important reduction of ReRAM writing
    activity is mainly due to an increase in the similarity of weights overwriting
    each other, not only because of the equal values but also a reduction in the magnitude
    of the weight deltas, requiring less pulses to update each ReRAM cell.


    #### <span id="page-9-5"></span>*B. ARAS Performance Evaluation*


    Figure [14](#page-10-0) shows the speedups of ARASBRW over the baseline configuration
    without any optimization, achieving an average performance improvement of 1.5×.
    ARAS provides significant speedups for the three CNNs that range from 1.5× (*DenseNet-161*)
    to 2.2× (*ResNet-50*). The reduction in execution time is due to the efficient
    adaptive weight replication scheme of the CONV layers, described in Section [V-B.](#page-6-0)
    Therefore, FC-based DNNs, like BERT, do not see any benefit. The other optimizations
    do not target nor provide performance improvements. The main reason is that the
    worst case latency when writing the weights of a layer, and the most common, is
    to write in all the rows of a crossbar sequentially, requiring the highest amount
    of pulses in a cell of each row, even tough multiple crossbars can be written
    in parallel. Consequently, the partial weight reuse optimization requires to reduce
    the ReRAM writing activity of all the slowest cells of multiple rows of different
    crossbars at the same time, an effect that has not occurred in our set of DNNs.


    In order to assess the effectiveness of the ARAS scheduler, we define an upper-bound
    of the performance as the inverse of the lowest execution time achievable per
    DNN, which is given by the total time required to write the weights of all the


    <span id="page-10-0"></span>![](_page_10_Figure_0.jpeg)


    <span id="page-10-1"></span>layers of a DNN model once, taking into account the
    amount of resources available. On average, the baseline performance is 66% of
    the upper bound. In contrast, ARAS exhibits significantly higher performance,
    reaching 88%, which is remarkably close to the upper-bound, and demonstrates the
    successful overlap between the computation and writing procedures.


    Furthermore, the throughput of ARAS scales with the amount of PEs and APUs. Our
    current configuration (see Section [VI\)](#page-8-4) has been selected to achieve
    real-time performance in all DNNs of our set, while limiting the resources so
    that none of the models fit entirely in the ReRAM crossbars. In particular, the
    throughput of ARAS in terms of inferences per second (Inf/s) is: 43 (VGG-16),
    132 (ResNet-50), 95 (DenseNet-161), 130 (BERT-Base, sequence length 128), and
    39 (BERT-Large, sequence length 128).


    #### *C. ARAS Energy Evaluation*


    Figure [15](#page-10-1) reports the energy breakdown of each configuration of
    ARAS normalized to the baseline without any optimization. We can observe that
    the computation energy is negligible compared to the writing and static energy.
    Specifically, NLP networks experience significant energy consumption due to weight
    writing, while in CNNs, static energy is quite high.


    ARAS leverages two optimizations to reduce static energy, which is primarily consumed
    by the Gbuffer. First, the adaptive bank selection, discussed in Section [V-A,](#page-5-1)
    reduces static energy by selecting the smallest set of banks for each layer to
    store input and output activations. On average, this optimization alone (ARASB)
    has a modest 3% reduction in total energy. However, note that it does not incur
    any additional overhead. Furthermore, the adaptive weight replication scheme (ARASBR)
    provides a reduction in static energy due to the performance improvements shown
    in Figure [14.](#page-10-0) Although these benefits come at the expense of a small
    increase in writing energy, the last optimization offsets the cost. On average,
    the second optimization by itself contributes to a 14% reduction in total energy.


    In most modern DNNs, ReRAM writing dominates the total energy consumption. The
    adaptive partial weight reuse


    TABLE IV ARAS LIFESPAN IN YEARS FOR DIFFERENT DNNS.


    <span id="page-10-2"></span>


    | Network               | VGG | ResNet | DenseNet | BERT-Base | BERT-Large | Average
    |

    |-----------------------|-----|--------|----------|-----------|------------|---------|

    | Real-Time (1011)      | 7   | 26     | 17       | 4         | 3          | 12      |

    | Max Throughput (1012) | 50  | 27     | 56       | 28        | 27         | 40      |


    optimization in ARAS aims to reduce energy consumption when updating ReRAM cells
    with new weights. To achieve this, ARAS increases the bitwise similarity between
    the values stored in the ReRAM cells and the new values that will overwrite those
    cells, as described in Section [VII-A.](#page-9-4) On average, this optimization
    provides 11% reduction in total energy consumption, which is well correlated with
    the reduction in ReRAM writing activity. Overall, ARASBRW achieves 28% energy
    savings by leveraging all the three optimizations.


    #### *D. ARAS Endurance Analysis*


    As described in [II-B,](#page-2-2) there is a growing body of research demonstrating
    that the endurance of ReRAM cells can reach up to 10<sup>11</sup> and 10<sup>12</sup>
    writing cycles. Table [IV](#page-10-2) reports the lifespan (in years) for real-time
    inference execution with an endurance of 10<sup>11</sup> cycles. In CNNs, real-time
    is defined as 30 Inf/s, while in BERT 100 Inf/s. Furthermore, it also shows the
    lifespan while the accelerator operates at maximum throughput (reported in [VII-B\)](#page-9-5)
    with an endurance of 10<sup>12</sup> cycles. The results indicate that employing
    a server with the ARAS scheme can offer a minimum of 27 years of lifespan with
    an endurance of 10<sup>12</sup> cycles. Besides, if the server is utilized for
    a diverse range of DNNs the lifespan can be extended to approximately 40 years
    (or more since this estimation assumes 100% utilization). Consequently, the opportunities
    for ReRAM cells with enhanced endurance in the PUM domain are evident, prompting
    companies to explore new insulators such as TaOx. These insulators offer prolonged
    lifespan, which are crucial for harnessing ReRAM cells in computing applications,
    a direction encouraged by ARAS and other proposals [\[40\]](#page-12-35).


    #### *E. Comparison With a TPU-Like Accelerator*


    We include a comparison of ARASBRW with a TPU-like accelerator in terms of performance
    and energy efficiency. For a fair comparison, TPU and ARAS have the same area
    and frequency. Figure [16](#page-11-19) shows the speedup of ARAS for the five
    DNNs, achieving an average performance improvement of 1.2×. Figure [17](#page-11-20)
    reports normalized energy. On average, ARAS reduces the energy consumption by
    33%. ARAS''s novel scheduler and its associated optimizations provide higher performance
    and energy efficiency over a TPU-like accelerator. Note that the moderate energy
    reduction observed in CNNs underscores the importance of our optimizations, proving
    that without them, the overhead of writing weights in the ReRAM crossbars could
    result in worse energy efficiency than the TPU. These results suggest that ARAS
    is a suitable candidate for DNN inference, effectively addressing the challenge
    of adaptability of previous ReRAM-based accelerators.


    ## *F. ARAS Overheads*


    ARAS does not require any additional hardware but has a slight overhead in terms
    of the on-chip memory size and


    <span id="page-11-19"></span>![](_page_11_Figure_0.jpeg)


    <span id="page-11-20"></span>amount of written weights in ReRAM crossbars. For
    the Adaptive Bank Selection optimization, described in section [VI,](#page-8-4)
    ARAS requires an additional 200 KB of Gbuffer memory due to the heterogeneous
    banks'' sizes, resulting in about 5% increase in the overall Gbuffer size. Moreover,
    the Adaptive Replication Scheme increases the total number of weights written
    in the ReRAM crossbars by an average of 6.4% due to the replication of weights.
    This overhead is compensated by the significant energy savings and performance
    gains achieved through this optimization. Finally, as discussed in section [V-C,](#page-7-0)
    the Adaptive Partial Weight Reuse optimization effectively compensates the shifting
    process by adjusting zp<sup>w</sup> in the de-quantization procedure without introducing
    any additional overhead for recomputing the original dot-product. All ARAS optimizations
    are employed by our offline scheduler, so they do not incur any extra hardware
    to manage them at runtime, except for a few extra gates and signals needed to
    control the Adaptive Bank Selection. The area overhead of ARAS due to the additional
    on-chip memory is less than 1.5%.


    #### VIII. CONCLUSIONS


    In this paper, we show that previous ReRAM-based DNN accelerators face an *adaptability*
    challenge due to the lack of flexibility and efficient mechanisms for updating
    weights in ReRAM crossbars at runtime. Then, we propose ARAS, a low-cost ReRAM-based
    accelerator that can be adapted to efficiently execute any present and future
    DNN, regardless of its model size, using limited resources. ARAS presents a novel
    scheduler that overlaps computations of a given layer with the writing of weights
    of subsequent layers. In addition, ARAS includes three optimizations aimed at
    mitigating the expensive overheads of writing weights in ReRAM. Our experimental
    results show that, on average, ARAS''s smart scheduler and its optimizations provide
    1.5× speedup and 28% reduction in energy consumption over the baseline without
    any optimization. Over a TPU-like accelerator, ARAS achieves 1.2× speedup and
    33% reduction in energy.


    #### REFERENCES


    - <span id="page-11-3"></span>[1] T. Andrulis, J. S. Emer, and V. Sze, "Raella:
    Reforming the arithmetic for efficient, low-resolution, and low-loss analog pim:
    No retraining required!" in *Proceedings of the 50th Annual International Symposium
    on Computer Architecture*, 2023.

    - <span id="page-11-4"></span>[2] A. Ankit, I. E. Hajj, S. R. Chalamalasetti,
    G. Ndu, M. Foltin, R. S. Williams, P. Faraboschi, W.-m. W. Hwu, J. P. Strachan,
    K. Roy, and D. S. Milojicic, "Puma: A programmable ultra-efficient memristorbased
    accelerator for machine learning inference," in *Proceedings of the Twenty-Fourth
    International Conference on Architectural Support for Programming Languages and
    Operating Systems (ASPLOS)*, 2019, p. 715–731.

    - <span id="page-11-8"></span>[3] T. B. Brown, B. Mann, N. Ryder, M. Subbiah,
    J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal,
    A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler,
    J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin *et al.*, "Language
    models are few-shot learners," 2020.

    - <span id="page-11-9"></span>[4] P.-Y. Chen, X. Peng, and S. Yu, "Neurosim: A
    circuit-level macro model for benchmarking neuro-inspired architectures in online
    learning," *IEEE Transactions on Computer-Aided Design of Integrated Circuits
    and Systems*, 2018.

    - <span id="page-11-1"></span>[5] P. Chi, S. Li, C. Xu, T. Zhang, J. Zhao, Y.
    Liu, Y. Wang, and Y. Xie, "Prime: A novel processing-in-memory architecture for
    neural network computation in reram-based main memory," in *ACM/IEEE 43rd Annual
    International Symposium on Computer Architecture (ISCA)*, 2016, pp. 27–39.

    - <span id="page-11-2"></span>[6] T. Chou, W. Tang, J. Botimer, and Z. Zhang,
    "Cascade: Connecting rrams to extend analog dataflow in an end-to-end in-memory
    processing paradigm," in *Proceedings of the 52nd Annual IEEE/ACM International
    Symposium on Microarchitecture*, 2019, p. 114–125.

    - <span id="page-11-7"></span>[7] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova,
    "Bert: Pre-training of deep bidirectional transformers for language understanding,"
    2019.

    - <span id="page-11-18"></span>[8] W. Foundation. Wikimedia downloads. [Online].
    Available: [https:](https://dumps.wikimedia.org) [//dumps.wikimedia.org](https://dumps.wikimedia.org)

    - <span id="page-11-10"></span>[9] L. Gao, I.-T. Wang, P.-Y. Chen, S. Vrudhula,
    J. sun Seo, Y. Cao, T.-H. Hou, and S. Yu, "Fully parallel write/read in resistive
    synaptic array for accelerating on-chip learning," *Nanotechnology*, 2015.

    - <span id="page-11-13"></span>[10] T. Gokmen, O. M. Onen, and W. Haensch, "Training
    deep convolutional neural networks with resistive cross-point devices," *Frontiers
    in Neuroscience*, vol. 11, 2017.

    - <span id="page-11-14"></span>[11] S. Han, J. Pool, J. Tran, and W. J. Dally,
    "Learning both weights and connections for efficient neural networks," 2015.

    - <span id="page-11-16"></span>[12] K. He, X. Zhang, S. Ren, and J. Sun, "Deep
    residual learning for image recognition," *arXiv*, 2015.

    - <span id="page-11-0"></span>[13] M. Hosomi, H. Yamagishi, T. Yamamoto, K. Bessho,
    Y. Higo, K. Yamane, H. Yamada, M. Shoji, H. Hachino, C. Fukumoto, H. Nagao, and
    H. Kano, "A novel nonvolatile memory with spin torque transfer magnetization switching:
    spin-ram," in *IEEE International Electron Devices Meeting (IEDM)*, 2005, pp.
    459–462.

    - <span id="page-11-11"></span>[14] C.-W. Hsu, I.-T. Wang, C.-L. Lo, M.-C. Chiang,
    W.-Y. Jang, C.-H. Lin, and T.-H. Hou, "Self-rectifying bipolar tao x/tio 2 rram
    with superior endurance over 10 12 cycles for 3d high-density storage-class memory,"
    in *2013 Symposium on VLSI Technology*. IEEE, 2013.

    - <span id="page-11-12"></span>[15] C.-W. Hsu, Y.-F. Wang, C.-C. Wan, I.-T. Wang,
    C.-T. Chou, W.-L. Lai, Y.-J. Lee, and T.-H. Hou, "Homogeneous barrier modulation
    of taox/tio2 bilayers for ultra-high endurance three-dimensional storageclass
    memory," *Nanotechnology*, mar 2014.

    - <span id="page-11-17"></span>[16] G. Huang, Z. Liu, L. van der Maaten, and K.
    Q. Weinberger, "Densely connected convolutional networks," *arXiv*, 2016.

    - <span id="page-11-5"></span>[17] M. Imani, M. Samragh Razlighi, Y. Kim, S. Gupta,
    F. Koushanfar, and T. Rosing, "Deep learning acceleration with neuron-to-memory
    transformation," in *IEEE International Symposium on High Performance Computer
    Architecture (HPCA)*, 2020, pp. 1–14.

    - <span id="page-11-15"></span>[18] N. P. Jouppi, C. Young, N. Patil, D. A. Patterson,
    G. Agrawal, R. Bajwa, S. Bates, and et al., "In-datacenter performance analysis
    of a tensor processing unit," in *Proceedings of the 44th Annual International
    Symposium on Computer Architecture*, 2017, p. 1–12.

    - <span id="page-11-6"></span>[19] B. Khabbazan, M. Riera, and A. Gonzalez, "Dna-teq:
    An adaptive ´ exponential quantization of tensors for dnn inference," in *2023
    IEEE 30th International Conference on High Performance Computing, Data, and Analytics
    (HiPC)*, 2023.

    - <span id="page-12-9"></span>[20] B. Khabbazan, M. Riera, and A. Gonzalez, "Qeihan:
    An energy-efficient ´ dnn accelerator that leverages log quantization in ndp architectures,"
    in *2023 32nd International Conference on Parallel Architectures and Compilation
    Techniques (PACT)*, 2023.

    - <span id="page-12-15"></span>[21] Y.-B. Kim, S. R. Lee, D. Lee, C. B. Lee, M.
    Chang, J. H. Hur, M.-J. Lee, G.-S. Park, C. J. Kim, U.-I. Chung, I.-K. Yoo, and
    K. Kim, "Bi-layered rram with unlimited endurance and extremely uniform switching,"
    in *2011 Symposium on VLSI Technology - Digest of Technical Papers*, 2011, pp.
    52–53.

    - <span id="page-12-10"></span>[22] A. Krizhevsky, I. Sutskever, and G. E. Hinton,
    "Imagenet classification with deep convolutional neural networks," in *Advances
    in Neural Information Processing Systems*, 2012.

    - <span id="page-12-32"></span>[23] S. V. Lab, "Imagenet," [https://www.image-net.org/.](https://www.image-net.org/)

    - <span id="page-12-0"></span>[24] B. C. Lee, E. Ipek, O. Mutlu, and D. Burger,
    "Phase change memory architecture and the quest for scalability," *Communications
    of the ACM*, vol. 53, no. 7, p. 99–106, 2010.

    - <span id="page-12-16"></span>[25] M.-J. Lee, C. B. Lee, D. Lee, S. R. Lee, M.
    Chang, J. H. Hur, Y.- B. Kim, C.-J. Kim, D. H. Seo, S. Seo *et al.*, "A fast,
    high-endurance and scalable non-volatile memory device made from asymmetric ta2o5
    x/tao2- x bilayer structures," *Nature materials*, 2011.

    - <span id="page-12-11"></span>[26] H. Li, T. F. Wu, S. Mitra, and H.-S. P. Wong,
    "Resistive ram-centric computing: Design and modeling methodology," *IEEE Transactions
    on Circuits and Systems I: Regular Papers*, 2017.

    - <span id="page-12-27"></span>[27] S. Li, Z. Yang, D. Reddy, A. Srivastava, and
    B. Jacob, "Dramsim3: A cycle-accurate, thermal-capable dram simulator," *IEEE
    Computer Architecture Letters*, 2020.

    - <span id="page-12-25"></span>[28] S. Li, K. Chen, J. H. Ahn, J. B. Brockman,
    and N. P. Jouppi, "Cactip: Architecture-level modeling for sram-based structures
    with advanced leakage reduction techniques," in *ICCAD: International Conference
    on Computer-Aided Design*, 2011.

    - <span id="page-12-3"></span>[29] W. Li, P. Xu, Y. Zhao, H. Li, Y. Xie, and Y.
    Lin, "Timely: Pushing data movements and interfaces in pim accelerators towards
    local and in time domain," in *Proceedings of the ACM/IEEE 47th Annual International
    Symposium on Computer Architecture*, 2020.

    - <span id="page-12-12"></span>[30] Y. Li, R. Katsumura, M. K. Gronroos, A. Tsurumaki-Fukuchi,
    M. Arita, ¨ H. Andoh, T. Morie, and Y. Takahashi, "Evaluation of multilevel memory
    capability of reram using ta2o5 insulator and different electrode materials,"
    in *2017 Silicon Nanoelectronics Workshop (SNW)*, 2017.

    - <span id="page-12-29"></span>[31] T. Norrie, N. Patil, D. H. Yoon, G. Kurian,
    S. Li, J. Laudon, C. Young, N. Jouppi, and D. Patterson, "The design process for
    google''s training chips: Tpuv2 and tpuv3," *IEEE Micro*, 2021.

    - <span id="page-12-33"></span>[32] NVIDIA. (2020) Nvidia Language Modeling. [Online].
    Available: <https://github.com/NVIDIA/DeepLearningExamples>

    - <span id="page-12-13"></span>[33] S. Park, A. Sheri, J. Kim, J. Noh, J. Jang,
    M. Jeon, B. Lee, B. R. Lee, B. H. Lee, and H. Hwang, "Neuromorphic speech systems
    using advanced reram-based synapse," in *2013 IEEE International Electron Devices
    Meeting*, 2013.

    - <span id="page-12-4"></span>[34] X. Peng, R. Liu, and S. Yu, "Optimizing weight
    mapping and data flow for convolutional neural networks on rram based processing-in-memory
    architecture," in *IEEE International Symposium on Circuits and Systems (ISCAS)*,
    2019, pp. 1–5.

    - <span id="page-12-23"></span>[35] X. Peng, R. Liu, and S. Yu, "Optimizing weight
    mapping and data flow for convolutional neural networks on processing-in-memory
    architectures," *IEEE Transactions on Circuits and Systems I: Regular Papers*,
    2020.

    - <span id="page-12-5"></span>[36] L. Pentecost, A. Hankin, M. Donato, M. Hempstead,
    G.-Y. Wei, and D. Brooks, "Nvmexplorer: A framework for cross-stack comparisons
    of embedded non-volatile memories," in *2022 IEEE International Symposium on High-Performance
    Computer Architecture (HPCA)*, 2022.

    - <span id="page-12-6"></span>[37] X. Qiao, X. Cao, H. Yang, L. Song, and H. Li,
    "Atomlayer: A universal reram-based cnn accelerator with atomic layer computation,"
    in *2018 55th ACM/ESDA/IEEE Design Automation Conference (DAC)*, 2018.

    - <span id="page-12-1"></span>[38] K. Qiu, N. Jao, M. Zhao, C. S. Mishra, G. Gudukbay,
    S. Jose, J. Sampson, M. T. Kandemir, and V. Narayanan, "Resirca: A resilient energy
    harvesting reram crossbar-based accelerator for intelligent embedded processors,"
    in *2020 IEEE International Symposium on High Performance Computer Architecture
    (HPCA)*, 2020.

    - <span id="page-12-34"></span>[39] P. Rajpurkar, J. Zhang, K. Lopyrev, and P.
    Liang, "Squad: 100,000+ questions for machine comprehension of text," 2016.

    - <span id="page-12-35"></span>[40] S. Resch, H. Cilasun, Z. Chowdhury, M. Zabihi,
    Z. Zhao, J.-P. Wang, S. Sapatnekar, and U. R. Karpuzcu, "On endurance of processing
    in (nonvolatile) memory," in *Proceedings of the 50th Annual International Symposium
    on Computer Architecture*. Association for Computing Machinery, 2023.

    - <span id="page-12-30"></span>[41] M. Riera, J. M. Arnau, and A. Gonzalez, "Crew:
    Computation reuse ´ and efficient weight storage for hardware-accelerated mlps
    and rnns," *Journal of Systems Architecture*, 2022.

    - <span id="page-12-21"></span>[42] M. Sabri, M. Riera, and A. Gonzalez, "Redy:
    A novel reram-centric ´ dynamic quantization approach for energy-efficient cnn
    inference," 2023.

    - <span id="page-12-28"></span>[43] A. Samajdar, Y. Zhu, P. Whatmough, M. Mattina,
    and T. Krishna, "Scalesim: Systolic cnn accelerator simulator," 2019.

    - <span id="page-12-7"></span>[44] A. Shafiee, A. Nag, N. Muralimanohar, R. Balasubramonian,
    J. P. Strachan, M. Hu, R. S. Williams, and V. Srikumar, "Isaac: A convolutional
    neural network accelerator with in-situ analog arithmetic in crossbars," in *ACM/IEEE
    43rd Annual International Symposium on Computer Architecture (ISCA)*, 2016, pp.
    14–26.

    - <span id="page-12-31"></span>[45] K. Simonyan and A. Zisserman, "Very deep convolutional
    networks for large-scale image recognition," *arXiv*, 2014.

    - <span id="page-12-20"></span>[46] L. Song, X. Qian, H. Li, and Y. Chen, "Pipelayer:
    A pipelined rerambased accelerator for deep learning," in *IEEE International
    Symposium on High Performance Computer Architecture (HPCA)*, 2017, pp. 541– 552.

    - <span id="page-12-26"></span>[47] Synopsys, "Design compiler," [https://www.synopsys.com/.](https://www.synopsys.com/)

    - <span id="page-12-24"></span>[48] G. Tech, "Neurosim," [https://github.com/neurosim/.](https://github.com/neurosim/)

    - <span id="page-12-17"></span>[49] W. Wen, Y. Zhang, and J. Yang, "Renew: Enhancing
    lifetime for reram crossbar based neural network accelerators," in *2019 IEEE
    37th International Conference on Computer Design (ICCD)*, 2019.

    - <span id="page-12-14"></span>[50] J. Woo, K. Moon, J. Song, S. Lee, M. Kwak,
    J. Park, and H. Hwang, "Improved synaptic behavior under identical pulses using
    alox/hfo2 bilayer rram array for neuromorphic systems," *IEEE Electron Device
    Letters*, 2016.

    - <span id="page-12-2"></span>[51] T.-H. Yang, H.-Y. Cheng, C.-L. Yang, I.-C.
    Tseng, H.-W. Hu, H.-S. Chang, and H.-P. Li, "Sparse reram engine: Joint exploration
    of activation and weight sparsity in compressed neural networks," in *ACM/IEEE
    46th Annual International Symposium on Computer Architecture (ISCA)*, 2019, pp.
    236–249.

    - <span id="page-12-18"></span>[52] X. Yang, H. Yang, J. R. Doppa, P. P. Pande,
    K. Chakrabartys, and H. Li, "Essence: Exploiting structured stochastic gradient
    pruning for endurance-aware reram-based in-memory training systems," *IEEE Transactions
    on Computer-Aided Design of Integrated Circuits and Systems*, 2023.

    - <span id="page-12-8"></span>[53] G. Yua, P. Behnam, Z. Li, A. Shafiee, S. Lin,
    X. Ma, H. Liu, X. Qian, M. Bojnordi, Y. Wang, and C. Ding, "Forms: Fine-grained
    polarized reram-based in-situ computation for mixed-signal dnn accelerator," in
    *ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)*,
    2021, pp. 265–278.

    - <span id="page-12-22"></span>[54] M. Zahedi, M. A. Lebdeh, C. Bengel, D. Wouters,
    S. Menzel, M. Le Gallo, A. Sebastian, S. Wong, and S. Hamdioui, "Mnemosene: Tile
    architecture and simulator for memristor-based computation-inmemory," *ACM Journal
    on Emerging Technologies in Computing Systems*, vol. 18, no. 3, 2022.

    - <span id="page-12-19"></span>[55] Y. Zhang, Z. Jia, Y. Pan, H. Du, Z. Shen,
    M. Zhao, and Z. Shao, "Pattpim: A practical reram-based dnn accelerator by reusing
    weight pattern repetitions," in *2020 57th ACM/IEEE Design Automation Conference
    (DAC)*, 2020.'
- title: "HPR-Mul: An Area and Energy-Efficient High-Precision Redundancy\n  Multiplier\
    \ by Approximate Computing"
  abstract: 'For critical applications that require a higher level of reliability,
    the

    Triple Modular Redundancy (TMR) scheme is usually employed to implement

    fault-tolerant arithmetic units. However, this method imposes a significant

    area and power/energy overhead. Also, the majority-based voter in the typical

    TMR designs is highly sensitive to soft errors and the design diversity of the

    triplicated module, which may result in an error for a small difference between

    the output of the TMR modules. However, a wide range of applications deployed

    in critical systems are inherently error-resilient, i.e., they can tolerate

    some inexact results at their output while having a given level of reliability.

    In this paper, we propose a High Precision Redundancy Multiplier (HPR-Mul) that

    relies on the principles of approximate computing to achieve higher energy

    efficiency and lower area, as well as resolve the aforementioned challenges of

    the typical TMR schemes, while retaining the required level of reliability. The

    HPR-Mul is composed of full precision (FP) and two reduced precision (RP)

    multipliers, along with a simple voter to determine the output. Unlike the

    state-of-the-art Reduced Precision Redundancy multipliers (RPR-Mul) that

    require a complex voter, the voter of the proposed HPR-Mul is designed based on

    mathematical formulas resulting in a simpler structure. Furthermore, we use the

    intermediate signals of the FP multiplier as the inputs of the RP multipliers,

    which significantly enhance the accuracy of the HPR-Mul. The efficiency of the

    proposed HPR-Mul is evaluated in a 15-nm FinFET technology, where the results

    show up to 70% and 69% lower power consumption and area, respectively, compared

    to the typical TMR-based multipliers. Also, the HPR-Mul outperforms the

    state-of-the-art RPR-Mul by achieving up to 84% higher soft error tolerance.'
  url: http://arxiv.org/abs/2410.20150v1
  keywords: ''
  document: "# HPR-Mul: An Area and Energy-Efficient High-Precision Redundancy Multiplier\
    \ by Approximate Computing\n\nJafar Vafaei, Omid Akbari\n\n*Abstract***— For critical\
    \ applications that require a higher level of reliability, the Triple Modular\
    \ Redundancy (TMR) scheme is usually employed to implement fault-tolerant arithmetic\
    \ units. However, this method imposes a significant area and power/energy overhead.\
    \ Also, the majority-based voter in the typical TMR designs is highly sensitive\
    \ to soft errors and the design diversity of the triplicated module, which may\
    \ result in an error for a small difference between the output of the TMR modules.\
    \ However, a wide range of applications deployed in critical systems are inherently\
    \ error-resilient, i.e., they can tolerate some inexact results at their output\
    \ while having a given level of reliability. In this paper, we propose a High\
    \ Precision Redundancy Multiplier (HPR-Mul) that relies on the principles of approximate\
    \ computing to achieve higher energy efficiency and lower area, as well as resolve\
    \ the aforementioned challenges of the typical TMR schemes, while retaining the\
    \ required level of reliability. The HPR-Mul is composed of full precision (FP)\
    \ and two reduced precision (RP) multipliers, along with a simple voter to determine\
    \ the output. Unlike the state-of-the-art Reduced Precision Redundancy multipliers\
    \ (RPR-Mul) that require a complex voter, the voter of the proposed HPR-Mul is\
    \ designed based on mathematical formulas resulting in a simpler structure. Furthermore,\
    \ we use the intermediate signals of the FP multiplier as the inputs of the RP\
    \ multipliers, which significantly enhance the accuracy of the HPR-Mul. The efficiency\
    \ of the proposed HPR-Mul is evaluated in a 15 nm FinFET technology, where the\
    \ results show up to 70% and 69% lower power consumption and area, respectively,\
    \ compared to the typical TMR-based multipliers. Also, the HPR-Mul outperforms\
    \ the state-of-the-art RPR-Mul by achieving up to 84% higher soft error tolerance.\
    \ Moreover, by employing the HPR-Mul in different image processing applications,\
    \ up to 13% higher output image quality is achieved in comparison with the state-ofthe-art\
    \ RPR multipliers.**\n\n*Index Terms— Approximate Compuitng, Reduced Precision\
    \ Redundancy, RPR, Energy-Efficiency, Multiplier, Soft Error, TMR.*\n\n#### I.\
    \ INTRODUCTION\n\n**M**ultiplier is one of the main arithmetic blocks in different\
    \ digital systems and its design metrics (i.e., delay, area, and power/energy\
    \ consumption) can significantly impact the system performance. Digital signal\
    \ processing (DSP), deep neural networks (DNNs), matrix multiplication (MM), and\
    \ stochastic computations are examples of these systems, in which the efficiency\
    \ of the multipliers has a vital role [\\[1\\].](#page-9-0) Moreover, in critical\
    \ applications where achieving a given level of reliability is essential, multipliers\
    \ are considered important parts that can be hardened against fault by employing\
    \ various redundancy schemes, such as Triple Modular Redundancy (TMR) [\\[2\\\
    ],](#page-9-1) residue code [\\[3\\],](#page-9-2) and parity predictio[n \\[4\\\
    ]](#page-9-3) methods.\n\nIn the TMR designs, the outputs of the replicated modules\
    \ are forwarded to a majority-based voter, in which to determine the valid output\
    \ by the voter, at least two modules have to provide the correct output. Also,\
    \ the replicated modules may be implemented in different ways but with the same\
    \ functionality to decrease the impact and possibility of common-mode failures\
    \ (CMFs) [\\[5\\].](#page-9-4) However, the design diversity of the replicated\
    \ modules may result in some small differences in the modules' outputs. Furthermore,\
    \ various types of errors induced by the different factors such as electromagnetic\
    \ inference (EMI), manufacturing process defects, aging, radiations, and the Singleevent\
    \ effects (SEEs) that are more likely to happen in the nanoscale era, can affect\
    \ the normal operation of the replicated modules. SEEs are the disruptions to\
    \ the normal operation of a device caused by a single energetic particle striking\
    \ a sensitive area in the device, which can be classified into different categories\
    \ such as Single Event Transients (SETs) and Single Event Upsets (SEUs[\\) \\\
    [6\\].](#page-9-5)\n\nSETs are temporary glitches or errors in electronic devices\
    \ when a single particle strikes a sensitive node, causing a momentary disruption\
    \ in the device's operation. SETs do not result in permanent damage to the device,\
    \ but they can cause transient errors that may affect the device's functionality.\
    \ SEUs are changes in the device's state or data. When a single energetic particle\
    \ strikes a sensitive node in the device, it can flip a bit in a memory cell or\
    \ register, leading to an error in the device's operation. SEUs can have significant\
    \ consequences, especially in critical applications where data integrity is essential.\
    \ Overall, SEEs are a critical consideration in fault-tolerant design, as they\
    \ can have implications for the reliability and safety of electronic systems.\
    \ Therefore, designers implement various strategies to mitigate the impact of\
    \ SEEs, such as error detection and correction mechanisms, shielding, redundancy,\
    \ and error recovery techniques. The aforementioned error factors can make some\
    \ small differences in the outputs of the replicated modules, which can be considered\
    \ as an error by the typical TMR voters [\\[5\\].](#page-9-4)\n\nReduced precision\
    \ redundancy (RPR) is one of the techniques that can resolve these challenges,\
    \ as well as decrease the power/energy and area overheads of the TMR designs [\\\
    [7\\]](#page-9-6)[\\[8\\].](#page-9-7)\n\nJafar Vafaei is with the School of Electrical\
    \ and Computer Engineering, University of Tehran, Tehran 14395-515, Iran (e-mail:\
    \ jafar.vafaei@ut.ac.ir). O. Akbari (corresponding author) is with the Department\
    \ of Electrical and Computer Engineering, Tarbiat Modares University, Tehran 14115-111,\
    \ Iran (e-mail[: o.akbari@modares.ac.ir\\)](mailto:o.akbari@modares.ac.ir).\n\n\
    ![](_page_1_Figure_0.jpeg)\n\n<span id=\"page-1-0\"></span>Fig. 1. Generic architecture\
    \ of a typical reduced precision redundancy (RPR) module [\\[9\\].](#page-9-8)\n\
    \nThe generic architecture of an RPR design is shown in [Fig. 1](#page-1-0) [\\\
    [9\\],](#page-9-8) which is composed of one full precision (FP) and two reduced\
    \ precision modules (e.g., adder [\\[10\\]](#page-9-9) or multiplier [\\[11\\\
    ]\\)](#page-9-10), along with an RPR voter. Note that the reduced precision modules\
    \ are usually obtained by the truncation method, in which some least significant\
    \ bits (LSBs) of the modules are removed. As shown in [Fig. 1,](#page-1-0) the\
    \ RPR voter is composed of several extra modules, including a subtractor, comparator,\
    \ logical gates, and a multiplexer, which can increase the area and delay overheads,\
    \ depending on the bit width of the modules' outputs.\n\nAs an example, in the\
    \ state-of-the-art RPR multiplier presented in [\\[11\\],](#page-9-10) the FP\
    \ multiplier (i.e., an *N*-bit multiplier) produces the full precision result,\
    \ and the RP multipliers (i.e., *N-M* bit multipliers) generate the reduced precision\
    \ outputs, where the *M* LSB bits of the RP multipliers were truncated. The subtractor\
    \ in the RPR voter measures the difference between the FP multiplier output and\
    \ one of the RP multipliers, and when this difference exceeds a predefined threshold,\
    \ an error is presumed to have occurred. In this case, if the comparator at the\
    \ output of the RP multipliers shows a difference, it is assumed the error has\
    \ occurred in the RP multipliers, and the output of the FP multiplier is selected\
    \ by the multiplexer to be forwarded to output. Otherwise, the error has occurred\
    \ in the FP multiplier, and the output of one of the RP multipliers is selected\
    \ by the multiplexer.\n\nIn recent years, approximate computing has been of great\
    \ interest since it employs the error resiliency of applications to achieve power/energy\
    \ consumption and area improvements at the cost of inducing some errors at the\
    \ output. Multimedia processing, digital signal processing, data mining, computer\
    \ vision, machine learning, and big data analysis, are some examples of error-resilient\
    \ applications, in which a span of outputs near to the exact (golden) one is acceptable\
    \ [\\[5\\].](#page-9-4) As an example, in an image processing application, the\
    \ output images may be perceived by the human eyes that are not sensitive to small\
    \ inaccuracies. However, in the error-resilient applications deployed in critical\
    \ systems, achieving a given level of reliability has more priority compared to\
    \ generating the exact outputs. Therefore, in these cases, the approximate computing\
    \ paradigm can be utilized for gaining the aforementioned\n\nadvantages, while\
    \ still providing the required level of reliability [\\[5\\]](#page-9-4)[\\[12\\\
    ].](#page-9-11)\n\nIn this paper, we propose a high precision redundancy multiplier\
    \ (HPR-Mul) that exploits approximate computing to achieve significant power/energy\
    \ and area improvements, while retaining the required level of reliability. Specifically,\
    \ we design the HPR voter based on mathematical formulas to achieve a simple voter\
    \ compared to the RPR voter shown in [Fig. 1.](#page-1-0) Furthermore, we use\
    \ the intermediate signals of the FP multiplier as the inputs of the RP multipliers\
    \ to increase their accuracy against soft errors. The novel contributions of this\
    \ paper in a nutshell are:\n\n- 1- Proposing a generic and systematic method to\
    \ design high precision redundancy (HPR) multipliers, while retaining the required\
    \ level of reliability (Section [III\\)](#page-2-0).\n- 2- Achieving a high accuracy\
    \ and fault tolerance by enhancing the accuracy of RP multipliers through exploiting\
    \ the intermediates signals of the FP multiplier (Subsectio[n III.](#page-2-0)[B\\\
    )](#page-3-0).\n- 3- A mathematical method to design the voter of HPR-Mul to achieve\
    \ a simpler voter compared to the state-of-theart RPR multipliers (Subsectio[n\
    \ III.](#page-2-0)[C\\)](#page-3-1).\n- 4- Investigating the soft error tolerance\
    \ of the proposed HPR-Mul, and comparing the results with typical TMR and state-of-the-art\
    \ RPR multipliers (Subsectio[n IV](#page-5-0)[.A\\)](#page-5-1).\n- 5- Extracting\
    \ the design metrics (power, delay, and area) of the HPR-Mul, and comparing them\
    \ with those of the typical TMR multipliers, as well as the state-of-the-art RPR\
    \ multipliers (Subsection [IV.](#page-5-0)[B\\)](#page-6-0).\n- 6- Assessing the\
    \ efficacy of the studied redundancy schemes in real-world image processing applications\
    \ (Subsectio[n IV](#page-5-0)[.C\\)](#page-6-1).\n\n**Paper Organization:** In\
    \ Section [II,](#page-1-1) prior related works on reduced precision redundancy-based\
    \ are reviewed. Then, the architecture of the proposed HPR-Mul is presented in\
    \ Section [III.](#page-2-0) Section [IV](#page-5-0) deals with the simulation\
    \ results and the effectiveness evaluation of the studied redundancy schemes.\
    \ Finally, the paper is concluded in Section [V.](#page-9-12)\n\n## II. RELATED\
    \ WORK\n\n<span id=\"page-1-1\"></span>In this section, different efforts dealing\
    \ with approximate computing in reliable systems are studied. Specifically, we\
    \ review key research focused on reduced precision redundancy methods are reviewed.\n\
    \nA comprehensive review of the different techniques of approximate computing\
    \ is found in [\\[13\\],](#page-9-13) where these techniques are classified into\
    \ different system layers, ranging from hardware and architecture to software\
    \ layers. Also, exhaustive surveys on approximate computing and its fault tolerance\
    \ have been conducted in [\\[14\\]](#page-9-14)[-\\[16\\].](#page-9-15)\n\nIn\
    \ [\\[17\\],](#page-9-16) an inexact double modular redundancy (IDMR) voter was\
    \ proposed, in which, when the difference between the outputs in the DMR system\
    \ is higher than a predefined threshold, an error signal is generated. Otherwise,\
    \ the average of outputs is computed as the correct output. The proposed method\
    \ of IDMR was extended to TMR voters in [\\[18\\].](#page-9-17) An approximate\
    \ TMR voter for loop-based algorithms was proposed in [\\[19\\],](#page-9-18)\
    \ where by varying the number of loop iterations, an accuracyperformance tradeoff\
    \ is achieved. In [\\[20\\],](#page-9-19) the authors have addressed the challenges\
    \ of selective hardening in the arithmetic circuits by exploring the different\
    \ tradeoffs between reliability and cost. In [\\[21\\],](#page-9-20) a Boolean\
    \ factorization-based method was proposed to compose approximate logics for redundant\
    \ systems and reduce the area overheads. In [\\[22\\],](#page-9-21) a partial\
    \ TMR-based design was proposed for the FPGAs that employ approximate logics in\
    \ the redundant modules. A transistor-level voter hardening method was proposed\
    \ i[n \\[23\\],](#page-9-22) that analyzed the voter inputs and constructed a\
    \ quadded transistor-based redundancy voter for masking the transient faults.\n\
    \nDifferent schemes were employed to protect the arithmetic units against faults.\
    \ In [\\[3\\],](#page-9-2) the modulo of output is checked to detect the error\
    \ occurrence, while in [\\[4\\],](#page-9-3) the parity method is used as an error\
    \ detection knob. In both of these works, two replicated modules were employed\
    \ to correct the output, when an error occurred. However, these methods impose\
    \ a considerable area cost.\n\nOne of the promising techniques to reduce the overheads\
    \ of the Modular Redundancy (NMR) based techniques is using ( − 1) reduced precision\
    \ redundant modules along with a main full precision module, which is known as\
    \ the reduced precision redundancy (RPR) scheme. Because of the significant area\
    \ and power/energy consumption reduction by the reduced precision modules, the\
    \ RPR scheme can achieve significant improvements. As an example, in [\\[24\\\
    ],](#page-9-23) it has been shown that the failure rate of an RPR design was improved\
    \ by 200× against Single Event Upsets (SEUs), whereas its area was reduced by\
    \ 50% compared to the typical TMR scheme. However, the overheads of the complex\
    \ voter in the RPR schemes, induced by error detection and correction are still\
    \ considerable [\\[25\\].](#page-10-0) An RPR voter for the carry propagate adders\
    \ (CPAs) was proposed in [\\[10\\].](#page-9-9) This work splits the structure\
    \ of the main adder (main module in a TMR design) into two higher and lower parts,\
    \ and only replicates the higher part as the redundant modules of the TMR design,\
    \ where the output carry of the main adder lower part is used as the input carry\
    \ of the replicated adders. Therefore, the required voter is simplified and its\
    \ cost is reduced.\n\nLoop perforation is one of the well-known software-level\
    \ approximate computing techniques, in which, the number of iterations in \"for\"\
    \ loops is modified manually. In [\\[26\\],](#page-10-1) loop perforation was\
    \ used to design an approximate software-based fault-tolerant system to diminish\
    \ the execution time overheads. In [\\[27\\],](#page-10-2) a duplication with\
    \ comparison (DWC)-based fault tolerant scheme for image processing applications\
    \ was proposed, in which one of the replicated modules was replaced with an approximated\
    \ one. Then, this design is used to distinguish between usable and unusable images\
    \ by a convolutional neural network (CNN). In [\\[28\\],](#page-10-3) approximate\
    \ computing methods are used along with radiation-induced mitigation techniques\
    \ to reduce the overheads of fault-tolerant systems. [\\[32\\]](#page-10-4) used\
    \ logic optimization techniques to find an appropriate combination of three approximate\
    \ modules to compose an approximate TMR design.\n\nAs reviewed in this section,\
    \ most state-of-the-art RPR works used expensive voters and almost were limited\
    \ to adders. However, the multipliers use significantly more area and power/energy\
    \ in digital systems. In this paper, we propose a systematic method to design\
    \ a high precision redundancy multiplier (HPR-Mul), which is based on simplifying\
    \ the voter design based on mathematical formulas, as well as enhancing the accuracy\
    \ and error tolerance of the design by exploiting some intermediate signals of\
    \ FP multiplier as the input of RP multipliers.\n\n#### <span id=\"page-2-0\"\
    ></span>III. PROPOSED HIGH PRECISION REDUNDANCY MULTIPLIER\n\nIn this section,\
    \ first, the implementation of a conventional block-level multiplier is discussed.\
    \ Then, the studied block level multiplier is used to compose the proposed high-precision\
    \ redundancy multiplier (HPR-Mul). Afterward, a mathematical method to design\
    \ the voter of the HPR-Mul is introduced.\n\n#### *A. Conventional Block-Level\
    \ Multipliers*\n\nA multiplier can be implemented by using smaller blocks [\\\
    [1\\].](#page-9-0) This design along with a numerical example is shown i[n Fig.\
    \ 2,](#page-2-1) where the two inputs of *A* and *B* are split into two higher\
    \ (*H*) and lower (*L*) parts. Then, the main multiplier is composed of four smaller\
    \ blocks, where () and ( ) are the upper and lower parts of the input (), respectively.\
    \ Note that for an input with the size of *N*, the size of the upper and lower\
    \ parts is (*N-K*) and *K*, respectively. We leverage this structure to compose\
    \ the FP multiplier of the proposed HPR-Mul, and then, employ its intermediate\
    \ signals to increase the accuracy of RP multipliers, to achieve higher accuracy\
    \ in faulty conditions compared to the typical TMR multipliers and RPR designs.\n\
    \n![](_page_2_Figure_10.jpeg)\n\n<span id=\"page-2-1\"></span>Fig. 2. Composing\
    \ a larger multiplier with smaller multiplier blocks, along with an example to\
    \ show the functionality of this method.\n\n![](_page_3_Figure_0.jpeg)\n\n<span\
    \ id=\"page-3-2\"></span>Fig. 3. Block Diagram of the HPR-MUL\n\n#### <span id=\"\
    page-3-0\"></span>*B. Proposed HPR-MUL*\n\nAs shown in [Fig. 1,](#page-1-0) in\
    \ an RPR multiplier, there is not any dependency between the main (i.e., the FP\
    \ multiplier) and redundant (i.e., the RP) multipliers. In our proposed HPR-MUL,\
    \ we create a dependency between these multipliers to simultaneously increase\
    \ its accuracy and fault tolerance feature with a slight overhead[. Fig. 3](#page-3-2)\
    \ shows the structure of the proposed HPR-MUL.\n\nTo compose the HPR-Mul, at first,\
    \ we implement the main multiplier in the block level form (see [Fig. 2\\)](#page-2-1)\
    \ consisting of four smaller multipliers, where their outputs are forwarded to\
    \ the summation block for computing the final result. Also, the redundant multipliers\
    \ are implemented with a smaller size that corresponds to the block in [Fig. 2.](#page-2-1)\
    \ Note that the RP modules can be obtained by the truncation method, e.g., the\
    \ *K* lower bits of the inputs of an *N*-bit multiplier are truncated, and therefore,\
    \ *(N-K)*-bit multipliers are used instead of full precision ones, where the output\
    \ of these multipliers is *2(N-K)*-bit. This method can significantly reduce the\
    \ area and power/energy consumption of the truncated module compared to the FP\
    \ module. Now, the intermediate results calculated by the , , and blocks of the\
    \ FP multiplier (red-colored dashed box in [Fig. 3\\)](#page-3-2) are forwarded\
    \ to the summation blocks of the RP multipliers, to enhance their accuracy.\n\n\
    Note that, depending on the size of the smaller blocks a tradeoff between the\
    \ accuracy and power/area of the HPR-Mul is achievable. In particular, the size\
    \ of the block is determinant since it is replicated in the redundant multipliers,\
    \ and thus, various levels of area and power/energy savings are achievable. Next,\
    \ the outputs of the FP multiplier and RP multipliers are forwarded to a simple\
    \ typical *2(N-K)*-bit TMR voter, instead of expensive voters used in typical\
    \ RPR designs (se[e Fig. 1\\)](#page-1-0), since the computed lower *2K*-bit of\
    \ the FP multiplier and RP multipliers are the same. Therefore, depending on the\
    \ *K*, the HPR voter may achieve lower design metrics (i.e., delay, power/energy\
    \ consumption, and area) compared to an *N*-bit typical TMR voter. It's worth\
    \ noting that the proposed HPR-Mul\n\nretains the reliability, i.e., still the\
    \ proposed design is triplicated in implementation, where only some LSB bits of\
    \ the reduced precision (RP) multipliers are relaxed. Moreover, employing the\
    \ approximate computing in TMR multipliers may resolve the challenge of typical\
    \ TMR multipliers and increase the system dependability.\n\nDetails of implementing\
    \ the HPR-Mul are depicted i[n Fig. 4,](#page-4-0) in which all the components\
    \ and their interconnections are depicted. Moreover, the required values for the\
    \ RP Multipliers are depicted with a numerical example on the right side of this\
    \ figure. As shown in this figure, the RP multipliers are composed only of a multiplier\
    \ and a − -bit adder.\n\nIt's worth noting that similar to some state-of-the-art\
    \ RPR works (such as [\\[10\\]\\)](#page-9-9), in our proposed HPR multiplier,\
    \ we created dependency between the FP and RP modules, and achieved a simpler\
    \ voter design, to obtain a moderate fault tolerance feature with a considerable\
    \ reduced overhead. Due to this dependency, the soft error effects may be propagated\
    \ from the lower part of the FP multiplier into the RP multipliers. However, the\
    \ weight of the AHB<sup>H</sup> block in the accuracy of the output of the summation\
    \ block is significantly higher than the lower parts. Based on the results that\
    \ will be presented in Subsection [IV.](#page-5-0)[A,](#page-5-1) in our soft\
    \ error tolerance examinations, we injected multiple faults in different parts\
    \ of all FP and RP multipliers, simultaneously, where the results show a significantly\
    \ higher soft error tolerance compared to the stateof-the-art RPR schemes.\n\n\
    In the next subsection, a mathematical method will be proposed to determine the\
    \ parameter *K* of the HPR-Mul, such that the accuracy of the HPR-Mul is retained\
    \ in the acceptable range, without requiring any extra hardware such as those\
    \ used in RPR voters.\n\n#### <span id=\"page-3-1\"></span>*C. Selecting Parameter\
    \ K for the proposed HPR Multiplier*\n\nIn this subsection, we propose a mathematical\
    \ method to select a proper value for *K*, such that the obtained configuration\
    \ of the proposed HPR-Mul meets the user-defined quality degradation upper bound\
    \ ().\n\nNote that, the quality degradation upper bound of the HPR-Mul can be\
    \ obtained from the higher level of abstraction (e.g., the application layer),\
    \ in which a number of the proposed HPR-Mul may be deployed. In such a case, a\
    \ quality translation (quality mapping) step, similar to those proposed in [\\\
    [29\\]](#page-10-5) and [\\[31\\]](#page-10-6) is required. I[n \\[29\\],](#page-10-5)\
    \ the adder and multiplier inside the ALU of all processing elements (PEs) of\
    \ a coarse-grained reconfigurable architecture (CGRA) were replaced with the quality\
    \ adjustable adder and multiplier proposed in [\\[30\\]](#page-10-7) and [\\[34\\\
    ],](#page-10-8) respectively, which result in approximate CGRA with quality scalable\
    \ processing elements (QSPEs) that provide the ability to implement an application\
    \ with various quality levels. To exploit this feature, a heuristic technique\
    \ was proposed that is applied to the data flow graph (DFG) of a given application,\
    \ and based on an integer linear programming (ILP) method, translates the desired\
    \ quality level of the application into the approximate mode (quality level) of\
    \ the QSPEs. Note that the desired quality level of the application can be obtained\
    \ from a\n\n![](_page_4_Figure_0.jpeg)\n\n<span id=\"page-4-0\"></span>Fig. 4.\
    \ The implementation of the Full Precision (FP) multiplier and required signals\
    \ for the Reduced Precision (RP) multipliers, along with a numerical example to\
    \ show its functionality.\n\nPareto optimal curve, in which the different quality\
    \ levels of the target application correspond to various approximate modes of\
    \ the QSPEs. Thus, through this process, from a desired quality (energy consumption)\
    \ level of the application, the quality level of all adders and multipliers inside\
    \ the QSPEs is determined. In [\\[31\\],](#page-10-6) an approximate architecture\
    \ composed of quality adjustable vector processors was proposed, where a quality\
    \ control unit translates the instruction-level quality specifications to hardware\
    \ accuracy for scaling the precision of the operations.\n\nNow, to select a proper\
    \ value for *K* while meeting the , an error metric is required to show the amount\
    \ of the output inexactness induced by relaxing the *K* lower bits of the RP multipliers\
    \ in the HPR-Mul. In the approximate computing literature, different error metrics\
    \ have been proposed, such as Error Rate (ER), Error Distance (ED), Mean Error\
    \ Distance (MED), Mean Relative Error Distance (MRED), and Mean Normalized Error\
    \ Distance (MNED), where their formula have been shown in [TABLE I.](#page-4-1)\
    \ As shown in this table, most of the error metrics are defined based on the ED,\
    \ which is the difference of the exact and approximate (inexact) output values.\
    \ We also employ the ED metric to calculate the parameter *K*.\n\n[Fig. 5](#page-4-2)\
    \ shows the steps of calculating the parameter *K*. First, by receiving the *N*\
    \ (i.e., the bit width of the multiplier inputs) and , the Maximum Tolerable Error\
    \ Distance (MTED) metric is calculated by [\\[5\\]:](#page-9-4)\n\n$$MTED = (2^N\
    \ - 1) \\times \\frac{Q\\_{DUB}}{100} \\tag{l}$$\n\nNow, the MTED is rounded down\
    \ to the nearest power of two in the form of 2, where is an integer value. This\
    \ rounding down ensures that the is always me[t \\[5\\].](#page-9-4)\n\n<span\
    \ id=\"page-4-1\"></span>TABLE I SOME ERROR METRICS DEFINED FOR EVALUATING THE\
    \ ACCURACY LOSS IN THE APPROXIMATE SYSTEMS\n\n| Error Metric | Formulation   \
    \                            | Description                       |\n|--------------|-------------------------------------------|-----------------------------------|\n\
    | ER           | #\U0001D452\U0001D45F\U0001D45F \U0001D45F \U0001D452 \U0001D460\
    <br>\U0001D460<br>\U0001D447                      | \U0001D447 \U0001D447<br>\U0001D44F\
    \U0001D452\U0001D45F \U0001D453<br>\U0001D460<br>\U0001D452\U0001D460        \
    \   |\n| ED           | ′<br>  −<br>                              | \U0001D438\
    \U0001D465<br>′<br>\U0001D434 \U0001D45F\U0001D465.                  |\n| MED\
    \          | \U0001D447<br>1<br>∑ \U0001D438\U0001D437\U0001D456<br> <br>\U0001D447\
    <br>\U0001D456=1          | -                                 |\n| MRED      \
    \   | \U0001D447<br> \U0001D438\U0001D437\U0001D456<br> <br>1<br>∑<br>\U0001D447\
    <br>\U0001D456<br>\U0001D456=1 | -                                 |\n| MNED \
    \        | \U0001D440\U0001D438\U0001D437<br>\U0001D437                      \
    \            | \U0001D437 \U0001D440 \U0001D465<br>\U0001D443 \U0001D460\U0001D460\
    \ \U0001D44F \U0001D452<br>\U0001D463<br>\U0001D452<br>ℎ\U0001D452 |\n\n![](_page_4_Figure_9.jpeg)\n\
    \n<span id=\"page-4-2\"></span>Fig. 5. Steps for calculating the *K* based on\
    \ the user-defined quality constraint (i.e., ).\n\n**Inputs:** *<sup>A</sup>*=151\
    \ (10010111), *B*=108 (01101100), *N*=8, =7% **Outputs:** *MTED*=17.85=> K=2\n\
    \n![](_page_5_Figure_1.jpeg)\n\n<span id=\"page-5-2\"></span>Fig. 6. A numerical\
    \ example to show the functionality of the HPR-MUL in the presence of error.\n\
    \nThen, by setting = , the size of FP and RP multipliers is determined, i.e.,\
    \ the parameter *K* is obtained by:\n\n$$K = \\frac{1}{2} |\\log\\_2 MTED|\\,\\\
    tag{2}$$\n\nNow, the *K* lower bits of the inputs of RP multipliers are relaxed,\
    \ i.e., the TMR technique is not applied to these bits. In this case, the output\
    \ of these multipliers is *2(N-K)*-bit, and thus, the size of the voter is also\
    \ *2(N-K)*-bit (se[e Fig. 3\\)](#page-3-2). Thus, instead of a *2N*-bit voter\
    \ used in typical TMR multipliers and the expensive voters used in typical RPR\
    \ designs shown i[n Fig. 1,](#page-1-0) a simpler voter with lower design metrics\
    \ (i.e., delay, power/energy consumption, and area) is required to compose the\
    \ HPR-Mul.\n\nTo show the functionality and error tolerance of the proposed HPR-Mul,\
    \ a numerical example of the proposed method is shown in [Fig. 6.](#page-5-2)\
    \ In this example, by assuming *N=8* and the = 7%, using (2), the value of parameter\
    \ *K* will be 2. Thus, 2-bit of the inputs of RP multipliers are truncated, and\
    \ thus, the outputs of RP multipliers are 12-bit (i.e., *2(N-K)*-bit) instead\
    \ of 16-bit (i.e., *2N*-bit). In this case, the size of the voter is also 12 bit.\n\
    \n#### IV. RESULTS AND DISCUSSION\n\n<span id=\"page-5-0\"></span>In this section,\
    \ first, we evaluate the effectiveness of the proposed HPR-Mul against soft errors\
    \ and compare it with the typical TMR-based multiplier, as well as the state-of-the-art\
    \ RPR multiplier presented in [\\[11\\].](#page-9-10) Next, we investigate the\
    \ design parameters (delay, power, and area) of these schemes. Finally, we examine\
    \ the efficacy of the studied TMR schemes in the different image processing applications.\n\
    \n## <span id=\"page-5-1\"></span>*A. Soft Error Tolerance*\n\nTo investigate\
    \ the effect of soft errors on the discussed redundancy schemes, we inject uniformly\
    \ distributed random errors in their components. [Fig. 7](#page-5-3) shows the\
    \ used noise model to inject errors, where inspired by [\\[5\\],](#page-9-4) we\
    \ proposed a controllable noise injection model, in which a noise source is placed\
    \ at the input of multipliers. Next, depending on a predefined probability of\
    \ the flip () factor and in a bitwise manner, each bit of the inputs of the multiplier\
    \ is flipped independently. For this, corresponding to each bit, a uniform random\
    \ value between 0 to 1 is generated, and if the generated value is less or equal\
    \ to the , the input bit of the multiplier is flipped. Otherwise, that bit remained\
    \ unchanged, i.e., it was not affected by the noise. Thus, in the following examinations,\
    \ multiple faults are injected on different multipliers with the factor of . Note\
    \ that for this evaluation, we used the same random inputs for all the studied\
    \ redundancy schemes. Finally, the Mean Square Error (MSE) value of the output\
    \ is calculated to show the soft error tolerance of the investigated redundancy\
    \ schemes [\\[11\\].](#page-9-10)\n\n![](_page_5_Figure_12.jpeg)\n\n<span id=\"\
    page-5-3\"></span>Fig. 7. The used noise model to insert errors at the inputs\
    \ of multipliers.\n\n[Fig. 8](#page-5-4) shows the MSE of RPR and HPR schemes,\
    \ for = 8 and the different values of , including 2, 4, and 6, where the results\
    \ are normalized to those of the typical TMR scheme. Based on the results, for\
    \ the in the range of [0.001, 0.02] and for = , 4, and 6, the proposed HPR schemes\
    \ achieve, on average, 77%, 81%, and 84% lower MSE, respectively, compared to\
    \ the state-of-the-art RPR multiplier.\n\n![](_page_5_Figure_15.jpeg)\n\n<span\
    \ id=\"page-5-4\"></span>Fig. 8. MSE of the HPR and RPR multipliers normalized\
    \ to the typical TMR multipliers, using the noise model presented in [Fig. 7.](#page-5-3)\n\
    \n![](_page_6_Figure_0.jpeg)\n\n<span id=\"page-6-2\"></span>Fig. 9. MSE of the\
    \ HPR and RPR multipliers normalized to the typical TMR multipliers, when the\
    \ errors are injected into the internal signals of the studied designs with a\
    \ uniform random distribution.\n\n![](_page_6_Figure_2.jpeg)\n\n<span id=\"page-6-3\"\
    ></span>Fig. 10. The simulation setup and tool flow for synthesizing the investigated\
    \ designs.\n\nIt can be seen that by increasing the value of the accuracy of the\
    \ RPR scheme is decreased drastically (see the range of MSE in [Fig. 8](#page-5-4)\
    \ a, b, and c), while for the HPR scheme, the MSE is significantly lower. These\
    \ improvements are due to enhancing the accuracy of RP multipliers in the HPR\
    \ structure, by exploiting intermediate signals of the FP multiplier (see [Fig.\
    \ 3\\)](#page-3-2). Note that the results in this figure were normalized to the\
    \ typical TMR multiplier, e.g., at some points (e.g., see pointer ① in Figure\
    \ 8.a) the normalized value of MSE is increased, which does not necessarily mean\
    \ an increase in the amount of MSE.\n\nWe also examined the studied designs, when\
    \ the errors are injected inside the designs. For this, first, we implemented\
    \ all the studied designs at the gate level, such that their internal signals\
    \ are available and we can inject errors into the internal signals. [Fig. 9](#page-6-2)\
    \ shows the achieved results when the errors are injected into the internal signals\
    \ of the designs. Based on the results, for *K*=2, 4, and 6, the proposed HPR\
    \ schemes achieve, on average, 78%, 82%, and 77% lower MSE, respectively, compared\
    \ to the state-of-the-art RPR multiplier. Note that compared to the case when\
    \ the noise source is placed at the inputs of modules (i.e., [Fig. 8\\)](#page-5-4),\
    \ the results of [Fig. 9](#page-6-2) show the absolute variation of 1.9%, 0.7%,\
    \ and 8.8%, respectively.\n\n#### <span id=\"page-6-0\"></span>*B. Design Metrics*\n\
    \nIn this subsection, we compare the design metrics (i.e., area, delay, and power)\
    \ of the proposed HPR scheme with those of the state-of-the-art RPR structure,\
    \ as well as the typical TMR schemes. The simulation setup and used tool flow\
    \ for these evaluations are shown in [Fig. 10.](#page-6-3) As shown in this figure,\
    \ the studied designs are implemented in Verilog HDL, and synthesized by the Synopsys\
    \ Design Compiler (DC) using a 15nm Open Cell Library technology [\\[33\\],](#page-10-9)\
    \ with the normal operating voltage of 0.8V. Then, the Value Change Dump (VCD)\
    \ file of the synthesized designs is created using the Modelsim HDL simulator,\
    \ which corresponds to the activity of internal nodes. Note that the VCD file\
    \ is obtained by injecting the real inputs (i.e., the studied image benchmarks)\
    \ into the studied TMR designs. Then, the VCD files are translated to Switching\
    \ Activity Interchange Format (SAIF) file that is used by Synopsys Prime Time\
    \ to obtain the accurate power consumption of the synthesized designs.\n\n[Fig.\
    \ 11](#page-7-0) shows the design metrics of the studied designs obtained for\
    \ the different values of *K*, ranging from 1 to 7. Based on the results, our\
    \ proposed HPR multiplier achieved up to 69% and 70% lower area and power consumption,\
    \ respectively, compared to the typical TMR-based multiplier, when *K* is 7. However,\
    \ the delay of the HPR is, on average, 5% higher than the typical TMR multiplier,\
    \ for the different studied values of K. This overhead is due to extra signals\
    \ in the HPR structure (see [Fig. 4\\)](#page-4-0) compared to the typical TMR\
    \ multiplier. In general, for the different values of *K*, the HPR multiplier\
    \ offers, on average, 44% and 45% lower area and power consumption, compared to\
    \ those of the typical TMR multiplier. Furthermore, the HPR multiplier led to\
    \ slight overheads of, on average, 3% and 2% higher area and delay, respectively,\
    \ when compared to the RPR multiplier, achieving a 3% lower delay. The extra delay\
    \ of the RPR multiplier is due to the complex structure of its voter (see [Fig.\
    \ 1\\)](#page-1-0) compared to the simple voter of the proposed HPR multiplier\
    \ (see [Fig. 3\\)](#page-3-2).\n\n#### <span id=\"page-6-1\"></span>*C. Image\
    \ Processing Applications*\n\nIn this subsection, we investigate the effectiveness\
    \ of the proposed HPR multiplier in image processing applications, including image\
    \ multiplication, image sharpening, and image smoothing filters, when given levels\
    \ of noise are injected into the applications.\n\n![](_page_7_Figure_0.jpeg)\n\
    \n<span id=\"page-7-0\"></span>Fig. 11. Normalized a) area, b) delay, and c) power\
    \ consumption of the studied redundancy-based multipliers compared to the typical\
    \ TMR multiplier.\n\nThe selection of image processing for evaluating our proposed\
    \ method in critical applications is due to getting the full potential of error-resilient\
    \ approximable applications deployed in critical systems. For instance, as demonstrated\
    \ in [\\[12\\],](#page-9-11)  cameras in an autonomous vehicle continuously receive\
    \ images from the surrounding environment to extract important features for safety\
    \ purposes, such as preventing lane departure. However, neither the human driver\
    \ nor the autonomous system requires all the details of the images. Instead, the\
    \ focus is on reliably extracting critical information from the images in the\
    \ event of a system fault. Therefore, in this scenario, reliability takes precedence\
    \ over the exact computation of unnecessary details. Our proposed HPR-Mul takes\
    \ advantage of this opportunity to fully utilize approximate computing and address\
    \ the challenges associated with typical TMR multiplier designs, such as their\
    \ \"strict majority\" property and high sensitivity to soft errors.\n\nFor these\
    \ evaluations, we leverage the noise injection model shown in [Fig. 7.](#page-5-3)\
    \ Also, in this work, six standard benchmark images from [\\[35\\]](#page-10-10)\
    \ are used. Note that in these image processing applications, similar to state-of-the-art\
    \ works such as [\\[11\\],](#page-9-10) and for simplicity, we consider a given\
    \ value of *K* (i.e., *K*=4), which can be obtained from the quality degradation\
    \ upper bound equations presented in equations (1) and (2). However, it would\
    \ be of great value in this area if work is conducted proposing a systematic method\
    \ for determining the quality level of approximate arithmetic of error-resilient\
    \ applications deployed in critical systems, considering the reliability issues.\n\
    \nFurthermore, to investigate the quality of output images, the mean structural\
    \ similarity index metric (MSSIM) is used. The MSSIM works based on the human\
    \ visual systems that extract the information based on the structure of the image.\
    \ MSSIM is calculated b[y \\[5\\]:](#page-9-4)\n\n$$MSSIM(X, Y) = \\frac{(2\\\
    mu\\_{\\text{x}}\\mu\\_{\\text{y}} + \\mathcal{C}\\_1)(2\\sigma\\_{\\text{xy}}\
    \ + \\mathcal{C}\\_2)}{(\\mu\\_{\\text{x}}^2 + \\mu\\_{\\text{y}}^2 + \\mathcal{C}\\\
    _1)(\\sigma\\_{\\text{x}}^2 + \\sigma\\_{\\text{y}}^2 + \\mathcal{C}\\_2)}\\tag{3}$$\n\
    \nwhere , , , , and are the local means, standard deviations, and cross-covariance\
    \ of images and . Note that the MSSIM is in the range of 0 to 1, where the higher\
    \ value of MSSIM corresponds to a higher quality.\n\n[TABLE II](#page-7-1) shows\
    \ the MSSIM of output images obtained by the image multiplication application,\
    \ for the two values of (i.e., 0.01 and 0.05), and when the parameter of the RPR\
    \ and HPR multipliers is 4. Based on the results, the image multiplication using\
    \ the HPR multiplier achieves, on average, 7% (24%) and 9% (177%) higher MSSIM\
    \ for the = 1% and\n\n<span id=\"page-7-1\"></span>TABLE II MSSIM OF IMAGE MULTIPLICATION\
    \ USING THE DIFFERENT TMR SCHEMES, UNDER THE TWO VALUES OF\n\n| Benchmark Image\
    \              | \U0001D477\U0001D487   | Typical TMR | RPR  | HPR  |\n|------------------------------|------|-------------|------|------|\n\
    |                              | 0.01 | 0.77        | 0.90 | 0.97 |\n| Lena ×\
    \ Sailboat on lake      | 0.05 | 0.25        | 0.59 | 0.65 |\n|              \
    \                | 0.01 | 0.84        | 0.93 | 0.98 |\n| Monarch Butterfly × Mandrill\
    \ | 0.05 | 0.28        | 0.69 | 0.74 |\n|                              | 0.01\
    \ | 0.75        | 0.90 | 0.97 |\n| Female × Tree                | 0.05 | 0.21\
    \        | 0.60 | 0.66 |\n|                              | 0.01 | 0.79       \
    \ | 0.91 | 0.97 |\n| Average                      | 0.05 | 0.25        | 0.63\
    \ | 0.68 |\n\n = 5%, respectively, compared to the RPR (Typical TMR) based image\
    \ multiplication. In the image sharpening application, the output image is obtained\
    \ by [\\[34\\]:](#page-10-8)\n\n$$Y(i,j) = 2X(i,j) - \\frac{1}{273} \\sum\\_{m=-2}^{2}\
    \ \\sum\\_{n=-2}^{2} X\\left(i+m, j+n\\right)$$\n\n$$\\cdot \\text{Mask}\\_{\\\
    text{Sharpening}}(m+3, n+3) \\tag{4}$$\n\nwhere *X* and *Y* are the input and\
    \ output images, respectively. Also, MaskSharpening matrix is\n\n$$\\text{Mask}\\\
    _{\\text{Sharpening}} = \\begin{bmatrix} 1 & 4 & 7 & 4 & 1\\\\ 4 & 16 & 26 & 16\
    \ & 4\\\\ 7 & 26 & 41 & 26 & 7\\\\ 4 & 16 & 26 & 16 & 4\\\\ 1 & 4 & 7 & 4 & 1\
    \ \\end{bmatrix} \\tag{5}$$\n\nIn [TABLE III,](#page-8-0) the MSSIM of output\
    \ images of the image sharpening filter under the different values of is presented.\
    \ Based on the results, the HPR-based image sharpening filter achieves, on average,\
    \ 11% (32%) and 13% (49%) higher MSSIM for the = 1% and = 5%, respectively, compared\
    \ to the RPR (Typical TMR)-based image sharpening filter.\n\nTo achieve the smoothed\
    \ output image, the following equation is used [\\[34\\]:](#page-10-8)\n\n$$Y(i,j)\
    \ = \\frac{1}{60} \\sum\\_{m=-2}^{2} \\sum\\_{n=-2}^{2} X\\left(i+m, j+n\\right)\
    \ \\cdot \\newline \\text{Mask}\\_{\\text{Smoothing}}\\left(m+3, n+3\\right) \\\
    tag{6}$$\n\nwhere *X* and *Y* are the input and output images, respectively, and\
    \ MaskSmoothing matrix is realized by:\n\n$$\\mathbf{M} \\text{ask}\\_{\\text{Smoothing}}\
    \ = \\begin{bmatrix} 1 & 1 & 1 & 1 & 1 \\\\ 1 & 4 & 4 & 4 & 1 \\\\ 1 & 4 & 12\
    \ & 4 & 1 \\\\ 1 & 4 & 4 & 4 & 1 \\\\ 1 & 1 & 1 & 1 & 1 \\end{bmatrix} \\tag{7}$$\n\
    \n| Benchmark Image   | \U0001D477\U0001D487   | Typ.  | RPR   | HPR   |\n|-------------------|------|-------|-------|-------|\n\
    |                   | 0.01 | 0.747 | 0.885 | 0.986 |\n| Sailboat on lake  | 0.05\
    \ | 0.607 | 0.823 | 0.929 |\n|                   | 0.01 | 0.793 | 0.892 | 0.992\
    \ |\n| Lena              | 0.05 | 0.670 | 0.855 | 0.960 |\n| Monarch Butterfly\
    \ | 0.01 | 0.784 | 0.884 | 0.985 |\n|                   | 0.05 | 0.683 | 0.824\
    \ | 0.931 |\n|                   | 0.01 | 0.725 | 0.884 | 0.985 |\n| Mandrill\
    \          | 0.05 | 0.617 | 0.825 | 0.929 |\n| Tree              | 0.01 | 0.737\
    \ | 0.877 | 0.980 |\n|                   | 0.05 | 0.603 | 0.813 | 0.921 |\n| Female\
    \            | 0.01 | 0.687 | 0.878 | 0.979 |\n|                   | 0.05 | 0.568\
    \ | 0.797 | 0.905 |\n|                   | 0.01 | 0.746 | 0.883 | 0.985 |\n| Average\
    \           | 0.05 | 0.625 | 0.823 | 0.929 |\n\n<span id=\"page-8-0\"></span>TABLE\
    \ III MSSIM OF IMAGE SHARPENING USING THE DIFFERENT TMR SCHEMES, FOR THE TWO VALUES\
    \ OF\n\nThe MSSIM of output images obtained by the image smoothing filter application,\
    \ are shown in [TABLE IV.](#page-8-1) Based on the results, the image smoothing\
    \ filter using the HPR multiplier results in, on average, 11% (212%) and 11% (1065%)\
    \ higher MSSIM for the = 1% and = 5%, respectively, compared to the RPR (Typical\
    \ TMR)-based image smoothing filter. Note that the output images of the typical\
    \ TMR-based image smoothing filter have a very low and almost unacceptable quality,\
    \ while the HPR-based filter offers the highest quality.\n\n[Fig. 12](#page-8-2)\
    \ shows the input and output images of the image sharpening and smoothing filters\
    \ leveraging the different studied redundancy-based multipliers for the \"*Monarch\
    \ Butterfly*\" benchmark image, under the two values of 0.01 and 0.05. As the\
    \ results of [TABLE IV](#page-8-1) and [Fig. 12](#page-8-2) show, the typical\
    \ TMRbased image smoothing filter has a very low quality compared to the other\
    \ studied image processing applications.\n\nThis behavior is because the image\
    \ smoothing filter generates output pixels with a lower value compared to the\
    \ other studied image processing applications. In detail, the range of matrix\
    \ elements (5) is between 1 and 41, while this is between 1 and 12 in (7), i.e.,\
    \ in the image smoothing filter, the pixels of input images are multiplied in\
    \ numbers with a significantly lower range, which results in lower output value\
    \ rather than other studied image processing applications. Moreover, the typical\
    \ TMR designs are sensitive to a slight difference between the module's output,\
    \ which is referred to as the \"strict majority\" property [\\[5\\].](#page-9-4)\
    \ Therefore, the typical TMR-based image smoothing filter has an almost unacceptable\
    \ quality compared to the other image processing applications, in the presence\
    \ of errors.\n\n<span id=\"page-8-1\"></span>TABLE IV MSSIM OF THE IMAGE SMOOTHING\
    \ FILTER USING THE DIFFERENT TMR SCHEMES, FOR THE TWO VALUES OF\n\n| Benchmark\
    \ Image   | \U0001D477\U0001D487   | Typ.  | RPR   | HPR   |\n|-------------------|------|-------|-------|-------|\n\
    |                   | 0.01 | 0.333 | 0.898 | 0.997 |\n| Sailboat on lake  | 0.05\
    \ | 0.092 | 0.876 | 0.976 |\n|                   | 0.01 | 0.370 | 0.899 | 0.999\
    \ |\n| Lena              | 0.05 | 0.095 | 0.892 | 0.992 |\n|                 \
    \  | 0.01 | 0.335 | 0.899 | 0.999 |\n| Monarch Butterfly | 0.05 | 0.089 | 0.888\
    \ | 0.988 |\n|                   | 0.01 | 0.298 | 0.898 | 0.998 |\n| Mandrill\
    \          | 0.05 | 0.068 | 0.883 | 0.983 |\n|                   | 0.01 | 0.260\
    \ | 0.898 | 0.998 |\n| Tree              | 0.05 | 0.060 | 0.884 | 0.985 |\n| \
    \                  | 0.01 | 0.326 | 0.898 | 0.998 |\n| Female            | 0.05\
    \ | 0.103 | 0.886 | 0.985 |\n|                   | 0.01 | 0.320 | 0.898 | 0.998\
    \ |\n| Average           | 0.05 | 0.085 | 0.885 | 0.985 |\n\n![](_page_8_Figure_7.jpeg)\n\
    \n<span id=\"page-8-2\"></span>Fig. 12. Input and output images of the image sharpening\
    \ and smoothing filters leveraging the different studied redundancy-based multipliers,\
    \ for the \"*Monarch Butterfly*\" benchmark image, under the two values of 0.01\
    \ and 0.05.\n\nFinally, [Fig. 13](#page-9-24) shows the power consumption of the\
    \ image multiplication application using the HPR-Mul scheme for the three different\
    \ values of *K* (i.e., *K*=2, 4, and 6), in which the power results were normalized\
    \ to the typical TMR multiplier. Based on the results shown in this figure, for\
    \ the Female × Tree, Lena × Sailboat on Lake, and Monarch Butterfly × Mandril\
    \ image multiplications, the proposed HPR-Mul achieved, on average, 42%, 47%,\
    \ and 37% lower power consumption compared to the typical TMR multiplier, respectively,\
    \ for the three studied values of K.\n\n![](_page_9_Figure_1.jpeg)\n\n<span id=\"\
    page-9-24\"></span>Fig. 13. Normalized power consumption of the HPR-Mul for the\
    \ image multiplication application under the various values of *K*.\n\n#### V.\
    \ CONCLUSION\n\n<span id=\"page-9-12\"></span>In this paper, we presented a high\
    \ precision redundancy multiplier (HPR-Mul) structure that leverages a full precision\
    \ multiplier along with the two reduced precision multipliers, where the output\
    \ is determined by a simple majority-based voter. Employing these reduced precision\
    \ modules provides a higher energy efficiency and lower area while resolving the\
    \ challenges of the typical TMR schemes, such as the high sensitivity to soft\
    \ errors and the design diversity of the triplicated module. Unlike prior state-of-the-art\
    \ reduced precision redundancy multipliers (RPR-Mul), the proposed HPR-Mul employs\
    \ some signals of the full precision multiplier to enhance the accuracy of the\
    \ reduced precision ones resulting in a considerably higher accuracy. The HPR-Mul\
    \ showed up to 69% and 70% lower area and power consumption compared to the typical\
    \ TMR multiplier, at the cost of 4% higher delay. Moreover, the HPR-Mul outperforms\
    \ the RPR-Mul by achieving up to 84% higher soft error tolerance. Finally, in\
    \ the examined image processing applications, the HPR-based applications showed\
    \ up to 13% (1065%) higher MSSIM in comparison with the state-of-the-art RPR (typical\
    \ TMR-based) multipliers, respectively.\n\n### REFERENCES\n\n- <span id=\"page-9-0\"\
    ></span>[1] A. A. Bahoo, O. Akbari and M. Shafique, \"An Energy-Efficient Generic\
    \ Accuracy Configurable Multiplier Based on Block-Level Voltage Overscaling,\"\
    \ in *IEEE Transactions on Emerging Topics in Computing*, vol. 11, no. 4, pp.\
    \ 851-867, Oct.-Dec. 2023.\n- <span id=\"page-9-1\"></span>[2] I. Koren and C.\
    \ Krishna, *Fault-Tolerant Systems*., Morgan Kaufmann, 2007.\n- <span id=\"page-9-2\"\
    ></span>[3] I. Alzaher-Noufal and M. Nicolaidis, \"A CAD Framework for Generating\
    \ Self-Checking Multipliers Based on Residue Codes\", in Proceedings of the Conference\
    \ on Design, Automation and Test in Europe (DATE), 1999.\n- <span id=\"page-9-3\"\
    ></span>[4] M. Nicolaidis and R. O. Duarte, \"Fault-Secure Parity Prediction Booth\
    \ Multipliers\", IEEE Design & Test of Computers, vol. 16, no. 3, pp. 90- 101,\
    \ 1999.\n- <span id=\"page-9-4\"></span>[5] J. Vafaei, O. Akbari, M. Shafique\
    \ and C. Hochberger, \"X-Rel: Energy-Efficient and Low-Overhead Approximate Reliability\
    \ Framework for Error-Tolerant Applications Deployed in Critical Systems,\" in\
    \ *IEEE Transactions on Very Large Scale Integration (VLSI) Systems*, vol. 31,\
    \ no. 7, pp. 1051-1064, July 2023.\n- <span id=\"page-9-5\"></span>[6] D. Kobayashi,\
    \ \"Scaling Trends of Digital Single-Event Effects: A Survey of SEU and SET Parameters\
    \ and Comparison With Transistor Performance,\" in *IEEE Transactions on Nuclear\
    \ Science*, vol. 68, no. 2, pp. 124-148, Feb. 2021.\n- <span id=\"page-9-6\"></span>[7]\
    \ S. Byonghyo, S. R. Sridhara, and N. R. Shanbhag, \"Reliable low-power digital\
    \ signal processing via reduced precision redundancy,\" IEEE Transac-tions on\
    \ Very Large Scale Integration (VLSI) Systems, vol. 12, no. 5, pp. 497-510, 2004.\n\
    - <span id=\"page-9-7\"></span>[8] B. Shim and N. R. Shanbhag, \"Energy-efficient\
    \ soft error-tolerant dig-ital signal processing,\" in IEEE Transactions on Very\
    \ Large Scale Integration (VLSI) Systems, vol. 14, no. 4, pp. 336-348, 2006.\n\
    - <span id=\"page-9-8\"></span>[9] S. LIU, K. Chen, P. Reviriego, W. Liu, A. LOURI\
    \ and F. Lombardi, \"Reduced Precision Redundancy for Reliable Processing of Data,\"\
    \ in *IEEE Transactions on Emerging Topics in Computing*, vol. 9, no. 4, pp. 1960-1971,\
    \ 1 Oct.-Dec. 2021.\n- <span id=\"page-9-9\"></span>[10] A. Ullah, P. Reviriego,\
    \ S. Pontarelli and J. A. Maestro, \"Majority Voting-Based Reduced Precision Redundancy\
    \ Adders,\" *IEEE Trans. Device and Materials Reliability,*, vol. 18, no. 1, pp.\
    \ 122-124, Mar. 2018.\n- <span id=\"page-9-10\"></span>[11] K. Chen, L. Chen,\
    \ P. Reviriego and F. Lombardi, \"Efficient Implementations of Reduced Precision\
    \ Redundancy (RPR) Multiply and Accumulate (MAC),\" in *IEEE Transactions on Computers*,\
    \ vol. 68, no. 5, pp. 784-790, 1 May 2019.\n- <span id=\"page-9-11\"></span>[12]\
    \ F. Baharvand and S. G. Miremadi, \"LEXACT: Low Energy N-Modular Redundancy Using\
    \ Approximate Computing for Real-Time Multicore Processors\", *IEEE Trans. Emerging\
    \ Topics in Computing*, pp. 1-1, 2017.\n- <span id=\"page-9-13\"></span>[13] W.\
    \ Liu, F. Lombardi and M. Shulte, \"A Retrospective and Prospective View of Approximate\
    \ Computing,\" in *Proceedings of the IEEE*, vol. 108, no. 3, pp. 394-399, March\
    \ 2020.\n- <span id=\"page-9-14\"></span>[14] G. Rodrigues, et al., \"Survey on\
    \ Approximate Computing and Its Intrinsic Fault Tolerance,\" in *Electronics*,\
    \ vol. 9, no. 4, p. 557, Mar. 2020.\n- [15] A. Aponte-Moreno, A. Moncada, F. Restrepo-Calle\
    \ and C. Pedraza, \"A review of approximate computing techniques towards fault\
    \ mitigation in HW/SW systems,\" *IEEE 19th Latin-American Test Symposium (LATS)*,\
    \ Brazil, 2018, pp. 1-6.\n- <span id=\"page-9-15\"></span>[16] A. Bosio, I. O'Connor,\
    \ G. S. Rodrigues, F. K. Lima and S. Hamdioui, \"Exploiting Approximate Computing\
    \ for implementing Low Cost Fault Tolerance Mechanisms,\" *15th Design & Technology\
    \ of Integrated Systems in Nanoscale Era (DTIS)*, Morocco, 2020, pp. 1-2.\n- <span\
    \ id=\"page-9-16\"></span>[17] K. Chen, F. Lombardi and J. Han, \"An approximate\
    \ voting scheme for reliable computing,\" *Design, Automation & Test in Europe\
    \ Conference & Exhibition (DATE)*, 2015, pp. 293-296.\n- <span id=\"page-9-17\"\
    ></span>[18] K. Chen, J. Han and F. Lombardi, \"Two Approximate Voting Schemes\
    \ for Reliable Computing,\" in *IEEE Transactions on Computers*, vol. 66, no.\
    \ 7, pp. 1227-1239, 1 July 2017.\n- <span id=\"page-9-18\"></span>[19] G.S. Rodrigues,\
    \ et al., \"Approximate TMR based on successive approximation and loop perforation\
    \ in microprocessors,\" in *Microelectronics Reliability*, vol. 100–101, sep.\
    \ 2019.\n- <span id=\"page-9-19\"></span>[20] I. Polian and J. P. Hayes, \"Selective\
    \ Hardening: Toward Cost-Effective Error Tolerance,\" *IEEE Design & Test of Computers*,\
    \ vol. 28, no. 3, pp. 54-63, May 2011.\n- <span id=\"page-9-20\"></span>[21] Iuri\
    \ A.C. Gomes, et al., \"Exploring the use of approximate TMR to mask transient\
    \ faults in logic with low area overhead,\" in *Microelectronics Reliability*,\
    \ Vol. 55, no. 9–10, pp. 2072-2076, 2015.\n- <span id=\"page-9-21\"></span>[22]\
    \ A. J. Sánchez-Clemente, L. Entrena and M. García-Valderas, \"Partial TMR in\
    \ FPGAs Using Approximate Logic Circuits,\" *IEEE Trans. Nuclear Science*, vol.\
    \ 63, no. 4, pp. 2233-2240, Aug. 2016.\n- <span id=\"page-9-22\"></span>[23] T.\
    \ Arifeen, et al., \"A Fault Tolerant Voter for Approximate Triple Modular Redundancy,\"\
    \ in *Electronics*, vol. 8, no. 3, pp. 332, Mar. 2019.\n- <span id=\"page-9-23\"\
    ></span>[24] B. Pratt, M. Fuller, M. Rice, and M. Wirthlin, \"Reduced-precision\
    \ redundancy for reliable FPGA communications systems in high-radiation environments,\"\
    \ *IEEE Trans. Aerosp. Electron. Syst.*, vol. 49, no. 1,pp. 369–380, Jan. 2013.\n\
    - <span id=\"page-10-0\"></span>[25] M. A. Sullivan, \"Reduced precision redundancy\
    \ applied to arithmetic operations in field programmable gate arrays for satellite\
    \ control and sensor systems,\" M.S. thesis, Dept. Elect. Comput. Eng., Naval\
    \ Postgraduate School, Monterey, CA, USA, 2008.\n- <span id=\"page-10-1\"></span>[26]\
    \ A. Aponte-Moreno, C. Pedraza and F. Restrepo-Calle, \"Reducing Overheads in\
    \ Software-based Fault Tolerant Systems using Approximate Computing,\" *IEEE Latin\
    \ American Test Symposium (LATS)*, Chile, 2019, pp. 1-6.\n- <span id=\"page-10-2\"\
    ></span>[27] M. Biasielli, C. Bolchini, L. Cassano, A. Mazzeo and A. Miele, \"\
    Approximation-Based Fault Tolerance in Image Processing Applications,\" *in IEEE\
    \ Transactions on Emerging Topics in Computing*, vol. 10, no. 2, pp. 648-661,\
    \ 1 April-June 2022.\n- <span id=\"page-10-3\"></span>[28] A. Aponte-Moreno, F.\
    \ Restrepo-Calle and C. Pedraza, \"FTxAC: Leveraging the Approximate Computing\
    \ Paradigm in the Design of Fault-Tolerant Embedded Systems to Reduce Overheads,\"\
    \ in *IEEE Transactions on Emerging Topics in Computing*, vol. 9, no. 2, pp. 797-\
    \ 810, 1 April-June 2021.\n- <span id=\"page-10-5\"></span>[29] O. Akbari, M.\
    \ Kamal, A. Afzali-Kusha, M. Pedram and M. Shafique, \"X-CGRA: An Energy-Efficient\
    \ Approximate Coarse-Grained Reconfigurable Architecture,\" in *IEEE Transactions\
    \ on Computer-Aided Design of Integrated Circuits and Systems*, vol. 39, no. 10,\
    \ pp. 2558-2571, Oct. 2020.\n- <span id=\"page-10-7\"></span>[30] O. Akbari, et\
    \ al., \"RAP-CLA: A Reconfigurable Approximate Carry Look-Ahead Adder,\" in *IEEE\
    \ Transactions on Circuits and Systems II: Express Briefs*, preprint, 23 May.\
    \ 2017.\n- <span id=\"page-10-6\"></span>[31] S. S. Venkataramani, et al., \"\
    Quality programmable vector processors for approximate computing,\" in *Proc.\
    \ 46th Annual International Symposium on Microarchitecture (MICRO)*, Dec. 2013,\
    \ pp. 1–12.\n- <span id=\"page-10-4\"></span>[32] A.S. Hassan, et al., \"Generation\
    \ Methodology for Good-Enough Approximate Modules of ATMR\", in *Journal of Electronic\
    \ Testing*, vol. 34, pp. 651–665, 2018.\n- <span id=\"page-10-9\"></span>[33]\
    \ M. Martins et al., \"Open cell library in 15nm FreePDK technology,\" in *Proc.\
    \ Symp. Int. Symp. Phys. Design (ISPD)*, 2015, pp. 171–178.\n- <span id=\"page-10-8\"\
    ></span>[34] O. Akbari, M. Kamal, A. Afzali-Kusha, and M. Pedram, \"Dual Quality\
    \ 4:2 Compressors for Utilizing in Dynamic Accuracy Configurable Multipliers,\"\
    \ *IEEE Trans. Very Large Scale Integr. (VLSI) Syst.*, vol. 25, no. 4, pp. 1352–1361,\
    \ Apr. 2017.\n- <span id=\"page-10-10\"></span>[35] *The USC-SIPI Image Database*.\
    \ Accessed: Mar. 16, 2024. [Online]. Available: http://sipi.usc.edu/database/\n\
    \n![](_page_10_Picture_11.jpeg)\n\n**Jafar Vafaei** received the B.Sc. degree\
    \ from the University of Tabriz, Tabriz, Iran, in 2013, and the M.Sc. degree from\
    \ University of Tehran, Tehran, Iran, in 2019, both in Electrical Engineering.\
    \ He is currently a research assistant at Tarbiat Modares University, Tehran,\
    \ Iran in the Computer Architecture and Dependable systems Laboratory (CADS-Lab).\
    \ His research interests include low power digital designs and machine learning,\
    \ reconfigurable computing,\n\nneuromorphic computing, and fault-tolerant system\
    \ design.\n\n![](_page_10_Picture_14.jpeg)\n\n**Omid Akbari** received the B.Sc.\
    \ degree from the University of Guilan, Rasht, Iran, in 2011, the M.Sc. degree\
    \ from Iran University of Science and Technology, Tehran, Iran, in 2013, and the\
    \ Ph.D. degree from the University of Tehran, Iran, in 2018, all in Electrical\
    \ Engineering, Electronics - Digital Systems subdiscipline. He was a visiting\
    \ researcher in the CARE-Tech Lab. at Vienna University of Technology (TU Wien),\
    \ Austria, from Apr. to\n\nOct. 2017, and a visiting research fellow under the\
    \ Future Talent Guest Stay program at Technische Universität Darmstadt (TU Darmstadt),\
    \ Germany, from Jul. to Sep. 2022. He is currently an assistant professor of Electrical\
    \ and Computer Engineering at Tarbiat Modares University, Tehran, Iran, where\
    \ he is also the Director of the Computer Architecture and Dependable Systems\
    \ Laboratory (CADS-Lab). His current research interests include embedded machine\
    \ learning, reconfigurable computing, energy-efficient computing, distributed\
    \ learning, and fault-tolerant system design."
- title: "A Host-SSD Collaborative Write Accelerator for LSM-Tree-Based Key-Value\n\
    \  Stores"
  abstract: 'Log-Structured Merge (LSM) tree-based Key-Value Stores (KVSs) are widely

    adopted for their high performance in write-intensive environments, but they

    often face performance degradation due to write stalls during compaction. Prior

    solutions, such as regulating I/O traffic or using multiple compaction threads,

    can cause unexpected drops in throughput or increase host CPU usage, while

    hardware-based approaches using FPGA, GPU, and DPU aimed at reducing compaction

    duration introduce additional hardware costs. In this study, we propose

    KVACCEL, a novel hardware-software co-design framework that eliminates write

    stalls by leveraging a dual-interface SSD. KVACCEL allocates logical NAND flash

    space to support both block and key-value interfaces, using the key-value

    interface as a temporary write buffer during write stalls. This strategy

    significantly reduces write stalls, optimizes resource usage, and ensures

    consistency between the host and device by implementing an in-device LSM-based

    write buffer with an iterator-based range scan mechanism. Our extensive

    evaluation shows that for write-intensive workloads, KVACCEL outperforms ADOC

    by up to 1.17x in terms of throughput and performance-to-CPU-utilization

    efficiency. For mixed read-write workloads, both demonstrate comparable

    performance.'
  url: http://arxiv.org/abs/2410.21760v1
  keywords: Key-Value Store, Log-Structured Merge Tree, Write Stall Mitigation
  document: '# arXiv:2410.21760v1 [cs.AR] 29 Oct 2024


    # A Host-SSD Collaborative Write Accelerator for LSM-Tree-Based Key-Value Stores


    Kihwan Kim1,<sup>∗</sup> , Hyunsun Chung1,<sup>∗</sup> , Seonghoon Ahn1,<sup>∗</sup>
    , Junhyeok Park<sup>1</sup> , Safdar Jamil<sup>1</sup>


    Hongsu Byun<sup>1</sup> , Myungcheol Lee<sup>2</sup> , Jinchun Choi<sup>2</sup>
    , Youngjae Kim1,†


    <sup>1</sup>Sogang University, Seoul, Republic of Korea, <sup>2</sup>ETRI, Daejeon,
    Republic of Korea


    *Abstract*—Log-Structured Merge (LSM) tree-based Key-Value Stores (KVSs) are widely
    adopted for their high performance in write-intensive environments, but they often
    face performance degradation due to write stalls during compaction. Prior solutions,
    such as regulating I/O traffic or using multiple compaction threads, can cause
    unexpected drops in throughput or increase host CPU usage, while hardware-based
    approaches using FPGA, GPU, and DPU aimed at reducing compaction duration introduce
    additional hardware costs. In this study, we propose KVACCEL, a novel hardware-software
    co-design framework that eliminates write stalls by leveraging a dual-interface
    SSD. KVACCEL allocates logical NAND flash space to support both block and key-value
    interfaces, using the key-value interface as a temporary write buffer during write
    stalls. This strategy significantly reduces write stalls, optimizes resource usage,
    and ensures consistency between the host and device by implementing an in-device
    LSM-based write buffer with an iterator-based range scan mechanism. Our extensive
    evaluation shows that for write-intensive workloads, KVACCEL outperforms ADOC
    by up to 17% in terms of throughput and performance-to-CPU-utilization efficiency.
    For mixed read-write workloads, both demonstrate comparable performance.


    *Index Terms*—Key-Value Store, Log-Structured Merge Tree, Write Stall Mitigation


    ### I. INTRODUCTION


    Log-Structured Merge (LSM) tree-based Key-Value Store (KVS) systems, such as RocksDB
    [\[1\]](#page-10-0) and LevelDB [\[2\]](#page-10-1), are commonly used in write-intensive
    applications due to their ability to handle high-throughput writes efficiently.
    However, LSM-based KVSs (LSM-KVSs) often experience performance degradation due
    to write stalls that occur during compaction [\[3\]](#page-10-2)– [\[8\]](#page-10-3).
    These write stalls block incoming write operations, resulting in a significant
    reduction in throughput and an increase in tail latency, which undermines system
    reliability in timesensitive workloads.


    To alleviate write stalls, RocksDB [\[1\]](#page-10-0), one of the most widely
    used LSM-KVS, implements a mechanism known as *slowdown* [\[9\]](#page-10-4).
    This slowdown mechanism anticipates potential write stalls and proactively reduces
    the write pressure on the LSM-KVS. Consequently, while it can prevent write stalls,
    it may unnecessarily decrease the throughput of RocksDB by limiting the write
    pressure directed to the LSM-KVS. Additionally, the state-of-the-art solution
    ADOC [\[5\]](#page-10-5) mitigates write stalls by dynamically increasing batch
    sizes and the number of compaction threads during a write slowdown, thereby reducing
    compaction duration. However, ADOC increases host CPU utilization by employing
    multiple compaction threads.


    Alternatively, hardware-based solutions have been investigated. Persistent memory
    (PM)-based designs [\[6\]](#page-10-6), [\[10\]](#page-10-7), [\[11\]](#page-10-8)
    buffer writes in PM before flushing them to the LSM-tree, while FPGA-based accelerators
    [\[12\]](#page-10-9)–[\[14\]](#page-10-10), GPU [\[15\]](#page-10-11)–[\[17\]](#page-10-12),
    and DPU [\[18\]](#page-10-13), [\[19\]](#page-10-14) speed up merge sort to reduce
    compaction time. Key-Value SSD (KV-SSD) architectures [\[20\]](#page-10-15)–[\[25\]](#page-10-16)
    handle key-value operations directly within storage devices, bypassing the OS
    and file system overheads. Although these approaches enhance performance, they
    require additional hardware (e.g., PM, FPGA, GPU, DPU), raising costs and complexity.


    The aforementioned software solutions suffer from unnecessary performance degradation
    due to inaccurate predictions or increased host CPU usage, while hardware solutions
    require additional hardware, raising costs. In this study, we propose a groundbreaking
    approach that avoids write stalls without compromising KVS performance, minimizes
    host CPU utilization, and requires no additional hardware costs. Our method represents
    a new paradigm, fundamentally different from existing approaches, by actively
    leveraging idle resources in existing storage devices to avoid write stalls while
    minimizing host CPU involvement.


    In this paper, we present KVACCEL, a novel hybrid hardware-software co-design
    framework that leverages a new dual-interface SSD architecture to mitigate write
    stalls and optimize the utilization of storage bandwidth. KVACCEL is built on
    the observation that during host-side write stalls, the underlying storage device''s
    available I/O bandwidth remains underutilized, despite its potential to handle
    additional I/O operations. KVACCEL then incorporates a dynamic I/O redirection
    mechanism that monitors the status of host-side LSM-KVS and, upon detecting a
    write stall, shifts writes from the LSM-KVS to the device-side key-value write
    buffer.


    KVACCEL presents a disaggregation of the SSD''s logical NAND flash address space
    into two regions: one for the traditional block interface, which is managed by
    the host-side LSM-KVS, and another for the key-value interface inspired by the
    KV-SSD, which serves as a temporary write buffer to serve pending write requests
    by bypassing the traditional LSM-based data path during stalls.


    To maintain consistency between the main LSM on the host and the write buffer
    on the device, KVACCEL introduces a range scan-based rollback mechanism. This
    mechanism structures the device-side write buffer as a separate LSM from the host-side
    main LSM and employs an iterator-based range scan over the buffer, enabling fast
    scan of buffered key-value


    <sup>∗</sup>They are first co-authors and have contributed equally.


    <sup>†</sup>Y. Kim is the corresponding author.


    pairs back to the host for merging. KVACCEL then merges them into the main LSM,
    maintaining the properties of the LSM and ensuring data consistency between the
    two interfaces.


    KVACCEL offers detector, I/O redirection, and rollback modules on top of RocksDB
    [\[1\]](#page-10-0). The dual-interface SSD was implemented using the Cosmos+
    OpenSSD platform [\[26\]](#page-10-17), an FPGA-based NVMe SSD development board.
    RocksDB operates on the block interface provided by a single OpenSSD, while the
    key-value interface of the same device handles the redirected key-value pairs.


    The key contributions of this paper are as follows:


    - We identify a critical opportunity to mitigate the fundamental issue of write
    stalls in LSM-KVS by leveraging the underutilized storage bandwidth during these
    stalls, transforming an inherent inefficiency into a performance optimization.

    - We propose a hybrid SSD architecture that integrates a new key-value interface
    alongside the traditional block interface within a singular device, allowing us
    to address write stalls without significantly modifying the existing LSM-KVS or
    deploying additional hardware in the system.

    - We develop efficient dynamic I/O redirection and rollback mechanisms to seamlessly
    manage data flow between the host-side LSM and device-side key-value interface,
    ensuring consistency and high performance.

    - Our approach demonstrates that by introducing an additional storage interface,
    separate from the traditional block interface, on a singular storage device in
    the system, we can provide an architecturally beneficial solution to address the
    inherent limitations of LSM-KVS, which required intentionally lowering the quality
    of write service to mitigate write stalls.


    Our extensive evaluation using db bench [\[27\]](#page-10-18) demonstrates that
    KVACCEL completely eliminates write halts and achieves up to a 17% increase in
    throughput compared to the state-of-theart solution by utilizing underutilized
    PCIe bandwidth during write stall periods, all while maintaining read performance.
    These results show that KVACCEL not only alleviates the performance bottlenecks
    of existing LSM-KVS systems but also introduces a novel, architecturally superior,
    cost-effective solution for optimizing write-intensive workloads in modern storage
    environments.


    ### II. BACKGROUND AND RELATED WORK


    This section reviews the compaction process in LSM-tree structures, where write
    stalls occur, and examines related research aimed at mitigating these stalls.


    ### *A. Log-Structured Merge Tree and Write Stall Issue*


    The Log-Structured Merge (LSM) tree [\[28\]](#page-10-19) is a writeoptimized
    data structure widely adopted in various NoSQL databases including LevelDB [\[2\]](#page-10-1),
    RocksDB [\[1\]](#page-10-0), and Cassandra [\[29\]](#page-10-20). The LSM-tree
    organizes data into memory and disk-based components with hierarchical levels
    increasing in size, as shown in Figure [1.](#page-1-0) The memory components include
    active MemTable (MT) which absorbs the incoming write requests from application.
    Once MT reaches a size threshold, a new active MT is allocated and old MT is converted
    into


    <span id="page-1-0"></span>![](_page_1_Figure_12.jpeg)


    Fig. 1: An architecture of LSM-tree.


    an Immutable MemTable (IMT). The flush operation picks the IMT and convert it
    into Sorted String Table (SST) file and write to storage device. The SSTs are
    organized in ascending levels, with each level having a size threshold. When a
    level reaches the size threshold, SSTs of current victim level n goes through
    a merge-sort operation, known as compaction, with SSTs of level n + 1. This process
    ensures key-value pairs within each SST to be sorted and unique.


    Write Stall Problem: Despite being write-optimized, LSM-KVSs suffer from the write
    stall problem. We define the write stall problem as blocking of incoming write
    requests by the internals of LSM-tree. SILK [\[3\]](#page-10-2) and ADOC [\[5\]](#page-10-5)
    categorize these write stalls into three different events. 1 Flush-based write
    stalls: when the flush operation is not able to keep up with the rate of incoming
    write requests resulting in exhaustion of memory. 2 L<sup>0</sup> to L<sup>1</sup>
    compaction-based write stalls: the SSTs in level 0 can hold overlapping key range,
    which necessitate the compaction operation to be serialized between L<sup>0</sup>
    to L<sup>1</sup> compaction. This serialization of L<sup>0</sup> to L<sup>1</sup>
    compaction can lead to blocking of flush operation when L<sup>0</sup> reaches
    its size threshold, resulting in L<sup>0</sup> to L<sup>1</sup> compaction-based
    write stall event. 3 Pending compaction bytes-based write stall: when the lower
    levels of LSM-KVS delays the compaction operation leading to high space amplification,
    resulting in blocking of incoming write requests.


    ### *B. Existing Optimizations for Addressing Write Stall Issue*


    To optimize LSM-KVS, there have been extensive research conducted by academia
    and industry which can be classified into two categories: (i) software-level and
    (ii) hardware-level.


    Software-Level Optimization: SILK [\[3\]](#page-10-2) introduces an I/O scheduler
    that mitigates write stalls by delaying flush and compaction operations to low-load
    periods, prioritizing flushes and lower-level compactions, and preempting compactions.
    Despite these strategies, SILK offers minimal performance improvement and exhibits
    ordinary tail latency under sustained write-intensive and long peak workloads.
    RocksDB [\[1\]](#page-10-0) indeed employs a slowdown mechanism [\[9\]](#page-10-4)
    that predicts potential write stalls and intentionally lowers the write throughput
    to prevent sudden performance drops, but this comes at the cost of increased latency
    and degraded service quality during heavy workloads. Blsm [\[30\]](#page-10-21)
    proposes a merge scheduler to coordinate compactions across multiple levels, but
    the L<sup>0</sup> to L<sup>1</sup> compaction still severely stalls foreground
    requests. The state-of-the-art solution, ADOC [\[5\]](#page-10-5), also reduces
    and restores


    <span id="page-2-0"></span>![](_page_2_Figure_0.jpeg)


    <span id="page-2-1"></span>200


    200


    <sup>0</sup> <sup>200</sup> <sup>400</sup> <sup>600</sup> <sup>0</sup> Fig. 2:
    Per-second throughput time-series for RocksDB and ADOC, based on write slowdown
    usage. 20 20 20


    P99 P99.9 <sup>0</sup>


    the write ratio as needed, and introduces a new mechanism to dynamically adjust
    write buffer size and background threads during write-intensive workloads, demonstrating
    superior write stall mitigation compared to existing solutions.


    Despite these efforts, *existing approaches aim to minimize compaction time to
    mitigate the write stall issue, but they ultimately rely on intentionally lowering
    the write request rate.* This trade-off negatively impacts service quality for
    users, highlighting the inherent limitation of ensuring uninterrupted write operations
    at the cost of degraded performance.


    Hardware-Level Optimization: To eliminate the high storage stack overhead during
    key-value writes and compaction, some studies have implemented LSM-KVS directly
    on SSDs, referred to as Key-Value SSDs (KV-SSDs) [\[20\]](#page-10-15)–[\[25\]](#page-10-16).
    iLSM [\[22\]](#page-10-22) bypasses the file system and block layer within the
    kernel, thereby improving the I/O latency and throughput of key-value clients.
    PinK [\[23\]](#page-10-23) proposed a resource-efficient LSM-KVS within the KV-SSD
    and demonstrated that KV-SSDs can reduce CPU and DRAM resource usage on both the
    host and device side. In contrast, there are studies that optimize LSM-KVS by
    leveraging Persistent Memory (PM). MatrixKV [\[6\]](#page-10-6) observes the shortcoming
    of the original SST format and points to slow L<sup>0</sup> to L<sup>1</sup> compaction
    as the root cause of write stalls when deploying PMs. It redesigns the format
    of SST for PM and proposes a new compaction scheme between the first two levels,
    which they call column-compaction. Zhang et. al. [\[13\]](#page-10-24) proposed
    an FPGA-based acceleration engine for LMS-KVS to speed up the compaction process
    via hardware-software collaboration, improving throughput and averting resource
    contention. However, these studies face significant limitations in terms of applicability,
    as they rely on new devices that *either require completely bypassing host-side
    LSM-KVS stacks or adding new hardware components*.


    ![](_page_2_Figure_5.jpeg)


    <sup>0</sup> <sup>200</sup> <sup>400</sup> <sup>600</sup> <sup>0</sup> Fig. 3:
    Throughput (a) and tail latency (b) results of RocksDB and ADOC, based on write
    slowdown usage.


    ### III. PROBLEM DEFINITION


    P99 P99.9 <sup>0</sup> 20 40 P99 P99.9 <sup>0</sup> In this section, we point
    out that both industry standard and state-of-the-art software-level solutions
    both rely on the write slowdown, an inefficient write stall prevention method.
    Furthermore, we highlight that during write stalls in LSM-KVS, the storage device
    is underutilized, even though it still has the capacity to process I/O requests.


    ## P99 P99.9 <sup>0</sup> *A. Slowdowns: The Inefficient Write Stall Solution*


    1000 2000 3000


    To prevent write stalls, the basic and most primitive solution is to slow down
    the writes itself before a write stall occurs. This is done by putting the write
    thread to sleep for a short duration of time, such as 1 ms [\[31\]](#page-10-25).
    Industry standard LSM-KVS such as RocksDB make liberal use of slowdowns during
    heavy write workloads to prevent write stalls. Meanwhile, ADOC [\[5\]](#page-10-5),
    the state of the art solution, still falls back to slowdowns as a last resort
    despite software optimizations such as dynamic allocation of compaction threads
    and batch size.


    To measure the effectiveness and frequency of write slowdowns, we used RocksDB''s
    benchmark tool, db bench [\[27\]](#page-10-18), and executed fillrandom workload
    for 600 seconds on RocksDB and ADOC. The experiments were conducted using an OpenSSD-based
    SSD prototype mounted with a traditional block based interface with the ext4 file-system.
    The SSD supports a peak bandwidth of approximately 630 MB/s, and is connected
    to the host via a PCIe Gen2.0 x8 interface, yielding a theoretical maximum PCIe
    bandwidth of 4 GB/s. Details about our experimental environment are provided in
    Section [VI-A.](#page-7-0)


    Figure [2](#page-2-0) (a)-(d) show RocksDB and ADOC''s time-series throughput.
    Here, we present two variants of RocksDB and ADOC, first where the slowdown feature
    of RocksDB and ADOC is disabled while in the second, the slowdown feature is enabled,
    respectively. Comparing Figures [2](#page-2-0) (a) and (c) with (b) and (d), respectively,
    we observe that when the slowdown feature is enabled for both RocksDB and ADOC,
    the issue of write throughput dropping to zero—i.e., write service halting momentarily—disappears.
    Instead, although the throughput is slightly lower, it remains stable at a base
    level, providing consistent service at up to 2 Kops/s. This demonstrates that
    the slowdown feature effectively mitigates write stalls, ensuring stable and uninterrupted
    service. However, it also highlights


    <span id="page-3-0"></span>![](_page_3_Figure_0.jpeg)


    PCIe Traffic Write stall region Maximum PCIe bandwidth


    PCIe Traffic Write stall region Maximum PCIe bandwidth


    800


    PCIe


    PCIe


    Traffic(MB/s)


    Fig. 4: Measurements of PCIe bandwidth utilization in the 100–200 seconds of execution
    range in RocksDB without applying slowdown techniques.


    that the extent of throughput mitigation achieved through the slowdown mechanism
    is not particularly significant.


    Surprisingly, looking at Figure [3](#page-2-1) reveals that these slowdowns actively
    harm performance in both throughput and P99 latency. While slowdown is in effect,
    the overall throughput of RocksDB and ADOC dropped by 34% and 47% respectively.
    Tail latency values were also elongated by 48% and 28% for RocksDB and ADOC respectively
    as well. Taking a more microscopic look into the slowdowns, we find that during
    the workload execution, RocksDB and ADOC experienced a total of 258 and 433 instances
    of write slowdowns, respectively. Additionally, ADOC also makes use of more CPU
    resources over RocksDB while still suffering write slowdowns, as seen in Figure
    [12](#page-8-0) in the evaluation section. As slowdowns ultimately throttle write
    operations over the course of the workload, the performance results inevitably
    suffer in comparison to results that do not employ slowdown. In addition, the
    hit in latency performance can be traced to each slowdown causing the write thread
    to sleep for a short period, worsening write response times. In other words, while
    the slowdown mechanism alleviates the occurrence of write stalls, it ultimately
    degrades the overall write performance of LSM-KVS, causing users to experience
    this performance drop.


    ### *B. Underutilized PCIe Bandwidth and Device Resources*


    In LSM-KVS during a write stall, all user write operations are blocked to allocate
    system resources for the compaction process. Once compaction is initiated, SSTables
    (SSTs) are loaded from the storage device/SSD to memory, where a merge-sort operation
    is performed. Newly created SSTs are then written back to the storage device.
    Importantly, during the merge-sort phase, no data transfer occurs between host''s
    memory and the storage device. This leaves an interval of time during a write
    stall where potential transfer bandwidth is being unused, yet write operations
    are not proceeding.


    To empirically verify this behavior, the used PCIe bandwidth of the previous fillrandom
    experiments were measured while measuring PCIe bandwidth at 1-second intervals
    using Intel PCM [\[32\]](#page-10-26). Note that since ADOC''s work depends on
    write slowdowns for its performance optimizations, they were


    <span id="page-3-1"></span>![](_page_3_Figure_7.jpeg)


    Fig. 5: A Cumulative Distribution Function (CDF) of PCIe bandwidth during a period
    of write stall on RocksDB. The numbers on the legend denote compaction thread
    count.


    excluded from these experiments. Figure [4](#page-3-0) illustrates the timeseries
    PCIe bandwidth measurements for RocksDB without slowdown, focusing on a 100-second
    segment of the total experiment duration.


    Figure [4](#page-3-0) (a)-(b) show the results when using one compaction thread
    (RocksDB(1)) and four compaction threads (RocksDB(4)), respectively. The red dotted
    lines in the figure indicate the maximum bandwidth of the SSD (630 MB/s), while
    the green dotted boxes mark periods of write stalls. From the figure, significant
    unused bandwidth can be observed during the write stall periods from both configurations
    of RocksDB.


    To further analyze the above, we conducted a statistical analysis of the PCIe
    bandwidth observed during the write stall periods over the entire 600-second experiment.
    Figure [5](#page-3-1) presents the cumulative distribution function (CDF) of PCIe
    bandwidth utilization during these periods. In RocksDB, with one compaction thread,
    30% of the write stall periods exhibit no PCIe bandwidth usage, while 49% use
    over 90% of available PCIe bandwidth. Four compaction threads improve usage somewhat,
    where 21% of the write stall periods exhibit no PCIe bandwidth usage and 55% use
    over 90% of available PCIe bandwidth. While one compaction thread does leave more
    periods of completely no PCIe bandwidth usage during a write stall, both configurations
    leave up to 90% of available PCIe bandwidth around 50% of the time during a write
    stall. Therefore, these results demonstrate that RocksDB in both configurations
    leaves a significant portion of the device''s available PCIe bandwidth underutilized
    during write stalls.


    ### *C. Exploring Available I/O Processing Capacity of the SSD*


    From the previous experimental results, the following observations can be made.


    Observation 1. Both state of the art and industry standard solutions make use
    of write slowdowns to prevent write stalls, which cause a sharp drop in overall
    throughput and tail latency.


    Observation 2. PCIe bandwidth is under-utilized during write stalls in industry
    standard LSM-KVS due to the compaction operation blocking device I/O.


    These observations lead to a dilemma between two currently possible paths. One
    can choose to keep slowdowns on and


    <span id="page-4-0"></span>![](_page_4_Figure_0.jpeg)


    Fig. 6: A comparison of software stacks for (a) NVMe Block Interface SSD and (b)
    NVMe Key-Value Inteface SSD.


    maintain I/O operation service, while coming at the great cost of throttling throughput
    and deteriorating tail latency. On the other hand, one can disable slowdowns and
    run the LSM-KVS at maximum capacity, keeping throughput and tail latency alive.
    However, this leads to enlonged write stalls to occur unpredictably.


    The experiments also show a third potential path with discovery of underutilized
    PCIe and device bandwidth during write stalls. This under-utilization is due to
    the key-value store halting I/O operations while compaction is in progress. If
    this underutilized bandwidth can be leveraged in times when the SSD still has
    available I/O processing capacity, the potential to mitigate write stalls and
    increase performance without sacrificing system resources can be realized.


    ### IV. OPPORTUNITIES IN KEY-VALUE INTERFACES


    Currently, KV-SSDs leverage NVMe extensions [\[33\]](#page-10-27) to support its
    key-value interface. The NVMe-based key-value interface API typically supports
    point and range queries [\[24\]](#page-10-28), such as PUT, GET, SEEK, and NEXT,
    and additionally offers buffered I/O capabilities like compound commands [\[34\]](#page-10-29).
    As described in Figure [6,](#page-4-0) the key-value interface enables efficient
    I/O processing by eliminating the need for file systems and block layers, effectively
    simplifying the storage software stack and reducing the overhead associated with
    multi-layer space management during processing writes and compaction.


    The KV-SSDs share the same NAND flash address space and use the same Flash Translation
    Layer (FTL) mechanisms as traditional block-based NVMe SSDs but internally implement
    a LSM-KVS at the controller level [\[22\]](#page-10-22), [\[23\]](#page-10-23),
    [\[25\]](#page-10-16). The controller abstracts logical addresses for point and
    range query executions, enabling direct key-value service within the device. Aside
    from executing point and range queries internally, *the rest of the storage infrastructure,
    such as the NVMe interface and FTL-managed logical-to-physical address mapping,
    remains identical to that of conventional SSDs*, ensuring compatibility while
    offering enhanced functionality. Based on this, we propose designing a hybrid
    dual-interface SSD that supports both block and key-value interfaces. This approach
    allows the SSD to leverage the available bandwidth and processing capacity during
    write stalls in LSM-KVS systems. By temporarily redirecting pending write requests
    through the key-value interface, the SSD can reduce the impact of write stalls
    and improve overall performance without disrupting ongoing operations in the LSM-KVS.


    <span id="page-4-1"></span>![](_page_4_Figure_7.jpeg)


    Fig. 7: (a) A software stack of KVACCEL and (b) The write path of KVACCEL shown
    using its software modules.


    ### V. DESIGN OF KVACCEL


    This section introduces the design objectives of KVACCEL, details the hardware
    and software components involved in its implementation, explains their operation,
    and discusses how crash consistency is ensured.


    ### *A. Design Goals*


    To address the aforementioned issues, we propose KVAC-CEL, a novel hybrid hardware-software
    co-design framework that leverages a dual-interface SSD architecture to eliminate
    write stalls and optimize the utilization of storage bandwidth. The design goals
    of KVACCEL are as follows:


    *G1*. Mitigating Write Stalls Effectively: Leverage the keyvalue interface of
    the hybrid SSD to serve as a temporary indevice write buffer during host-side
    write stalls. By redirecting writes to the key-value interface, KVACCEL can prevent
    the host-side LSM from becoming overloaded during compaction. *G2*. Maximizing
    I/O Bandwidth Utilization: Ensure that the SSD''s available bandwidth and I/O
    processing capacity are fully utilized during write stalls by dynamically switching
    between the block and key-value interfaces.


    *G3*. Seamless Integration for Consistency and Performance: Achieve seamless integration
    between the hybrid SSD and host LSM-KVS by employing efficient metadata management
    and a rollback mechanism. This ensures data consistency between the host''s LSM
    and the device''s key-value write buffer, even when switching between interfaces.


    ### *B. Overall Architecture*


    Hardware and Software. KVACCEL is system that offers dynamic redirection and rollback
    techniques to a LSM-KVS to both mitigate write stalls and fully utilize available
    I/O bandwidth. This is achieved through the close co-design of software and hardware
    components. The *Software* components assign I/O commands to the correct interface
    depending on real-time information of the database. Maintaining the consistency
    of the database between the two interfaces during database operations is also
    paramount in the software design. The *Hardware* components implements the disaggregation
    of separate block and key-value interfaces to allow for the hybrid interface of
    the SSD. The hardware also implements support


    for bulk range scan operations over its write buffer to perform the rollback operation
    for consistency of our system.


    Disaggregation and Aggregation. The design of KVACCEL is based on two key factors:
    disaggregation and aggregation. *Disaggregation* facilitates the division of the
    SSD into the hybrid interface, as well as the software required for the I/O pathways
    for each interface. KVACCEL disaggregates a SSD into a hybrid interface with separate
    block and keyvalue interfaces, each with its own separate LSM-tree that each interface
    manages. *Aggregation* focuses on managing the data stored in the hybrid interface
    SSD as if it were a single database instance. This includes unifying the host-side
    and device-side LSM-trees during rollback operations by efficiently merging cached
    key-value pairs from the device back into the host''s LSM structure. Additionally,
    KVACCEL maintains a global metadata manager to track the locations of key-value
    pairs across both interfaces, ensuring transparent access to data regardless of
    its physical placement in the SSD.


    Figure [7\(](#page-4-1)a) shows the potential of writing using the keyvalue interface
    during periods of write stall. Through the keyvalue interface, I/O operations
    can bypass the file system and block layer and drill a path straight to the NVMe
    controller via the driver. This path offers a method to service I/O requests uninterrupted
    through the key-value interface, even while write stalls are occurring on the
    database running on the block interface. A key point in disaggregation is that
    the hybrid interface of both block and key-value interfaces are implemented in
    a singular device. This is significant in that to see the benefits of KVACCEL,
    only the one storage device programmed to run is required. A single device solution
    enables KVACCEL to bypass the burdens of additional hardware deployment (e.g.,
    PM, FPGA) that previous hardware-level solutions to the write stall issue introduce.


    ### *C. Interface Pathing via Software Modules*


    To make use of the hybrid interface, the decision to use which interface needs
    to be made every time a operation is requested by the database. To do this, KVACCEL
    makes use of the following four software components shown in Figure [7\(](#page-4-1)b)
    to make this decision to make full use of unused device bandwidth. The LSM-tree
    residing on the block interface is labeled *Main-LSM*, while the LSM-tree on the
    key-value interface labeled *Dev-LSM*. Main-LSM is used by the LSM-KVS running
    on the host machine, and uses the block interface to serve write operations during
    periods when write stall is not present. On the other hand, Dev-LSM runs entirely
    within the hybrid SSD, and uses the key-value interface to serve write operations
    when Main-LSM is facing a write stall as secondary cache storage.


    - Detector: The Detector periodically checks three components of Main-LSM that
    are associated with the characteristics of a write stall: the number of SSTs in
    L0, MT size, and pending compaction size. The Detector then reports this information
    to the Controller to use for path determination.

    - Controller: The Controller uses the information reported by the Detector to
    issue I/O operations to the correct interface. If the Detector reports that no
    write stalls are occurring,


    the Controller directs the operation to Main-LSM. If the Detector reports a write
    stall, the Controller performs the operation to the Dev-LSM.


    - Metadata Manager: As the SSD has been disaggregated into a hybrid interface,
    the data written can be in either Main-LSM or Dev-LSM. To keep track of which
    interface the database needs to use for future read operations, the keyvalue pairs
    that are redirected to the Dev-LSM are kept track of. This metadata of a key-value
    pair''s location is captured in a hash table in memory, and is used for membership
    testing for future operations that need to know the location of a certain key-value
    pair. In the case of a system failure and data loss of the metadata manager were
    to happen, the data can be recovered by a range scan covering every key-value
    pair in the key-value interface.

    - Rollback Manager: To aggregate the two LSM-trees into one, returning the cachced
    key-value pairs from Dev-LSM to Main-LSM is required. To facilitate this, the
    Rollback Manager is tasked to initiate the rollback operation depending on the
    contention status of Main-LSM. The Rollback Manager receives information of the
    presence of a write stall from the Detector. Further details on the rollback mechanism
    can be found in Section [V-E.](#page-6-0)


    With these modules, the read and write paths of KVACCEL, depending on the status
    of the Metadata Manager and the presence of a write stall, can be seen as follows.


    - Read Path: *(1)* The Metadata Manager checks the location of the queried key.
    *(2)* If the key-value pair is in the Main-LSM or if the Dev-LSM is empty, the
    Controller directs the read operation to the Main-LSM. *(3)* If the key-value
    pair is found in the Dev-LSM, the Controller redirects the read operation to the
    Dev-LSM.

    - Write Path: *(1)* The Detector checks for the presence of a write stall. *(2)*
    If a write stall is detected, the Controller, through the Metadata Manager, updates
    the record to indicate that the key-value pair is now in the Dev-LSM, and the
    pair is written to the Dev-LSM. *(3)* If no write stall is detected, the Controller
    directs the key-value pair to be written to the Main-LSM. *(3-1)* If the Metadata
    Manager indicates that an overlapping key-value pair already exists in the Dev-LSM,
    it updates the record to indicate that the latest key-value pair is now in the
    Main-LSM.


    Note that these paths only refer to the point queries of Put() and Get(). For
    range queries, refer to Section [V-F.](#page-7-1)


    # *D. Hybrid Dual-Interface SSD*


    To support a storage device with a dual-interface, the SSD''s logical NAND flash
    address space range is disaggregated into two address ranges, as shown in Figure
    [8.](#page-6-1) One address range is used for the block interface, and the other
    for the key-value interface. The address ranges are defined by the disaggregation
    point, which is a logical address that defines the end of one interface and the
    start of the next. The SSD''s controller issues different commands for each respective
    interfaced based on the given opcode of the NVMe command. Block interface commands
    perform FTL mapping over the logical address


    <span id="page-6-1"></span>![](_page_6_Figure_0.jpeg)


    Fig. 8: A dynamic, namespace-aware hybrid NAND flash space allocation of disaggregated
    NAND flash address space.


    space allocated for the block interface. Key-value interface commands allocates
    NAND pages from the logical address space allocated for the key-value interface.
    Both interfaces make use of existing NVMe command set specifications made for
    each respective interface type [\[33\]](#page-10-27), [\[35\]](#page-10-30).


    As the FTL maps logical address spaces for each interface separately, there are
    no issues of overlapping logical NAND pages between the two interfaces. When a
    file system is created for the block interface, the file system only sees the
    address range that was allocated for the block interface, and reports storage
    capacity to reflect said address range. Likewise, the key-value interface will
    only store key-value pairs up to the limits of its allocated address range.


    Multi-Tenancy Support: The ability to create multiple isolated regions on a singular
    device is a key requirement in multi-tenancy environments. To offer multi-tenancy
    in KVACCEL, both the block and key-value interface needs to support such features
    of isolated divisions. Multi-tenancy on the block interface is supported by namespaces
    as specified in the NVMe standard [\[36\]](#page-10-31), while previous works
    on supporting namespaces on the key-value interface [\[37\]](#page-10-32) are
    compatible with KVACCEL''s key-value interface implementation. By utilizing both
    namespace implementations for each interface and matching namespaces in both interfaces
    for each tenant, KVACCEL can fully support the requirements of multi-tenancy with
    both interfaces.


    # <span id="page-6-0"></span>*E. Rollback Operation*


    To return the two separated LSM-KVSs back into a singular database, the cached
    key-value pairs in Dev-LSM needs to be returned back to Main-LSM. This is done
    in a process called rollback. Figure [9](#page-6-2) displays an overview of the
    rollback operation, and the interactions between the host and the device during
    said operation. A rollback operation starts with the Detector and Rollback Manager.
    1 As rollback is only performed during periods when write stall is not present
    in Main-LSM, the Detector needs to notify the Rollback Manager the appropriate
    moment to start rollback. 2 When no write stalls are detected and there are key-value
    pairs in Dev-LSM, the rollback operation is initiated.


    <span id="page-6-2"></span>![](_page_6_Figure_7.jpeg)


    Fig. 9: An in-device iterator-based range scan to accelerate host-device co-managed
    rollback mechanism in KVACCEL.


    Rollback Scheduling: The rollback manager can schedule a rollback *eagerly* or
    *lazily* depending on the characteristics of a workload. An eager rollback scheme
    will trigger rollback as soon as the rollback manager detects that there are enough
    leftover resources in the LSM-KVS. Such a scheme is better suited for read oriented
    workloads, as point read query on the Dev-LSM are much slower than its counterpart
    in the Main-LSM, as such a read operation requires querying the slower device
    storage every time for a read operation. On the other hand, a lazy rollback scheme
    will trigger rollback when it is certain no other workload will interfere or be
    interfered by the rollback. This scheme is designed for write intensive workloads,
    as there is little penalty to keep the key-value pairs in Dev-LSM in this workload
    and therefore less urgency to perform rollback.


    Iterator-Based Bulky Range Scan: Regardless of the chosen rollback scheduling
    scheme, rollback needs to be performed as fast as the system allows. This is mainly
    due to the possibility of I/O operation conflicts. Performance can especially
    be crippled in cases where read and write operations happen simultaneously, where
    a time-consuming read operations can impact write operations. Such a conflict
    can occur with the aforementioned slower point read query on the Dev-LSM. To accelerate
    the rollback operation, 3 an iterator first identifies the range of the entire
    Dev-LSM to perform a range query by using the start and end keys of Dev-LSM. 4
    The iterator will search over the entire Dev-LSM, and 5 cache key-value pairs
    are serialized in bulk and transferred to host via device memory using NAND I/O.
    6 key-value pairs are then saved to system memory in chunks of 512 KB, so that
    the host can access the key-value pairs using Direct Memory Access (DMA). This
    size was chosen as 512 KB is the maximum size unit that DMA supports for data
    transfer on our platform. 7 Finally, the host can retrieve and unpacks the key-value
    pairs to merge back in Main-LSM. 8 After one rollback operation is finished, a
    reset is performed on the Dev-LSM to prevent consistency issues in the next rollback
    operation. By resetting Dev-LSM, the key-value pairs redirected to be involved
    in the next rollback can be the most up to date data. The reset also ensures the
    rollback of all key-value pairs in Dev-LSM to be completely written back to Main-LSM.


    An important point to keep mind of is as the duration of a write stall is relatively
    short, Dev-LSM does not have a large amount of SSTs that needs to be rolled back.
    This fact, along


    <span id="page-7-2"></span>![](_page_7_Figure_0.jpeg)


    Fig. 10: A range query operation in KVACCEL.


    with the aforementioned iterator-based range scan method, can ensure that every
    rollback operation can be finished in between periods of write stall.


    ### <span id="page-7-1"></span>*F. Range Query Support*


    Range queries work with the combination of iterator implementations of each respective
    interface in KVACCEL. Main-LSM can use the chosen LSM-KVS''s implementation of
    iterator and range scan. Meanwhile, Dev-LSM''s key-value interface has support
    for iterators and range scan functionality [\[24\]](#page-10-28), and KVACCEL
    utilizes the same bulky range scan mechanism from the rollback operation. Each
    interface will have its own iterator to perform Seek() and Next() operations over
    its LSM-trees. The two iterators will be aggregated to work in tandem to perform
    a range query over the entire LSM-KVS. An example range query operation is shown
    in Figure [10.](#page-7-2) 1 An iterator for both Main-LSM and Dev-LSM are created,
    and 2 a Seek() operation is performed for both LSM-trees. 3 The values returned
    from the Seek() operation are sent to the iterator comparator to be compared and
    saved. The iterator that returned the desired start key, or the smaller key if
    the desired start key was not found, is selected. 4 The selected iterator than
    procedes to perform Next() operations, until the iterator returns a key larger
    than the key saved from the opposing iterator''s first Seek() operation. 5 The
    used iterator is then switched, and 6 Next() operations are continued on the switched
    iterator. 7 This process of switching iterators when necessary continues until
    the desired end point is reached or the final key-value pair is reached.


    ### *G. ACID Property Management*


    KVACCEL maintains the ACID properties of database transactions by leveraging its
    dual-interface SSD design. First of all, for atomicity, the disaggregation of
    NAND flash address space within the dual-interface SSD handles operations between
    the Main-LSM and Dev-LSM in a completely independent manner. The rollback manager
    then monitors and reverts any changes made during incomplete transactions, ensuring
    that any partial or failed transactions during write redirection or rollback are
    consistently cleaned up by a rollback manager. Consistency is upheld through real-time
    metadata tracking and validation across both interfaces, with a dynamic consistency


    checker enforcing strict rollback protocols during high-pressure situations to
    maintain data accuracy in Main-LSM. The Metadata Manager directs all read and
    write operations to the appropriate structure, ensuring a seamless transition
    from Dev-LSM to Main-LSM. To achieve isolation, KVACCEL segregates concurrent
    I/O operations between the two LSM structures through the Controller Module, isolating
    Dev-LSM as a temporary cache during write stalls and preventing interference between
    the interfaces. Each range query is executed independently with separate iterators
    for each LSM, thereby ensuring query consistency even during ongoing write operations.
    Durability is guaranteed through a two-stage commit protocol that writes data
    first to Dev-LSM''s non-volatile NAND space before committing it to Main-LSM.
    This method secures committed transactions even during unexpected power failures
    or system crashes. In the event of a failure during rollback, the data remains
    in Dev-LSM until the system is restored, ensuring no loss of committed transactions.
    This robust architecture makes KVACCEL capable of maintaining database integrity
    and performance under various system conditions.


    ### VI. EVALUATION


    # <span id="page-7-0"></span>*A. Experimental Setup*


    We implemented KVACCEL''s hardware components by extending the state-of-the-art
    NVMe KV-SSD [\[38\]](#page-10-33) based on Cosmos+ OpenSSD platform [\[26\]](#page-10-17).
    The SoC of the platform operates the KVACCEL''s hybrid-interface SSD controller,
    the PCIe interface controller, the DRAM controller, and the NAND flash controller.
    A single ARM core of the Cosmos+ is used to run Dev-LSM''s I/O operations, as
    well as other required operations such as flush and compaction operations. The
    host system runs a modified version of the Linux kernel to facilitate the hybrid-interface
    SSD, as well as the NVMe block and key-value interface drivers. Table [I](#page-7-3)
    and Table [II](#page-7-4) present the hardware and software specifications of
    our setup.


    <span id="page-7-3"></span>TABLE I: Specifications of the OpenSSD platform.


    | SoC          | Xilinx Zynq-7000 with ARM Cortex-A9 Core |

    |--------------|------------------------------------------|

    | NAND Module  | 1TB, 4 Channel & 8 Way                   |

    | Interconnect | PCIe Gen2 ×8 End-Points                  |


    TABLE II: Specifications of the host system.


    <span id="page-7-4"></span>


    | CPU    | Intel(R) Xeon(R) Gold 6226R CPU @ 2.90GHz (32 cores),<br>CPU usage
    limited to 8 cores. |  |

    |--------|----------------------------------------------------------------------------------------|--|

    | Memory | 384GB DDR4                                                                             |  |

    | OS     | Ubuntu 22.04.4, Linux Kernel 6.6.31                                                    |  |


    KVACCEL''s software components were implemented on RocksDB v8.3.2. The Detector,
    Controller, Metadata Manager, and Rollback Manager software modules are all implemented
    on top of RocksDB. The Detector and Rollback Manager in particular run a thread
    detached from the RocksDB thread, refreshing the status of Main-LSM and checking
    for conditions of rollback every 0.1 seconds.


    For performance evaluations, we slightly modified db bench [\[27\]](#page-10-18),
    a widely recognized benchmarking tool used in RocksDB. We enabled db bench to
    send NVMe key-value commands to the Cosmos+ OpenSSD platform through the <span
    id="page-8-1"></span>TABLE III: LSM-KVS configurations. For all figures, the numbers
    next to each LSM-KVS refer to compaction thread count. For KVACCEL, the settings
    refer to the Main-LSM.


    | LSM-KVS    | Compaction Threads (n) | MT Size |

    |------------|------------------------|---------|

    | KVACCEL(n) | 1                      |         |

    |            | 2                      |         |

    |            | 4                      |         |

    | RocksDB(n) | 1                      |         |

    |            | 2                      | 128 MB  |

    |            | 4                      |         |

    | ADOC(n)    | 1                      |         |

    |            | 2                      |         |

    |            | 4                      |         |


    <span id="page-8-2"></span>TABLE IV: db bench workload configurations. Each benchmark
    was run with a 4 B key and 4 KB value size. Workload A,B,C were run for 600 seconds,
    and Workload D performed 60K read operations.


    | Name            | Type             | Characteristics      | Notes (write/read
    ratio) |

    |-----------------|------------------|----------------------|--------------------------|

    | A               | fillrandom       | 1 write thread       | No write limit           |

    | B               | readwhilewriting | 1 write thread       | 9:1                      |

    | C               |                  | + 1 read thread      | 8:2                      |

    | D<br>seekrandom |                  | 1 range query thread | Run after initial        |

    |                 |                  | (Seek + 1024 Next)   | 20GB fillrandom          |


    NVMe passthrough. The LSM-KVSs and the configurations used for the evaluations
    are detailed in Table [III.](#page-8-1) The various patterns of the workloads
    to verify our proposed design are described in Table [IV.](#page-8-2)


    ### *B. Write Stall Mitigation Evaluation*


    This section demonstrates KVACCEL''s ability to mitigate write stalls via I/O
    redirection. Figure [11](#page-8-3) displays the persecond throughput of all three
    LSM-KVS (RocksDB, ADOC, and KVACCEL) during the entirety of workload A. Figure
    [11](#page-8-3) (a) and (b) focus on the periods of lower throughput in order
    to examine the decrease in throughput that occurred during the slowdown phase.
    ADOC and RocksDB can be both seen suffering from slowdowns to 2 Kop/s in order
    to prevent a write stall. In similar periods, KVACCEL proceeds to write upwards
    of 30 Kop/s, showing I/O redirection response of KVACCEL allowing for the avoidance
    of write stalls.


    A point to emphasize here is KVACCEL does not employ any slowdown mechanisms to
    avoid a write stall. This is because KVACCEL is inherently designed to accept
    writes in its full capacity during a write stall via redirection instead of intentionally
    throttling write flow to attempt to avoid a write stall. This different approach
    to the write stall problem allows KVACCEL to maintain write operations while greatly
    lowering performance compromises, while other LSM-KVSs suffer from slowdowns or
    face a write stall depending on workload settings.


    ### *C. Performance Evaluation*


    In this section, the read/write performance, and efficiency of KVACCEL will be
    demonstrated with the workloads of Table [IV.](#page-8-2) Here, we introduce a
    scoring metric of the ratio between throughput and resources used to better demonstrate
    the efficiency of throughput in relation to resources used.


    <span id="page-8-3"></span>![](_page_8_Figure_10.jpeg)


    Fig. 11: Per-second throughput for each LSM-KVS while running workload A.


    <span id="page-8-0"></span>![](_page_8_Figure_12.jpeg)


    Thread 1 Thread 2 Thread 4 <sup>0</sup> 20 Fig. 12: (a) Throughput, (b) P99 Latency,
    and (c) Efficiency scores of all evaluated LSM-KVS for workload A. Thread counts
    here denote compaction thread count.


    $$\text{Efficiency} = \frac{\text{Avg. Through} (\text{MB/s})}{\text{Avg. CPU
    usage} (\%)} \tag{1}$$


    Figure [12](#page-8-0) shows the average throughput, P99 latency, and efficiency
    respectively of all LSM-KVS configurations performing workload A. To demonstrate
    the full potential of KVACCEL in a write-only operation, rollback and compaction
    operations in Dev-LSM were disabled for workload A. This is because for a write-only
    workload phase, a lazy rollback scheme that performs rollback after the workload
    completes is the most sensible option.


    KVACCEL shows at most a 37% and 17% improvement over its respective configuration
    in compaction thread count than RocksDB and ADOC, respectively. In P99 latency,
    a maximum of 42% and 20% decrease in latency was also observed between respective
    configurations between KVACCEL and RocksDB, ADOC, respectively. A key point of
    interest that can be observed is that KVACCEL with only one compaction thread
    shows similar write throughput with ADOC employing four compaction threads. This
    is due to the fact that KVACCEL is able to contribute more to throughput when
    write stalls are longer and happen more frequently. Increasing the compaction
    thread count ultimately reduces write stall length and frequency, thus lowering
    the effectiveness of KVACCEL.


    Referring to the efficiency metric for the results, KVACCEL also maintains the
    better efficiencies in host machine''s resources between all LSM-KVS compared,
    with KVACCEL(1) shows the best efficiency over all configurations. This is because
    KVACCEL is able to achieve the higher throughput results while maintaining the
    same CPU utilization.


    In order to handle more diverse scenarios, rather than using write- optimized
    solution, workloads A to C were also performed to evaluate KVACCEL under different
    rollback


    <span id="page-9-0"></span>![](_page_9_Figure_0.jpeg)


    RocksDB


    RocksDB ADOC


    RocksDB


    Fig. 13: Read and write throughput comparison of different workloads based on
    rollback schemes choice. KVACCEL-L uses a lazy rollback scheme, and KVACCEL-E
    uses an eagar rollback scheme. All LSM-KVS configurations in this figure use 4
    compaction threads. Read throughput is non-applicable for workload A, as workload
    A is a 100% write workload, and is thus excluded.


    50 100 150 200 Throughput(Kops/s) 50 100 150 200 Throughput(Kops/s) Write Read
    0 50 100 150 200 Throughput(Kops/s) Write Read A 0 50 100 150 200 Throughput(Kops/s)
    schemes. The results of these workloads of all LSM-KVS configurations can be seen
    in Figure [13,](#page-9-0) where comparisons of rollback schemes based on workload
    type are also made. Here, KVACCEL-L and KVACCEL-E refer to KVACCEL with lazy and
    eager rollback schemes respectively. For workload A, KVACCEL-L shows superior
    write performance over KVACCEL-E, as it is a write only workload, leading rollback
    operations to take away bandwidth from actual write operations. However, both
    configurations show lower performance in comparison to the write optimized KVACCEL
    as shown in Figure [12.](#page-8-0)


    Write Read A 0 Write Read A 0 A Workload B and C present a read-write mix workload,
    where both rollback schemes achieve similar write throughput, both holding a lead
    of 36% and 51% over ADOC respectively. However, KVACCEL-E shows an increase in
    read performance, due to rollback allowing more read operations to be performed
    from Main-LSM, showing that a eager rollback scheme can be more effective for
    a write/read mixed workload.


    <span id="page-9-1"></span>TABLE V: Throughput of range queries for RocksDB, ADOC,
    and KVACCEL performing workload D.


    | LSM-KVS | Range Query Throughput (Kops/s) |

    |---------|---------------------------------|

    | RocksDB | 302                             |

    | ADOC    | 351                             |

    | KVACCEL | 100                             |


    Table [V](#page-9-1) shows the results of range query workloads from workload
    D. These results prove that KVACCEL is able to fully support the range query operation
    across the hybrid interfaces. However, KVACCEL still suffers a significant performance
    hit in comparison to other LSM-KVS. This is in large part due to a lack of read
    caching mechanism for iterator operations on the Dev-LSM to be accelerated in
    contrast to the Main-LSM''s iterator.


    ### *D. Overhead Analysis*


    Through the additional software modules that KVACCEL implements, there are unavoidable
    overhead processes on top of the core LSM-KVS operations. A breakdown of all the
    potential overheads of KVACCEL are covered in Table [VI.](#page-9-2)


    <span id="page-9-2"></span>TABLE VI: Detailed breakdown of time overheads for
    KVAC-CEL''s operations.


    | Operation  | Average Elapsed Time (us) |

    |------------|---------------------------|

    | Detector   | 1.37                      |

    | Key Insert | 0.45                      |

    | Key Check  | 0.20                      |

    | Key Delete | 0.28                      |


    <span id="page-9-3"></span>![](_page_9_Figure_11.jpeg)


    0 100 200 300 400 500 600 10<sup>1</sup> Fig. 14: Overview of PCIe bandwidth usage
    for (a) RocksDB(1) (b) KVACCEL(1) in logarithmic scale.


    The Detector module has the largest overhead impact, with an average of 1.37 microseconds
    every 0.1 seconds it is used. The Metadata Module is also a required overhead,
    due to the requirement of maintaining consistency between the dual interfaces.
    For this there are the key insert, check and delete operations, which on average,
    takes 0.45, 0.2 and 0.28 microseconds respectively. In practice, during workloads,
    the largest overhead observed related to the Metadata Manager was the combination
    of a key check and delete operation, which took 0.48 microseconds.


    # *E. Microscopic Analysis of PCIe Usage*


    To verify the usage of PCIe bandwidth of KVACCEL, we conducted experiments with
    Workload A and measured the bandwidth utilization by using Intel PCM [\[32\]](#page-10-26).
    Figure [14](#page-9-3) shows the results in time series in comparison to baseline
    RocksDB. It can be observed that KVACCEL takes advantage of its dual interface
    and demonstrate high PCIe utilization which aligns with the results presented
    in Figure [11.](#page-8-3)


    # VII. CONCLUSION


    There has been extensive research on mitigating write stalls in LSM-tree-based
    key-value stores. However, these existing studies fall short in overcoming the
    write stalls and limits the performance gain. This study introduces KVACCEL, the
    first hardware-software co-design that revitalizes the underutilized computational
    power of SSDs during compaction to avoid write stalls. KVACCEL integrates a dual-interface
    SSD architecture, dynamically redirecting writes to a key-value interface during
    host-side write stalls, eliminating the need for complex hostside optimizations,
    high CPU usage, or additional hardware. We implemented KVACCEL by extending RocksDB
    to support I/O redirection during write stalls. Our evaluation shows that KVACCEL
    outperforms ADOC in throughput and CPU efficiency for write-heavy workloads, while
    both systems perform comparably in mixed read-write scenarios.


    ### REFERENCES


    - <span id="page-10-0"></span>[1] Facebook, "RocksDB." [http://rocksdb.org,](http://rocksdb.org)
    2012.

    - <span id="page-10-1"></span>[2] Google, "LevelDB." [https://github.com/google/leveldb,](https://github.com/google/leveldb)
    2017.

    - <span id="page-10-2"></span>[3] O. Balmau, F. Dinu, W. Zwaenepoel, K. Gupta,
    R. Chandhiramoorthi, and D. Didona, "SILK: Preventing Latency Spikes in Log-Structured
    Merge Key-Value Stores," in *Proceedings of the 2019 USENIX Annual Technical Conference
    (USENIX ATC 19)*, (Renton, WA), pp. 753–766, USENIX Association, July 2019.

    - [4] S. Jamil, A. Khan, K. Kim, J.-K. Lee, D. An, T. Hong, S. Oral, and Y. Kim,
    "Denkv: Addressing design trade-offs of key-value stores for scientific applications,"
    in *Proceedings of the 2022 IEEE/ACM International Parallel Data Systems Workshop
    (PDSW)*, pp. 20–25, 2022.

    - <span id="page-10-5"></span>[5] J. Yu, S. H. Noh, Y.-r. Choi, and C. J. Xue,
    "ADOC: Automatically harmonizing dataflow between components in log-structured
    key-value stores for improved performance," in *Proceedings of the 21st USENIX
    Conference on File and Storage Technologies (FAST 23)*, pp. 65–80, 2023.

    - <span id="page-10-6"></span>[6] T. Yao, Y. Zhang, J. Wan, Q. Cui, L. Tang, H.
    Jiang, C. Xie, and X. He, "MatrixKV: Reducing write stalls and write amplification
    in LSM-tree based KV stores with matrix container in NVM," in *Proceedings of
    the 2020 USENIX Annual Technical Conference (USENIX ATC 20)*, pp. 17–31, 2020.

    - [7] O. Balmau, D. Didona, R. Guerraoui, W. Zwaenepoel, H. Yuan, A. Arora, K.
    Gupta, and P. Konka, "TRIAD: Creating synergies between memory, disk and log in
    log structured key-value stores," in *Proceedings of the 2017 USENIX Annual Technical
    Conference (USENIX ATC 17)*, pp. 363– 375, 2017.

    - <span id="page-10-3"></span>[8] C. Ding, T. Yao, H. Jiang, Q. Cui, L. Tang,
    Y. Zhang, J. Wan, and Z. Tan, "TriangleKV: Reducing write stalls and write amplification
    in LSM-tree based KV stores with triangle container in NVM," *IEEE Transactions
    on Parallel and Distributed Systems*, vol. 33, no. 12, pp. 4339–4352, 2022.

    - <span id="page-10-4"></span>[9] Facebook, "Write Stalls." [https://github.com/facebook/rocksdb/wiki/](https://github.com/facebook/rocksdb/wiki/Write-Stalls)
    [Write-Stalls,](https://github.com/facebook/rocksdb/wiki/Write-Stalls) 2021.

    - <span id="page-10-7"></span>[10] S. Kannan, N. Bhat, A. Gavrilovska, A. Arpaci-Dusseau,
    and R. Arpaci-Dusseau, "Redesigning LSMs for nonvolatile memory with NoveLSM,"
    in *Proceedings of the 2018 USENIX Annual Technical Conference (USENIX ATC 18)*,
    pp. 993–1005, 2018.

    - <span id="page-10-8"></span>[11] O. Kaiyrakhmet, S. Lee, B. Nam, S. H. Noh,
    and Y.-r. Choi, "SLM-DB: Single-Level Key-Value store with persistent memory,"
    in *Proceedings onf the 17th USENIX Conference on File and Storage Technologies
    (FAST 19)*, pp. 191–205, 2019.

    - <span id="page-10-9"></span>[12] X. Sun, J. Yu, Z. Zhou, and C. J. Xue, "FPGA-based
    Compaction Engine for Accelerating LSM-tree Key-Value Stores," in *2020 IEEE 36th
    International Conference on Data Engineering (ICDE)*, pp. 1261–1272, 2020.

    - <span id="page-10-24"></span>[13] T. Zhang, J. Wang, X. Cheng, H. Xu, N. Yu,
    G. Huang, T. Zhang, D. He, F. Li, W. Cao, Z. Huang, and J. Sun, "FPGA-Accelerated
    compactions for LSM-Based key-value store," in *Proceedings of the 18th USENIX
    Conference on File and Storage Technologies*, FAST''20, (USA), p. 225–238, USENIX
    Association, 2020.

    - <span id="page-10-10"></span>[14] G. Huang, X. Cheng, J. Wang, Y. Wang, D. He,
    T. Zhang, F. Li, S. Wang, W. Cao, and Q. Li, "X-engine: An optimized storage engine
    for largescale e-commerce transaction processing," in *Proceedings of the 2019
    International Conference on Management of Data*, pp. 651–665, 2019.

    - <span id="page-10-11"></span>[15] P. Xu, J. Wan, P. Huang, X. Yang, C. Tang,
    F. Wu, and C. Xie, "Luda: boost lsm key value store compactions with gpus," *arXiv
    preprint arXiv:2004.03054*, 2020.

    - [16] H. Zhou, Y. Chen, L. Cui, G. Wang, and X. Liu, "A gpu-accelerated compaction
    strategy for lsm-based key-value store system,"

    - <span id="page-10-12"></span>[17] H. Sun, J. Xu, X. Jiang, G. Chen, Y. Yue,
    and X. Qin, "glsm: Using gpgpu to accelerate compactions in lsm-tree-based key-value
    stores," *ACM Transactions on Storage*, vol. 20, no. 1, pp. 1–41, 2024.

    - <span id="page-10-13"></span>[18] C. Ding, J. Zhou, J. Wan, Y. Xiong, S. Li,
    S. Chen, H. Liu, L. Tang, L. Zhan, K. Lu, *et al.*, "Dcomp: Efficient offload
    of lsm-tree compaction with data processing units," in *Proceedings of the 52nd
    International Conference on Parallel Processing*, pp. 233–243, 2023.

    - <span id="page-10-14"></span>[19] C. Ding, J. Zhou, K. Lu, S. Li, Y. Xiong,
    J. Wan, and L. Zhan, "D 2 comp: Efficient offload of lsm-tree compaction with
    data processing units on disaggregated storage," *ACM Transactions on Architecture
    and Code Optimization*, 2024.

    - <span id="page-10-15"></span>[20] Samsung Electronics Co., "Samsung Key-Value
    SSD Enables High Performance Scaling," 2017.

    - [21] Y. Jin, H.-W. Tseng, Y. Papakonstantinou, and S. Swanson, "KAML: A Flexible,
    High-Performance Key-Value SSD," in *Proceedings of the IEEE International Symposium
    on High Performance Computer Architecture (HPCA)*, pp. 373–384, IEEE, 2017.

    - <span id="page-10-22"></span>[22] C.-G. Lee, H. Kang, D. Park, S. Park, Y. Kim,
    J. Noh, W. Chung, and K. Park, "iLSM-SSD: An intelligent LSM-tree based key-value
    SSD for data analytics," in *Proceedings of the 27th International Symposium on
    Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)*,
    pp. 384–395, IEEE, 2019.

    - <span id="page-10-23"></span>[23] J. Im, J. Bae, C. Chung, Arvind, and S. Lee,
    "PinK: High-speed Instorage Key-value Store with Bounded Tails," in *Proceedings
    of the USENIX Annual Technical Conference (ATC)*, pp. 173–187, USENIX, 2020.

    - <span id="page-10-28"></span>[24] S. Lee, C.-G. Lee, D. Min, I. Park, W. Chung,
    A. Sivasubramaniam, and Y. Kim, "Iterator interface extended lsm-tree-based kvssd
    for range queries," in *Proceedings of the 16th ACM International Conference on
    Systems and Storage*, SYSTOR ''23, (New York, NY, USA), p. 60–70, Association
    for Computing Machinery, 2023.

    - <span id="page-10-16"></span>[25] J. Park, C.-G. Lee, S. Hwang, S. Yang, J.
    Noh, W. Chung, J. Lee, and Y. Kim, "BandSlim: A Novel Bandwidth and Space-Efficient
    KV-SSD with an Escape-from-Block Approach," in *Proceedings of the 53rd International
    Conference on Parallel Processing (ICPP)*, ICPP ''24, p. 1187–1196, 2024.

    - <span id="page-10-17"></span>[26] "Cosmos+ OpenSSD Platform." [http://www.openssd.io/,](http://www.openssd.io/)
    2017.

    - <span id="page-10-18"></span>[27] Facebook, "DB Bench." [https://github.com/facebook/rocksdb/wiki/](https://github.com/facebook/rocksdb/wiki/Benchmarking-tools)
    [Benchmarking-tools,](https://github.com/facebook/rocksdb/wiki/Benchmarking-tools)
    2017.

    - <span id="page-10-19"></span>[28] P. O''Neil, E. Cheng, D. Gawlick, and E. O''Neil,
    "The Log-Structured Merge-Tree (LSM-tree)," *Acta Informatica*, vol. 33, no. 4,
    pp. 351–385, 1996.

    - <span id="page-10-20"></span>[29] "Cassandra." [https://cassandra.apache.org/,](https://cassandra.apache.org/)
    2008. Accessed: 2024-02-14.

    - <span id="page-10-21"></span>[30] R. Sears and R. Ramakrishnan, "bLSM: a general
    purpose log structured

    - merge tree," in *Proceedings of the 2012 ACM SIGMOD International Conference
    on Management of Data*, pp. 217–228, 2012.

    - <span id="page-10-25"></span>[31] Facebook, "Write Stalls," 2023.

    - <span id="page-10-26"></span>[32] "Intel® performance counter monitor - a better
    way to measure cpu utilization," 2022. Last Accessed: Oct 1, 2024.

    - <span id="page-10-27"></span>[33] NVM Express Inc., "NVM Express Key Value Command
    Set Specification," 2021.

    - <span id="page-10-29"></span>[34] S.-H. Kim, J. Kim, K. Jeong, and J.-S. Kim,
    "Transaction support using compound commands in key-value ssds," in *Proceedings
    of the 11th USENIX Conference on Hot Topics in Storage and File Systems*, HotStorage''19,
    (USA), p. 1, USENIX Association, 2019.

    - <span id="page-10-30"></span>[35] NVM Express Inc., "NVM Express Specification,"
    2011.

    - <span id="page-10-31"></span>[36] NVM Express Inc., "NVMe Namespaces," 2022.

    - <span id="page-10-32"></span>[37] D. Min and Y. Kim, "Isolating namespace and
    performance in key-value ssds for multi-tenant environments," in *Proceedings
    of the 13th ACM Workshop on Hot Topics in Storage and File Systems*, HotStorage
    ''21, (New York, NY, USA), p. 8–13, Association for Computing Machinery, 2021.

    - <span id="page-10-33"></span>[38] S. Lee, C.-G. Lee, D. Min, I. Park, W. Chung,
    A. Sivasubramaniam, and Y. Kim, "Iterator Interface Extended LSM-tree-based KVSSD
    for Range Queries," in *Proceedings of the 16th ACM International Conference on
    Systems and Storage*, pp. 60–70, 2023.'
- title: Online Alignment and Addition in Multi-Term Floating-Point Adders
  abstract: 'Multi-term floating-point addition appears in vector dot-product

    computations, matrix multiplications, and other forms of floating-point data

    aggregation. A critical step in multi-term floating point addition is the

    alignment of fractions of the floating-point terms before adding them.

    Alignment is executed serially by identifying first the maximum of all

    exponents and then shifting the fraction of each term according to the

    difference of its exponent from the maximum one. Contrary to common practice,

    this work proposes a new online algorithm that splits the identification of the

    maximum exponent, the alignment shift for each fraction, and their addition to

    multiple fused incremental steps that can be computed in parallel. Each fused

    step is implemented by a new associative operator that allows the incremental

    alignment and addition for arbitrary number of operands. Experimental results

    show that employing the proposed align-and-add operators for the implementation

    of multi-term floating point adders can improve delay or save significant area

    and power. The achieved area and power savings range between 3%-23% and 4%-26%,

    respectively.'
  url: http://arxiv.org/abs/2410.21959v1
  keywords: Floating point arithmetic, Multi-term adders, Online algorithm, Energy
    Efficiency
  document: "# Online Alignment and Addition in Multi-Term Floating-Point Adders\n\
    \nKosmas Alexandridis and Giorgos Dimitrakopoulos\n\n*Abstract*—Multi-term floating-point\
    \ addition appears in vector dot-product computations, matrix multiplications,\
    \ and other forms of floating-point data aggregation. A critical step in multiterm\
    \ floating point addition is the alignment of fractions of the floating-point\
    \ terms before adding them. Alignment is executed serially by identifying first\
    \ the maximum of all exponents and then shifting the fraction of each term according\
    \ to the difference of its exponent from the maximum one. Contrary to common practice,\
    \ this work proposes a new *online* algorithm that splits the identification of\
    \ the maximum exponent, the alignment shift for each fraction, and their addition\
    \ to multiple fused incremental steps that can be computed in parallel. Each fused\
    \ step is implemented by a new associative operator that allows the incremental\
    \ alignment and addition for arbitrary number of operands. Experimental results\
    \ show that employing the proposed align-and-add operators for the implementation\
    \ of multi-term floating point adders can improve delay or save significant area\
    \ and power. The achieved area and power savings range between 3%–23% and 4%–26%,\
    \ respectively.\n\n*Index Terms*—Floating point arithmetic, Multi-term adders,\
    \ Online algorithm, Energy Efficiency\n\n#### I. INTRODUCTION\n\nMachine learning\
    \ (ML) algorithms have been widespread in various application domains. Their efficient\
    \ and accurate computation relies mostly on matrix multiplication kernels and\
    \ floating-point (FP) arithmetic for data representation [\\[1\\]](#page-4-0),\
    \ [\\[2\\]](#page-4-1).\n\nThe FP representations used in ML algorithms cover\
    \ IEEE-754 compliant formats as well as reduced-precision formats that use 16\
    \ or fewer bits in total, in an effort to balance numerical performance, and hardware\
    \ and storage costs [\\[3\\]](#page-4-2), [\\[4\\]](#page-4-3). In most cases,\
    \ a FP number consists of three fields: the sign bit (s), the exponent (e), and\
    \ the fraction (m) and its value is given by (−1)<sup>s</sup>×1.m×2 <sup>e</sup>−bias,\
    \ where bias is a constant that depends on the bit width of the exponent. Corner\
    \ cases, such as not-a-number, infinity, or de-normals can be also encoded or\
    \ skipped depending on the chosen format [\\[4\\]](#page-4-3).\n\nTo reduce the\
    \ overhead of FP arithmetic when implementing vector-wide operations, designers\
    \ have turned to fusing individual FP operations to more complex ones that implement\
    \ the needed computation at once [\\[5\\]](#page-4-4), [\\[6\\]](#page-4-5), [\\\
    [7\\]](#page-4-6), [\\[8\\]](#page-4-7). This approach allows alignment, normalization,\
    \ and rounding steps to be shared among independent operations, ultimately resulting\
    \ in more efficient hardware architectures.\n\nMulti-term addition, the core of\
    \ fused operators, involves adding multiple FP numbers with potentially different\
    \ exponents. To align the addends for addition, the fraction of each number is\
    \ shifted according to the difference of its own exponent to the maximum exponent\
    \ of all terms. This serial dependency across fraction alignment and addition\
    \ impacts negatively the overall hardware efficiency.\n\nIn this work, inspired\
    \ by online softmax computation [\\[9\\]](#page-4-8), we propose a new *online*\
    \ approach for alignment and addition in multi-term FP adders. In this way, *all\
    \ serial dependencies* that traditionally characterize alignment and addition\
    \ steps *are removed* and maximum exponent calculation, as well as alignment and\
    \ addition of fractions, are computed *incrementally and in parallel*. In practice,\
    \ alignment and addition is performed using trees built from the newly proposed\
    \ alignand-add operators.\n\nThe experimental evaluation shows that the proposed\
    \ approach simplifies fundamentally the complexity of alignment and addition in\
    \ multi-term FP adders. The corresponding hardware units that adopt the online\
    \ alignment and addition paradigm, require significantly less area and power than\
    \ traditional approaches. The area and power savings range between 3%–23% and\
    \ 4%–26%, respectively, for various examined configurations. Also, when opting\
    \ for high-speed implementations, they can also improve delay under the same number\
    \ of pipeline stages.\n\n# II. ALIGNMENT AND ADDITION IN MULTI-TERM FLOATING POINT\
    \ ADDERS\n\nA high-level description of multi-term fused addition is shown in\
    \ Algorithm [1.](#page-0-0) The input is an array of FP numbers f<sup>i</sup>\
    \ and the output is their sum S. The algorithm begins by finding the exponent\
    \ with the maximum value in step 1. Then, the fractions are aligned based on the\
    \ difference of the local exponent and the maximum one (step 2). With the fractions\
    \ aligned, the summation operation is performed in step 3. The sum is normalized\
    \ and rounded in step 4.\n\n<span id=\"page-0-0\"></span>\n\n| Algorithm 1 Multi-term\
    \ fused floating point addition |\n|------------------------------------------------------|\n\
    | Input: Floats f1, f2, , fN                           |\n\nOutput: S = P<sup>N</sup>\
    \ <sup>i</sup>=1 f<sup>i</sup>\n\n- 1: Find maximum exponent emax = max(e1, e2,\
    \ . . . , e<sup>N</sup> )\n- 2: Align every fraction 1.m<sup>i</sup> by shifting\
    \ right by emax − e<sup>i</sup> positions\n- 3: Sum the aligned fractions S =\
    \ P<sup>N</sup> <sup>i</sup>=1 aligned(1.mi)\n- 4: Normalize and round S to produce\
    \ the final FP sum\n\nThis work was supported by a Siemens EDA research grant\
    \ to Democritus University of Thrace on \"High-level synthesis research for Systems-on-Chip\"\
    .\n\nKosmas Alexandridis and Giorgos Dimitrakopoulos are with the Department of\
    \ Electrical and Computer Engineering, Democritus University of Thrace, Xanthi,\
    \ Greece. E-mail: {koalexan, dimitrak}@ee.duth.gr\n\n![](_page_1_Figure_1.jpeg)\n\
    \n<span id=\"page-1-1\"></span>Fig. 1. Baseline approach for multi-term fraction\
    \ alignment and addition.\n\nThe baseline implementation of alignment and addition\
    \ (steps 1–3), which are the focus of this work, is detailed in Algorithm [2.](#page-1-0)\
    \ The first loop corresponds to the first step in Algorithm [1](#page-0-0) that\
    \ computes the maximum exponent and stores it in λ<sup>N</sup> at the end of the\
    \ loop. The second loop performs steps 2 and 3 of Algorithm [1;](#page-0-0) each\
    \ fraction m<sup>i</sup> is aligned in line 5 and accumulated to a partial sum\
    \ o<sup>i</sup> in line 6. To simplify presentation in Algorithm [2,](#page-1-0)\
    \ each fraction 1.m<sup>i</sup> is denoted as m<sup>i</sup> , which is assumed\
    \ to be in signed (2's complement) form according to the sign s<sup>i</sup> of\
    \ f<sup>i</sup> .\n\n<span id=\"page-1-0\"></span>Algorithm 2 Serial fraction\
    \ alignment and addition\n\n| 1: for i ← 1 : N do         |                  \
    \                 |\n|-----------------------------|-----------------------------------|\n\
    | ← max(λi−1, ei)<br>λi<br>2: |                                   |\n| 3: end\
    \ for                  | ▷ Maximum exponent in λN          |\n| 4: for i ← 1 :\
    \ N do         |                                   |\n| ami<br>← mi<br>≫ (λN<br>5:\
    \  | − ei)<br>▷ Alignment shift        |\n| ← oi−1<br>oi<br>+ ami<br>6: | ▷ Accumulate\
    \ the aligned fraction |\n| 7: end for                  |                    \
    \               |\n| 8: S = oN                   |                           \
    \        |\n\nThe two loops of Algorithm [2](#page-1-0) cannot be merged. Thus,\
    \ in hardware, each part is unrolled separately and the second loop can begin\
    \ execution only after the first loop has computed the maximum exponent λ<sup>N</sup>\
    \ . This approach for alignment and addition is followed in the majority of hardware\
    \ architectures for multi-term adders [\\[10\\]](#page-4-9), [\\[11\\]](#page-4-10),\
    \ [\\[12\\]](#page-4-11) and is shown in Fig. [1.](#page-1-1)\n\nTo reduce delay,\
    \ other architectures perform fraction alignment based on the relative difference\
    \ of exponents, and avoid the dependency to the maximum exponent that is computed\
    \ in parallel [\\[5\\]](#page-4-4), [\\[6\\]](#page-4-5), [\\[7\\]](#page-4-6),\
    \ [\\[13\\]](#page-4-12). However, in all cases, this concept is applied only\
    \ for 3- or 4-term adders and cannot be generalized to arbitrary number of terms.\
    \ This limitation is removed by the formulation in this work. Other solutions,\
    \ such as Kaul *et al.* [\\[14\\]](#page-4-13), split the alignment of fractions\
    \ into global and local alignment. In this way, computing exponent differences\
    \ and the alignment shift are partially overlapped in time at the circuit level.\
    \ However, still addition is performed separately in a following step.\n\nOther\
    \ approaches avoid the need for fraction alignment by mapping floating point accumulation\
    \ to fixed-point arithmetic [\\[15\\]](#page-4-14), [\\[16\\]](#page-4-15). Effectively,\
    \ alignment is performed implicitly when transforming FP numbers to their equivalent\
    \ fixed-point integers. Such approaches are practical when accumulation is done\
    \ in time. In this work, we focus on wide parallel architectures that perform\
    \ addition in space.\n\n# III. ONLINE ALIGNMENT AND ADDITION\n\nThis work aims\
    \ to fuse the serial alignment and addition steps into one combined step that\
    \ would perform maximum exponent calculation, alignment shift and addition incrementally\
    \ and in parallel for various groups of inputs. Effectively, this transformation\
    \ would allow us to merge the two separate loops of Algorithm [2](#page-1-0) into\
    \ one single loop.\n\nTo present the proposed algorithm for online alignment and\
    \ addition, we first merge the shift and add operations, shown in lines 5 and\
    \ 6 of Algorithm [2,](#page-1-0) into one equation as follows:\n\n<span id=\"\
    page-1-2\"></span>\n$$o\\_i = o\\_{i-1} + m\\_i \\gg (\\lambda\\_N - e\\_i), \\\
    quad \\text{with} \\quad \\lambda\\_N = \\max\\_i \\{ e\\_i \\} \\tag{1}$$\n\n\
    The right shift in [\\(1\\)](#page-1-2) can be equivalently written as a multiplication\
    \ with a negative power of two, i.e.,\n\n<span id=\"page-1-3\"></span>\n$$o\\\
    _i = o\\_{i-1} + m\\_i \\, 2^{-(\\lambda\\_N - e\\_i)}.\\tag{2}$$\n\nFully unrolling\
    \ [\\(2\\)](#page-1-3) we can write the final sum o<sup>N</sup> as follows:\n\n\
    <span id=\"page-1-5\"></span>\n$$o\\_N = o\\_{N-1} + o\\_{N-2} + \\dots + o\\\
    _1 = \\sum\\_{i=1}^{N} m\\_i \\, 2^{-\\left(\\lambda\\_N - e\\_i\\right)} \\quad\
    \ (3)$$\n\n# *A. Basic online algorithm for alignment and addition*\n\nTo remove\
    \ the dependency to λ<sup>N</sup> for the computation of the final sum o<sup>N</sup>\
    \ we define a new sequence o ′ i\n\n<span id=\"page-1-4\"></span>\n$$o\\_i' =\
    \ \\sum\\_{j=1}^{i} m\\_j 2^{-(\\lambda\\_i - e\\_j)} \\quad \\text{with } \\\
    lambda\\_i = \\max(\\lambda\\_{i-1}, e\\_i) \\quad (4)$$\n\nSequence [\\(4\\)](#page-1-4)\
    \ has the interesting property that its last term o ′ N is equal to o<sup>N</sup>\
    \ defined in [\\(3\\)](#page-1-5). Beginning from [\\(4\\)](#page-1-4), our goal\
    \ is to form a recursive relation that would connect o ′ i to o ′ i−1 . Initially,\
    \ in [\\(4\\)](#page-1-4), we separate the ith term m<sup>i</sup> 2 −(λi−ei) from\
    \ the rest:\n\n<span id=\"page-1-6\"></span>\n$$o'\\_i = \\left(\\sum\\_{j=1}^{i-1}\
    \ m\\_j \\, 2^{-(\\lambda\\_i - e\\_j)}\\right) + m\\_i \\, 2^{-(\\lambda\\_i\
    \ - e\\_i)}$$\n\nThen, inside the parenthesis, we add and subtract the helper\
    \ term λi−<sup>1</sup>\n\n$$o'\\_i = \\left(\\sum\\_{j=1}^{i-1} m\\_j \\, 2^{-(\\\
    lambda\\_i - \\lambda\\_{i-1} + \\lambda\\_{i-1} - e\\_j)}\\right) + m\\_i \\\
    , 2^{-(\\lambda\\_i - e\\_i)}$$\n\nFinally, we factor out the term 2 −(λi−λi−1)\n\
    \n$$o\\_i' = \\left(\\sum\\_{j=1}^{i-1} m\\_j \\, 2^{-(\\lambda\\_{i-1} - e\\\
    _j)}\\right) 2^{-(\\lambda\\_i - \\lambda\\_{i-1})} + m\\_i \\, 2^{-(\\lambda\\\
    _i - e\\_i)} \\tag{5}$$\n\nAccording to [\\(4\\)](#page-1-4), the term left in\
    \ the parenthesis corresponds to o ′ i−1 . Thus, introducing o ′ i−1 into [\\\
    (5\\)](#page-1-6) we get the sought recursive relation:\n\n<span id=\"page-1-7\"\
    ></span>\n$$o\\_i^\\prime = o\\_{i-1}^\\prime 2^{-(\\lambda\\_i - \\lambda\\_{i-1})}\
    \ + m\\_i \\, 2^{-(\\lambda\\_i - e\\_i)} \\tag{6}$$\n \n$$\\text{with } \\lambda\\\
    _i = \\max(\\lambda\\_{i-1}, e\\_i)$$\n\n<span id=\"page-2-1\"></span>Algorithm\
    \ 3 Online fused fraction alignment and addition\n\n1: for i ← 1 : N do 2: λ<sup>i</sup>\
    \ ← max(λi−1, ei) 3: o ′ <sup>i</sup> ← o ′ <sup>i</sup>−<sup>1</sup> ≫ (λ<sup>i</sup>\
    \ − λi−1) + m<sup>i</sup> ≫ (λ<sup>i</sup> − ei) 4: end for 5: S = o ′ N\n\nRemapping\
    \ multiplications with negative powers of two back to equivalent right arithmetic\
    \ shift operations, the recursive relation in [\\(6\\)](#page-1-7) can be equivalently\
    \ expressed as follows:\n\n<span id=\"page-2-0\"></span>\n$$o\\_i' = o\\_{i-1}'\
    \ \\gg (\\lambda\\_i - \\lambda\\_{i-1}) + m\\_i \\gg (\\lambda\\_i - e\\_i) \\\
    tag{7}$$\n\nThis mapping to shift operations is valid since λ<sup>i</sup> is the\
    \ maximum of λi−<sup>1</sup> and e<sup>i</sup> and thus the shift amounts λi−λi−<sup>1</sup>\
    \ and λ<sup>i</sup> − e<sup>i</sup> in [\\(7\\)](#page-2-0), are always greater\
    \ or equal to zero.\n\nAlgorithm [3](#page-2-1) uses recursive relation [\\(7\\\
    )](#page-2-0) to compute alignment and addition *online*. At each iteration, a\
    \ local maximum exponent is identified that drives local alignment shifts and\
    \ accumulation of the output sum. Even if this fused align and add operation needs\
    \ an extra subtraction and shift per iteration relative to Algorithm [2,](#page-1-0)\
    \ the experimental results show that it leads to more efficient unrolled and pipelined\
    \ hardware implementations.\n\n#### *B. Parallel computation of fraction alignment\
    \ and addition*\n\nThe computation of the sum of aligned fractions S and the identification\
    \ of the maximum exponent can be performed in parallel using a new operator ⊚\
    \ that is defined as:\n\n<span id=\"page-2-3\"></span>\n$$\n\\begin{bmatrix}\n\
    \\lambda\\_i \\\\\n o\\_i\n\\end{bmatrix} \\odot \\begin{bmatrix}\n\\lambda\\\
    _j \\\\\no\\_j\n\\end{bmatrix} = \\\\\n\\begin{bmatrix}\n\\max(\\lambda\\_i, \\\
    lambda\\_j) \\\\\no\\_i \\gg (\\max(\\lambda\\_i, \\lambda\\_j) - \\lambda\\_i)\
    \ + o\\_j \\gg (\\max(\\lambda\\_i, \\lambda\\_j) - \\lambda\\_j)\n\\end{bmatrix}\n\
    $$\n\nIt can be shown by induction using a derivation similar to [\\(5\\)](#page-1-6)\
    \ that the final sum S and the maximum exponent of a set of FP numbers can be\
    \ computed using the newly defined operator ⊚ as follows:\n\n$$\n\\begin{bmatrix}\n\
    \\max\\{e\\_i\\} \\\\\nS\n\\end{bmatrix} = \\begin{bmatrix}\ne\\_1 \\\\ m\\_1\
    \ \\end{bmatrix} \\odot \\begin{bmatrix}\ne\\_2 \\\\ m\\_2 \\end{bmatrix} \\odot\
    \ \\dots \\odot \\begin{bmatrix}\ne\\_N \\\\ m\\_N \\end{bmatrix} \\tag{9}\n$$\n\
    \nAlso, it can be proven that the operator ⊚ is associative since\n\n$$\\left(\
    \ \\begin{bmatrix} e\\_1 \\\\ m\\_1 \\end{bmatrix} \\odot \\begin{bmatrix} e\\\
    _2 \\\\ m\\_2 \\end{bmatrix} \\right) \\odot \\begin{bmatrix} e\\_3 \\\\ m\\_3\
    \ \\end{bmatrix} = \\begin{bmatrix} e\\_1 \\\\ m\\_1 \\end{bmatrix} \\odot \\\
    left( \\begin{bmatrix} e\\_2 \\\\ m\\_2 \\end{bmatrix} \\odot \\begin{bmatrix}\
    \ e\\_3 \\\\ m\\_3 \\end{bmatrix} \\right) \\tag{10}$$\n\n## *C. Hardware Organization\
    \ of Alignment and Addition*\n\nUsing the new associative operator ⊚, fraction\
    \ alignment and addition can be performed using various hardware configurations.\
    \ For instance, Fig. [2\\(](#page-2-2)a) depicts a binary-tree architecture of\
    \ ⊚ operators. Following the definition of the ⊚ operator in [\\(8\\)](#page-2-3),\
    \ at each node of the tree, the local maximum exponent is identified first and\
    \ in turn drives local fraction alignment and addition.\n\nThe ⊚ operator can\
    \ be generalized to higher radices as well. Fig. [2\\(](#page-2-2)b) shows an\
    \ example of an 8-term alignment and addition using a mixture of radix-4 and radix-2\
    \ operators.\n\n![](_page_2_Figure_17.jpeg)\n\n![](_page_2_Figure_18.jpeg)\n\n\
    ![](_page_2_Figure_19.jpeg)\n\n<span id=\"page-2-2\"></span>Fig. 2. Tree-based\
    \ organization of parallel alignment and addition for an 8 term floating point\
    \ addition using (a) the radix-2 ⊚ operator in all nodes of the tree and (b) a\
    \ mixture of radix-4 and radix-2 operators.\n\nRadix-4 operators are used in the\
    \ first level and a radix-2 operator at the last level. For the rest of the paper\
    \ this configuration would be denoted as a 4-2 solution. Equivalently, the 8-term\
    \ adder of Fig. [2\\(](#page-2-2)a) would be denoted as a 2-2-2 solution highlighting\
    \ the radix of the operators used in each level of the tree.\n\nA radix-4 operator\
    \ effectively follows the baseline architecture shown in Fig. [1](#page-1-1) for\
    \ 4 inputs, i.e., it finds first the maximum of the 4 exponents and subtracts\
    \ it from all input exponents. The exponent differences are used for aligning\
    \ the 4 fractions before adding them. In fact, the proposed approach is a generalization\
    \ of the baseline alignment and addition. The baseline approach for an N-term\
    \ adder, shown in Fig. [1,](#page-1-1) is effectively a sub-solution of the proposed\
    \ approach and uses a *single* radix-N operator.\n\n#### IV. EVALUATION\n\nExperimental\
    \ evaluation aims at exploring the effectiveness of the proposed alignment and\
    \ addition architecture, for building multi-term fused FP adders relative to the\
    \ widely-used baseline approach. For this reason, we implemented 16, 32 and 64-term\
    \ adders for the four FP-arithmetic formats shown in Fig. [3](#page-2-4) covering\
    \ single and reduced-precision formats [\\[1\\]](#page-4-0). For the proposed\
    \ designs, for each multi-term adder we explored all possible configurations using\
    \ align-and-add operators of various radices (i.e., number of inputs).\n\n![](_page_2_Figure_25.jpeg)\n\
    \n<span id=\"page-2-4\"></span>Fig. 3. Structure of commonly used FP data types.\n\
    \n![](_page_3_Figure_1.jpeg)\n\n<span id=\"page-3-1\"></span>Fig. 4. The a) area\
    \ and b) average power of 32-term BFloat16 adders designed with the baseline approach\
    \ and the proposed approach that uses the newly introduced align-and-add operator\
    \ ⊚ in various mixed-radix configurations.\n\nAll the multi-term FP adders under\
    \ comparison, were implemented in C++[1](#page-3-0) and synthesized to Verilog\
    \ using Catapult HLS, using a 28-nm standard-cell library. All designs (i.e.,\
    \ proposed and *baseline*) operate at a clock frequency of 1 GHz and implement\
    \ a complete multi-term fused FP addition that includes fraction alignment and\
    \ addition as well as normalization and rounding of the final sum. To achieve\
    \ the target clock frequency, HLS synthesis was instructed to produce designs\
    \ with appropriate pipeline depth, depending on the number of input terms and\
    \ their data type. As the number of input terms increases so does the design's\
    \ pipeline depth. For an N-term FP32 adder we aimed for log<sup>2</sup> N pipeline\
    \ stages. HLS can derive many other pipelined solutions. However, to simplify\
    \ comparisons across designs, we selected the same configuration for all cases.\
    \ For lower-precision data types, such as BFloat16 and FP8, one pipeline stage\
    \ less relative to FP32 is enough to reach the targeted clock frequency due to\
    \ smaller mantissa and exponent bit widths. The final area results were derived\
    \ from Oasys logic synthesis tool. The power consumption was estimated after synthesis\
    \ using the PowerPro power analysis and optimization tool. For power estimation,\
    \ we employed multi-term adders in matrix multiplication kernels for the BERT\
    \ Transformer [\\[17\\]](#page-4-16) using input data from the GLUE dataset [\\\
    [18\\]](#page-4-17).\n\n### *A. Design-space exploration for 32-term BFloat16\
    \ adders*\n\n<span id=\"page-3-0\"></span>In order to assess how mixed-radix configurations\
    \ perform relative to the baseline align-and-add approach in multi-term floating-point\
    \ adders, we initially focused on the case of 32-term BFloat16 adders. The designs\
    \ presented represent complete multi-term floating point adders and the baseline\
    \ approach differs from the proposed designs only in the alignment and addition\
    \ logic. Normalization and rounding are the same for all designs under comparison.\n\
    \n![](_page_3_Figure_8.jpeg)\n\n<span id=\"page-3-2\"></span>Fig. 5. The most\
    \ area efficient designs achieved by each configuration for 32-term BFloat16 for\
    \ various clock period targets using 1–4 pipeline stages.\n\nFig. [4](#page-3-1)\
    \ depicts the area and power of the proposed adders, that follow different mixed-radix\
    \ configurations relative to the baseline approach, which effectively uses a single\
    \ N-input operator. In all cases, utilizing a mixed-radix configuration proves\
    \ more efficient than the radix-32 *baseline* configuration. From the results\
    \ shown in Fig. [4\\(](#page-3-1)a) the proposed designs can achieve area savings\
    \ that range between 3% and 15%. The 4-4-2 configuration offers the best area\
    \ efficiency, reducing area by 15%. As shown in Fig. [4\\(](#page-3-1)b), the\
    \ proposed mixedradix designs achieve power reductions of 6% to 26%. The optimal\
    \ configuration, in terms of power consumption, is the 8-2-2 design, achieves\
    \ a notable 26% power reduction.\n\nThe proposed formulation splits alignment\
    \ and addition to smaller hardware blocks thus increasing hardware modularity.\
    \ In effect, this transformation, allows HLS to schedule intermediate alignment\
    \ and addition steps to pipeline stages with better flexibility that results in\
    \ more efficient designs.\n\nThis modular approach enhances also the delay characteristics\
    \ of multi-term adders across different pipelined configurations. Fig. [5](#page-3-2)\
    \ illustrates the most area-efficient 32-term BFloat16 adders produced for various\
    \ clock period targets. For high-frequency applications, 4-stage pipelines excel,\
    \ while 2 stage pipelines are more area-optimal at lower frequencies. The proposed\
    \ 2-2-8 configuration stands out for its speed, offering a 16.6% faster clock\
    \ cycle than the baseline design with the same number of pipeline stages. In terms\
    \ of area, similar to Fig. [4\\(](#page-3-1)a), the 4-4-2 design is the most compact\
    \ at 1 ns. However, for less stringent clock requirements, the baseline design\
    \ provides the best area-performance trade-off. For completeness, we also included\
    \ the fastest single-cycle (1-stage) implementations for each design. In all cases,\
    \ their equivalent pipelined solutions offer a superior combination of speed and\
    \ area efficiency for 32-term adders.\n\n## *B. Multi-term adders for various\
    \ FP formats*\n\nAs previously demonstrated, the proposed approach performs well\
    \ for building 32-term BFloat16 adders. Nevertheless, it is essential to verify\
    \ that this efficiency extends to adders with fewer or more inputs and to other\
    \ FP data types. A more extensive analysis will offer a comprehensive understanding\
    \ of the effectiveness of FP adders built using the proposed parallel align-and-add\
    \ architecture.\n\n<span id=\"page-4-18\"></span>TABLE I THE AREA AND POWER FOR\
    \ (A) 16, (B) 32 AND (C) 64-INPUT MULTI-TERM ADDERS AND FOR VARIOUS FP DATA TYPES.\n\
    \n| N = 16   | Area (×103µm2<br>) |                 |      | Power (mW) |    \
    \             |      |\n|----------|--------------------|-----------------|------|------------|-----------------|------|\n\
    |          | Base               | Proposed        | Save | Base       | Proposed\
    \        | Save |\n| FP32     | 8.87               | 6.8<br>(8-2)    | 23%  |\
    \ 3.03       | 2.65<br>(8-2)   | 13%  |\n| BFloat16 | 2.92               | 2.69<br>(8-2)\
    \   | 8%   | 1.61       | 1.35<br>(8-2)   | 16%  |\n| FP8 e4m3 | 1.29        \
    \       | 1.23<br>(8-2)   | 4%   | 0.83       | 0.69<br>(8-2)   | 17%  |\n| FP8\
    \ e5m2 | 1.17               | 1.23<br>(2-4-2) | -5%  | 0.62       | 0.70<br>(2-4-2)\
    \ | -13% |\n| FP8 e6m1 | 1.33               | 1.36<br>(4-2-2) | -2%  | 0.49  \
    \     | 0.54<br>(4-2-2) | -10% |\n\n(a) 16-term FP adders\n\nN = 32 Area (×10<sup>3</sup>µm<sup>2</sup>\
    \ ) Power (mW) Base Proposed Save Base Proposed Save FP32 16.24 14.02 14% 6.69\
    \ 5.78 14% (2-2-2-2-2) (2-2-2-2-2) BFloat16 6.44 5.5 15% 3.97 2.92 26% (8-2-2)\
    \ (8-2-2) FP8 e4m3 3.02 2.51 17% 1.85 1.53 17% (8-2-2) (8-2-2) FP8 e5m2 2.73 2.44\
    \ 11% 1.74 1.44 17% (8-2-2) (8-2-2) FP8 e6m1 2.80 2.48 11% 0.76 0.63 18% (8-2-2)\
    \ (8-2-2)\n\n(b) 32-term FP adders\n\n| N = 64   | Area (×103µm2<br>) |      \
    \                |      | Power (mW) |                      |      |\n|----------|--------------------|----------------------|------|------------|----------------------|------|\n\
    |          | Base               | Proposed             | Save | Base       | Proposed\
    \             | Save |\n| FP32     | 32.51              | 28.67<br>(2-2-2-2-4)\
    \ | 12%  | 13.26      | 10.82<br>(2-2-2-2-4) | 19%  |\n| BFloat16 | 12.84    \
    \          | 11.73<br>(2-4-2-2-2) | 9%   | 7.30       | 7.05<br>(2-4-2-2-2)  |\
    \ 4%   |\n| FP8 e4m3 | 5.79               | 5.09<br>(8-4-2)      | 12%  | 3.62\
    \       | 3.01<br>(8-4-2)      | 17%  |\n| FP8 e5m2 | 5.34               | 4.78<br>(8-8)\
    \        | 11%  | 3.35       | 2.78<br>(8-8)        | 17%  |\n| FP8 e6m1 | 5.39\
    \               | 4.86<br>(2-8-4)      | 10%  | 1.62       | 1.35<br>(2-8-4) \
    \     | 17%  |\n\n(c) 64-term FP adders\n\nTable [I](#page-4-18) presents the\
    \ area and power performance of all designs under comparison for 16, 32 and 64\
    \ inputs and for the FP formats shown in Fig. [3.](#page-2-4) To examine also\
    \ a corner case, where the exponent differences are large relative to the mantissa's\
    \ bit width, we included also an additional 8-bit FP datatype FP8\\_e6m1.\n\n\
    For the proposed designs, we only report the configuration with the best area/power\
    \ performance. The selected configuration is indicated inside the parenthesis\
    \ below the results of the proposed designs.\n\nThe performance gains achieved\
    \ by the proposed designs depend mainly on the number of input terms and are consistent\
    \ across all examined FP data types. As shown in Table [I,](#page-4-18) adders\
    \ with a large number of input terms, like 32 or 64, demonstrate a more pronounced\
    \ benefit compared to those with a lower number of inputs. The size of the exponent\
    \ field also influences the effectiveness of mixed-radix designs. As the size\
    \ of the exponent increases, exponent calculation and fraction alignment and addition\
    \ become equally critical. This\n\n### V. CONCLUSIONS\n\nThis work reformulates\
    \ the decades-old problem of serial alignment and addition appearing in multi-term\
    \ FP adders in a new *online* form. The proposed computation paradigm allows maximum\
    \ exponent identification, exponent subtraction, alignment shift and addition\
    \ to be computed incrementally and in parallel. Alignment and addition logic can\
    \ be structured in a tree-like structure using the newly introduced align-andadd\
    \ operator. Operators with varying numbers of inputs can be employed at each level\
    \ of the tree. Hardware evaluation confirms that this approach substantially reduces\
    \ the complexity of alignment and addition, resulting in faster multiterm FP adders\
    \ or designs with smaller area and lower power consumption compared to conventional\
    \ approaches.\n\n#### REFERENCES\n\n- <span id=\"page-4-0\"></span>[1] L. Bertaccini\
    \ *et al.*, \"MiniFloat-NN and ExSdotp: An ISA extension and a modular open hardware\
    \ unit for low-precision training on RISC-V cores,\" in *IEEE Symp. on Computer\
    \ Arithmetic (ARITH)*, 2022.\n- <span id=\"page-4-1\"></span>[2] N. P. Jouppi\
    \ *et al.*, \"Ten lessons from three generations shaped google's tpuv4i: Industrial\
    \ product,\" in *Int. Symp. on Computer Architecture (ISCA)*, 2021, pp. 1–14.\n\
    - <span id=\"page-4-2\"></span>[3] S. Wang and P. Kanwar, \"BFloat16: The secret\
    \ to high performance on Cloud TPUs,\" *Google Cloud Blog*, vol. 4, 2019.\n- <span\
    \ id=\"page-4-3\"></span>[4] P. Micikevicius *et al.*, \"Fp8 formats for deep\
    \ learning,\" *arXiv preprint arXiv:2209.05433*, 2022.\n- <span id=\"page-4-4\"\
    ></span>[5] Y. Tao *et al.*, \"Correctly rounded architectures for floating-point\
    \ multioperand addition and dot-product computation,\" in *IEEE Int. Conf. on\
    \ Application-Specific Systems, Architectures and Proc. (ASAP)*, 2013.\n- <span\
    \ id=\"page-4-5\"></span>[6] J. Sohn and E. E. Swartzlander, \"A fused floating-point\
    \ three-term adder,\" *IEEE Trans. on Circuits and Systems I*, vol. 61, no. 10,\
    \ pp. 2842–2850, 2014.\n- <span id=\"page-4-6\"></span>[7] ——, \"A fused floating-point\
    \ four-term dot product unit,\" *IEEE Trans. on Circuits and Systems I*, vol.\
    \ 63, no. 3, pp. 370–378, 2016.\n- <span id=\"page-4-7\"></span>[8] D. Filippas,\
    \ C. Peltekis, G. Dimitrakopoulos, and C. Nicopoulos, \"Reduced-precision floating-point\
    \ arithmetic in systolic arrays with skewed pipelines,\" in *IEEE Int. Conf. on\
    \ Artificial Intelligence Circuits and Systems (AICAS)*, 2023.\n- <span id=\"\
    page-4-8\"></span>[9] M. Milakov and N. Gimelshein, \"Online normalizer calculation\
    \ for softmax,\" *arXiv preprint arXiv:1805.02867*, 2018.\n- <span id=\"page-4-9\"\
    ></span>[10] B. Hickmann *et al.*, \"Intel nervana neural network processor-t\
    \ (NNP-T) fused floating point many-term dot product,\" in *IEEE Symp. on Comp.\
    \ Arith. (ARITH)*, 2020.\n- <span id=\"page-4-10\"></span>[11] D. Filippas, C.\
    \ Nicopoulos, and G. Dimitrakopoulos, \"Templatized fused vector floating-point\
    \ dot product for high-level synthesis,\" *Journal of Low Power Electronics and\
    \ Applications*, vol. 12, no. 4, 2022.\n- <span id=\"page-4-11\"></span>[12] O.\
    \ Desrentes, B. D. de Dinechin, and F. de Dinechin, \"Exact fused dot product\
    \ add operators,\" in *IEEE Symp. on Comp. Arith. (ARITH)*, 2023.\n- <span id=\"\
    page-4-12\"></span>[13] A. F. Tenca, \"Multi-operand floating-point addition,\"\
    \ in *IEEE Symp. on Computer Arithmetic (ARITH)*, 2009.\n- <span id=\"page-4-13\"\
    ></span>[14] H. Kaul *et al.*, \"Optimized fused floating-point many-term dot-product\
    \ hardware for machine learning accelerators,\" in *IEEE Symp. on Computer Arithmetic\
    \ (ARITH)*, 2019.\n- <span id=\"page-4-14\"></span>[15] Y. Uguen and F. de Dinechin.\
    \ (2017) Design-space exploration for the Kulisch accumulator. [Online]. Available:\
    \ [https://hal.science/](https://hal.science/hal-01488916) [hal-01488916](https://hal.science/hal-01488916)\n\
    - <span id=\"page-4-15\"></span>[16] J. Koenig *et al.*, \"A hardware accelerator\
    \ for computing an exact dot product,\" in *IEEE Symp. on Comp. Arith. (ARITH)*,\
    \ 2017.\n- <span id=\"page-4-16\"></span>[17] J. Devlin, M.-W. Chang, K. Lee,\
    \ and K. Toutanova, \"BERT: Pre-training of deep bidirectional transformers for\
    \ language understanding,\" in *North American Chapter of the Assoc. for Comp.\
    \ Linguistics*, 2019.\n- <span id=\"page-4-17\"></span>[18] A. Wang, A. Singh,\
    \ J. Michael, F. Hill, O. Levy, and S. R. Bowman, \"GLUE: A multi-task benchmark\
    \ and analysis platform for natural language understanding,\" *arXiv preprint\
    \ arXiv:1804.07461*, 2018."
- title: "Communication Characterization of AI Workloads for Large-scale\n  Multi-chiplet\
    \ Accelerators"
  abstract: 'Next-generation artificial intelligence (AI) workloads are posing challenges

    of scalability and robustness in terms of execution time due to their intrinsic

    evolving data-intensive characteristics. In this paper, we aim to analyse the

    potential bottlenecks caused due to data movement characteristics of AI

    workloads on scale-out accelerator architectures composed of multiple chiplets.

    Our methodology captures the unicast and multicast communication traffic of a

    set of AI workloads and assesses aspects such as the time spent in such

    communications and the amount of multicast messages as a function of the number

    of employed chiplets. Our studies reveal that some AI workloads are potentially

    vulnerable to the dominant effects of communication, especially multicast

    traffic, which can become a performance bottleneck and limit their scalability.

    Workload profiling insights suggest to architect a flexible interconnect

    solution at chiplet level in order to improve the performance, efficiency and

    scalability of next-generation AI accelerators.'
  url: http://arxiv.org/abs/2410.22262v2
  keywords: AI Accelerators, Multi-Chiplet Accelerators, Communication, Network-on-Package
  document: '# Communication Characterization of AI Workloads for Large-scale Multi-chiplet
    Accelerators


    Mariam Musavi, Emmanuel Irabor, Abhijit Das, Eduard Alarcon, and Sergi Abadal
    ´ NaNoNetworking Center in Catalunya (N3Cat), Universitat Politecnica de Catalunya
    (UPC), Barcelona, Spain `


    *Abstract*—Next-generation artificial intelligence (AI) workloads are posing challenges
    of scalability and robustness in terms of execution time due to their intrinsic
    evolving data-intensive characteristics. In this paper, we aim to analyse the
    potential bottlenecks caused due to data movement characteristics of AI workloads
    on scale-out accelerator architectures composed of multiple chiplets. Our methodology
    captures the unicast and multicast communication traffic of a set of AI workloads
    and assesses aspects such as the time spent in such communications and the amount
    of multicast messages as a function of the number of employed chiplets. Our studies
    reveal that some AI workloads are potentially vulnerable to the dominant effects
    of communication, especially multicast traffic, which can become a performance
    bottleneck and limit their scalability. Workload profiling insights suggest to
    architect a flexible interconnect solution at chiplet level in order to improve
    the performance, efficiency and scalability of next-generation AI accelerators.


    *Index Terms*—AI Accelerators, Multi-Chiplet Accelerators, Communication, Network-on-Package


    #### I. INTRODUCTION


    Artificial Intelligence (AI) applications have revolutionized multiple fields
    such as natural language processing, genomics, medical and health systems, graph
    analytics, and data analytics, and others [\[1\]](#page-4-0) [\[2\]](#page-4-1)
    [\[3\]](#page-4-2). However, the impressive feats that AI can achieve are often
    accompanied by very intense computational demands, which are saturating the boundaries
    of status-quo computing infrastructures [\[4\]](#page-4-3).


    To address this, specialized hardware (HW) accelerators are designed to target
    specific computational-intensive AI workloads in an efficient manner. GPUs can
    be considered as hardware accelerators for graphics, that have been adapted to
    also better support AI workloads. However, the past decade has seen the emergence
    of AI accelerators with tensor or transformer cores, which cater to the specific
    computational needs of AI [\[5\]](#page-4-4). Generally, the architecture of AI
    accelerators is comprised of an off-chip memory, a shared on-chip memory known
    as global buffer, and an array of processing elements (PEs) connected via a Network-on-Chip
    (NoC) [\[6\]](#page-4-5).


    As modern AI models are evolving in size and diversity to accommodate multiple
    applications [\[7\]](#page-4-6) or to implement input-dependent models such as
    Graph Neural Networks (GNNs) [\[8\]](#page-4-7), versatile and high-performance
    accelerators are required to support their execution. This can be achieved with
    reconfigurable dataflows in FPGAs [\[9\]](#page-4-8), flexible accelerator


    <span id="page-0-0"></span>![](_page_0_Figure_10.jpeg)


    Fig. 1: An illustration of multi-chiplet architecture with 3x3 computing chiplets
    and 4 DRAM chiplets.


    architectures [\[10\]](#page-4-9), [\[11\]](#page-4-10), or by resorting to less
    efficient but more general-purpose GPU and CPU architectures [\[12\]](#page-4-11).


    Another alternative compatible with the scaling and adaptation of AI accelerators
    is the use of chiplets. Indeed, chiplet technology is a promising enabler that
    provides a way to scale AI accelerators, by combining together multiple specialized
    (and potentially heterogeneous) AI accelerator chiplets in a single computing
    platform, as illustrated in Fig. [1.](#page-0-0) These chiplets are interconnected
    among themselves and to memory via on-package links, typically through silicon
    interposers or organic substrates, in order to create a Network-on-Package (NoP)
    [\[13\]](#page-4-12)–[\[15\]](#page-4-13). This has been proposed in multiple
    works, including SIMBA [\[16\]](#page-4-14) or WIENNA [\[17\]](#page-4-15), among
    others.


    It is worth noting that AI accelerator chiplets can spend more than 90 percent
    of the total system energy on memorybound tasks, to fetch data, as illustrated
    in [\[18\]](#page-4-16). This is due to not only the limited speed of memory modules,
    but more importantly, the relatively slow chiplet-to-chiplet data transfers, which
    can often dominate the computation energy due to traversing long interconnects.
    This issue is exacerbated by the need to use multicast communication in many of
    the dataflows that are used in AI accelerators [\[17\]](#page-4-15), [\[19\]](#page-4-17).


    Despite the importance of chiplet-level communication in multi-chip AI accelerators,
    the communication traffic within such systems has not been explored in depth nor
    characterized. Traffic characterization and modeling has been well studied in
    CPU and GPU platforms for a variety of workloads [\[20\]](#page-4-18)–[\[22\]](#page-4-19),
    yet a similar analysis is missing in the proposed context.


    The main contribution of this work is the profiling of workload-specific data
    movements across a set of popular AI workloads in multi-chiplet AI accelerators
    under increasing size. We augment GEMINI [\[23\]](#page-4-20) to register communication
    packets and then parse the resulting traces to analyze their characteristics,
    for instance, the number of messages, destinations per multicast, or the number
    of NoP hops per message.


    Authors gratefully acknowledge funding from the European Commission through projects
    with GA 101042080 (WINC) and 101189474 (EWiC) and from Generalitat de Catalunya
    through the ICREA Academia Award 2024.


    The remainder of the paper is structured as follows. Section [II](#page-1-0) presents
    the state of the art in multi-chip AI accelerators and workload characterization.
    Our methodology is presented in Section [III.](#page-1-1) Section [IV](#page-2-0)
    presents the results obtained, followed by the conclusion and future work in Section
    [V.](#page-3-0)


    #### II. RELATED WORKS


    # <span id="page-1-0"></span>*A. Scheduling and Mapping for AI Accelerators*


    In the pioneering work of [\[16\]](#page-4-14), authors developed and characterized
    a multi-chip AI inference accelerator with a scalable and hierarchical interconnect
    architecture on a package, called SIMBA. The objective was to enhance the energy
    efficiency and reduce accelerator''s inference latency by partitioning the non-uniform
    workload, considering communication-aware data placement, and implementing cross-layer
    pipelining. The mapper used in this work is Timeloop [\[24\]](#page-4-21), and
    the cost of the HW architecture is estimated with Accelergy [\[25\]](#page-4-22).


    The authors of [\[23\]](#page-4-20) developed and characterized a multichip AI
    inference accelerator design framework, called GEM-INI, that explores the design
    space to deliver architectures that, for a particular workload, minimize monetary
    cost and Energy-Delay Product (EDP). The mapper employed in this work is inter-layer
    pipelining using SET [\[26\]](#page-4-23), and a customized cost model was used
    to evaluate the cost of the architecture. These improvements significantly enhanced
    the overall system performance compared to the baseline mapping variants.


    In recent work, the authors of [\[7\]](#page-4-6) developed a multi-chipletbased
    multi-model AI inference accelerator that is scalable under heterogeneous traffic
    (i.e., data center and AR/VR) models with the objective to minimize EDP. The mapper
    used in this work is based on SET [\[26\]](#page-4-23), and the hybrid cost model
    is customized using MAESTRO [\[27\]](#page-4-24).


    In all these works discussed above and similar ones like MOHaM [\[28\]](#page-4-25),
    MAGMA [\[29\]](#page-4-26), Herald [\[30\]](#page-4-27), etc., communication is
    either coarsely classified depending on the chosen dataflow or indirectly used
    as a metric for optimization of mapping. However, none of the previous works have
    conducted an in-depth characterization of the communication workload across AI
    algorithms and architectures.


    # *B. Workload Parallelism*


    Work that considered to determine spatial and temporal tiling factors such as,
    PE dimensions, accumulator SRAM sizing, scratchpad SRAM sizing, and finds the
    best compute/buffer sizes using a mapping-first approach includes Timeloop [\[24\]](#page-4-21),
    Sparseloop [\[31\]](#page-4-28), COSA [\[32\]](#page-4-29) and DOSA [\[33\]](#page-4-30)
    to reduce memory access overheads and data replication. Works that exploit both
    intra- and layer loop ordering using inter-layer pipelining include TANGRAM [\[34\]](#page-4-31),
    SET [\[26\]](#page-4-23), GEMINI [\[23\]](#page-4-20), and most recently SCAR
    [\[7\]](#page-4-6) to improve data re-usability reduce pipelining delays. Again,
    in these cases, communication cost is used directly or indirectly as a metric
    for the optimization of the mapping, but without providing a proper characterization
    of the different workloads.


    <span id="page-1-2"></span>![](_page_1_Figure_9.jpeg)


    Fig. 2: Methodology for characterizing data movement of AI workloads in multi-chip
    architectures, based on GEMINI [\[23\]](#page-4-20).


    ## *C. Communication Workload Analysis*


    In addition, there are a few existing works that characterize communication in
    CPU workloads based on cycle-accurate simulation, obtaining spatial-temporal distributions
    or correlations between traffic flows, among others, for unicast and multicast
    communication [\[20\]](#page-4-18), [\[21\]](#page-4-32), [\[35\]](#page-4-33).
    Similar investigations have been conducted on GPU architectures [\[22\]](#page-4-19).
    Undoubtedly, all the above work outperformed the performance on workloads-chiplets
    mapping; however, there is still a gap left to fulfill characterizing workloads
    from inter-chiplet multicast communication perspective. This characterization
    has significant potential to assist mappers/schedulers to efficiently orchestrate
    computing resources and tailor data movement strategies according to the underlying
    communication distance bottlenecks in executing the next-generation AI workloads.
    To the best of our knowledge, there is a critical need to characterize multicast
    data movement pattern in order to obtain the key metrics in the realm of diverse
    AI workloads-chiplet mappings under scalability (i.e., increasing the size of
    chiplet array configurations).


    ## III. CHARACTERIZATION METHODOLOGY


    <span id="page-1-1"></span>The main objective of this work is to perform a traffic
    analysis to obtain communication behaviors of various AI workloads, in order to
    assess their scalability. To this end, we employ the methodology outlined in Fig.
    [2,](#page-1-2) to capture important communication-related characteristics (e.g.,
    multicast data movement) of all benchmark AI workloads. The communication workloads
    to be analyzed depend on multiple factors such as AI workload itself, the size
    of architecture, or how the workload is mapped into the architecture. Mapping
    requires decisions on the loop execution order and also regarding dataflow dependencies.
    The former determines the order in which the layer is mapped and executed on architecture,
    while later determines the inter-dependency of data provisioning between layers
    (i.e., the movement of output activation and partial sums, between preceding and
    subsequent layers).


    To abstract away from these decisions, here we use GEMINI [\[23\]](#page-4-20)
    to determine the best mapping for a particular combination of AI workload and
    multi-chip accelerator configuration. We wrote C++ functional scripts to obtain
    a set of communication traces from GEMINI, which are later parsed with the


    TABLE I: Simulation Parameters


    <span id="page-2-2"></span><span id="page-2-1"></span>![](_page_2_Figure_1.jpeg)


    Fig. 3: Fraction of overall execution time (in clock cycles) spent by on-chip,
    chip-to-chip and chip-to-DRAM data movement related tasks for each AI workload
    across configurations.


    aim to extract profiles, which can further be processed and visualized using Python
    3.10. We mainly focused on capturing multicast communication metrics, due to the
    severity of their effects when scaled, across all workloads and configurations.
    In particular, the communication metrics extracted are:


    - Amount of time in clock cycles spent performing the data movement by the relevant
    compute/memory chiplets.

    - Number of unicast and multicast messages.

    - Number of chiplet-to-chiplet (NoP) hops per message.

    - Number of chiplets involved in a multicast message.


    Table [I](#page-2-1) presents the parameters explored in the mapping of 12 AI
    inference workloads on the best HW architecture parameters obtained from GEMINI.
    We choose benchmark models that contain multi-branch classic residual, e.g. ResNet50,
    ResNet152, GoogleNet, Transformer (TF), TF Cell, and inception (iRES) structures
    with more intricate dependencies which are prevalent and widely used in various
    scenarios such as image classification and language processing. The bandwidths
    in the different communication levels, as well as the three multi-chiplet arrangements
    from 2 to 18 chiplets, are chosen based on GEMINI baselines. Regardless of the
    number of AI compute chiplets, the number of DRAM chiplets is set to four.


    #### IV. WORKLOAD CHARACTERIZATION RESULTS


    <span id="page-2-0"></span>This section presents our characterization results
    and discusses the insights obtained for multiple AI workloads on different multi-chip
    accelerator configurations.


    #### *A. Communication Source Analysis*


    Fig. [3](#page-2-2) reveals the sources of data movement by assessing the time
    spent by NoC, NoP, and DRAM across all workloads and averaged across system sizes.
    Notably, it can be observed that in most cases, the inter-chiplet data movement
    is the highest contribution among the three, especially for workloads such as
    iRES, LSTM, or TF.


    This aspect is further analyzed in Fig. [4,](#page-2-3) which shows the amount
    of time spent on data movement (i.e., communication


    <span id="page-2-3"></span>![](_page_2_Figure_14.jpeg)


    Fig. 4: Fraction of data movement time spent in the DRAM, NoP and NoC across all
    AI workloads and chiplet array configurations. The total time in clock cycles
    is shown in red at the bottom of the plot.


    time) contributed by each NoC, NoP and DRAM blocks for the different system sizes
    considered. During each workload execution, an increase in the number of chiplets
    directly contributes to an increase in NoP time, while indirectly impacting NoC
    time. As the number of chiplets grows, the number of computation cores also increases,
    enabling more efficient handling of computations. However, it is evident that,
    under 6x3 configuration, some AI workloads such as TF and TF Cell are heavily
    bottlenecked by the inter-chiplet communication time by 72.6% and 70.7%, respectively.
    In contrast, GoogleNet and DarkNet19 appear to be memory-bounded by 46.5% and
    44.5%, respectively.


    In summary, Fig. [5](#page-3-1) shows in the form of a box plot, the fraction
    of data movement time spent by NoP over the sum of NoC, NoP and DRAM time for
    all workloads as a function of the number of chiplets in the system. It is evident
    from the plot that the amount of NoP time is directly proportional to the increase
    in network size, reaching values well above 50% and even beyond 70%, which will
    eventually throttle the execution of the AI workload. Therefore, it is necessary
    to improve and design flexible data movement-aware architectures to eliminate
    the communication bottleneck.


    #### *B. Multicast Traffic Analysis*


    Fig. [6](#page-3-2) shows the fraction of multicast messages generated by each
    workload and the number of chiplets combination that are included in the destination
    set of each multicast message. First, it is evident from the plot that algorithms
    such as DenseNet (15.2, 36.8 and 41.2) ×10<sup>6</sup> and TF (9.2, 14.8 and 13.6)
    ×10<sup>6</sup> have a high number of multicast messages for 1x2, 3x3 and 6x3
    chiplet configurations, respectively. Overall, it is observed that the importance
    of muilticasts with two, four, and six chiplet destinations is high. However,
    the number of multicast messages with the highest number of visited chiplets is
    very


    <span id="page-3-1"></span>![](_page_3_Figure_0.jpeg)


    Fig. 5: Fraction of time spent by NoP over the total communication time across
    all chiplet array configurations.


    <span id="page-3-2"></span>![](_page_3_Figure_2.jpeg)


    Fig. 6: Number of multicast messages and number of destinations per message across
    all AI workloads and for multiple chiplet array configurations. The number of
    messages is shown in red at the bottom of the plot, whereas the number of destinations
    is shown in the form of a stacked bar where each color represents a given number
    of destinations.


    significant, i.e., 12, especially in workloads such as LSTM (30.1%) or GNMT (30.5%).


    Finally, Fig. [7](#page-3-3) shows the distribution of unicast and multicast messages
    as a function of the number of NoP hops required in the 6x3 chiplet array. This
    figure offers multiple insights. On one hand, the notion of spatial locality is
    not common in multi-chip AI accelerators, as the increasing number of messages
    with high number of NoP hops suggest. In contrast, even though the number of multicasts
    is significantly smaller than the number of unicasts, they are more prone to establish
    long-range connections.


    # *C. Discussion*


    In summary, workload-specific layers consist of datadependent operators, with
    different compute, memory, and data movement requirements. As the number of chiplets
    increases, the number of communication hops also increases, which leads to a rise
    in the latency incurred due to moving data from memory to compute units. Hence,
    workload-specific multicast profiling is particularly useful to identify underlying
    communication-distance bottlenecks and to alleviate by improving the interconnect
    fabrics.


    <span id="page-3-3"></span>![](_page_3_Figure_8.jpeg)


    Fig. 7: Number of unicast and multicast messages across all AI workloads for the
    6x3 chiplet array configuration.


    Emerging interconnect technologies, i.e., wireless [\[17\]](#page-4-15), [\[36\]](#page-4-34)–[\[38\]](#page-4-35)
    and/or optical-enabled interconnects [\[39\]](#page-4-36), [\[40\]](#page-4-37))
    are promising candidates to tackle the communication overhead in multi-chip accelerators
    [\[41\]](#page-4-38). An alternative is to reduce the communication intensity
    through novel memory and computing approaches such as in-memory computing (IMC),
    nearmemory computing (NMC), or adopting vertical chip stacking technologies such
    as 3D-stacked memory.


    Communication-wise, optical interconnects offer higher bandwidth and lower power
    consumption per transmitted bit. For instance, Hummingbird [\[40\]](#page-4-37)
    employs an optical interconnect that allows all-to-all broadcast network with
    great benefits of reduced latency for long-distance data transfers, yet the scaling
    of such architecture remains unclear. Wireless technology, on the other hand,
    can become a powerful complement to wired interconnect fabrics offering dynamic
    bandwidth sharing, broadcast support, yet at the cost of reduced bandwidth. In
    this scheme, priority-based packet transmission of certain critical messages can
    be exploited to alleviate specific communication bottlenecks that happen if the
    workload is unbalanced within the wired and wireless planes of the multi-chip
    accelerator architecture. Hence, scaling-driven architecture, enabled by wireless
    communication or optical communication, has huge potential to address these challenging
    end-to-end latency constraints with high reliability and stability.


    # V. CONCLUSION AND FUTURE WORK


    <span id="page-3-0"></span>We presented a multicast characterization tool, based
    on GEMINI [\[23\]](#page-4-20), to register and analyze the communication workloads
    of AI algorithms mapped onto multi-chip AI accelerators. We observed that the
    communication latency has the potential to become an important bottleneck in most
    of the analyzed workloads, an aspect that is exacerbated as the communication
    intensity grows proportionally to the number of chiplets in the system. In particular,
    we identified multicast data movement as an important contributor to the overall
    required communication. We posit that this analysis and modeling approach is critical
    to take into account the relevant trade-offs in designing the architecture for
    large-scale multi-chip AI accelerator systems. Specifically, we suggest addressing
    the communication bottlenecks by supplementing existing interconnects with emerging
    enablers such as wireless or optical interconnect technologies.


    #### REFERENCES


    - <span id="page-4-0"></span>[1] N. P. Jouppi, D. Hyun Yoon, M. Ashcraft, M. Gottscho,
    T. B. Jablin, G. Kurian, J. Laudon, S. Li, P. Ma, X. Ma, T. Norrie, N. Patil,
    S. Prasad, C. Young, Z. Zhou, and D. Patterson, "Ten Lessons From Three Generations
    Shaped Google''s TPUv4i: Industrial Product," in *Proceedings of ISCA*, 2021.

    - <span id="page-4-1"></span>[2] Y. Gu, A. Subramaniyan, T. Dunn, A. Khadem, K.-Y.
    Chen, S. Paul, M. Vasimuddin, S. Misra, D. Blaauw, S. Narayanasamy *et al.*, "GenDP:
    A Framework of Dynamic Programming Acceleration for Genome Sequencing Analysis,"
    in *Proceedings of ISCA*, 2023.

    - <span id="page-4-2"></span>[3] V. Dadu, S. Liu, and T. Nowatzki, "PolyGraph:
    Exposing the Value of Flexibility for Graph Processing Accelerators," in *Proceedings
    of ISCA*, 2021, pp. 595–608.

    - <span id="page-4-3"></span>[4] C.-J. Wu, R. Raghavendra, U. Gupta, B. Acun,
    N. Ardalani, K. Maeng, G. Chang, F. Aga, J. Huang, C. Bai *et al.*, "Sustainable
    AI: Environmental implications, challenges and opportunities," *Proceedings of
    Machine Learning and Systems*, vol. 4, pp. 795–813, 2022.

    - <span id="page-4-4"></span>[5] R. Garg, H. Kwon, E. Qin, Y.-H. Chen, T. Krishna,
    and L. Lai, "PipeOrgan: Efficient Inter-operation Pipelining with Flexible Spatial
    Organization and Interconnects," *arXiv preprint arXiv:2405.01736*, 2024.

    - <span id="page-4-5"></span>[6] P. Chatarasi, H. Kwon, A. Parashar, M. Pellauer,
    T. Krishna, and V. Sarkar, "Marvel: A data-centric approach for mapping deep learning
    operators on spatial accelerators," *ACM Transactions on Architecture and Code
    Optimization (TACO)*, vol. 19, no. 1, pp. 1–26, 2021.

    - <span id="page-4-6"></span>[7] M. Odema, L. Chen, H. Kwon, and M. A. Al Faruque,
    "Scar: Scheduling multi-model ai workloads on heterogeneous multi-chiplet module
    accelerators," in *Proceedings of MICRO*, 2024, pp. 565–579.

    - <span id="page-4-7"></span>[8] R. Garg, E. Qin, F. Munoz-Matr ˜ ´ınez, R. Guirado,
    A. Jain, S. Abadal, J. L. Abellan, M. E. Acacio, E. Alarc ´ on, S. Rajamanickam
    ´ *et al.*, "Understanding the design-space of sparse/dense multiphase GNN dataflows
    on spatial accelerators," in *Proceedings of IPDPS*, 2022, pp. 571–582.

    - <span id="page-4-8"></span>[9] R. Hwang, T. Kim, Y. Kwon, and M. Rhu, "Centaur:
    A chiplet-based, hybrid sparse-dense accelerator for personalized recommendations,"
    in *Proceedings of ISCA*, 2020, pp. 968–981.

    - <span id="page-4-9"></span>[10] S.-C. Kao, H. Kwon, M. Pellauer, A. Parashar,
    and T. Krishna, "A Formalism of DNN Accelerator Flexibility," *Proceedings of
    the ACM on Measurement and Analysis of Computing Systems*, vol. 6, no. 2, 2022.

    - <span id="page-4-10"></span>[11] H. Kwon, A. Samajdar, and T. Krishna, "MAERI:
    Enabling flexible dataflow mapping over dnn accelerators via reconfigurable interconnects,"
    *ACM SIGPLAN Notices*, vol. 53, no. 2, pp. 461–475, 2018.

    - <span id="page-4-11"></span>[12] B. Li, R. Arora, S. Samsi, T. Patel, W. Arcand,
    D. Bestor, C. Byun *et al.*, "AI-enabling workloads on large-scale GPU-accelerated
    system: Characterization, opportunities, and implications," in *Proceedings of
    HPCA*, 2022, pp. 1224–1237.

    - <span id="page-4-12"></span>[13] N. Beck, S. White, M. Paraschou, and S. Naffziger,
    "''Zeppelin'': An SoC for multichip architectures," in *Proceedings of ISSCC*,
    2018, pp. 40–42.

    - [14] A. Kannan, N. E. Jerger, and G. H. Loh, "Enabling interposer-based disintegration
    of multi-core processors," in *Proceedings of MICRO*, 2015, pp. 546–558.

    - <span id="page-4-13"></span>[15] P. Vivet, E. Guthmuller, Y. Thonnart, G. Pillonnet,
    G. Moritz, I. Miro-Panades, C. Fuguet, J. Durupt ` *et al.*, "2.3 a 220gops 96-core
    processor with 6 chiplets 3d-stacked on an active interposer offering 0.6 ns/mm
    latency, 3tb/s/mm 2 inter-chiplet interconnects and 156mw/mm 2@ 82%-peak-efficiency
    dc-dc converters," in *Proceedings of ISSCC*, 2020, pp. 46–48.

    - <span id="page-4-14"></span>[16] Y. S. Shao, J. Clemons, R. Venkatesan, B. Zimmer,
    M. Fojtik, N. Jiang, B. Keller, A. Klinefelter, N. Pinckney, P. Raina *et al.*,
    "Simba: Scaling deep-learning inference with multi-chip-module-based architecture,"
    in *Proceedings of MICRO*, 2019, pp. 14–27.

    - <span id="page-4-15"></span>[17] R. Guirado, H. Kwon, S. Abadal, E. Alarcon,
    and T. Krishna, "Dataflow- ´ architecture co-design for 2.5D DNN accelerators
    using wireless network-on-package," in *Proceedings of ASP-DAC*, 2021, pp. 806–812.

    - <span id="page-4-16"></span>[18] A. Boroumand, S. Ghose, Y. Kim, R. Ausavarungnirun,
    E. Shiu, R. Thakur, D. Kim, A. Kuusela, A. Knies, P. Ranganathan *et al.*, "Google
    workloads for consumer devices: Mitigating data movement bottlenecks," in *Proceedings
    of ASPLOS*, 2018, pp. 316–331.

    - <span id="page-4-17"></span>[19] V. Sze, Y.-H. Chen, T.-J. Yang, and J. S. Emer,
    "Efficient processing of deep neural networks: A tutorial and survey," *Proceedings
    of the IEEE*, vol. 105, no. 12, pp. 2295–2329, 2017.

    - <span id="page-4-18"></span>[20] V. Soteriou, H. Wang, and L. Peh, "A statistical
    traffic model for onchip interconnection networks," in *14th IEEE International
    Symposium on Modeling, Analysis, and Simulation*, 2006, pp. 104–116.

    - <span id="page-4-32"></span>[21] S. Abadal, A. Mestres, R. Martinez, E. Alarcon,
    A. Cabellos-Aparicio, and R. Martinez, "Multicast on-chip traffic analysis targeting
    manycore NoC design," in *2015 23rd Euromicro International Conference on Parallel,
    Distributed, and Network-Based Processing*, 2015.

    - <span id="page-4-19"></span>[22] S. Lal, J. Lucas, M. Andersch, M. Alvarez-Mesa,
    A. Elhossini, and B. Juurlink, "GPGPU workload characteristics and performance
    analysis," in *IProceedings of SAMOS*, 2014.

    - <span id="page-4-20"></span>[23] J. Cai, Z. Wu, S. Peng, Y. Wei, Z. Tan, G.
    Shi, M. Gao, and K. Ma, "Gemini: Mapping and Architecture Co-exploration for Large-scale
    DNN Chiplet Accelerators," in *Proceedings of HPCA*, 2024, pp. 156– 171.

    - <span id="page-4-21"></span>[24] A. Parashar, P. Raina, Y. S. Shao, Y.-H. Chen,
    V. A. Ying, A. Mukkara, R. Venkatesan, B. Khailany, S. W. Keckler, and J. Emer,
    "Timeloop: A systematic approach to DNN accelerator evaluation," in *Proceedings
    of ISPASS*, 2019, pp. 304–315.

    - <span id="page-4-22"></span>[25] Y. N. Wu, J. S. Emer, and V. Sze, "Accelergy:
    An architecture-level energy estimation methodology for accelerator designs,"
    in *Proceedings of ICCAD*, 2019, pp. 1–8.

    - <span id="page-4-23"></span>[26] J. Cai, Y. Wei, Z. Wu, S. Peng, and K. Ma,
    "Inter-layer scheduling space definition and exploration for tiled accelerators,"
    in *Proceedings of ISCA*, 2023.

    - <span id="page-4-24"></span>[27] H. Kwon, P. Chatarasi, V. Sarkar, T. Krishna,
    M. Pellauer, and A. Parashar, "MAESTRO: A data-centric approach to understand
    reuse, performance, and hardware cost of DNN mappings," *IEEE Micro*, vol. 40,
    no. 3, pp. 20–29, 2020.

    - <span id="page-4-25"></span>[28] A. Das, E. Russo, and M. Palesi, "Multi-objective
    hardware-mapping co-optimisation for multi-dnn workloads on chiplet-based accelerators,"
    *IEEE Transactions on Computers*, 2024.

    - <span id="page-4-26"></span>[29] S.-C. Kao and T. Krishna, "Magma: An optimization
    framework for mapping multiple dnns on multiple accelerator cores," in *Proceedings
    of HPCA*, 2022, pp. 814–830.

    - <span id="page-4-27"></span>[30] H. Kwon, L. Lai, M. Pellauer, T. Krishna, Y.-H.
    Chen, and V. Chandra, "Heterogeneous dataflow accelerators for multi-dnn workloads,"
    in *Proceedings of HPCA*, 2021, pp. 71–83.

    - <span id="page-4-28"></span>[31] Y. N. Wu, P.-A. Tsai, A. Parashar, V. Sze,
    and J. S. Emer, "Sparseloop: An analytical approach to sparse tensor accelerator
    modeling," in *Proceedings of MICRO*, 2022, pp. 1377–1395.

    - <span id="page-4-29"></span>[32] Q. Huang, M. Kang, G. Dinh, T. Norell, A. Kalaiah,
    J. Demmel, J. Wawrzynek, and Y. S. Shao, "COSA: Scheduling by constrained optimization
    for spatial accelerators," in *Proceedings of ISCA*, 2021, pp. 554–566.

    - <span id="page-4-30"></span>[33] C. Hong, Q. Huang, G. Dinh, M. Subedar, and
    Y. S. Shao, "DOSA: Differentiable model-based one-loop search for dnn accelerators,"
    in *Proceedings of MICRO*, 2023, pp. 209–224.

    - <span id="page-4-31"></span>[34] M. Gao, X. Yang, J. Pu, M. Horowitz, and C.
    Kozyrakis, "Tangram: Optimized coarse-grained dataflow for scalable NN accelerators,"
    in *Proceedings of ASPLOS*, 2019, pp. 807–820.

    - <span id="page-4-33"></span>[35] S. Abadal, R. Mart´ınez, J. Sole-Pareta, E.
    Alarc ´ on, and A. Cabellos- ´ Aparicio, "Characterization and modeling of multicast
    communication in cache-coherent manycore processors," *Computers & Electrical
    Engineering*, vol. 51, pp. 168–183, 2016.

    - <span id="page-4-34"></span>[36] E. Irabor, M. Musavi, A. Das, and S. Abadal,
    "Exploring the potential of wireless-enabled multi-chip ai accelerators," in *7th
    Workshop on Accelerated Machine Learning (AccML) at HiPEAC*, 2025.

    - [37] M. Palesi, E. Russo, A. Das, and J. Jose, "Wireless enabled interchiplet
    communication in dnn hardware accelerators," in *Proceedings of IPDPSW*, 2023,
    pp. 477–483.

    - <span id="page-4-35"></span>[38] S. Abadal, R. Guirado, H. Taghvaee, A. Jain,
    E. P. de Santana, P. H. Bol´ıvar, M. Saeed, R. Negra, Z. Wang, K.-T. Wang *et
    al.*, "Graphenebased wireless agile interconnects for massive heterogeneous multi-chip
    processors," *IEEE Wireless Communications*, 2022.

    - <span id="page-4-36"></span>[39] E. Taheri, M. A. Mahdian, S. Pasricha, and
    M. Nikdast, "SwInt: A Non-Blocking Switch-Based Silicon Photonic Interposer Network
    for 2.5 D machine learning Accelerators," *IEEE Journal on Emerging and Selected
    Topics in Circuits and Systems*, 2024.

    - <span id="page-4-37"></span>[40] M. Steinman, "Hummingbird™ Low-Latency Computing
    Engine," in *2023 IEEE Hot Chips 35 Symposium (HCS)*, 2023, pp. 1–20.

    - <span id="page-4-38"></span>[41] A. Das, M. Palesi, J. Kim, and P. P. Pande,
    "Chip and package-scale interconnects for general-purpose, domain-specific and
    quantum computing systems-overview, challenges and opportunities," *IEEE Journal
    on Emerging and Selected Topics in Circuits and Systems*, 2024.'
- title: "Systolic Array Data Flows for Efficient Matrix Multiplication in Deep\n\
    \  Neural Networks"
  abstract: 'The paper discusses how Systolic Arrays can improve matrix multiplication
    for

    deep neural networks (DNNs). With AI models like OpenAI''s GPT now containing

    trillions of parameters, the need for efficient matrix multiplication is more

    critical than ever. In this paper, the three main systolic array data flows:

    Weight Stationary (WS), Input Stationary (IS), and Output Stationary (OS) are

    discussed. Each data flow''s energy consumption and efficiency across various

    matrix sizes are calculated using the SCALE-Sim simulator. The results show

    that selecting the right data flow for specific matrix configurations can

    drastically reduce energy consumption. The conclusions provide helpful insights

    into optimizing hardware for AI and machine learning applications, offering

    potential improvements in designing energy-efficient DNN accelerators.'
  url: http://arxiv.org/abs/2410.22595v1
  keywords: systolic arrays, hardware accelerators, DNN
  document: '# Systolic Array Data Flows for Efficient Matrix Multiplication in Deep
    Neural Networks


    Tejas Raja *Chelmsford High School C*helmsford, MA tejasraja21@gmail.com


    *Abstract*—The paper discusses how Systolic Arrays can improve matrix multiplication
    for deep neural networks (DNNs). With AI models like OpenAI''s GPT now containing
    trillions of parameters, the need for efficient matrix multiplication is more
    critical than ever. In this paper, the three main systolic array data flows: Weight
    Stationary (WS), Input Stationary (IS), and Output Stationary (OS) are discussed.
    Each data flow''s energy consumption and efficiency across various matrix sizes
    are calculated using the SCALE-Sim simulator. The results show that selecting
    the right data flow for specific matrix configurations can drastically reduce
    energy consumption. The conclusions provide helpful insights into optimizing hardware
    for AI and machine learning applications, offering potential improvements in designing
    energy-efficient DNN accelerators.


    *Index Terms*—systolic arrays, hardware accelerators, DNN


    #### I. INTRODUCTION


    In recent years, the increase in the size of artificial intelligence (AI) models
    has made it critical for large general matrix multiplication (GEMM) operations
    to be highly energy efficient. For instance, Open AI''s ChatGPT has increased
    its parameters exponentially over the years [\[11\]](#page-4-0), starting from
    117 million in 2018 to 1.8 trillion parameters in 2023, requiring substantial
    matrix multiplication during training and inference as shown in Figure [1.](#page-0-0)
    Larger matrix sizes require stronger computing hardware. However, the energy and
    performance costs are also increasing with these large matrix sizes, requiring
    the design of specified hardware for efficient matrix multiplication.


    ![](_page_0_Figure_7.jpeg)


    <span id="page-0-0"></span>Fig. 1. ChatGPT Parameters vs. Year [\[11\]](#page-4-0)


    Traditionally, Von-Neumann architecture-based Central Processing Units (CPU) were
    used to compute matrix multiplications [\[1\]](#page-3-0). However, its bottleneck
    made it challenging to perform at high efficiency. Graphical Processing Units
    (GPU) were used to combat this, allowing for parallel computation [\[8\]](#page-4-1).
    However, as Capra et al. [\[3\]](#page-3-1) point out, GPUs still follow the Von
    Neumann architecture and therefore suffer from similar memory access bottlenecks,
    especially in high-demand deep learning applications. Most recently, Google''s
    Tensor Processing Units (TPU) have emerged as the strongest way to perform extensive
    matrix operations using systolic arrays that are not limited by Von-Neumann bottlenecks
    [\[6\]](#page-4-2).


    Previous works on systolic arrays do not evaluate the specific benefits of using
    different data flow customizations. The three main data flows are weight stationary
    (WS), input stationary (IS), and output stationary (OS). Each of the data flows
    has varied performance and energy efficiency across a range of matrix dimensions
    and sizes. Having insights into optimal data flow, for given matrix sizes, is
    crucial in designing and optimizing hardware accelerators for DNN applications.


    This paper comprehensively explains the energy required to multiply various matrix
    sizes with the weight, input, and output stationary data flows. The number of
    clock cycles required is calculated using the SCALE-Sim [\[9\]](#page-4-3) simulator.
    The minimum dimension of the systolic array is calculated based on the data flow.
    The total energy required for each data flow for a range of matrix sizes is calculated.


    #### II. BACKGROUND


    This section discusses the motivation towards specialized hardware for AI/ML computation.


    #### *A. Von Neumann Architecture*


    In 1945, Hungarian mathematician and physicist John Von Neumann created the premise
    of all computer architecture design: Von Neumann Architecture. Von Neumann architecture
    executes numerical computation in an independent unit called, CPU, which consists
    of an arithmetic logic unit (ALU) and a control unit (CU) as shown in Figure [2.](#page-1-0)
    The output and instructions are stored in a separate memory block. When executed,
    the instructions from memory feed the CPU to perform necessary computations. The
    output from the CPU is stored back in the central memory unit as shown in Figure
    [2.](#page-1-0) Most computer systems today are based on this architecture and
    for a good reason. Von Neumann Architecture has a straightforward design, only
    needing a single memory space for instructions and data. It is also a general-purpose
    architecture that can handle various applications, from basic computing to executing
    complex algorithms. Following Moore''s Law, such computer architecture''s performance
    has been scaling until recently. Moore''s law states that the number of transistors
    in an integrated circuit doubles approximately every two years, ultimately increasing
    speed while decreasing power consumption and maintaining cost. **Von Neumann Design**


    ![](_page_1_Figure_1.jpeg)


    <span id="page-1-0"></span>Fig. 2. Von Neumann Diagram [\[1\]](#page-3-0)


    ## *B. Issues With Von Neumann*


    Although there are many positives to Von Neumann''s architecture, its main setback
    is its inability to efficiently perform extensive matrix multiplication. Matrix
    multiplication is crucial today because of its extensive role in scientific computing,
    computer graphics, data processing, and, most importantly, Deep Neural Networks
    (DNN) training and inference. Training DNNs involves multiplying a matrix of inputs
    with a matrix of weights to model the connections between neurons. As DNNs grow,
    similar to the exponential increase in parameters of large language models (LLM)
    like ChatGPT, the necessity to perform matrix multiplication efficiently becomes
    increasingly important.


    Von Neumann''s architecture is inefficient for matrix multiplication, primarily
    due to its memory bottleneck. The Von Neumann Bottleneck happens due to the shared
    bus between the memory and the processing units, forcing the processor to fetch
    data from the memory sequentially, or one at a a time. This slows down the overall
    speed of the tasks because the processor is often left to sit idle while the data
    moves from the memory to the processor. In matrix multiplication, which requires
    frequent memory access due to the repetitive nature of multiplication and addition,
    this bottleneck severely limits performance by the back-and-forth data transfer.


    # *C. CPU vs. GPU vs. TPU*


    To tackle the lack of efficiency exhibited by standard Von Neumann designs in
    performing extensive matrix multiplication operations, an alternative system,
    called a graphical processing unit (GPU), has been used. A GPU differs from a
    CPU in that while a CPU is required for all general processing, a GPU is specialized
    to handle computing and parallel processing [\[8\]](#page-4-1). Although GPUs
    are often seen as more efficient for matrix multiplication, they are still built
    on Von Neumann, leading to the same inefficiencies due to a shared bus and a memory
    bottleneck. Overcoming these shortcomings, Google''s Tensor Processing Unit (TPU)
    seems to be the most efficient. Figure [3](#page-1-1) compares the peak floating
    point operations per second (FLOPS) between each of these processors [\[5\]](#page-4-4).
    Google''s TPU differs from standard CPUs and GPUs because of the overall architecture.
    While CPUs and GPUs use Von Neumann, TPUs use systolic arrays. Unlike traditional
    Von Neumann architectures, systolic arrays can limit bottlenecks by passing data
    through a localized array of processing elements (PE) in parallel. This allows
    for Multiplication Accumulate (MAC) operations to be performed simultaneously
    rather than sequentially, significantly reducing the runtime and energy demands
    of large matrix multiplications.


    ![](_page_1_Figure_9.jpeg)


    <span id="page-1-1"></span>Fig. 3. Peak Flops Comparison [\[5\]](#page-4-4)


    ### III. SYSTOLIC ARRAY DATA FLOWS


    # *A. Weights, Inputs, and Outputs*


    Machine learning (ML) requires the efficient multiplication of two matrices, the
    input and the weight matrices. In DNN calculations, the inputs are a set of numbers
    from the activation values of the previous layer or the primary input values.
    The weights are trainable and data-dependent numbers are used to alter the inputs
    to predict specific outputs.


    ## *B. What are Systolic Arrays?*


    A systolic array is a system made up of a two-dimensional array of units known
    as processing elements (PE). The operations necessary to multiply matrices are
    simply specific combinations of multiplication and addition. A processing element
    has Multiplication and Accumulate (MAC) units for this purpose. The MAC unit multiplies
    two inputs, corresponding to elements from two matrices, and adds the result to
    a running total (partial sum). This operation is repeated over time to accumulate
    results for the output. A processing element also has memory, often noted as local
    registers, to store incoming data and partial sums. The structure of systolic
    arrays allows for two major advantages. First, all processing elements can be
    run simultaneously rather than sequentially. Secondly, the data can be fed in
    an interleaved way resulting in a pipelined mechanism to compute the output. Systolic
    arrays are stronger alternatives to traditional Von Neumann as they limit the
    bottleneck. Systolic arrays do this by not relying on the slow transfer of memory
    to the processing units to compute tasks sequentially since each processing element
    in the array has its local memory that can be propagated through each computation.


    #### *C. Mapping Matrices to Systolic Arrays*


    To compute matrix multiplications on a systolic array, the matrices are mapped
    onto the array''s physical structure using three dimensions: Spatial Columns (S<sup>C</sup>
    ), Spatial Rows (SR), and Temporal (T). S<sup>C</sup> represents the number of
    columns of processing elements in the array, and S<sup>R</sup> represents the
    number of rows; the spatial dimension corresponds to the physical requirements
    of the systolic array, as shown in Figure [4.](#page-2-0) T, however, refers to
    the dimension where data from the matrices propagates through the systolic array
    over time, synchronized by each clock cycle.


    ![](_page_2_Figure_3.jpeg)


    <span id="page-2-0"></span>Fig. 4. Spatial Rows and Columns to Systolic Array


    ## *D. Data Flows*


    Different data flows, such as Weight Stationary (WS), Input Stationary (IS), and
    Output Stationary (OS), can be used in specific scenarios to optimize the speed
    and efficiency of the calculations. Let the weight matrix be W ∈ RM×<sup>N</sup>
    , the input matrix be I ∈ R <sup>N</sup>×<sup>P</sup> , and the output matrix
    be O ∈ RM×<sup>P</sup> as shown in Figure [5.](#page-2-1)


    ![](_page_2_Figure_7.jpeg)


    <span id="page-2-1"></span>Fig. 5. Dimensions of Weight, Input, and Output Matrix


    Weight Stationary: In weight stationary (WS) data flow, weight matrix values are
    pre-filled into the systolic array, while the inputs and partial sums are propagated
    through the systolic array for each clock cycle. The spatial requirements, or
    the minimum size of the systolic array, NP E, needed is the size of the weight
    matrix, M × N, while the temporal dimension is P.


    Input Stationary: Similar to WS data flow, the IS data flow pre-fills the systolic
    array with the input matrix values, N × P. The spatial requirement, NP E, for
    an IS data flow is therefore the size of the input matrix, N × P, while the temporal
    dimension is mapped to M.


    Output Stationary: Finally, the output stationary data flow is where the output
    matrix is stationary while both the weight and input values are streamed into
    the array. In this flow, each PE accumulates the partial sum without propagating
    it until the entire operation is completed. After the multiplication, the output
    matrix is read to the output buffer. The spatial dimensions, NP E, required to
    compute matrix multiplication through an OS data flow is the size of the output
    matrix, M × P, while the temporal dimension is mapped to N.


    ## *E. Characterizing Systolic Arrays*


    To qualify the data flow for a range of matrix sizes, one needs to estimate the
    throughput and energy consumption. The total energy consumption is given by Equation
    [\(1\)](#page-2-2).


    <span id="page-2-2"></span>

    $$E = N\_{PE} \cdot P\_{PE} \cdot N\_C \cdot T\_{clk} \tag{1}$$


    where E is the total energy, NP E is the number of PEs in the array, PP E is the
    power consumption per PE, N<sup>C</sup> is the total number of cycles needed to
    compute the matrix multiplication and Tclk is the clock period which is 1/Fclk,
    where Fclk is the clock frequency to which the PEs are designed. Samajdar et al,
    in [\[9\]](#page-4-3), proposed SCALE-Sim, a cycle simulator to estimate systolic
    array size and throughput for various data flows. The equation for the number
    of cycles for input, output, and weight stationary is given by Equation [\(2\)](#page-2-3).


    <span id="page-2-3"></span>

    $$N\_C = 2 \cdot S\_R + S\_C + T - 2 \tag{2}$$


    where S<sup>R</sup> corresponds to the spatial rows, S<sup>C</sup> corresponds
    to the spatial columns, and T corresponds to the temporal dimension. The total
    number of PEs is calculated as follows.


    <span id="page-2-4"></span>

    $$N\_{PE} = S\_R \cdot S\_C \tag{3}$$


    Based on the type of data flow, the mapping of SR, S<sup>C</sup> , and T to the
    matrix dimension are different, as shown in table [I.](#page-3-2) To determine
    the most efficient data flow, it is necessary to test different matrix sizes under
    each data flow.


    ### IV. EVALUATION


    In this section, the optimal systolic array data flow for different matrix size
    configurations is determined by calculating the total energy consumption. For
    each dimension of the matrices (M × N × P), the extreme {min : max} values chosen
    were


    <span id="page-3-2"></span>TABLE I MAPPING OF MATRIX DIMENSIONS FOR SYSTOLIC ARRAY
    DATA-FLOWS


    | Data flow              | Spatial<br>Rows (SR) | Spatial<br>Columns (SC ) | Temporal<br>(T)
    |

    |------------------------|----------------------|--------------------------|-----------------|

    | Weight Stationary (WS) | M                    | N                        | P               |

    | Input Stationary (IS)  | N                    | P                        | M               |

    | Output Stationary (OS) | M                    | P                        | N               |


    {5 : 500}. Based on these extreme values 8 different matrix configurations can
    be evaluated.


    A cycle simulator was developed based on the SCALE-Sim paper [\[9\]](#page-4-3)
    to calculate the total energy required to multiply different configurations of
    matrix size under each data flow. The findings are plotted in Figure [6.](#page-3-3)
    The total energy is computed using the Equation [\(1\)](#page-2-2). Based on the
    PE designed by He et. al [\[4\]](#page-4-5) for a 700MHz clock frequency, the
    power for a regular TPU-based PE, PP E, for a 32-bit floating MAC on 28nm technology
    is 2.17mW. The number of processing elements required is calculated using Equation
    [\(3\)](#page-2-4), with specific mappings as shown in Table [I.](#page-3-2) Finally,
    to find the number of cycles, the Equation [\(2\)](#page-2-3), using cycle simulator
    algorithm [\[9\]](#page-4-3) was used. The total energy is plotted for each configuration
    of matrix sizes for each of the data flows as shown in figure [6.](#page-3-3)


    ![](_page_3_Figure_4.jpeg)


    <span id="page-3-3"></span>Fig. 6. Data Flow Energy Comparison


    To reduce the energy required to perform matrix multiplication, the systolic array
    size needs to be minimal. The number of rows and columns of the systolic array
    (SR, S<sup>C</sup> ) that define the array size, as in Equation [\(3\)](#page-2-4),
    is mapped to the matrix dimensions (M, N, P) based on the data flow as shown in
    Table [I.](#page-3-2) Hence, total energy consumption is minimal when the smallest
    matrix dimension pair is mapped to the systolic array. The mapping of stationary
    matrix size to the data flow is defined as (M × N) 7→ W S, (N × P) 7→ IS, and
    (M × P) 7→ OS.


    Based on the graph in Figure [6,](#page-3-3) it is clear that when the three dimensions,
    (M, N, P) are equal or similar, all data flows have similar total energy consumption.
    However, keeping the smaller two dimensions of the matrices (M, N, P) as the spatial
    dimensions (SR, S<sup>C</sup> ), will result in the least amount of energy consumed.
    For example, for configuration 5×5×500, (M ×N ×P), dimensions (M, N) are smaller
    than P, making weight stationary the optimal data flow.


    ## V. RELATED WORK


    Batten et al. [\[2\]](#page-3-4) shows a promising way to optimize systolic data
    flows by using PyHDL-Eval, an LLM evaluation framework designed to automate and
    improve hardware design tasks. By using LLMs to generate efficient hardware configurations
    and design code, PyHDL-Eval could be adapted to optimize data flows such as Weight
    Stationary, Input Stationary, and Output Stationary. This approach has the potential
    to streamline the process of configuring systolic arrays for matrix multiplication
    in deep neural networks, enhancing both computational efficiency and energy consumption
    across various matrix dimensions. However, it is important to note that besides
    optimizing systolic array data flow, key design considerations are memory hierarchy,
    data interconnection, and multi-core capabilities.


    Besides systolic array-based hardware accelerators, Jung et al. [\[7\]](#page-4-6)
    pioneered Hammerblade, an open-source RISC-V many-core architecture. Being able
    to scale the number of cores in standard RISC-V processors also significantly
    increases efficiency for DNN calculations. This shows that as important as it
    is to focus on the specific cores of processing units, critical advancements can
    be made by focusing on the System-on-Chip (SoC) design itself.


    ## VI. CONCLUSION


    Understanding the impact of systolic array data flows is necessary to optimize
    deep neural network (DNN) accelerators for matrix multiplication. The WS, IS,
    and OS data flow analysis shows that the choice of data flow significantly affects
    energy consumption based on matrix dimensions. Using the SCALE-Sim-based cycle
    simulator, it is clear that the smallest dimensions should correspond to the spatial
    dimensions to require the least amount of energy. This reinforces the need to
    carefully select data flows depending on matrix configurations to achieve optimal
    performance in DNN accelerators. With the end of Moore''s law [\[10\]](#page-4-7),
    the need for fast processors to train DNNs is more critical now than ever. Thus,
    optimizing current architectures is an important step towards efficient DNN accelerators.
    Future work can extend this analysis by exploring advanced data flows and hardware
    configurations to reduce energy costs further while maintaining high computational
    efficiency.


    ### REFERENCES


    - <span id="page-3-0"></span>[1] I. Arikpo, F. Ogban, and I. Eteng, "Von neumann
    architecture and modern computers," *Global Journal of Mathematical Sciences*,
    vol. 6, no. 2, pp. 97–103, 2007.

    - <span id="page-3-4"></span>[2] C. Batten, N. Pinckney, M. Liu, H. Ren, and B.
    Khailany, "Pyhdl-eval: An llm evaluation framework for hardware design using pythonembedded
    dsls," in *Proceedings of the 2024 ACM/IEEE International Symposium on Machine
    Learning for CAD*, ser. MLCAD ''24. New York, NY, USA: Association for Computing
    Machinery, 2024. [Online]. Available:<https://doi.org/10.1145/3670474.3685948>

    - <span id="page-3-1"></span>[3] M. Capra, B. Bussolino, A. Marchisio, G. Masera,
    M. Martina, and M. Shafique, "Hardware and software optimizations for accelerating
    deep neural networks: Survey of current trends, challenges, and the road ahead,"
    *IEEE Access*, vol. 8, pp. 225 134–225 180, 2020.

    - <span id="page-4-5"></span>[4] X. He, S. Pal, A. Amarnath, S. Feng, D.-H. Park,
    A. Rovinski, H. Ye, Y. Chen, R. Dreslinski, and T. Mudge, "Sparse-tpu: adapting
    systolic arrays for sparse matrices," in *Proceedings of the 34th ACM International
    Conference on Supercomputing*, ser. ICS ''20. New York, NY, USA: Association for
    Computing Machinery, 2020. [Online]. Available:<https://doi.org/10.1145/3392717.3392751>

    - <span id="page-4-4"></span>[5] N. P. Jouppi, D. H. Yoon, G. Kurian, S. Li, N.
    Patil, J. Laudon, C. Young, and D. Patterson, "A domain-specific supercomputer
    for training deep neural networks," *Commun. ACM*, vol. 63, no. 7, p. 67–78, Jun.
    2020. [Online]. Available:<https://doi.org/10.1145/3360307>

    - <span id="page-4-2"></span>[6] N. P. Jouppi, C. Young, N. Patil, D. Patterson,
    G. Agrawal, R. Bajwa, S. Bates, S. Bhatia, N. Boden, A. Borchers, R. Boyle, P.-l.
    Cantin, C. Chao, C. Clark, J. Coriell, M. Daley, M. Dau, J. Dean, B. Gelb, T.
    V. Ghaemmaghami, R. Gottipati, W. Gulland, R. Hagmann, C. R. Ho, D. Hogberg, J.
    Hu, R. Hundt, D. Hurt, J. Ibarz, A. Jaffey, A. Jaworski, A. Kaplan, H. Khaitan,
    D. Killebrew, A. Koch, N. Kumar, S. Lacy, J. Laudon, J. Law, D. Le, C. Leary,
    Z. Liu, K. Lucke, A. Lundin, G. MacKean, A. Maggiore, M. Mahony, K. Miller, R.
    Nagarajan, R. Narayanaswami, R. Ni, K. Nix, T. Norrie, M. Omernick, N. Penukonda,
    A. Phelps, J. Ross, M. Ross, A. Salek, E. Samadiani, C. Severn, G. Sizikov, M.
    Snelham, J. Souter, D. Steinberg, A. Swing, M. Tan, G. Thorson, B. Tian, H. Toma,
    E. Tuttle, V. Vasudevan, R. Walter, W. Wang, E. Wilcox, and D. H. Yoon, "In-datacenter
    performance analysis of a tensor processing unit," in *Proceedings of the 44th
    Annual International Symposium on Computer Architecture*, ser. ISCA ''17. New
    York, NY, USA: Association for Computing Machinery, 2017, p. 1–12. [Online]. Available:<https://doi.org/10.1145/3079856.3080246>

    - <span id="page-4-6"></span>[7] D. C. Jung, M. Ruttenberg, P. Gao, S. Davidson,
    D. Petrisko, K. Li, A. K. Kamath, L. Cheng, S. Xie, P. Pan, Z. Zhao, Z. Yue, B.
    Veluri, S. Muralitharan, A. Sampson, A. Lumsdaine, Z. Zhang, C. Batten, M. Oskin,
    D. Richmond, and M. B. Taylor, "Scalable, programmable and dense: The hammerblade
    open-source risc-v manycore," in *2024 ACM/IEEE 51st Annual International Symposium
    on Computer Architecture (ISCA)*, 2024, pp. 770–784.

    - <span id="page-4-1"></span>[8] S. J. Park, "An analysis of gpu parallel computing,"
    in *2009 DoD High Performance Computing Modernization Program Users Group Conference*,
    2009, pp. 365–369.

    - <span id="page-4-3"></span>[9] A. Samajdar, J. M. Joseph, Y. Zhu, P. Whatmough,
    M. Mattina, and T. Krishna, "A systematic methodology for characterizing scalability
    of dnn accelerators using scale-sim," in *2020 IEEE International Symposium on
    Performance Analysis of Systems and Software (ISPASS)*, 2020, pp. 58–68.

    - <span id="page-4-7"></span>[10] T. N. Theis and H.-S. P. Wong, "The end of moore''s
    law: A new beginning for information technology," *Computing in Science & Engineering*,
    vol. 19, no. 2, pp. 41–50, 2017.

    - <span id="page-4-0"></span>[11] A. Yosifova. (2023) The evolution of chatgpt:
    History and future. [Online]. Available: [https://365datascience.com/trending/the-evolution](https://365datascience.com/trending/the-evolution-of-chatgpt-history-and-future/)[of-chatgpt-history-and-future/](https://365datascience.com/trending/the-evolution-of-chatgpt-history-and-future/)'
- title: "Educating for Hardware Specialization in the Chiplet Era: A Path for the\n\
    \  HPC Community"
  abstract: 'The advent of chiplet technology introduces cutting-edge opportunities
    for

    constructing highly heterogeneous platforms with specialized accelerators.

    However, the HPC community currently lacks expertise in hardware development,
    a

    gap that must be bridged to leverage these advancements. Additionally,

    technologies like chiplet is cutting-edge with limited educational resource

    available. This paper addresses potential hardware specialization direction in

    HPC and how to cultivate these skills among students and staff, emphasizing the

    importance of understanding and developing custom hardware (e.g., rapid

    prototyping and resource estimation). We have been mentoring graduate-level

    students and new staff in hardware designs in a hands-on manner, encouraging

    them to utilize modern open-source hardware tools for their designs, which

    facilitates the sharing of research ideas. Additionally, we provide a summary

    of theses tools as part of our approach to prototyping and mentoring.'
  url: http://arxiv.org/abs/2410.23127v1
  keywords: Hardware specialization, Chiplet, open-source hardware tools, digital
    design, simulation
  document: '# Educating for Hardware Specialization in the Chiplet Era: A Path for
    the HPC Community


    Kazutomo Yoshii *Argonne National Laboratory* Lemont, IL, USA


    Mohamed El-Hadedy *California State Polytechnic University* Pomona, CA, USA


    *Abstract*—The advent of chiplet technology introduces cuttingedge opportunities
    for constructing highly heterogeneous platforms with specialized accelerators.
    However, the HPC community currently lacks expertise in hardware development,
    a gap that must be bridged to leverage these advancements. Additionally, technologies
    like chiplet is cutting-edge with limited educational resource available. This
    paper addresses potential hardware specialization direction in HPC and how to
    cultivate these skills among students and staff, emphasizing the importance of
    understanding and developing custom hardware (e.g., rapid prototyping and resource
    estimation). We have been mentoring graduate-level students and new staff in hardware
    designs in a hands-on manner, encouraging them to utilize modern opensource hardware
    tools for their designs, which facilitates the sharing of research ideas. Additionally,
    we provide a summary of theses tools as part of our approach to prototyping and
    mentoring.


    *Index Terms*—Hardware specialization, Chiplet, open-source hardware tools, digital
    design, simulation


    # I. INTRODUCTION


    Hardware specialization has emerged as a promising approach for sustaining performance
    growth in the post-Moore era, given the increasing visibility of inefficiencies
    inherent in general-purpose architectures. Specialized accelerators (e.g., audio/video
    codecs, encryption engines), which can be found in system-on-chips, have proofread
    significant improvements in power, performance, and chip area over general-purpose
    architectures. Furthermore, emerging AI accelerators (e.g., Cerebras, Groq) are
    noteworthy examples of specialized architecture tailored to their target AI workloads.
    In highperformance computing (HPC), most platforms widely employ heterogeneous
    node configurations incorporating CPUs and GPUs. However, as of today, no notable
    specialized accelerator integration has been found in HPC. While co-design activities
    between HPC laboratories and vendors exist, the primary objective is to design
    scalable platforms that support existing application portfolios. Creating dedicated
    accelerators for specific workloads or applications has remained a secondary objective,
    which is understandable as adding new functionality to the chip can introduce
    more complexity to the entire chip development process, particularly verification.
    Interestingly, industries are adopting chiplet technology [\[1\]](#page-1-0),
    a modular design approach, to improve the yield, reusability, and scalability
    of their products. As chiplet technology matures and becomes more accessible technologically
    and economically, it could offer opportunities for the HPC community to evaluate
    custom, specialized accelerators tailored to our workloads.


    A ''what-if'' question arises: if we could one day construct a new platform by
    combining multiple chiplets (many of which vendors offer common functions like
    memory blocks) without concerning many factors (e.g., license, technology accessibility,
    and economics), can we develop chiplet accelerators tailored to our specific workloads?
    [\[2\]](#page-1-1) To this end, we must cultivate our expertise in hardware design,
    verification, and simulation, as well as our proficiency in system software stacks
    and compilers for custom accelerators. As it is challenging to hire hardware experts
    (e.g., shortage of hardware experts), we must be innovative in training existing
    staff and students whose major is computer science or domain science. While commercial
    EDA tools, IPs, and process design kits remain essential for chip designs with
    cutting-edge transistor nodes today, the emergence of open hardware standards
    like RISC-V [\[3\]](#page-1-2) and open-source hardware tools and designs is becoming
    increasingly pronounced. For training and idea sharing purposes, we harness open-source
    hardware tools. These tools leverage modern software paradigms (e.g., functional
    programming, intermediate representation) and practices (e.g., test automation),
    making them particularly appealing to individuals whose expertise is in software,
    as well as accessibility (no license is required) extensively and possibly good
    community support.


    # II. PROTOTYPING AND MENTORING


    At this point, we focus on mentoring students who actively participate in our
    projects rather than teaching a course. We provide design problems to students
    and work with them in a hands-on manner. Tools can often be showstoppers due to
    limited seat availability (commercial licenses), non-disclosure agreements, cost,
    and other barriers. To address these challenges, we have been evaluating open-source
    hardware tools for prototyping designs and mentoring, and we have successfully
    published several papers on hardware designs using these open-source tools [\[4\]](#page-1-3)–[\[10\]](#page-1-4).


    Here is a brief summary of the open-source hardware tools that we frequently use
    for prototyping and mentoring:


    Chisel Hardware Construction Language: Chisel [\[11\]](#page-1-5), a domain-specific
    language for digital circuits, is implemented by harnessing the extensibility
    of the Scala programming language. Chisel has been used for many real-world tapeout
    chips [\[12\]](#page-1-6) (e.g., RISC-V processors, Google''s Edge TPU) and the
    Chisel community keeps growing. Chisel assumes positive-edge triggered registers
    and synchronous reset by default, resulting in more concise code, which works
    well for newcomers. Chisel has fewer dependencies, which is an important aspect
    of a teaching and prototyping tool. Various documents/tutorials and open-source
    codes written in Chisel are available online. Furthermore, Chisel is compatible
    with the Jupyter notebook [\[13\]](#page-1-7), which aids in the teaching process.
    We have also created a teaching material for internal purposes.


    Cocotb Verification Framework: Cocotb [\[14\]](#page-1-8) is a verification library
    written in Python and converts user testbenches into an underlying simulator tool.
    It supports Icarus, Verilator, Synopsys VCS, and several other simulator tools.
    It has been gaining popularity among hobbyists, academia, and even industries
    [\[15\]](#page-1-9). Python ecosystems such as Numpy benefit test bench development
    and teaching (e.g., most of our input data are stored in a Numpy format). Due
    to Python''s popularity, introducing cocotb to software folks is relatively straightforward
    as long as the audiences have the basic concept of digital circuits (e.g., clock,
    wire, register). Cocotb can be installed using the pip tool, similar to other
    Python tools.


    OpenRoad: OpenRoad [\[16\]](#page-1-10) is an electronic design automation (EDA)
    tool that transforms user''s register-transfer level (RTL) codes, such as Verilog
    codes, into GDS, a database file format for data exchange of integrated circuit
    layout for fabrication. We utilize OpenRoad primarily to obtain various reports
    from the EDA flow, such as die area, cell count, wire length, power, critical
    path, etc., to evaluate the physical aspect of the design instead of generating
    GDS for actual tape-out.


    # III. CONCLUSION


    We present a concise overview of three open-source tools that we have utilized
    for prototyping our own designs while mentoring students. The list of open-source
    hardware tools can be extensive, especially for description languages like Chisel
    [\[17\]](#page-1-11). Other notable open-source hardware tools include CIRCT [\[18\]](#page-1-12),
    FireSIM [\[19\]](#page-1-13), and Chipyard [\[20\]](#page-1-14). Open-source hardware
    ecosystems are gaining momentum and hold potential as effective prototyping and
    training/education tools for Chiplet accelerator designs. While we are still in
    the early stages of the educational aspect, we have gained expertise in mentoring
    using open-source tools and have developed design examples. We will continue working
    on creating a more concrete set of educational materials.


    # ACKNOWLEDGMENTS


    This work is based on work supported by the U.S. Department of Energy, Office
    of Science, under contract DE-AC02- 06CH11357.


    # REFERENCES


    <span id="page-1-0"></span>[1] J. H. Lau and J. H. Lau, "Chiplet heterogeneous
    integration," *Semiconductor Advanced Packaging*, pp. 413–439, 2021.


    - <span id="page-1-1"></span>[2] K. Yoshii, J. Tramm, B. Allen, T. Ueno, K. Sano,
    A. Siegel, and P. Beckman, "Hardware specialization: Estimating monte carlo crosssection
    lookup kernel performance and area," in *Proceedings of the SC''23 Workshops of
    The International Conference on High Performance Computing, Network, Storage,
    and Analysis*, 2023, pp. 1274–1278.

    - <span id="page-1-2"></span>[3] K. Asanovi´c and D. A. Patterson, "Instruction
    sets should be free: The case for RISC-V," *EECS Department, University of California,
    Berkeley, Tech. Rep. UCB/EECS-2014-146*, 2014.

    - <span id="page-1-3"></span>[4] K. Yoshii, R. Sankaran, S. Strempfer, M. Levental,
    M. Hammer, and A. Miceli, "A hardware co-design workflow for scientific instruments
    at the edge," in *Driving Scientific and Engineering Discoveries Through the Integration
    of Experiment, Big Data, and Modeling and Simulation: 21st Smoky Mountains Computational
    Sciences and Engineering, SMC 2021, Virtual Event, October 18-20, 2021, Revised
    Selected Papers*. Springer International Publishing Cham, 2022, pp. 209–226.

    - [5] M. Hammer, K. Yoshii, and A. Miceli, "Strategies for on-chip digital data
    compression for x-ray pixel detectors," *Journal of Instrumentation*, vol. 16,
    no. 01, p. P01025, 2021.

    - [6] S. Strempfer, K. Yoshii, M. Hammer, D. Bycul, and A. Miceli, "Designing
    a streaming data coalescing architecture for scientific detector asics with variable
    data velocity," in *2021 3rd Annual Workshop on Extremescale Experiment-in-the-Loop
    Computing (XLOOP)*. IEEE, 2021, pp. 8–14.

    - [7] M. El-Hadedy, R. Hua, K. Yoshii, W.-M. Hwu, and M. Margala, "Recolfsr: Reconfigurable
    low-power cryptographic processor based on lfsr for trusted iot platforms," in
    *2023 24th International Symposium on Quality Electronic Design (ISQED)*. IEEE,
    2023, pp. 1–7.

    - [8] M. El-Hadedy, R. Hua, K. Yoshii, and W.-M. Hwu, "Optimizing ascon permutation
    in multi-clock domains with chisel: Resource efficiency and critical path reduction,"
    in *2024 IEEE 17th Dallas Circuits and Systems Conference (DCAS)*, 2024, pp. 1–6.

    - [9] K. Yoshii, T. Ueno, K. Sano, A. Miceli, and F. Cappello, "Streaming hardware
    compressor generator framework," in *Proceedings of the SC''23 Workshops of The
    International Conference on High Performance Computing, Network, Storage, and
    Analysis*, 2023, pp. 289–297.

    - <span id="page-1-4"></span>[10] M. El-Hadedy, R. Hua, S. Saqib, K. Yoshii, W.-M.
    Hwu, and M. Margala, "Bltesti: Benchmarking lightweight tinyjambu on embedded
    systems for trusted iot," in *2023 IEEE 36th International System-on-Chip Conference
    (SOCC)*. IEEE, 2023, pp. 1–6.

    - <span id="page-1-5"></span>[11] J. Bachrach, H. Vo, B. Richards, and Y. L. D.
    a. d. . Design, "Chisel: constructing hardware in a Scala embedded language,"
    *DAC Design Automation Conference*, pp. 1212–1221, 2012.

    - <span id="page-1-6"></span>[12] C. Celio, P.-F. Chiu, B. Nikolic, D. A. Patterson,
    and K. Asanovic, "BOOMv2: an open-source out-of-order RISC-V core," in *First
    Workshop on Computer Architecture Research with RISC-V (CARRV)*, 2017.

    - <span id="page-1-7"></span>[13] T. Kluyver, B. Ragan-Kelley, F. P´erez, B. E.
    Granger, M. Bussonnier, J. Frederic, K. Kelley, J. B. Hamrick, J. Grout, S. Corlay
    *et al.*, "Jupyter notebooks-a publishing format for reproducible computational
    workflows." *Elpub*, vol. 2016, pp. 87–90, 2016.

    - <span id="page-1-8"></span>[14] B. J. Rosser, "Cocotb: a python-based digital
    logic verification framework," in *Micro-electronics Section seminar*. CERN, Geneva,
    Switzerland, 2018.

    - <span id="page-1-9"></span>[15] cocotb development team. (2023, 6) cocotb user
    survey 2023. [Online]. Available:<https://www.cocotb.org/2023/06/17/user-survey-2023.html>

    - <span id="page-1-10"></span>[16] A. B. Kahng and T. Spyrou, "The OpenROAD project:
    Unleashing hardware innovation."

    - <span id="page-1-11"></span>[17] L. Truong and P. Hanrahan, "A golden age of
    hardware description languages: applying programming language techniques to improve
    design productivity," in *3rd Summit on Advances in Programming Languages (SNAPL
    2019)*. Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2019.

    - <span id="page-1-12"></span>[18] S. Eldridge, P. Barua, A. Chapyzhenka, A. Izraelevitz,
    J. Koenig, C. Lattner, A. Lenharth, G. Leontiev, F. Schuiki, R. Sunder *et al.*,
    "Mlir as hardware compiler infrastructure," in *Workshop on Open-Source EDA Technology
    (WOSET)*, 2021.

    - <span id="page-1-13"></span>[19] S. Karandikar, H. Mao, D. Kim, D. Biancolin,
    A. Amid, D. Lee, N. Pemberton, E. Amaro, C. Schmidt, A. Chopra *et al.*, "FireSim:
    FPGA-accelerated cycle-exact scale-out system simulation in the public cloud,"
    in *2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture
    (ISCA)*. IEEE, 2018, pp. 29–42.

    - <span id="page-1-14"></span>[20] A. Amid, D. Biancolin, A. Gonzalez, D. Grubb,
    S. Karandikar, H. Liew, A. Magyar, H. Mao, A. Ou, N. Pemberton *et al.*, "Chipyard:
    Integrated design, simulation, and implementation framework for custom SoCs,"
    *IEEE Micro*, vol. 40, no. 4, pp. 10–21, 2020.'
- title: "UpANNS: Enhancing Billion-Scale ANNS Efficiency with Real-World PIM\n  Architecture"
  abstract: 'Approximate Nearest Neighbor Search (ANNS) is a critical component of
    modern

    AI systems, such as recommendation engines and retrieval-augmented large

    language models (RAG-LLMs). However, scaling ANNS to billion-entry datasets

    exposes critical inefficiencies: CPU-based solutions are bottlenecked by memory

    bandwidth limitations, while GPU implementations underutilize hardware

    resources, leading to suboptimal performance and energy consumption. To address

    these challenges, we introduce \emph{UpANNS}, a novel framework leveraging

    Processing-in-Memory (PIM) architecture to accelerate billion-scale ANNS.

    UpANNS integrates four key innovations, including 1) architecture-aware data

    placement to minimize latency through workload balancing, 2) dynamic resource

    management for optimal PIM utilization, 3) co-occurrence optimized encoding to

    reduce redundant computations, and 4) an early-pruning strategy for efficient

    top-k selection. Evaluation on commercial UPMEM hardware demonstrates that

    UpANNS achieves 4.3x higher QPS than CPU-based Faiss, while matching GPU

    performance with 2.3x greater energy efficiency. Its near-linear scalability

    ensures practicality for growing datasets, making it ideal for applications

    like real-time LLM serving and large-scale retrieval systems.'
  url: http://arxiv.org/abs/2410.23805v2
  keywords: ''
  document: "# UpANNS: Enhancing Billion-Scale ANNS Efficiency with Real-World PIM\
    \ Architecture\n\nSitian Chen Hong Kong Baptist University\n\nAmelie Chi Zhou<sup>∗</sup>\
    \ Hong Kong Baptist University\n\nYucheng Shi Hong Kong Baptist University\n\n\
    Yusen Li Nankai University Xin Yao Huawei\n\n# ABSTRACT\n\nApproximate Nearest\
    \ Neighbor Search (ANNS) is a critical component of modern AI systems, such as\
    \ recommendation engines and retrieval-augmented large language models (RAG-LLMs).\
    \ However, scaling ANNS to billion-entry datasets exposes critical inefficiencies:\
    \ CPU-based solutions are bottlenecked by memory bandwidth limitations, while\
    \ GPU implementations underutilize hardware resources, leading to suboptimal performance\
    \ and energy consumption. To address these challenges, we introduce UpANNS, a\
    \ novel framework leveraging Processing-in-Memory (PIM) architecture to accelerate\
    \ billion-scale ANNS. UpANNS integrates four key innovations, including 1) architecture-aware\
    \ data placement to minimize latency through workload balancing, 2) dynamic resource\
    \ management for optimal PIM utilization, 3) co-occurrence optimized encoding\
    \ to reduce redundant computations, and 4) an early-pruning strategy for efficient\
    \ top-k selection. Evaluation on commercial UPMEM hardware demonstrates that UpANNS\
    \ achieves 4.3x higher QPS than CPU-based Faiss, while matching GPU performance\
    \ with 2.3x greater energy efficiency. Its near-linear scalability ensures practicality\
    \ for growing datasets, making it ideal for applications like real-time LLM serving\
    \ and large-scale retrieval systems.\n\n#### ACM Reference Format:\n\nSitian Chen,\
    \ Amelie Chi Zhou, Yucheng Shi, Yusen Li, and Xin Yao. 2025. UpANNS: Enhancing\
    \ Billion-Scale ANNS Efficiency with Real-World PIM Architecture. In The International\
    \ Conference for High Performance Computing, Networking, Storage and Analysis\
    \ (SC '25), November 16–21, 2025, St Louis, MO, USA. ACM, New York, NY, USA, [13](#page-12-0)\
    \ pages. [https://doi.org/10.](https://doi.org/10.1145/3712285.3759777) [1145/3712285.3759777](https://doi.org/10.1145/3712285.3759777)\n\
    \n#### 1 INTRODUCTION\n\nApproximate Nearest Neighbor Search (ANNS) is a foundational\
    \ technology for modern AI systems, enabling applications like realtime recommendations\
    \ [\\[15,](#page-11-0) [41,](#page-11-1) [50\\]](#page-12-1) and retrieval-augmented\
    \ large language models (RAG-LLMs) [\\[32,](#page-11-2) [36,](#page-11-3) [48\\\
    ]](#page-12-2). As datasets grow to billions of entries, which is common in social\
    \ networks [\\[55\\]](#page-12-3) or enterprise-scale LLM serving, efficient ANNS\
    \ becomes critical.\n\nSC '25, November 16–21, 2025, St Louis, MO, USA\n\n© 2025\
    \ Association for Computing Machinery.\n\nACM ISBN 979-8-4007-1466-5/2025/11.\
    \ . . \\$15.00\n\n<https://doi.org/10.1145/3712285.3759777>\n\nANNS algorithms\
    \ can be categorized into four classes: hashbased [\\[6,](#page-11-4) [23,](#page-11-5)\
    \ [59\\]](#page-12-4), tree-based [\\[8,](#page-11-6) [14,](#page-11-7) [58\\\
    ]](#page-12-5), graph-based [\\[37,](#page-11-8) [45,](#page-11-9) [51\\]](#page-12-6)\
    \ and compression-based [\\[31,](#page-11-10) [34,](#page-11-11) [43\\]](#page-11-12).\
    \ While graph-based, tree-based and hash-based methods excel at million-scale\
    \ searches, they encounter scalability challenges for billion-scale datasets due\
    \ to prohibitive memory requirements. For example, HNSW [\\[44\\]](#page-11-13),\
    \ a state-of-the-art graph-based method, requires 60-450 bytes of memory per vertex.\
    \ This results in a memory requirement of up to 450GB for a billionvertex graph,\
    \ making it impractical for real-world deployments. In contrast, compression-based\
    \ methods dramatically reduce storage footprints by encoding data into compact\
    \ representations, enabling scalable and cost-efficient billion-scale searches.\
    \ Thus, this paper focuses mainly on compression-based ANNS algorithms. Specifically,\
    \ we focus on IVFPQ [\\[29\\]](#page-11-14), one of the most popular compressionbased\
    \ algorithms and widely used for accelerating massive industrial systems (e.g.,\
    \ recommendation systems in ByteDance [\\[12\\]](#page-11-15) and long-context\
    \ LLM serving [\\[56\\]](#page-12-7)).\n\nLimitation of Existing Architectures.\
    \ IVFPQ integrates two key techniques, including cluster-based filtering (IVF)\
    \ and product quantization (PQ). IVF partitions data into clusters to narrow search\
    \ scope and PQ compresses data into compact codes, greatly reducing the storage\
    \ requirements while maintaining search accuracy. However, our analysis of IVFPQ's\
    \ four-stage pipeline, as shown in Figure [1,](#page-1-0) reveals critical scalability\
    \ limitations in existing CPU/GPU implementations. When datasets scale from 1M\
    \ to 1B entries, we identify that: 1) IVFPQ's runtime bottleneck shifts with dataset\
    \ scale. For example, when dataset scales from 1M to 1B, the performance bottleneck\
    \ of CPU implementation shifts from the LUT construction stage, which is mainly\
    \ compute-intensive, to the distance calculation stage, which is mainly memory-bound.\
    \ Thus, existing performance optimizations for million-scale ANNS [\\[5,](#page-11-16)\
    \ [31\\]](#page-11-10) may not be suitable to billion-scale. 2) At billion-scale,\
    \ both CPU and GPU implementations face various limitations due to architectural\
    \ mismatch. CPUs become memory bandwidth-limited, struggling to fetch compressed\
    \ vectors from DRAM for distance calculation. On the other hand, although GPUs\
    \ excel at parallel distance calculations, they stall during the low-parallelism\
    \ top-k stage (64% of runtime), causing resource waste.\n\nOpportunities of PIM.\
    \ To bridge this gap, we adopt a new hardware architecture, namely the UPMEM Processing-in-Memory\
    \ (PIM) hardware [\\[2\\]](#page-11-17), which embed computation within memory\
    \ to minimize data movement and enhance efficiency [\\[17\\]](#page-11-18). Unlike\
    \ existing PIM products [\\[9,](#page-11-19) [28,](#page-11-20) [38,](#page-11-21)\
    \ [49,](#page-12-8) [54\\]](#page-12-9), UPMEM PIM is a standard DIMM device that\
    \ features multi-threaded DPU cores with direct access to memory, significantly\
    \ improving performance for dataintensive applications. While recent studies have\
    \ explored the use of PIM/near-memory technology for ANNS, they are either designed\n\
    \n<sup>∗</sup>Corresponding author.\n\nPermission to make digital or hard copies\
    \ of all or part of this work for personal or classroom use is granted without\
    \ fee provided that copies are not made or distributed for profit or commercial\
    \ advantage and that copies bear this notice and the full citation on the first\
    \ page. Copyrights for components of this work owned by others than ACM must be\
    \ honored. Abstracting with credit is permitted. To copy otherwise, or republish,\
    \ to post on servers or to redistribute to lists, requires prior specific permission\
    \ and/or a fee. Request permissions from permissions@acm.org.\n\nSC '25, November\
    \ 16–21, 2025, St Louis, MO, USA Chen et al.\n\n<span id=\"page-1-0\"></span>![](_page_1_Figure_1.jpeg)\n\
    \nFigure 1: Query processing time breakdown of the IVFPQ [\\[29\\]](#page-11-14)\
    \ algorithm, using SIFT dataset [\\[30\\]](#page-11-22) at different scales (1M,\
    \ 100M and 1B). Hardware specifications see Section [5.](#page-6-0)\n\nfor million-scale\
    \ [\\[52\\]](#page-12-10) or rely on simulated architectures [\\[39,](#page-11-23)\
    \ [42\\]](#page-11-24). To the best of our knowledge, this is the first study\
    \ to leverage practical PIM hardware to enhance billion-scale ANNS performance.\n\
    \nChallenges. While seem promising, leveraging UPMEM PIM for billion-scale ANNS\
    \ introduces three challenges: 1 UPMEM's high aggregated bandwidth depends on\
    \ evenly distributed workloads across its PIM chips. However, real-world datasets\
    \ exhibit severe imbalances, skewing compute and memory demands across PIM chips.\
    \ 2 UPMEM's unique hardware architecture, e.g., distributed DPU cores, tiered\
    \ memory, and lack of inter-DPU communication, requires rethinking IVFPQ's pipeline\
    \ to maximize efficiency. 3 Despite reducing data movement, UPMEM's DPUs (350\
    \ MHz RISC cores) lack the raw compute power of CPUs/GPUs. This necessitates the\
    \ design of pruning methods to bypass non-critical computations (e.g., low-impact\
    \ distance calculations) to maintain low latency while preserving ANN query accuracy.\n\
    \nInnovations. We propose UpANNS, the first framework to enable efficient billion-scale\
    \ ANNS on commercial PIM hardware. UpANNS integrates four key innovations to address\
    \ the aforementioned challenges: 1 We introduce architecture-aware data placement\
    \ and query scheduling to dynamically balance workloads across PIM chips to maximize\
    \ memory bandwidth utilization (Section [4.1\\)](#page-3-0). 2 We design an efficient\
    \ thread scheduling approach to fully utilize the multi-threaded computing power\
    \ on PIM, together with efficient memory management to maximize cache reuse on\
    \ PIM to further improve the performance (Section [4.2\\)](#page-4-0). 3 On observing\
    \ that real-world datasets often contain correlated items in data point vectors,\
    \ we redesign the encoding in IVFPQ and prestore frequently accessed item combinations\
    \ to minimize redundant memory accesses (Section [4.3\\)](#page-5-0). 4 We optimize\
    \ the low-parallelism top-k stage with thread-local top-k heaps and early termination\
    \ pruning to accelerate the stage on PIM's limited cores (Section [4.4\\)](#page-6-1).\n\
    \nWe evaluated the performance of UpANNS using seven UPMEM PIM modules and three\
    \ real-world billion-scale datasets. Results demonstrates three key advantages\
    \ over conventional architectures: 1) Performance: UpANNS improves the Query per\
    \ Second (QPS) by 4.3x compared to CPU-based Faiss [\\[18\\]](#page-11-25), and\
    \ is comparable to GPU-based Faiss. 2) Energy efficiency: UpANNS provides 2.3x\
    \ higher QPS/Watt than GPUs, enabling superior throughput under identical power\
    \ budgets. 3) Scalability: UpANNS demonstrates near-linear scaling with dataset\
    \ size, ensuring practical viability for applications like large model serving\
    \ [\\[42\\]](#page-11-24).\n\n<span id=\"page-1-1\"></span>![](_page_1_Figure_8.jpeg)\n\
    \nFigure 2: An example of the IVFPQ algorithm (L2 distance).\n\n#### 2 BACKGROUND\
    \ AND MOTIVATION\n\n#### 2.1 Approximate Nearest Neighbor Search\n\nApproximate\
    \ Nearest Neighbor Search (ANNS) algorithms are commonly employed to identify\
    \ the top-k vectors most similar to a query vector within large datasets. Among\
    \ the various ANNS algorithms, we focus on the Inverted File with Product Quantization\
    \ (IVFPQ) method [\\[29\\]](#page-11-14), which is one of the most popular ANNS\
    \ algorithms for its efficiency in billion-scale searches. As shown in Figure\
    \ [2,](#page-1-1) IVFPQ operates in two phases, namely the offline preprocessing\
    \ phase (top) and the online querying phase (bottom).\n\nOffline phase preprocesses\
    \ dataset to enable rapid searches. First, the Inverted File (IVF) technique partitions\
    \ data points into || clusters (e.g., via K-means), and each point is represented\
    \ as a residual (i.e., the difference between the point and its centroid). Next,\
    \ Product Quantization (PQ) compresses these residuals by splitting each vector\
    \ into subvectors and encoding them into compact codes using a codebook. For example,\
    \ a 128-dimensional vector can be reduced from 512 bytes to 64 bytes using uint8\
    \ encoding with = 16, achieving an 8x compression rate. This compressed representation\
    \ minimizes storage and accelerates distance computations during queries.\n\n\
    Online phase processes incoming queries in four stages. (a) Cluster filtering\
    \ computes distances between the query and all cluster centroids, retaining only\
    \ the closest clusters to limit the search scope. (b) Lookup table (LUT) construction\
    \ precomputes partial distances between the query and codebook entries for each\
    \ subvector, eliminating redundant calculations. (c) During distance calculation,\
    \ the algorithm aggregates these precomputed LUT values to approximate the distance\
    \ between the query and every candidate point in the selected clusters. (d) Top-K\
    \ selection sorts these approximate distances and returns the nearest neighbors.\n\
    \n### 2.2 UPMEM PIM Architecture\n\nProcessing-in-Memory (PIM) represents a promising\
    \ paradigm for addressing the data movement bottleneck encountered in many large-scale\
    \ data analytics applications including ANNS. Recently,\n\n<span id=\"page-2-0\"\
    ></span>![](_page_2_Figure_1.jpeg)\n\nFigure 3: UPMEM PIM architecture.\n\nreal\
    \ PIM hardware has started to enter the market, with UPMEM PIM being the first\
    \ commercially available solution [\\[2,](#page-11-17) [17\\]](#page-11-18). While\
    \ prior work has demonstrated UPMEM's utility in recommendation systems [\\[16\\\
    ]](#page-11-26), graph processing [\\[13,](#page-11-27) [21\\]](#page-11-28),\
    \ machine learning [\\[22,](#page-11-29) [47\\]](#page-12-11) and databases [\\\
    [11,](#page-11-30) [33\\]](#page-11-31), its potential for ANNS remains unexplored.\n\
    \nA UPMEM module (Figure [3\\)](#page-2-0) is a standard DDR4 DIMM housing 16\
    \ PIM chips, each containing 8 DRAM Processing Units (DPUs). Each DPU is a 350\
    \ MHz RISC core supporting 24 hardware threads and a 14-stage pipeline to hide\
    \ memory latency [\\[3\\]](#page-11-32). The memory hierarchy comprises three\
    \ tiers, including 1) MRAM (64MB/DPU), which is a slow bulk storage accessed via\
    \ high-latency DMA transfers; 2) WRAM (64KB/DPU), a fast scratchpad memory with\
    \ singlecycle access used for active computations and 3) IRAM (24KB/DPU) which\
    \ is the instruction memory for thread execution.\n\nThe host CPU orchestrates\
    \ data transfers to MRAM, which occur concurrently only if buffer sizes across\
    \ all DPUs are uniform; otherwise, sequential transfers degrade performance. Critically,\
    \ DPUs lack direct intercommunication and all coordination must route through\
    \ the host, introducing overhead for collaborative tasks like top-k aggregation.\
    \ Despite these constraints, UPMEM's distributed architecture achieves 20 Tb/s\
    \ aggregate bandwidth [\\[13\\]](#page-11-27) by parallelizing access across 2,560\
    \ DPUs (20 DIMMs), significantly surpassing the memory bandwidth typically available\
    \ to CPUs.\n\n#### 2.3 Motivation and Challenges\n\nIn this subsection, we discuss\
    \ the opportunities and challenges associated with PIM-based ANNS acceleration.\n\
    \nOpportunities. Scaling ANNS to billion-entry datasets exposes critical inefficiencies\
    \ in conventional architectures, as highlighted in Figure [1.](#page-1-0) Specifically,\
    \ for a dataset with one billion vectors, the distance calculation stage alone\
    \ requires 250 million random memory accesses per query (with = 32, || = 4096\
    \ and = 32 as in Figure [1\\)](#page-1-0). This overwhelms CPU memory bandwidth\
    \ (e.g., 85 GB/s) and exceeds GPU memory capacity (e.g., 80GB), creating a prohibitive\
    \ memory wall. UPMEM PIM directly addresses this bottleneck through its 7.2 TB/s\
    \ aggregated memory bandwidth [\\[13\\]](#page-11-27), enabled by parallel access\
    \ across thousands of DPUs. Unlike GPUs, UPMEM's standard DIMM form factor supports\
    \ scalable memory expansion, allowing it to manage datasets far beyond GPU memory\
    \ limits. By executing computations in-memory, UPMEM eliminates costly data transfers\
    \ between CPU and memory, making it uniquely suited for memory-intensive ANNS\
    \ workloads like IVFPQ.\n\nChallenges. While UPMEM PIM offers great potential,\
    \ realizing its benefits for billion-scale ANNS requires overcoming three key\
    \ challenges tied to its architecture and workload characteristics:\n\n<span id=\"\
    page-2-1\"></span>![](_page_2_Figure_11.jpeg)\n\nFigure 4: The distribution of\
    \ access frequency, cluster size and workload in each cluster in SPACEV1B [\\\
    [1\\]](#page-11-33).\n\nChallenge 1: Imbalanced data access pattern. IVFPQ's clustercentric\
    \ design interacts poorly with UPMEM's distributed DPUs. Real-world datasets exhibit\
    \ extreme skew: popular clusters receive 500x more queries than others (Figure\
    \ [4a\\)](#page-2-1), and large clusters contain 106x more data points than others\
    \ (Figure [4b\\)](#page-2-1). This imbalance creates \"hot\" DPUs overwhelmed\
    \ by frequent and large cluster accesses while others idle, wasting UPMEM's parallel\
    \ bandwidth. Compounding this, DPUs lack direct communication. Data communications\
    \ between DPUs have to go through slow host CPU, negating PIM's latency benefits.\n\
    \nChallenge 2: Complicated PIM resource management. UPMEM provides parallel structure\
    \ between DPUs. Each DPU equips with 24 hardware threads and 14-stage pipeline,\
    \ with exclusive access to its local memory. How to adapt IVFPQ to fully utilize\
    \ the multithreads and leverage the 14-stage pipeline to maximally reduce the\
    \ pipeline bubble and hide the memory read latency is a challenging problem. In\
    \ addition, the fast WRAM on DPU has a limited capacity of 64KB only. Due to the\
    \ significant bandwidth difference between WRAM and MRAM, we should maximize the\
    \ amount of data stored in WRAM to improve performance. However, UPMEM lacks a\
    \ Memory Management Unit (MMU) to virtualize its physical memory. DPUs use physical\
    \ addresses when accessing memory [\\[26\\]](#page-11-34), which prohibits efficient\
    \ utilization of the limited cache space.\n\nChallenge 3: Limited power of UPMEM\
    \ PIM. While UPMEM reduces data movement, its DPUs lack the computational density\
    \ of CPUs/GPUs. Processing millions of memory accesses per query on 350 MHz cores\
    \ risks prohibitive latency. This necessitates accuracyaware pruning strategies\
    \ to skip non-critical computation without degrading query accuracy.\n\n#### 3\
    \ SYSTEM OVERVIEW\n\nThis paper presents UpANNS, an efficient system that accelerates\
    \ ANN search using UPMEM PIM hardware. UpANNS incorporates four optimizations,\
    \ targeting specific challenges mentioned above.\n\nOpt1: PIM-AwareWorkload Distribution.\
    \ To mitigate skewed cluster access patterns (challenge 1), we dynamically replicate\
    \ and distribute IVF clusters across DPUs during offline preprocessing. High-demand\
    \ clusters are replicated to avoid hotspots, while spatially proximate clusters\
    \ are co-located on the same DPU to minimize costly inter-DPU communication. This\
    \ ensures balanced memory accesses and maximizes the utilization of UPMEM's 7.2\
    \ TB/s aggregated bandwidth.\n\nOpt2: PIM Resource Management. With UPMEM's unique\
    \ hardware architecture (challenge 2), we propose two optimizations to fully utilize\
    \ its limited resources: 1) Thread scheduling: we utilize <span id=\"page-3-1\"\
    ></span>Vector 0\n\n**Encoded points Co-occurrence**\n\n![](_page_3_Figure_1.jpeg)\n\
    \n![](_page_3_Figure_2.jpeg)\n\nFigure 5: Overview of UpANNS. The grey blocks\
    \ in each vector represent high frequency co-occurrences in the encoded vectors.\n\
    \n24 hardware threads per DPU to overlap MRAM transfers with computations via\
    \ a 14-stage pipeline; 2) Memory management: we carefully reuse WRAM buffers across\
    \ IVFPQ stages (e.g., codebook, LUT to encoded vectors) to minimize MRAM accesses\
    \ (Figure [6\\)](#page-5-1).\n\n**High Frequent** \n\nOpt3: Co-occurrence Aware\
    \ Encoding. Targeting challenge 3, we identify frequent element combinations in\
    \ encoded vectors and precompute partial sums during offline processing. This\
    \ can greatly reduce online memory accesses without compromising accuracy.\n\n\
    Opt4: Top-K Pruning. We further address challenge 3 by eliminating redundant computations.\
    \ For the bottlenecked top-k selection stage, we combine thread-local heaps with\
    \ early termination, which enables skipping 68% of redundant comparisons and accelerates\
    \ the final stage by 3.1x.\n\nFigure [5](#page-3-1) shows the optimized workflow\
    \ of IVFPQ. During offline phase, clusters are encoded (Opt3), replicated and\
    \ distributed (Opt1). During online phase, queries are scheduled to balanced DPUs\
    \ (Opt1), and processed using WRAM-optimized pipelines (Opt2) and pruned top-k\
    \ selection (Opt4).\n\n# 4 TECHNICAL DETAILS\n\n### <span id=\"page-3-0\"></span>4.1\
    \ PIM-Aware Workload Distribution\n\nTo address the severe workload imbalance\
    \ caused by skewed cluster access patterns, UpANNS integrates two synergistic\
    \ strategies: offline data placement and online query scheduling. The data placement\
    \ task selectively replicates and distributes clusters across DPUs based on their\
    \ access frequency patterns and sizes, enhancing system flexibility for diverse\
    \ query workloads. Meanwhile, the query scheduling task efficiently maps filtered\
    \ clusters from batch queries to appropriate DPUs, dynamically balancing workloads\
    \ at runtime.\n\n<span id=\"page-3-3\"></span>4.1.1 Data Placement. The offline\
    \ data placement strategy optimizes cluster distribution across DPUs based on\
    \ three three key insights: (1) cluster-level locality preservation reduces partial\
    \ result transfers, (2) workload skew necessitates adaptive replication, and (3)\
    \ spatial proximity between frequently co-accessed clusters\n\n<span id=\"page-3-2\"\
    ></span>\n\n| Algorithm 1 Data Placement for Cluster \U0001D456              \
    \ |                                                                          \
    \          |  |  |  |  |\n|--------------------------------------------------------|------------------------------------------------------------------------------------|--|--|--|--|\n\
    | Input: \U0001D45B\U0001D451\U0001D45D\U0001D462: the number of available DPUs;\
    \ \U0001D460\U0001D456          | : #vectors in cluster \U0001D456;          \
    \                                                 |  |  |  |  |\n| \U0001D453\U0001D456\
    \                                                     | : the access frequency\
    \ of cluster \U0001D456;\U0001D44A : the average workload per DPU;           \
    \   |  |  |  |  |\n|                                                        |\
    \ \U0001D440\U0001D434\U0001D44B_\U0001D437\U0001D443\U0001D448_\U0001D446\U0001D43C\
    \U0001D44D\U0001D438: the maximum number of vectors each DPU can have;       \
    \              |  |  |  |  |\n| Output: \U0001D451\U0001D45D\U0001D462_\U0001D456\
    \U0001D451;                                        |                         \
    \                                                           |  |  |  |  |\n| 1:\
    \ \U0001D451_\U0001D456\U0001D451 ← \U0001D45B\U0001D451\U0001D45D\U0001D462,\
    \ \U0001D461ℎ\U0001D459\U0001D451 ← 1.0, \U0001D450\U0001D45C\U0001D462\U0001D45B\
    \U0001D461 ← 0 and \U0001D45F\U0001D44E\U0001D461\U0001D452 ← 0.02; |        \
    \                                                                            |\
    \  |  |  |  |\n| 2: \U0001D45B\U0001D450\U0001D45D\U0001D466 = ⌈\U0001D460\U0001D456\
    \ ∗ \U0001D453\U0001D456 /\U0001D44A ⌉;                               | ⊲ \U0001D45B\
    \U0001D450\U0001D45D\U0001D466 denotes #DPUs that cluster \U0001D456 is distributed\
    \ onto;                           |  |  |  |  |\n| 3: \U0001D464\U0001D456 = \U0001D460\
    \U0001D456 ∗ \U0001D453\U0001D456 /\U0001D45B\U0001D450\U0001D45D\U0001D466; \
    \                                | ⊲ \U0001D464\U0001D456 denotes the per DPU\
    \ workload of cluster \U0001D456;                                    |  |  | \
    \ |  |\n| 4: while \U0001D45B\U0001D450\U0001D45D\U0001D466 > 0 do           \
    \                        |                                                   \
    \                                 |  |  |  |  |\n| 5:                        \
    \                             | if \U0001D44A [\U0001D451_\U0001D456\U0001D451\
    \ ] + \U0001D464\U0001D456 ≤ \U0001D44A ∗ \U0001D461ℎ\U0001D459\U0001D451 and\
    \ \U0001D446 [\U0001D451_\U0001D456\U0001D451 ] + \U0001D460\U0001D456 ≤ \U0001D440\
    \U0001D434\U0001D44B_\U0001D437\U0001D443\U0001D448_\U0001D446\U0001D43C\U0001D44D\
    \U0001D438 then                |  |  |  |  |\n| \U0001D450\U0001D45C\U0001D462\
    \U0001D45B\U0001D461 ← 0;<br>6:                                       |      \
    \                                                                            \
    \  |  |  |  |  |\n| \U0001D451\U0001D45D\U0001D462_\U0001D456\U0001D451.push(\U0001D451\
    _\U0001D456\U0001D451), \U0001D45B\U0001D450\U0001D45D\U0001D466 ← \U0001D45B\U0001D450\
    \U0001D45D\U0001D466 − 1;<br>7:              |                               \
    \                                                     |  |  |  |  |\n| 8:<br>else\
    \                                             |                              \
    \                                                      |  |  |  |  |\n| \U0001D450\
    \U0001D45C\U0001D462\U0001D45B\U0001D461 ← \U0001D450\U0001D45C\U0001D462\U0001D45B\
    \U0001D461 + 1;<br>9:                               |                        \
    \                                                            |  |  |  |  |\n|\
    \ \U0001D451_\U0001D456\U0001D451 ← (\U0001D451_\U0001D456\U0001D451 + 1) mod\
    \ \U0001D45B\U0001D451\U0001D45D\U0001D462;<br>10:                     |     \
    \                                                                            \
    \   |  |  |  |  |\n| if \U0001D450\U0001D45C\U0001D462\U0001D45B\U0001D461 ==\
    \ \U0001D45B\U0001D451\U0001D45D\U0001D462 then<br>11:                       \
    \    | ⊲ no suitable DPU found;                                              \
    \             |  |  |  |  |\n| 12:                                           \
    \         | \U0001D461ℎ\U0001D459\U0001D451 ← \U0001D461ℎ\U0001D459\U0001D451\
    \ + \U0001D45F\U0001D44E\U0001D461\U0001D452; ⊲ enlarge threshold to loosen the\
    \ workload balance constraint; |  |  |  |  |\n\nenables communication optimizations.\
    \ Our data placement strategy carefully considered these three insights to achieve\
    \ optimal performance and the key steps are shown in Algorithm [1.](#page-3-2)\n\
    \nFirstly, entire clusters are placed on a single DPU to avoid partial result\
    \ transfers on the bottlenecked communication between DPUs and the host CPU. For\
    \ a cluster , its workload is mainly contributed by the memory-intensive distance\
    \ calculation stage, which performs excessive memory search in LUTs to calculate\
    \ distances between a query and all search points in the cluster. Thus, we can\
    \ estimate the workload as = ∗ , where is the cluster size and is its historical\
    \ access frequency. Given DPUs and a batch of queries, the balanced memory accesses\
    \ per DPU that we need to achieve is = 1 Í =1 , where is the number of clusters.\n\
    \nSecondly, cluster access frequency is highly skew, as shown in Figure [4.](#page-2-1)\
    \ Thus, some clusters may have workloads significantly exceeding . We create copies\
    \ for these high-demand clusters (Line 2) and distribute them across multiple\
    \ DPUs. During placement, replicas are assigned to DPUs with the least residual\
    \ capacity, iteratively increasing the acceptable workload threshold ℎ until all\
    \ replicas are placed (Lines 5–12). At the end of this algorithm, each cluster\
    \ is placed on a list of DPUs.\n\n<span id=\"page-4-1\"></span>\n\n| Algorithm\
    \ 2 Query Scheduling for a batch \U0001D444                                  \
    \                 |  |  |  |  |  |  |\n|----------------------------------------------------------------------------------------------|--|--|--|--|--|--|\n\
    | Input: \U0001D460\U0001D456<br>: #vectors in cluster \U0001D456; \U0001D436\
    : centroid vectors;                                   |  |  |  |  |  |  |\n| \U0001D440\
    : the cluster to DPU mapping generated by Algorithm 1;                       \
    \               |  |  |  |  |  |  |\n| Output: \U0001D434\U0001D460\U0001D460\U0001D456\
    \U0001D454\U0001D45B\U0001D452\U0001D451: the query scheduling result;       \
    \                                        |  |  |  |  |  |  |\n| 1: \U0001D44A\
    \ [\U0001D456] ← 0 for \U0001D456 ∈ {0, , \U0001D45B\U0001D451\U0001D45D\U0001D462\
    \ − 1};<br>⊲\U0001D44A [\U0001D456] denotes the workload on DPU \U0001D456   \
    \             |  |  |  |  |  |  |\n| 2: for \U0001D456 ∈ [0,  \U0001D444   − 1]\
    \ do                                                                  |  |  |\
    \  |  |  |  |\n| \U0001D439 [\U0001D456] ← cluster_filtering(\U0001D444[\U0001D456\
    ] );<br>⊲ IDs of selected clusters for query \U0001D456<br>3:            |  |\
    \  |  |  |  |  |\n| ⊲ clusters with one replica<br>for each cluster \U0001D457\
    \ in \U0001D439 [\U0001D456] and \U0001D440 [\U0001D457].\U0001D460\U0001D456\U0001D467\
    \U0001D452 = 1 do<br>4:       |  |  |  |  |  |  |\n| \U0001D434\U0001D460\U0001D460\
    \U0001D456\U0001D454\U0001D45B\U0001D452\U0001D451 [\U0001D440 [\U0001D457] [0]\
    \ ].push( ⟨\U0001D456, \U0001D457⟩ ); ⊲ Schedule cluster \U0001D457 of query \U0001D456\
    \ to DPU \U0001D440 [\U0001D457] [0]<br>5: |  |  |  |  |  |  |\n| \U0001D44A [\U0001D440\
    \ [\U0001D457] [0] ] ← \U0001D44A [\U0001D440 [\U0001D457] [0] ] + \U0001D460\
    \ \U0001D457<br>6:<br>;                                             |  |  |  |\
    \  |  |  |\n| Remove cluster \U0001D457 from \U0001D439 [\U0001D456];<br>7:  \
    \                                                         |  |  |  |  |  |  |\n\
    | 8: Sort all clusters in \U0001D439 in descending order according to their sizes;\
    \                      |  |  |  |  |  |  |\n| 9: for each cluster \U0001D450 in\
    \ \U0001D439 do                                                              \
    \  |  |  |  |  |  |  |\n| \U0001D451\U0001D45D\U0001D462\U0001D460 ← \U0001D440\
    \ [\U0001D450 ];<br>⊲ the list of DPUs containing replicas of cluster \U0001D450\
    <br>10:                 |  |  |  |  |  |  |\n| for each query \U0001D444[\U0001D456\
    ] and \U0001D439 [\U0001D456] contains \U0001D450 do<br>11:                  \
    \                         |  |  |  |  |  |  |\n| find \U0001D457 with min(\U0001D44A\
    \ [\U0001D451\U0001D45D\U0001D462\U0001D460 [\U0001D457] ] + \U0001D460\U0001D450\
    \ );<br>⊲ find the least loaded DPU<br>12:                  |  |  |  |  |  | \
    \ |\n| \U0001D434\U0001D460\U0001D460\U0001D456\U0001D454\U0001D45B\U0001D452\U0001D451\
    \ [\U0001D451\U0001D45D\U0001D462\U0001D460 [\U0001D457] ].push( ⟨\U0001D456,\
    \ \U0001D450 ⟩ );<br>13:                                                 |  |\
    \  |  |  |  |  |\n| \U0001D44A [\U0001D451\U0001D45D\U0001D462\U0001D460 [\U0001D457\
    ] ] ← \U0001D44A [\U0001D451\U0001D45D\U0001D462\U0001D460 [\U0001D457] ] + \U0001D460\
    \U0001D450 ;<br>14:                                                  |  |  | \
    \ |  |  |  |\n\nThirdly, clusters selected for a single query are typically in\
    \ close proximity (e.g., neighboring centroids). We co-locate such clusters on\
    \ the same DPU, which allows local aggregation of partial top-k results during\
    \ multi-cluster queries, dramatically cutting inter-DPU communication overhead.\n\
    \n4.1.2 Query Scheduling. At runtime, the host CPU assigns queries to DPUs using\
    \ a greedy scheduling algorithm (Algorithm [2\\)](#page-4-1) to achieve workload\
    \ balance dynamically. Each query selects clusters by comparing the centroid vectors\
    \ with the query vector, thus is relatively light-weighted compared to the other\
    \ online stages. We execute cluster filtering and query scheduling on the host\
    \ CPU and send the scheduling results to PIM for query searching.\n\nSince some\
    \ popular clusters are replicated and distributed across DPUs, the query scheduling\
    \ algorithm dynamically selects the appropriate cluster replicas to ensure workload\
    \ balance. Specifically, we first schedule the clusters with only one replica\
    \ (Line 4-5). Next, we update the workload on each DPU (Line 6). Clusters with\
    \ multiple replicas are ordered by size (descending) and assigned to the least-loaded\
    \ DPU (Line 8-14). This ensures larger clusters do not overload individual DPUs.\
    \ Algorithm [2](#page-4-1) runs at runtime with a complexity of (|| ×), where\
    \ is the set of queries. Since both || and are significantly smaller than billion-scale\
    \ datasets, the overhead of this algorithm is negligible.\n\nDue to hardware limitations\
    \ preventing direct DPU-DPU communication, PIM faces challenges in handling changing\
    \ query patterns. UpANNS targets applications such as retrieval-augmented LMs\
    \ and recommendation systems, where query patterns typically change regularly\
    \ (e.g., every few days [\\[31\\]](#page-11-10)) and incrementally. To handle\
    \ such changes, UpANNS implements an adaptive approach: (1) adjusting the number\
    \ of cluster copies (Section [4.1.1\\)](#page-3-3) to accommodate minor query\
    \ pattern changes in the short term, and (2) performing full data relocation to\
    \ address major pattern shifts over longer time periods. Furthermore, our experiments\
    \ demonstrate that the PIM hardware achieves significantly higher query throughput\
    \ than CPU implementations even with random data distribution, highlighting the\
    \ substantial advantages of PIM architectures.\n\n#### <span id=\"page-4-0\"></span>4.2\
    \ PIM Resource Management\n\nTo maximize UPMEM's limited per-DPU resources (small\
    \ WRAM cache and 350 MHz DPU cores), UpANNS employs two synergistic optimizations:\
    \ thread scheduling and memory management. These ensure full utilization of PIM's\
    \ parallel compute and memory bandwidth while mitigating latency bottlenecks.\n\
    \n4.2.1 Thread Scheduling. The UPMEM architecture provides a parallel structure\
    \ between DPUs. Further, each DPU's 24 hardware threads and 14-stage pipeline\
    \ enable fine-grained parallelism, but WRAM constraints (64KB) restrict concurrent\
    \ cluster processing.\n\nTo efficiently utilize the multi-threads, we can explore\
    \ hardware parallelism at various levels: inter-queries, between different clusters\
    \ of a single query, or intra-cluster. Note that multiple threads operating on\
    \ the same DPU share the limited 64KB WRAM. Since accessing data from WRAM is\
    \ much faster than from MRAM, we try to fit all data needed during thread execution\
    \ in WRAM, including the codebook, LUT and encoded data in selected clusters.\
    \ The codebook size can be estimated as × 256, where represents the dimension\
    \ of data points, amounting to 32KB for the SIFT dataset. The LUT size can be\
    \ estimated as × 256 × (16), which is 8KB when the dimension of encoded points\
    \ is 16. If we pursue parallelism at the query or cluster level, the WRAM space\
    \ will be over-utilized since the combined size of the LUTs for more than four\
    \ clusters will easily exceed 64KB. Given this hardware constraint, we implement\
    \ intra-cluster parallelism, where threads collaborate on a single cluster at\
    \ a time.\n\nOn each DPU, multiple threads execute concurrently to accelerate\
    \ the processing of a single cluster. Figure [6](#page-5-1) illustrates our multi-threading\
    \ strategy. Specifically, in the LUT construction stage, threads concurrently\
    \ fetch codebook segments from MRAM into WRAM, constructing partial lookup tables\
    \ (LUTs). For = 16 subvectors, this reduces LUT build time by 4.8x compared to\
    \ singlethreaded execution. After the LUT is constructed, the partial sum of high-frequency\
    \ co-occurrence combinations are computed using the LUT (details introduced in\
    \ Section [4.3\\)](#page-5-0). The partial sums and the LUT will be combined to\
    \ construct the full LUT. Once the full LUT is constructed, multiple threads concurrently\
    \ read the encoded points from MRAM to WRAM to calculate the partial distances\
    \ between the query and search points. Pipelining is enabled to overlap MRAM reads\
    \ and distance calculations to hide the high latency. Each thread maintains a\
    \ thread-local priority queue (PQ), implemented as a max heap with a size of ,\
    \ to store the local top-k partial distances. After a thread finishes the partial\
    \ top-k insertion, it moves on to process the next cluster.\n\nFour barriers are\
    \ introduced to synchronize the threads and guarantee the correctness of processing.\
    \ Barrier 0 prevents premature LUT updates while some threads are still calculating\
    \ DIST based on LUT. Barrier 1 ensures the LUT is fully constructed before combination\
    \ sums are created. Barrier 2 secures the updated LUT and combination sums, preventing\
    \ erroneous reads. Barrier 3 confirms that all threads have completed the distance\
    \ calculations and result insertions for the current query, allowing for aggregating\
    \ the final top-k from the thread-local priority queues.\n\nThe number of threads\
    \ (#threads) is an important parameter in the implementation. A DPU can support\
    \ up to 24 threads. However, since the size of the MRAM-WRAM transfer must be\
    \ a multiple of 8, at least equal to 8 and not greater than 2048, each thread\
    \ requires up to 2KB WRAM space to store the search points read from MRAM. Next,\
    \ we discuss how to efficiently reuse the limited WRAM capacity to allow more\
    \ threads for better performance.\n\n<span id=\"page-5-1\"></span>![](_page_5_Figure_1.jpeg)\n\
    \nFigure 6: Parallel processing of each cluster inside a single DPU. Red texts\
    \ represent that the WRAM space is reused.\n\n![](_page_5_Figure_3.jpeg)\n\n4.2.2\
    \ Memory Management. Due to the significant bandwidth difference between WRAM\
    \ and MRAM, we aim to maximize the data stored in WRAM. The UPMEM DPU lacks a\
    \ Memory Management Unit (MMU) to virtualize its physical memory, and the DPU\
    \ is constrained to the limited physical memory capacity. Therefore, we propose\
    \ a WRAM reuse strategy specifically tailored to the query processing stages on\
    \ PIM. This strategy enables threads to operate in a larger address space than\
    \ the physical WRAM space available.\n\nFigure [6](#page-5-1) illustrates the\
    \ reuse strategy using the SIFT dataset as an example. During the LUT construction\
    \ stage, we use multiple threads to read the codebooks (32KB) and compute the\
    \ corresponding entries in the LUT (8KB). Once the complete LUT is obtained, we\
    \ use the same capacity to construct the sum of the combinations (8KB). Since\
    \ the codebooks are not used in the following stages, they can be safely overwritten\
    \ to conserve space. As a result, the total space required for the codebooks and\
    \ the full LUT is only 48KB. Once the full LUT is obtained, we proceed to the\
    \ costly distance calculation stage, which needs to fetch encoded point from MRAM\
    \ to WRAM. To optimize this process, we reuse the WRAM space occupied by codebooks\
    \ to allow more threads to load encoded data points concurrently. In the example\
    \ shown in Figure [6,](#page-5-1) we utilize 16 threads consuming 32KB of memory.\n\
    \nThe read latency of MRAM does not increase linearly with size, as shown in Figure\
    \ [7.](#page-5-1) The latency increases slowly as data size grows from 8B to 256B\
    \ and increases almost linearly beyond 256B. This suggests that smaller MRAM read\
    \ sizes (under 256B) yield greater benefits. Larger MRAM reads can consume significant\
    \ WRAM space with minimal returns. We explore an optimal MRAM read size (buffer\
    \ size) to enhance overall efficiency.\n\n#### <span id=\"page-5-0\"></span>4.3\
    \ Co-occurrence Aware Encoding\n\nThe limited computational and memory resources\
    \ of individual DPUs necessitate aggressive workload reduction without sacrificing\
    \ accuracy. Existing works have proposed various workload pruning techniques using\
    \ distance bounds [\\[7,](#page-11-35) [20\\]](#page-11-36) and machine learning\
    \ [\\[49\\]](#page-12-8). These methods are complementary to UpANNS.\n\nEncoded\
    \ points are indices pointing to codebooks, resulting in a limited value range\
    \ (typically within [0, 255]). This property allows frequent co-occurring element\
    \ combinations. For instance, in the SIFT1B dataset, the triplet (1, 15, 26) appears\
    \ in 5.7% of vectors. During distance calculation stage (Figure [2\\(](#page-1-1)c)),\
    \ each element in an\n\n<span id=\"page-5-2\"></span>![](_page_5_Figure_10.jpeg)\n\
    \nFigure 8: An example of co-occurrence aware encoding.\n\nencoded vector triggers\
    \ a LUT lookup, summing to compute the final distance. By caching the partial\
    \ sums of the high-frequency combinations, we can reduce redundant memory accesses\
    \ and computations during distance calculation at runtime, hence improving the\
    \ querying performance.\n\nLeveraging the above observations, we propose a co-occurrence\
    \ aware data encoding strategy. Specifically, we first identify highfrequency\
    \ co-occurring element combinations in encoded points using an Element Co-occurrence\
    \ Graph (ECG). The ECG is constructed such that nodes represent elements and edges\
    \ represent co-occurrence relationships, with edge weights corresponding to co-occurrence\
    \ frequencies. For each cluster, we select the top most frequent combinations\
    \ of length 3. Longer combinations can be selected if a larger cache size (e.g.,\
    \ WRAM capacity) is available. The parameter is determined by the WRAM size; by\
    \ default, we set =256. Our goal is to cache the partial sums of these combinations\
    \ to avoid redundant computations during online searches. However, since distances\
    \ are unknown during the offline phase, partial sums cannot be precomputed. To\
    \ address this, we reserve buffer space in WRAM to store partial sums generated\
    \ after the LUT construction stage. We preallocate this buffer space and assign\
    \ memory addresses to each partial sum. These addresses are then\n\nused to re-encode\
    \ the IVFPQ-encoded points. Figure [8](#page-5-2) illustrates this process with\
    \ a detailed example.\n\nFirst, as shown on the top of the figure, we identify\
    \ three cooccurrence element sets, namely (1, 15, 26), (79, 25, 77) and (2, 14,\
    \ 31). Note that, we must also take into account the positions where these occurrences\
    \ happen. For example, when utilizing the cached partial sum of the combination\
    \ (1, 15, 26), it is crucial that these elements appear in the columns (0, 1,\
    \ 2) respectively to make the sum meaningful. Second, we calculate the cache address\
    \ for the partial sum of all possible combinations in the co-occurrence sets.\
    \ Third, we use the generated cache address to re-encode an encoded vector, which\
    \ has 16 dimensions and each dimension is a 8 value in the 0-255 range. This vector\
    \ contains the highfrequency combinations (1, 15, 26), (79, 25, 77) and (14, 31)\
    \ on the correct positions. According to the pre-computed cache address, we can\
    \ find the partial sum of the three combinations at 0x00111 (7), 0x01111 (15)\
    \ and 0x10110 (22) respectively. Thus, in the new encoded vector, we use the address\
    \ of the partial sum to replace the original elements in the vector. Specifically,\
    \ the new encoded vector contains three parts. The first digit represents the\
    \ total length of the vector, followed by the original codes which are not identified\
    \ as high-frequency co-occurred elements and the combination codes which record\
    \ the cache address of the partial sum.\n\nDuring online phase, we calculate the\
    \ partial sums using constructed LUT and store the sums in WRAM according to prearranged\
    \ layout. For example, for combination (1, 15, 26) on position (0, 1, 2), the\
    \ partial sum is calculated as [1+0×256] + [15+ 1 × 256] + [26 + 2 × 256], and\
    \ stored at address 0x00111. By reusing the cached partial sum, we can greatly\
    \ reduce the computation and memory accesses during distance calculation stage.\n\
    \nNote that, during partial sum calculation, we need multiplication operations\
    \ to access the LUT (e.g., 15 + 1 × 256). However, existing work [\\[25\\]](#page-11-37)\
    \ shows that multiplication operations on PIM are less efficient. To mitigate\
    \ this issue, as shown in Figure [8,](#page-5-2) we further modify the new encoded\
    \ vector by converting the original code and combination code into direct addresses.\
    \ For the original code, we calculate the direct address of the LUT by 256× +\\\
    _. For the combination code, we directly use the cache address with an offset\
    \ of the LUT size (256 × 16). The length of the new encoded vector is stored in\
    \ the second digit, which is 11 in this example. The reason for storing in the\
    \ second digit is to ensure that the length of the new encoded vector is no more\
    \ than the original length. If the new vector doesn't have a high frequency combination,\
    \ the length doesn't need to be stored and the second digit is larger than 256\
    \ × 1. Otherwise, the length is stored in the second digit and the second digit\
    \ is less than 256 × 1. We could identify the second digit to determine the length\
    \ of the encoded vector.\n\nIn the example, our co-occurrence aware encoding reduces\
    \ the length of the encoded vector from 16 to 12, achieving a 25% reduction in\
    \ length. Our empirical studies indicate that, a higher length reduction rate\
    \ corresponds to a greater decrease in distance calculation time, as demonstrated\
    \ in Figure [14.](#page-8-0)\n\n#### <span id=\"page-6-1\"></span>4.4 Top-K Pruning\n\
    \nThe above optimizations try to alleviate the memory bottleneck in the LUT construction\
    \ and distance calculation stages. In this\n\n<span id=\"page-6-2\"></span>![](_page_6_Figure_9.jpeg)\n\
    \nFigure 9: Top-K pruning in DPU. Grey nodes are pruned.\n\nsubsection, we look\
    \ at the performance issue in the top-k selection stage to further enhance UpANNS\
    \ efficiency.\n\nRecall from Figure [6,](#page-5-1) each thread maintains a thread-local\
    \ priority queue for local top-k. We get the total top-k on the current DPU by\
    \ aggregating the local top-k results from different threads. Directly transferring\
    \ all local top-k to the CPU would lead to too much communication between the\
    \ CPU and DPUs. Instead, we propose to efficiently insert the values from the\
    \ local top-k queues into the total top-k queue on the current DPU, with pruning\
    \ enabled. Figure [9](#page-6-2) shows a detailed example.\n\nConsidered four\
    \ threads, each maintaining a priority queue of six elements. A max heap is employed\
    \ to construct the priority queues. Once all four threads have completed their\
    \ maintenance of the max heaps (Barrier 3), they can use semaphores to concurrently\
    \ insert values from their thread-local heaps into the total top-k. To minimize\
    \ unnecessary insertion, we propose to convert the thread-local max heaps into\
    \ min heaps. We use the sem\\_take() and sem\\_give() semaphore functions to manage\
    \ concurrent insertions of the top element from each min heap into the total result\
    \ max heap. For any thread-local min heap, if the top value is greater than the\
    \ maximum value in the total max heap, it indicates that the remaining values\
    \ in the thread-local heap cannot contribute to the overall top-k and can therefore\
    \ be pruned.\n\n#### <span id=\"page-6-0\"></span>5 EVALUATION\n\n#### 5.1 Experimental\
    \ Setup\n\nCompared Baselines. We compare UpANNS with three baselines: PIM-naive,\
    \ Faiss-CPU and Faiss-GPU. PIM-naive is the naive implementation of IVFPQ on PIM\
    \ with our PIM resource management strategy. Faiss-CPU and Faiss-GPU adopt the\
    \ CPU- and GPU-based implementations of IVFPQ from Faiss library [\\[18\\]](#page-11-25),\
    \ the most popular ANNS library developed by Meta.\n\nWhile there are other advanced\
    \ works to accelerate ANNS methods, their methodologies or hardware compatibility\
    \ differ fundamentally from UpANNS. For example, FANNS [\\[31\\]](#page-11-10)\
    \ accelerates IVFPQ using FPGAs but faces device memory constraints, limiting\
    \ scalability to billion-scale datasets. Some studies [\\[39\\]](#page-11-23)\
    \ optimizes IVFPQ via simulator-based architectures, thus are incomparable to\
    \ UpANNS, which targets real-world hardware. To the best of our knowledge, no\
    \ accessible PIM-based ANNS implementation\n\n<span id=\"page-7-0\"></span>\n\n\
    | Harware  | Specifications                                | Approx. Price | Memory\
    \ capacity | Peak Power | Bandwidth  |  |  |\n|----------|-----------------------------------------------|---------------|-----------------|------------|------------|--|--|\n\
    | CPU [27] | 2xIntel Xeon Silver 4110@2.10GHz, 4xDDR4 DRAM | 1,400USD      | 128\
    \ GB          | 190W       | 85.3 GB/s  |  |  |\n| GPU [46] | Nvidia A100 PCI-e\
    \ 80GB                        | 20,000USD     | 80 GB           | 300W       |\
    \ 1935 GB/s  |  |  |\n| PIM [19] | 7xUPMEM PIM (896 DPUs)                    \
    \    | 2,800USD      | 56 GB           | 162W       | 612.5 GB/s |  |  |\n|  \
    \        |                                               |               |   \
    \              |            |            |  |  |\n\nTable 1: Specifics of evaluated\
    \ hardware architectures.\n\nexists for direct comparison. For example, Proxima\
    \ [\\[52\\]](#page-12-10) estimated its performance using an in-house simulator.\
    \ To bridge this gap, we implemented a rigorous naive PIM baseline (PIM-naive)\
    \ that serves as a foundation for UpANNS. Notably, the above works [\\[31,](#page-11-10)\
    \ [39,](#page-11-23) [52\\]](#page-12-10) all use Faiss as the state-of-the-art\
    \ method, making it a reasonable choice for comparison.\n\nHardware Setup. Table\
    \ [1](#page-7-0) shows the hardware specifications of the four compared solutions.\
    \ Except for the components noted, the remaining hardware is the same across all\
    \ setups. For the CPU-based platform, we use two Intel Xeon Silver 4110@2.10GHz\
    \ CPUs with 4 DDR4-2666Hz DRAM modules, providing 128 GB memory capacity and 85.3\
    \ GB/s bandwidth. We use one NVIDIA A100 GPU [\\[46\\]](#page-12-12) with 80GB\
    \ memory capacity and 1,935 GB/s bandwidth. For PIM-naive and UpANNS, we use 7\
    \ UPMEME PIM modules, with 56 GB total memory capacity and 612.5 GB/s aggregated\
    \ bandwidth. Existing study [\\[19\\]](#page-11-39) indicates that the peak power\
    \ of each PIM DIMM is 23.22W, leading to 162W total peak power.\n\nBenchmark.\
    \ We evaluate the compared baselines using three billion-scale datasets, namely\
    \ the DEEP1B [\\[10\\]](#page-11-40), SIFT1B [\\[30\\]](#page-11-22) and SPACEV1B\
    \ [\\[1\\]](#page-11-33). The DEEP1B dataset contains 1 billion 96-dimensional\
    \ vectors encoded into 12 dimensions. The SIFT1B dataset contains 1 billion 128-dimensional\
    \ vectors encoded into 16 dimensions. The SPACEV1B dataset contains 1 billion\
    \ 100-dimensional vectors encoded into 20 dimensions. We process 1,000 queries\
    \ at a time. The optimizations in UpANNS do not impact the accuracy.\n\nEvaluation\
    \ Metrics. We compare different solutions mainly based on Query per second (QPS).\
    \ For GPU-based Faiss, we also compare the QPS per Watt (QPS/W) to evaluate the\
    \ cost-effectiveness of different solutions.\n\n#### 5.2 Overall Performance Results\n\
    \nWe first compare the performance and efficiency of the four compared solutions.\
    \ Results are shown in Figure [10](#page-8-1) and Figure [12.](#page-8-2) The\
    \ term IVF4096 indicates that 1 billion points are partitioned into 4096 clusters.\
    \ We vary nprobe from 64, 128 to 256, which indicates the number of clusters selected\
    \ for each query search. All results are normalized to the values obtained from\
    \ Faiss-CPU or Faiss-GPU when nprobe=256 and IVF=4096 for each dataset. For DEEP1B,\
    \ due to GPU out-of-memory errors (marked with blue 'X' in Figure [12\\)](#page-8-2),\
    \ results are normalized to Faiss-GPU with nprobe=64 and IVF=4096.\n\nCompared\
    \ to Faiss-CPU and PIM-naive, UpANNS consistently achieves the highest queries\
    \ per second (QPS) across all settings. Specifically, UpANNS accelerates query\
    \ search times by 1.6x-3.8x for DEEP1B, by 2.3x-4.3x for SIFT1B and by 2.1x-4.0x\
    \ for SPACEV1B when compared to Faiss-CPU. Notably, under the same IVF setting,\
    \ all solutions experience a decrease in QPS as nprobe increases, due to the increased\
    \ search workload. Under the same nprobe, UpANNS obtains a higher QPS improvement\
    \ compared to Faiss-CPU when\n\nthe IVF increases. The reason is that a higher\
    \ number of clusters results in smaller cluster sizes, which leads to fewer encoded\
    \ points to search within each cluster and reduces data locality. Hence, the CPU,\
    \ which has multiple cache layers, does not exhibit a linear increase in QPS with\
    \ increasing IVF. In contrast, the DPU, which has a small-sized WRAM for cache\
    \ and MRAM for main memory, is less affected by data locality, enabling UpANNS\
    \ to achieve greater speedup as IVF increases. Although PIM-naive also surpasses\
    \ Faiss-CPU, it underperforms UpANNS by up to 3.1x, underscoring the critical\
    \ role of architectural optimizations in the proposed solution.\n\nComparing UpANNS\
    \ with Faiss-GPU, Figure [12a](#page-8-2) shows the performance results in terms\
    \ of QPS, and Figure [12b](#page-8-2) shows the energy efficiency results in terms\
    \ of QPS per watt. Overall, UpANNS obtains comparable QPS as Faiss-GPU in most\
    \ cases, except when IVF is 16384 and nprobe is 64. We used Nvidia Nsight [\\\
    [4\\]](#page-11-41) to look into the results and found that the significantly\
    \ high performance of Faiss-GPU under this setting is because of the different\
    \ parallelism of the top-k selection. The parallelism of this stage on SIFT1B\
    \ is 9x higher than the one on SPACEV1B under the same IVF and nprobe parameters.\
    \ Note that the comparable performance of UpANNS and Faiss-GPU are obtained under\
    \ a huge gap in the memory bandwidth and computing capabilities between PIM and\
    \ A100 hardware, as detailed in Table [1.](#page-7-0)\n\nFrom the energy efficiency\
    \ perspective, the 7 UPMEM DIMMs consume 162W peak power while A100 GPU consumes\
    \ a significant 300W. Although the actual energy consumption during runtime differs\
    \ from the peak power, we can use it as an approximation to compare the energy\
    \ efficiency of UpANNS and Faiss-GPU. Results in Figure [12b](#page-8-2) show\
    \ that, UpANNS achieves 2x higher QPS/W compared to Faiss-GPU in most cases, demonstrating\
    \ better power efficiency. While newer GPU models like H100 SXM ( 3.5TB/s bandwidth,\
    \ 700W) and GH200 ( 4.9TB/s bandwidth, 1000W) provide 2-3× higher bandwidth, their\
    \ power requirements scale proportionally, reinforcing UpANNS's position as the\
    \ more energy-efficient solution. Beyond energy consumption, we also find that\
    \ the per dollar QPS of UpANNS can be up to 9.3x higher than that of Faiss-GPU,\
    \ indicating that UpANNS can significantly reduce costs in real production environments.\n\
    \n#### 5.3 Ablation Study\n\nIn this subsection, we conduct ablation studies to\
    \ evaluate the effectiveness of our key optimizations in UpANNS, including the\
    \ PIM-Aware Workload Distribution, multi-threading design, Cooccurrence Aware\
    \ Encoding, and Top-K Pruning strategy.\n\n5.3.1 PIM-Aware Workload Distribution.\
    \ Since the workloads need to be distributed among DPUs and executed in parallel,\
    \ the largest workload among DPUs determines the overall performance. We\n\n<span\
    \ id=\"page-8-1\"></span>![](_page_8_Figure_2.jpeg)\n\nFigure 10: QPS of different\
    \ ANNS solutions normalized to that of Faiss-CPU when IVF is 4096 and nprobe is\
    \ 256. nprobe varies from 64, 128 to 256. #clusters varies from 4096, 8192 to\
    \ 16384.\n\n<span id=\"page-8-2\"></span>![](_page_8_Figure_4.jpeg)\n\n![](_page_8_Figure_5.jpeg)\n\
    \nFigure 11: The ratio of maximum process and average process improvement due\
    \ to the PIM-Aware Workload Distribution strategy under different nprobe and IVF\
    \ settings.\n\n![](_page_8_Figure_7.jpeg)\n\nFigure 12: QPS and QPS/W of Faiss-GPU\
    \ and UpANNS, normalized to those of Faiss-GPU when IVF is 4096 and nprobe is\
    \ 256. nprobe varies from 64, 128 to 256. #clusters varies from 4096, 8192 to\
    \ 16384. The results are normalized to in SIFT1B and SPACEV1B, nprobe is 64 in\
    \ DEEP1B.\n\n<span id=\"page-8-3\"></span>![](_page_8_Figure_9.jpeg)\n\nFigure\
    \ 13: QPS of UpANNS as the #threads increases\n\nevaluate the performance improvement\
    \ due to the PIM-Aware Workload Distribution strategy compared to the naive distribution\
    \ strategy that assigns clusters randomly to DPUs. Figure [11](#page-8-1) shows\
    \ the ratio of maximum process to average process under different nprobe and IVF\
    \ settings. A ratio closer to 1 indicates that the workload is more evenly distributed\
    \ among DPUs. We observe that the ratio for PIM-naive is significantly higher\
    \ than 1, especially when the IVF and nprobe are small. In contrast, the ratio\
    \ for Up-ANNS is close to 1 under all settings for all datasets, indicating that\
    \ the PIM-Aware Workload Distribution strategy effectively balances the workload\
    \ among DPUs.\n\n5.3.2 Number of Threads per DPU. UPMEM PIM could support up to\
    \ 24 threads per DPU [\\[2\\]](#page-11-17). Therefore, we evaluate the performance\
    \ of UpANNS by varying the number of threads from 1 to 24. Figure [13](#page-8-3)\
    \ shows our results for three datasets with different nprobe values. We normalize\
    \ the values to those when the number of tasklets is one. We have similar observations\
    \ in all settings. That is, the QPS increases linearly as the number of tasklets\
    \ increases up to\n\n<span id=\"page-8-0\"></span>![](_page_8_Figure_13.jpeg)\n\
    \nFigure 14: Performance improvement due to Co-occurrence Aware Encode strategy\
    \ (CAE) under different nprobe and corresponding length reduction rates.\n\n11.\
    \ Beyond 11 tasklets, the performance nearly saturates. The QPS of UpANNS with\
    \ 11 tasklets is almost 11x higher than that with a single tasklet. This is because\
    \ each DPU has a 14-stage pipeline, and only the last three stages can execute\
    \ in parallel with the first two stages of the next instruction within the same\
    \ thread. Using more than 11 tasklets makes full use of the pipeline and keeps\
    \ the DPU busy. Thus, by default, we set #threads to 11 per DPU.\n\n5.3.3 Co-occurrence\
    \ Aware Encoding. To evaluate the performance improvement due to the Co-occurrence\
    \ Aware Encoding strategy, we select queries that choose top-nprobe clusters with\
    \ the highest vector length reduction rates and calculate the average vector length\
    \ reduction rate in the maximum workload DPU. Figure [14](#page-8-0) shows the\
    \ performance improvement due to the Co-occurrence Aware Encoding strategy under\
    \ different nprobe values and corresponding length reduction rates. We observe\
    \ the following for\n\nSC '25, November 16–21, 2025, St Louis, MO, USA Chen et\
    \ al.\n\n<span id=\"page-9-0\"></span>![](_page_9_Figure_1.jpeg)\n\n![](_page_9_Figure_2.jpeg)\n\
    \nFigure 15: Time reduction with proposed top-k selection strategy.\n\nFigure\
    \ 16: The impact of different batch size on query latency.\n\nall settings. First,\
    \ the performance improvement correlates positively with the length reduction\
    \ rate, which is consistent with our design. Second, the LUT construction time\
    \ increases slightly due to the additional time needed to construct the partial\
    \ sum. Third, the performance improvement becomes more significant with higher\
    \ length reduction rates. This is because a higher length reduction rate means\
    \ less data needs to be transferred between MRAM and WRAM, allowing calculations\
    \ to be done more efficiently. The time breakdown also shows that distance calculation\
    \ time decreases more when the length reduction rate is higher.\n\n5.3.4 Top-K\
    \ Pruning. We evaluate the time of the top-k selection stage with and without\
    \ the proposed top-k selection strategy. Figure [15](#page-9-0) shows the time\
    \ reduction due to our proposed strategy. The results are normalized to the time\
    \ with the proposed strategy in top-10. We observe that the top-k selection time\
    \ increases linearly with the top-k value. The proposed strategy significantly\
    \ reduces the time for top-k selection, especially when the top-k value is large.\
    \ This is because our strategy reduces the number of comparisons needed to find\
    \ the top-k results.\n\n## 5.4 Sensitivity Study\n\nWe study the impact of three\
    \ parameters to the performance of UpANNS, including the batch size of queries,\
    \ the data size of each MRAM read and the required k size in the top-k query.\n\
    \n5.4.1 Batch Size. Figure [16](#page-9-0) shows the impact of different batch\
    \ sizes on query latency. We set the IVF to 4096 and nprobe to 64, varying the\
    \ batch size (#BS) from 10, 100 to 1000. We observe that UpANNS achieves the lowest\
    \ query latency in all settings, and the speedup of UpANNS over Faiss-CPU and\
    \ PIM-naive increases as the batch size increases. This is because the overhead\
    \ of the preprocessing and post-processing stages is amortized over a larger number\
    \ of queries. This makes it suitable for real-world applications where the number\
    \ of queries is usually large.\n\n5.4.2 MRAM Read Size. As described in Section\
    \ [4.2,](#page-4-0) the MRAM read latency does not increase linearly with size.\
    \ To decide an optimal MRAM read size for each thread, we evaluate the performance\
    \ of UpANNS with varied MRAM read sizes. Since the MRAM read size must be between\
    \ 8 Bytes - 2048 Bytes and aligned with 8 Bytes, we varied the number of vectors\
    \ read at once. For example, we vary the number of vectors fetched in one MRAM\
    \ read from 2, 4, ..., to\n\n<span id=\"page-9-1\"></span>![](_page_9_Figure_11.jpeg)\n\
    \n![](_page_9_Figure_12.jpeg)\n\nFigure 17: The impact of MRAM read size. Figure\
    \ 18: The impact of topk size.\n\n<span id=\"page-9-2\"></span>![](_page_9_Figure_14.jpeg)\n\
    \nFigure 19: Query search time breakdown\n\n64, making the MRAM read size vary\
    \ from 64 Bytes, 128 Bytes, ..., to 2 KB to fit in the 2048 Bytes limit in SIFT1B.\
    \ We set IVF to 4096 and top-k to 10. Figure [17](#page-9-1) shows the evaluation\
    \ results.\n\nFor all datasets, the QPS increases more rapidly when the number\
    \ of vectors increases from 2 to 16, while becoming much more stable when the\
    \ number of vectors exceeds 16. This is consistent with our observation in Figure\
    \ [7,](#page-5-1) which shows that the MRAM read latency grows slowly when the\
    \ data transfer size increases from 8 Bytes to 512 Bytes and grows dramatically\
    \ beyond 512 Bytes. By default, we set the MRAM read size to 16 vectors to have\
    \ good QPS and reasonable WRAM size at the same time.\n\n5.4.3 Top-K Size. To\
    \ evaluate the impact of top-k size, we vary k from 1 to 100 while maintaining\
    \ default parameters. Figure [18](#page-9-1) presents normalized QPS results (relative\
    \ to Faiss-CPU's top-100 performance) for UpANNS, Faiss-CPU, and Faiss-GPU. UpANNS\
    \ achieves consistent performance advantages, achieving an average 2.5x higher\
    \ QPS than Faiss-CPU and 1.6x higher than Faiss-GPU across datasets. While Faiss-CPU's\
    \ QPS remains stable across k values, UpANNS and Faiss-GPU exhibit slight QPS\
    \ degradation as k increases. This stems from the growing top-k list size, which\
    \ amplifies CPU-DPU communication costs for UpANNS and CUDA stream synchronization\
    \ overhead for Faiss-GPU during top-k selection.\n\nA detailed processing time\
    \ breakdown (Figure [19\\)](#page-9-2) reveals critical insights. UpANNS significantly\
    \ reduces the distance calculation stage's time ratio from 99.5% (Faiss-CPU) to\
    \ 75.5–80% across datasets, demonstrating its effectiveness in mitigating CPU\
    \ memory bottlenecks. While GPUs leverage high memory bandwidth to alleviate memory\
    \ access challenges, their performance is constrained by CUDA synchronization\
    \ during identifying top-k, consuming over 85% of processing time for large datasets.\
    \ DPUs avoid this bottleneck, achieving GPU-competitive performance. As k increases,\
    \ the top-k stage's time ratio grows from 76% to 89% on GPUs and 9% to 17% on\
    \ DPUs, while remaining negligible on CPUs due to their dominance by distance\
    \ calculations. These trends align with the observed QPS variations under different\
    \ k sizes.\n\n<span id=\"page-10-0\"></span>![](_page_10_Figure_2.jpeg)\n\nFigure\
    \ 20: Scalability of UpANNS under different #DPUs. Red triangles are measured\
    \ from real hardware and red dash line represents predicted QPS. The yellow line\
    \ indicates the QPS of Faiss-GPU. The blue vertical dash line shows when DPUs\
    \ consume power equal to that of an A100 GPU.\n\n#### 5.5 Scalability Study\n\n\
    Existing systems can hold up to 20 UPMEM DIMMs, each containing 128 DPUs [\\[25\\\
    ]](#page-11-37), totaling 2560 DPUs. To study the performance of UpANNS beyond\
    \ the seven PIM DIMMs we have, we evaluate the QPS of UpANNS with 500, 600, ...,\
    \ 900 DPUs and use the regression method to predict the QPS of UpANNS with up\
    \ to 2560 DPUs. Due to the limited memory capacity when the number of DPUs is\
    \ low, we evaluate the performance using 500 million scale.\n\nAs shown in Figure\
    \ [20,](#page-10-0) the regression curve fits perfectly with the QPS results measured\
    \ from 500-900 DPUs. When the number of DPUs increases, the QPS of UpANNS increases\
    \ almost linearly, demonstrating good scalability of our system. When the number\
    \ of DPUs is 2560, UpANNS can achieve up to 2.6x higher QPS compared to Faiss-GPU.\
    \ Note that, even with 20 DIMMs, the cost of PIM is still much lower than that\
    \ of A100 GPU (\\$8000 vs. \\$20,000). If we compare the performance of UpANNS\
    \ and Faiss-GPU under the same peak power (i.e., 300W), we can use 1654 DPUs.\
    \ As shown by the blue vertical dash line in Figure [20,](#page-10-0) UpANNS obtains\
    \ higher QPS than Faiss-GPU at the same peak power constraint under all settings.\
    \ This again demonstrates the energy efficiency of UpANNS, making it practical\
    \ for real-world large-scale production systems. While our current evaluation\
    \ focuses on single-host configurations, UpANNS can be easily extended to multi-host\
    \ configurations. Only query distribution and result aggregation require cross-host\
    \ communication. The core memory-intensive search operations remain local to each\
    \ host, ensuring efficient scalability.\n\n#### 6 RELATED WORK\n\nExisting methods\
    \ to improve ANNS performance can be categorized into software-based and hardware-based\
    \ methods.\n\nSoftware-based Methods. Software-based methods primarily enhance\
    \ the performance of ANNS through algorithm-level optimizations. For instance,\
    \ VDTuner [\\[53\\]](#page-12-13) utilizes a learning-based parameter tuning method\
    \ to more quickly find the optimal parameter configuration for ANNS databases.\
    \ Works such as [\\[7,](#page-11-35) [20,](#page-11-36) [40\\]](#page-11-42) reduce\
    \ memory access frequency by establishing a distance bound or using pruning methods.\
    \ In contrast, our proposed method accelerates the ANNS algorithm based on new\
    \ hardware, complementing the aforementioned software-based methods.\n\nHardware-based\
    \ Methods. Hardware-based methods primarily accelerate ANNS by utilizing existing\
    \ hardware or designing\n\nnew hardware. GPUs are the most commonly used hardware\
    \ to accelerate ANNS. For example, works such as [\\[24,](#page-11-43) [35,](#page-11-44)\
    \ [57\\]](#page-12-14) employ GPU-optimized graph algorithms for ANNS but face\
    \ scalability challenges due to limited GPU memory. Juno [\\[43\\]](#page-11-12)\
    \ addresses this using GPU ray-tracing cores (RT-cores) to accelerate the IVFPQ\
    \ compression-based method, reducing storage costs compared to graph-based methods.\
    \ However, GPU solutions generally exhibit lower energy efficiency than UpANNS.\n\
    \nIn addition to GPUs, many other hardware options have also been used to accelerate\
    \ ANNS. For example, works such as [\\[5,](#page-11-16) [31,](#page-11-10) [54\\\
    ]](#page-12-9) utilize FPGA to achieve high throughput on million-scale datasets\
    \ but struggle with billion-scale data due to constrained on-chip memory. SmartSSD-based\
    \ systems [\\[37,](#page-11-8) [49\\]](#page-12-8) offer ample memory but risk\
    \ PCIe bandwidth bottlenecks when coexisting with applications like LLMs. Compute\
    \ Express Link (CXL) [\\[28\\]](#page-11-20) accelerates ANNS by expanding memory\
    \ via additional controllers, yet raises system maintenance costs. In contrast,\
    \ UpANNS is a cost-effective and easily scalable solution that offers near-linear\
    \ performance scalability for large-scale datasets.\n\nIn addition to existing\
    \ hardware, many works design custom hardware specifically tailored to the characteristics\
    \ of ANNS for acceleration. For example, [\\[39\\]](#page-11-23) proposed a specialized\
    \ architecture, which combines the benefits of a specialized dataflow pipeline\
    \ and efficient data reuse to accelerate ANNS. The work [\\[51\\]](#page-12-6)\
    \ proposed a near-data processing (NDP) architecture based on SmartSSD to accelerate\
    \ the graph-traversal-based ANNS task. However, the aforementioned works are still\
    \ at the simulation or prototype stage, and are far from practical application.\n\
    \n#### 7 CONCLUSION\n\nIn this work, we addressed the significant performance\
    \ bottlenecks faced by CPU and GPU-based Approximate Nearest Neighbor Search (ANNS)\
    \ solutions at billion-scale datasets, where CPUbased solutions are bounded by\
    \ limited memory bandwidth and GPU-based solutions encounter memory capacity and\
    \ resource utilization issues. We present UpANNS, a novel framework leveraging\
    \ UPMEM's Processing-in-Memory (PIM) architecture to overcome the memory bottleneck\
    \ in billion-scale ANNS algorithms. UpANNS effectively addresses memory bottlenecks\
    \ by employing architecture-aware data placement, efficient resource management,\
    \ and a novel encoding approach for the IVFPQ algorithm. Our extensive evaluation\
    \ demonstrates that UpANNS significantly improves performance, achieving a remarkable\
    \ 4.3x increase in QPS compared to CPU-based implementations of Faiss, while also\
    \ matching the performance of GPU-based Faiss systems. Furthermore, UpANNS shows\
    \ a commendable 2.3x improvement in QPS per Watt compared to GPU solutions, highlighting\
    \ its superior cost-effectiveness and potential for large-scale applications such\
    \ as large model serving.\n\nAlthough our current implementation is tuned for\
    \ IVFPQ, the core techniques, namely workload distribution, resource management,\
    \ and top-k pruning, are transferable. Future work will generalize UpANNS to broader\
    \ ANNS algorithms, and exploit nextgeneration PIM hardware with higher frequency\
    \ and bandwidth to further improve competitiveness against high-end accelerators.\n\
    \nSC '25, November 16–21, 2025, St Louis, MO, USA Chen et al.\n\n#### REFERENCES\n\
    \n- <span id=\"page-11-33\"></span>[1] 2021. SPACEV1B: A billion-Scale vector\
    \ dataset for text descriptors. [https:](https://github.com/microsoft/SPTAG/tree/main/datasets/SPACEV1B)\
    \ [//github.com/microsoft/SPTAG/tree/main/datasets/SPACEV1B.](https://github.com/microsoft/SPTAG/tree/main/datasets/SPACEV1B)\n\
    - <span id=\"page-11-17\"></span>[2] 2023. Introducing the most advanced Processing\
    \ In Memory product. [https:](https://www.upmem.com/) [//www.upmem.com/.](https://www.upmem.com/)\n\
    - <span id=\"page-11-32\"></span>[3] 2023. UPMEM User Manual. [https://sdk.upmem.com/2023.1.0/index.html.](https://sdk.upmem.com/2023.1.0/index.html)\n\
    - <span id=\"page-11-41\"></span>[4] 2024. NVIDIA Nsight Systems. [https://developer.nvidia.com/nsight-systems.](https://developer.nvidia.com/nsight-systems)\n\
    - <span id=\"page-11-16\"></span>[5] Ameer MS Abdelhadi, Christos-Savvas Bouganis,\
    \ and George A Constantinides. 2019. Accelerated approximate nearest neighbors\
    \ search through hierarchical product quantization. In 2019 International Conference\
    \ on Field-Programmable Technology (ICFPT). IEEE, 90–98.\n- <span id=\"page-11-4\"\
    ></span>[6] Alexandr Andoni, Piotr Indyk, Thijs Laarhoven, Ilya Razenshteyn, and\
    \ Ludwig Schmidt. 2015. Practical and optimal LSH for angular distance. Advances\
    \ in neural information processing systems 28 (2015).\n- <span id=\"page-11-35\"\
    ></span>[7] Fabien André, Anne-Marie Kermarrec, and Nicolas Le Scouarnec. 2016.\
    \ Cache locality is not enough: High-performance nearest neighbor search with\
    \ product quantization fast scan. In 42nd International Conference on Very Large\
    \ Data Bases, Vol. 9. 12.\n- <span id=\"page-11-6\"></span>[8] Akhil Arora, Sakshi\
    \ Sinha, Piyush Kumar, and Arnab Bhattacharya. 2018. Hdindex: Pushing the scalability-accuracy\
    \ boundary for approximate knn search in high-dimensional spaces. arXiv preprint\
    \ arXiv:1804.06829 (2018).\n- <span id=\"page-11-19\"></span>[9] Bahar Asgari,\
    \ Ramyad Hadidi, Jiashen Cao, Da Eun Shim, Sung-Kyu Lim, and Hyesoon Kim. 2021.\
    \ Fafnir: Accelerating sparse gathering by using efficient near-memory intelligent\
    \ reduction. In 2021 IEEE International Symposium on High-Performance Computer\
    \ Architecture (HPCA). IEEE, 908–920.\n- <span id=\"page-11-40\"></span>[10] Artem\
    \ Babenko and Victor Lempitsky. 2016. Efficient indexing of billion-scale datasets\
    \ of deep descriptors. In Proceedings of the IEEE Conference on Computer Vision\
    \ and Pattern Recognition. 2055–2063.\n- <span id=\"page-11-30\"></span>[11] Arthur\
    \ Bernhardt, Andreas Koch, and Ilia Petrov. 2023. Pimdb: From mainmemory dbms\
    \ to processing-in-memory dbms-engines on intelligent memories. In Proceedings\
    \ of the 19th International Workshop on Data Management on New Hardware. 44–52.\n\
    - <span id=\"page-11-15\"></span>[12] BytePlus. 2025. Advanced Feature: Vector\
    \ Retrieval. [https://docs.byteplus.com/zh-](https://docs.byteplus.com/zh-CN/docs/bytehouse/vector-retrieval)[CN/docs/bytehouse/vector-retrieval.](https://docs.byteplus.com/zh-CN/docs/bytehouse/vector-retrieval)\n\
    - <span id=\"page-11-27\"></span>[13] Shuangyu Cai, Boyu Tian, Huanchen Zhang,\
    \ and Mingyu Gao. 2024. PimPam: Efficient Graph Pattern Matching on Real Processing-in-Memory\
    \ Hardware. Proceedings of the ACM on Management of Data 2, 3 (2024), 1–25.\n\
    - <span id=\"page-11-7\"></span>[14] Faquan Chen, Rendong Ying, Jianwei Xue, Fei\
    \ Wen, and Peilin Liu. 2023. Parallelnn: A parallel octree-based nearest neighbor\
    \ search accelerator for 3d point clouds. In 2023 IEEE International Symposium\
    \ on High-Performance Computer Architecture (HPCA). IEEE, 403–414.\n- <span id=\"\
    page-11-0\"></span>[15] Rihan Chen, Bin Liu, Han Zhu, Yaoxuan Wang, Qi Li, Buting\
    \ Ma, Qingbo Hua, Jun Jiang, Yunlong Xu, Hongbo Deng, et al. 2022. Approximate\
    \ nearest neighbor search under neural similarity metric for large-scale recommendation.\
    \ In Proceedings of the 31st ACM International Conference on Information & Knowledge\
    \ Management. 3013–3022.\n- <span id=\"page-11-26\"></span>[16] Sitian Chen, Haobin\
    \ Tan, Amelie Chi Zhou, Yusen Li, and Pavan Balaji. 2024. UpDLRM: Accelerating\
    \ Personalized Recommendation using Real-World PIM Architecture. Proceedings of\
    \ the 56th Annual Design Automation Conference (DAC) (2024).\n- <span id=\"page-11-18\"\
    ></span>[17] Fabrice Devaux. 2019. The true processing in memory accelerator.\
    \ In HCS'19. IEEE Computer Society, 1–24.\n- <span id=\"page-11-39\"></span><span\
    \ id=\"page-11-25\"></span>[18] Facebook AI Research. [n. d.]. Faiss. [https://github.com/facebookresearch/faiss.](https://github.com/facebookresearch/faiss)\
    \ [19] Yann Falevoz and Julien Legriel. 2023. Energy efficiency impact of processing\
    \ in memory: A comprehensive review of workloads on the upmem architecture. In\
    \ European Conference on Parallel Processing. Springer, 155–166.\n- <span id=\"\
    page-11-36\"></span>[20] Jianyang Gao and Cheng Long. 2023. High-dimensional approximate\
    \ nearest neighbor search: with reliable and efficient distance comparison operations.\
    \ Proceedings of the ACM on Management of Data 1, 2 (2023), 1–27.\n- <span id=\"\
    page-11-28\"></span>[21] Christina Giannoula, Peiming Yang, Ivan Fernandez Vega,\
    \ Jiacheng Yang, Yu Xin Li, Juan Gomez Luna, Mohammad Sadrosadati, Onur Mutlu,\
    \ and Gennady Pekhimenko. 2024. Accelerating Graph Neural Networks on Real Processing-In-Memory\
    \ Systems. arXiv[:2402.16731](https://arxiv.org/abs/2402.16731) [cs.AR]<https://arxiv.org/abs/2402.16731>\n\
    - <span id=\"page-11-29\"></span>[22] Kailash Gogineni, Sai Santosh Dayapule,\
    \ Juan Gómez-Luna, Karthikeya Gogineni, Peng Wei, Tian Lan, Mohammad Sadrosadati,\
    \ Onur Mutlu, and Guru Venkataramani. 2024. SwiftRL: Towards Efficient Reinforcement\
    \ Learning on Real Processing-In-Memory Systems. arXiv preprint arXiv:2405.03967\
    \ (2024).\n- <span id=\"page-11-5\"></span>[23] Long Gong, Huayi Wang, Mitsunori\
    \ Ogihara, and Jun Xu. 2020. iDEC: indexable distance estimating codes for approximate\
    \ nearest neighbor search. Proceedings of the VLDB Endowment 13, 9 (2020).\n-\
    \ <span id=\"page-11-43\"></span>[24] Fabian Groh, Lukas Ruppert, Patrick Wieschollek,\
    \ and Hendrik PA Lensch. 2022. Ggnn: Graph-based gpu nearest neighbor search.\
    \ IEEE Transactions on Big Data 9, 1 (2022), 267–279.\n- <span id=\"page-11-37\"\
    ></span>[25] Juan Gómez-Luna, Izzat El Hajj, Ivan Fernandez, Christina Giannoula,\
    \ Geraldo F. Oliveira, and Onur Mutlu. 2022. Benchmarking a New Paradigm: Experimental\
    \ Analysis and Characterization of a Real Processing-in-Memory System. IEEE Access\
    \ 10 (2022), 52565–52608.\n- <span id=\"page-11-34\"></span>[26] Bongjoon Hyun,\
    \ Taehun Kim, Dongjae Lee, and Minsoo Rhu. 2024. Pathfinding Future PIM Architectures\
    \ by Demystifying a Commercial PIM Technology. In 2024 IEEE International Symposium\
    \ on High-Performance Computer Architecture\n- <span id=\"page-11-38\"></span>(HPCA).\
    \ IEEE, 263–279. [27] Intel. 2017. Intel Xeon Silver 4110 Processor. [https://www.intel.com/content/](https://www.intel.com/content/www/us/en/products/sku/123547/intel-xeon-silver-4110-processor-11m-cache-2-10-ghz/specifications.html)\
    \ [www/us/en/products/sku/123547/intel-xeon-silver-4110-processor-11m](https://www.intel.com/content/www/us/en/products/sku/123547/intel-xeon-silver-4110-processor-11m-cache-2-10-ghz/specifications.html)[cache-2-10-ghz/specifications.html.](https://www.intel.com/content/www/us/en/products/sku/123547/intel-xeon-silver-4110-processor-11m-cache-2-10-ghz/specifications.html)\n\
    - <span id=\"page-11-20\"></span>[28] Junhyeok Jang, Hanjin Choi, Hanyeoreum Bae,\
    \ Seungjun Lee, Miryeong Kwon, and Myoungsoo Jung. 2023. CXL-ANNS: Software-Hardware\
    \ Collaborative Memory Disaggregation and Computation for Billion-Scale Approximate\
    \ Nearest Neighbor Search. In 2023 USENIX Annual Technical Conference (USENIX\
    \ ATC 23). Boston, MA, 585–600.\n- <span id=\"page-11-14\"></span>[29] Herve Jegou,\
    \ Matthijs Douze, and Cordelia Schmid. 2010. Product quantization for nearest\
    \ neighbor search. IEEE transactions on pattern analysis and machine intelligence\
    \ 33, 1 (2010), 117–128.\n- <span id=\"page-11-22\"></span>[30] Hervé Jégou, Romain\
    \ Tavenard, Matthijs Douze, and Laurent Amsaleg. 2011. Searching in one billion\
    \ vectors: re-rank with source coding. In 2011 IEEE International Conference on\
    \ Acoustics, Speech and Signal Processing (ICASSP). IEEE, 861–864.\n- <span id=\"\
    page-11-10\"></span>[31] Wenqi Jiang, Shigang Li, Yu Zhu, Johannes De Fine Licht,\
    \ Zhenhao He, Runbin Shi, Cedric Renggli, Shuai Zhang, Theodoros Rekatsinas, Torsten\
    \ Hoefler, and Gustavo Alonso. 2023. Co-design Hardware and Algorithm for Vector\
    \ Search. In Proceedings of the International Conference for High Performance\
    \ Computing, Networking, Storage and Analysis (SC '23). Article 87, 15 pages.\n\
    - <span id=\"page-11-2\"></span>[32] Wenqi Jiang, Marco Zeller, Roger Waleffe,\
    \ Torsten Hoefler, and Gustavo Alonso. 2023. Chameleon: a heterogeneous and disaggregated\
    \ accelerator system for retrieval-augmented language models. arXiv preprint arXiv:2310.09949\
    \ (2023).\n- <span id=\"page-11-31\"></span>[33] Muhammad Attahir Jibril, Hani\
    \ Al-Sayeh, and Kai-Uwe Sattler. 2024. Accelerating Aggregation Using a Real Processing-in-Memory\
    \ System. In 2024 IEEE 40th International Conference on Data Engineering (ICDE).\
    \ Los Alamitos, CA, USA, 3920–3932.\n- <span id=\"page-11-11\"></span>[34] Jeff\
    \ Johnson, Matthijs Douze, and Hervé Jégou. 2019. Billion-scale similarity search\
    \ with GPUs. IEEE Transactions on Big Data 7, 3 (2019), 535–547.\n- <span id=\"\
    page-11-44\"></span>[35] Saim Khan, Somesh Singh, Harsha Vardhan Simhadri, Jyothi\
    \ Vedurada, et al. 2024. BANG: Billion-Scale Approximate Nearest Neighbor Search\
    \ using a Single GPU. arXiv preprint arXiv:2401.11324 (2024).\n- <span id=\"page-11-3\"\
    ></span>[36] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and\
    \ Mike Lewis. 2019. Generalization through memorization: Nearest neighbor language\
    \ models. arXiv preprint arXiv:1911.00172 (2019).\n- <span id=\"page-11-8\"></span>[37]\
    \ Ji-Hoon Kim, Yeo-Reum Park, Jaeyoung Do, Soo-Young Ji, and Joo-Young Kim. 2023.\
    \ Accelerating Large-Scale Graph-Based Nearest Neighbor Search on a Computational\
    \ Storage Platform. IEEE Trans. Comput. 72, 1 (2023), 278–290.\n- <span id=\"\
    page-11-21\"></span>[38] Hunjun Lee, Minseop Kim, Dongmoon Min, Joonsung Kim,\
    \ Jongwon Back, Honam Yoo, Jong-Ho Lee, and Jangwoo Kim. 2022. 3D-FPIM: An extreme\
    \ energyefficient DNN acceleration system using 3D NAND flash-based in-situ PIM\
    \ unit. In 2022 55th IEEE/ACM International Symposium on Microarchitecture (MICRO).\
    \ IEEE, 1359–1376.\n- <span id=\"page-11-23\"></span>[39] Yejin Lee, Hyunji Choi,\
    \ Sunhong Min, Hyunseung Lee, Sangwon Beak, Dawoon Jeong, Jae W Lee, and Tae Jun\
    \ Ham. 2022. Anna: Specialized architecture for approximate nearest neighbor search.\
    \ In 2022 IEEE International Symposium on High-Performance Computer Architecture\
    \ (HPCA). IEEE, 169–183.\n- <span id=\"page-11-42\"></span>[40] Daniel LeJeune,\
    \ Reinhard Heckel, and Richard Baraniuk. 2019. Adaptive estimation for approximate\
    \ -nearest-neighbor computations. In The 22nd International Conference on Artificial\
    \ Intelligence and Statistics. PMLR, 3099–3107.\n- <span id=\"page-11-1\"></span>[41]\
    \ Shiyu Li, Yitu Wang, Edward Hanson, Andrew Chang, Yang Seok Ki, Hai Helen Li,\
    \ and Yiran Chen. 2024. NDRec: A Near-Data Processing System for Training Large-Scale\
    \ Recommendation Models. IEEE Trans. Comput. (2024).\n- <span id=\"page-11-24\"\
    ></span>[42] Shengwen Liang, Ziming Yuan, Ying Wang, Dawen Xu, Huawei Li, and\
    \ Xiaowei Li. 2024. HyQA: Hybrid Near-Data Processing Platform for Embedding Based\
    \ Question Answering System. In 2024 Design, Automation & Test in Europe Conference\
    \ & Exhibition (DATE). IEEE, 1–6.\n- <span id=\"page-11-12\"></span>[43] Zihan\
    \ Liu, Wentao Ni, Jingwen Leng, Yu Feng, Cong Guo, Quan Chen, Chao Li, Minyi Guo,\
    \ and Yuhao Zhu. 2024. JUNO: Optimizing High-Dimensional Approximate Nearest Neighbour\
    \ Search with Sparsity-Aware Algorithm and Ray-Tracing Core Mapping. In Proceedings\
    \ of the 29th ACM International Conference on Architectural Support for Programming\
    \ Languages and Operating Systems, Volume 2 (<conf-loc>, <city>La Jolla</city>,\
    \ <state>CA</state>, <country>USA</country>, </conf-loc>) (ASPLOS '24). 549–565.\n\
    - <span id=\"page-11-13\"></span>[44] Yu A Malkov and Dmitry A Yashunin. 2018.\
    \ Efficient and robust approximate nearest neighbor search using hierarchical\
    \ navigable small world graphs. IEEE transactions on pattern analysis and machine\
    \ intelligence 42, 4 (2018), 824–836.\n- <span id=\"page-11-9\"></span>[45] Magdalen\
    \ Dobson Manohar, Zheqi Shen, Guy E. Blelloch, Laxman Dhulipala, Yan Gu, Harsha\
    \ Vardhan Simhadri, and Yihan Sun. 2024. ParlayANN: Scalable and Deterministic\
    \ Parallel Graph-Based Approximate Nearest Neighbor Search Algorithms. In Proceedings\
    \ of the 29th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel\
    \ Programming, PPoPP 2024, Edinburgh, United Kingdom, March 2-6, 2024, Michel\
    \ Steuwer, I-Ting Angelina Lee, and Milind Chabbi (Eds.). ACM, 270–285.\n\n- <span\
    \ id=\"page-12-12\"></span><span id=\"page-12-0\"></span>[46] Nvidia. 2020. NVIDIA\
    \ A100. [https://www.nvidia.cn/data-center/a100/.](https://www.nvidia.cn/data-center/a100/)\n\
    - <span id=\"page-12-11\"></span>[47] Steve Rhyner, Haocong Luo, Juan Gómez-Luna,\
    \ Mohammad Sadrosadati, Jiawei Jiang, Ataberk Olgun, Harshita Gupta, Ce Zhang,\
    \ and Onur Mutlu. 2024. PIM-Opt: Demystifying Distributed Optimization Algorithms\
    \ on a Real-World Processing-In-Memory System. In Proceedings of the 2024 International\
    \ Conference on Parallel Architectures and Compilation Techniques (Long Beach,\
    \ CA, USA) (PACT '24). 201–218.\n- <span id=\"page-12-2\"></span>[48] Weijia Shi,\
    \ Sewon Min, Maria Lomeli, Chunting Zhou, Margaret Li, Gergely Szilvasy, Rich\
    \ James, Xi Victoria Lin, Noah A Smith, Luke Zettlemoyer, et al. 2023. In-context\
    \ pretraining: Language modeling beyond document boundaries. arXiv preprint arXiv:2310.10638\
    \ (2023).\n- <span id=\"page-12-8\"></span>[49] Bing Tian, Haikun Liu, Zhuohui\
    \ Duan, Xiaofei Liao, Hai Jin, and Yu Zhang. 2024. Scalable Billion-point Approximate\
    \ Nearest Neighbor Search Using SmartSSDs. In 2024 USENIX Annual Technical Conference\
    \ (USENIX ATC 24).\n- <span id=\"page-12-1\"></span>[50] Yitu Wang, Shiyu Li,\
    \ Qilin Zheng, Andrew Chang, Hai Li, and Yiran Chen. 2023. EMS-i: An Efficient\
    \ Memory System Design with Specialized Caching Mechanism for Recommendation Inference.\
    \ ACM Transactions on Embedded Computing Systems 22, 5s (2023), 1–22.\n- <span\
    \ id=\"page-12-6\"></span>[51] Yitu Wang, Shiyu Li, Qilin Zheng, Linghao Song,\
    \ Zongwang Li, Andrew Chang, Hai \"Helen\" Li, and Yiran Chen. 2024. NDSEARCH:\
    \ Accelerating Graph-Traversal-Based Approximate Nearest Neighbor Search through\
    \ Near Data Processing. In 2024 ACM/IEEE 51st Annual International Symposium on\
    \ Computer Architecture (ISCA). 368–381.\n- <span id=\"page-12-10\"></span>[52]\
    \ Weihong Xu, Junwei Chen, Po-Kai Hsu, Jaeyoung Kang, Minxuan Zhou, Sumukh Pinge,\
    \ Shimeng Yu, and Tajana Rosing. 2023. Proxima: Near-storage Acceleration for\
    \ Graph-based Approximate Nearest Neighbor Search in 3D NAND. arXiv preprint arXiv:2312.04257\
    \ (2023).\n- <span id=\"page-12-13\"></span>[53] T. Yang, W. Hu, W. Peng, Y. Li,\
    \ J. Li, G. Wang, and X. Liu. 2024. VDTuner: Automated Performance Tuning for\
    \ Vector Data Management Systems. In 2024 IEEE 40th International Conference on\
    \ Data Engineering (ICDE). 4357–4369.\n- <span id=\"page-12-9\"></span>[54] Wei\
    \ Yuan and Xi Jin. 2025. FANNS: An FPGA-Based Approximate Nearest-Neighbor Search\
    \ Accelerator. IEEE Transactions on Very Large Scale Integration (VLSI) Systems\
    \ (2025).\n- <span id=\"page-12-3\"></span>[55] Jiaqi Zhai, Lucy Liao, Xing Liu,\
    \ Yueming Wang, Rui Li, Xuan Cao, Leon Gao, Zhaojie Gong, Fangda Gu, Jiayuan He,\
    \ Yinghai Lu, and Yu Shi. 2024. Actions Speak Louder than Words: Trillion-Parameter\
    \ Sequential Transducers for Generative Recommendations. In Proceedings of the\
    \ 41st International Conference on Machine Learning (Proceedings of Machine Learning\
    \ Research, Vol. 235), Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian\
    \ Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (Eds.). PMLR,\
    \ 58484–58509.\n- <span id=\"page-12-7\"></span>[56] Hailin Zhang, Xiaodong Ji,\
    \ Yilin Chen, Fangcheng Fu, Xupeng Miao, Xiaonan Nie, Weipeng Chen, and Bin Cui.\
    \ 2025. PQCache: Product Quantization-based KVCache for Long Context LLM Inference.\
    \ arXiv[:2407.12820](https://arxiv.org/abs/2407.12820) [cs.CL] [https:](https://arxiv.org/abs/2407.12820)\
    \ [//arxiv.org/abs/2407.12820](https://arxiv.org/abs/2407.12820)\n- <span id=\"\
    page-12-14\"></span>[57] Weijie Zhao, Shulong Tan, and Ping Li. 2020. Song: Approximate\
    \ nearest neighbor search on gpu. In 2020 IEEE 36th International Conference on\
    \ Data Engineering (ICDE). IEEE, 1033–1044.\n- <span id=\"page-12-5\"></span>[58]\
    \ Bolong Zheng, Zhao Xi, Lianggui Weng, Nguyen Quoc Viet Hung, Hang Liu, and Christian\
    \ S Jensen. 2020. PM-LSH: A fast and accurate LSH framework for high-dimensional\
    \ approximate NN search. Proceedings of the VLDB Endowment 13, 5 (2020), 643–655.\n\
    - <span id=\"page-12-4\"></span>[59] Yuxin Zheng, Qi Guo, Anthony KH Tung, and\
    \ Sai Wu. 2016. Lazylsh: Approximate nearest neighbor search for multiple distance\
    \ functions with a single index. In Proceedings of the 2016 International Conference\
    \ on Management of Data. 2023– 2037."
- title: "Multilayer Dataflow: Orchestrate Butterfly Sparsity to Accelerate\n  Attention\
    \ Computation"
  abstract: 'Recent neural networks (NNs) with self-attention exhibit competitiveness

    across different AI domains, but the essential attention mechanism brings

    massive computation and memory demands. To this end, various sparsity patterns

    are introduced to reduce the quadratic computation complexity, among which the

    structured butterfly sparsity has been proven efficient in computation

    reduction while maintaining model accuracy. However, its complicated data

    accessing pattern brings utilization degradation and makes parallelism hard to

    exploit in general block-oriented architecture like GPU. Since the

    reconfigurable dataflow architecture is known to have better data reusability

    and architectural flexibility in general NN-based acceleration, we want to

    apply it to the butterfly sparsity for acquiring better computational

    efficiency for attention workloads. We first propose a hybrid

    butterfly-sparsity network to obtain better trade-offs between attention

    accuracy and performance. Next, we propose a scalable multilayer dataflow

    method supported by coarse-grained streaming parallelism designs, to

    orchestrate the butterfly sparsity computation on the dataflow array. The

    experiments show that compared with Jetson Xavier NX, our design has a speedup

    of up to $14.34\times$ ($9.29\times$ on average) as well as $11.14\times$

    energy efficiency advancement in attention workloads. In comparison with SOTA

    attention accelerators of the same peak performance, our dataflow architecture

    acquires $2.38\times$-$4.7\times$ efficiency improvement as well as

    $6.60\times$-$15.37\times$ energy reduction with butterfly sparsity

    optimization.'
  url: http://arxiv.org/abs/2411.00734v2
  keywords: attention mechanism, reconfigurable dataflow architecture, structured
    sparsity, butterfly computation
  document: '# Multi-layer Dataflow: Orchestrate Butterfly Sparsity to Accelerate
    Attention Computation


    Haibin Wu<sup>1</sup> , Wenming Li1,<sup>∗</sup> , Kai Yan1,<sup>∗</sup> , Zhihua
    Fan1,<sup>∗</sup> , Peiyang Wu<sup>1</sup> , Yuqun Liu<sup>1</sup> ,


    Yanhuan Liu<sup>1</sup> , Ziqing Qiang<sup>1</sup> , Meng Wu<sup>1</sup> , Kunming
    Zhang<sup>1</sup> , Xiaochun Ye<sup>1</sup> , Dongrui Fan<sup>1</sup> <sup>1</sup>
    State Key Lab of Processors, Institute of Computing Technology, CAS, Beijing,
    China


    arXiv:2411.00734v2 [cs.AR] 25 Nov 2024


    *Abstract*—Recent neural networks (NNs) with self attention exhibits competitiveness
    across different AI domains, but the essential attention mechanism brings massive
    computation and memory demands. To this end, various sparsity patterns are introduced
    to reduce the quadratic computation complexity, among which the structured butterfly
    sparsity has been proven efficient in computation reduction while maintaining
    model accuracy. However, its addressing inefficiency brings utilization degradation
    and makes parallelism hard to exploit in general block-oriented architecture like
    GPU. Since the reconfigurable dataflow accelerators have shown the superiority
    of better data reusability and architectural flexibility towards general NN-based
    acceleration, we propose a scalable multilayer dataflow method supported with
    coarse-grained streaming parallelism, to orchestrate the butterfly sparsity computation
    on dataflow array. The experiments show that compared with Jetson Xavier NX, our
    design has a speedup of up to 14.34× (9.29× on average) as well as 12.3× energy
    efficiency advancement in attention workloads. In comparison with SOTA butterfly
    accelerator, our design acquires 1.17× speedup and 3.36× energy efficiency improvement,
    at the same peak performance.


    *Index Terms*—attention mechanism, reconfigurable dataflow architecture, structured
    sparsity, butterfly computation


    #### I. INTRODUCTION


    Prevailing deep learning method, such as *Transformer*, has dominated various
    domains in Natural Language Processing (NLP) and Computer Vision (CV) [\[1\]](#page-8-0).
    Its prominent attention mechanisms [\[2\]](#page-8-1) capture comprehensive relationships
    between tokens and features, through linear mapping implemented by matrix-vector
    multiplication. However, the computational complexity of attention increases quadratically
    with the token length, which can extend up to 64K in long sequences, as observed
    in BERT [\[3\]](#page-8-2), resulting in substantial computation and memory demands.
    Various speedup methods with diverse sparsity patterns [\[4\]](#page-8-3)–[\[6\]](#page-8-4)
    have been introduced in the computation process of Q, K, V and sof tmax(Q(K) T
    )V to alleviate the computation and memory demands.


    Based on the *theoretical evidence* provided by recent research [\[7\]](#page-8-5),
    [\[8\]](#page-8-6), structured sparsity does not significantly compromise model
    accuracy, compared with dynamic sparsity [\[9\]](#page-8-7), [\[10\]](#page-8-8).
    Dynamic sparsity poses challenges in general architecture, like GPU due to the
    inefficiency associated with *randomized addressing*. They often require customized
    hardware to achieve dynamic element compression or rearrangement within matrices
    [?], [\[11\]](#page-8-9). In contrast, structured sparsity offers predictable
    addressing regulations that can be efficiently applied through matrix layout optimization
    during programming or compiling.


    <span id="page-0-0"></span>![](_page_0_Figure_9.jpeg)


    <span id="page-0-1"></span>**\*v \*v** Fig. 2: Profiling original dense-based
    attention and fft-based attention kernels of VIT and BERT on the GPU platform
    - Jetson Xavier NX.


    Among various structured sparsity, *butterfly sparsity* has been recently proven
    efficient in enhancing performance of attention workloads while maintaining accuracy
    across diverse AI tasks [\[8\]](#page-8-6). As shown in Fig. [1a](#page-0-0)&1b,
    one of the butterfly sparsity approach is to simplify the linear weight matrices
    for Q, K, V by using a set of O(logN) butterfly matrices B<sup>i</sup> that have
    the same sparsity rate of 2/N respectively [\[7\]](#page-8-5), [\[12\]](#page-8-10).
    This is a technique we refer to as *butterfly pattern matrix-vector multiplication
    (BPMM)*. Another approach, such as *FNet* [\[13\]](#page-8-11), applies *2D Fast
    Fourier Transform* (2D-FFT) on tokens to heavily decrease the computation amount
    in attention layers, as illustrated in Fig. [1c](#page-0-0). With these simplifications,
    the original dense product is replaced with butterfly computation to reduce the
    complexity and weight size from O(N<sup>2</sup> ) to O(N log N).


    General block-oriented architecture - GPU excels in dense matrix computation,
    though it lacks energy efficiency, compared to dataflow architectures like TPU
    [\[14\]](#page-8-12) and Plasticine [\[15\]](#page-8-13). In terms of butterfly
    sparsity, however, the cacheunfriendly addressing results in performance degradation
    on GPUs. We conducted analyses on NVIDIA - Jetson Xavier NX, by comparing the
    original dense attention kernels (to qkv and sof tmax(qk) ∗ v) and the butterfly
    attention kernels (*fftsequence* and *fft-hidden*) of VIT and BERT. Detailed profiling
    results are presented in Fig. [2.](#page-0-1) Although the data throughput is
    given in *adequate batch parallelism* of 128 in GPU,


    <span id="page-1-0"></span>![](_page_1_Figure_0.jpeg)


    Fig. 3: Fundamental patterns in existing sparsity variants.


    the *hit rates*, especially for the L1 cache, exhibit significant degradation
    in the FFT-based kernels compared to the dense q, k, v kernels in both VIT and
    BERT. Furthermore, their overall performance indicated by duration time fails
    to exhibit obvious speedup (even a reduced performance for BERT in large scales),
    despite the *theoretical reduction of computational complexity* mentioned above
    with the butterfly sparsity.


    Aiming for tackling the challenges of accessing inefficiency and parallelism exploitation
    hardness in structured butterfly sparsity computation, we introduce a *scalable
    multilayer dataflow orchestration* method applied on a flexible reconfigurable
    dataflow architecture, to speed up butterfly computation for attention workloads.
    Our contributions are as follows:


    #### • Algorithm aspects:


    - i. We propose a *multilayer dataflow method* to tackle the hardness of orchestrate
    the data reuse of butterfly sparsity on a *reconfigurable dataflow architecture
    substrate*. ii. we equip this dataflow substrate with *block-level DFG node scheduling*
    as well as the decoupled function unit design, to exploit effective *coarse-grained
    streaming parallelism*.

    - iii. We introduce a *multi-stage division method* based on the *Cooley-Tukey
    algorithm* to enhance the scalability of our dataflow approach towards butterfly
    sparsity in various data scales. iv. Moreover, we architect the scratchpad memory
    with a multi-line design to implement *transposefree SIMD parallelism* in large-scale
    vector sequence for boosting the overall computation performance.

    - Experiment results demonstrate the superiority of the proposed multilayer dataflow
    method with outstanding speedup and energy efficiency in attention workloads over
    the advanced GPU and state-of-the-art accelerators.


    #### II. BACKGROUND AND RELATED WORK


    In this session, we begin by discussing the existing variants of sparsity in attention
    workloads and emphasize the superiority of butterfly sparsity for global relationships
    capturing. Next, we summarize the shortcomings in recent architectural works towards
    butterfly sparsity computation.


    #### *A. Sparsity Variants for Attention Computation*


    The prominent self-attention mechanism effectively facilitates interaction among
    tokens and features, overcoming the limitations of CNNs and RNNs. But it is plagued
    by quadratic computation and memory requirements. Recent sparse attention approaches
    introduce diverse sparsity patterns in attention matrices or linear weighted matrices,
    as shown in Fig. [3.](#page-1-0) One notable implementation is dynamic sparsity,
    which is determined by the *sequence content*. Kitaev *et al.* [\[9\]](#page-8-7)
    proposed Locality Sensitive Hashing (LSH) to infer content-based sparsity. Qu
    *et*


    <span id="page-1-1"></span>![](_page_1_Figure_12.jpeg)


    Fig. 4: The consecutive weight matrices multiplication of butterfly sparsity in
    diverse stride patterns.


    *al*. utilized low-rank linear transformations to estimate attention scores [\[10\]](#page-8-8)
    and select the sparse pattern dynamically through top-k selection. However, these
    content-based methods scale poorly in long sequence and may be *application-fixed
    and awkward* when facing with variable sparse density.


    Static structured sparsity as an alternative method addresses these drawbacks
    by applying regular sparsity patterns to attention and linear matrices. Common
    structured approaches often *combine several basic patterns* shown in Fig. [3.](#page-1-0)(a,b,d).
    For instance, Child *et al*. [\[16\]](#page-8-14) introduced a sparsity model
    combining sliding and dilated sliding window pattern, and acquired state-of-the-art
    accuracy results in long sequences. Also, in Longformer [\[17\]](#page-8-15),
    Beltagy *et al*. proposed the globalplus-sliding-window pattern that achieved
    linear scalability of sequence length while maintaining high accuracy in long
    document tasks. These combination methods aim to improve patterns capturing within
    attention matrix, as using only one partial sparsity has *limited expression capability*
    for *token relationships* and *weight features*.


    #### *B. Principle of Butterfly Matrices and Fourier Transform*


    Butterfly sparsity is a special global pattern beyond the restriction of limited
    relationship. In Fig. [4,](#page-1-1) it involves a set of regular structured
    matrices whose product - *BPMM* captures the overall relationships of tokens or
    features in a binary reductive way with logarithmic complexity. A. The first sparse
    approach is applying these matrices to the linear layers of Q, K, V and *feed
    forward network* (FFN), correspond to Fig. [1b](#page-0-0). In the works [\[7\]](#page-8-5),
    [\[12\]](#page-8-10), decomposition methods that represent linear weights as the
    multiplication of butterfly matrices are proposed and *proven effective in parameter
    simplification*. B. The second further sparse approach in attention workloads
    is applying *Fourier Transform* to replace or approximate computation of *attention
    matrix* [\[13\]](#page-8-11), [\[18\]](#page-8-16), correspond to Fig. [1c](#page-0-0).
    The principle of *Discrete Fourier Transform* (DFT) is illustrated in Eq [\(1\)](#page-1-2).
    The result vector X is obtained by performing matrix multiplication between an
    input vector x and a dense matrix Ω whose entry at row k and column n is represented
    as ω kn <sup>N</sup> .


    <span id="page-1-2"></span>

    $$X\_k = \sum\_{n=0}^{N-1} x\_n \omega\_N^{kn}, \ k = 0, \dots, N-1. \qquad \text{(l)}$$


    A famous fast algorithm for DFT computing is the *Cooley-Tukey decimation-in-time*
    FFT [\[19\]](#page-8-17), which recursively divides the sequence into odd and
    even subsequences to reduce the computation from O(N<sup>2</sup> ) to O(N log
    N), as shown in Eq [\(2\)](#page-2-0).


    <span id="page-2-3"></span>![](_page_2_Figure_0.jpeg)


    (b) Partial-order dataflow dependence reconstructed in layers.


    (a) Original butterfly dataflow between DFG nodes is not in partial-order relation
    because of the mutual data dependence for element swapping.


    Fig. 5: Butterfly sparsity computation expressed as the dataflow execution with
    data dependence in multiple layers.


    <span id="page-2-0"></span>

    $$\begin{aligned} X\_k &= \sum\_{n=0}^{\frac{N}{2}-1} x\_{2n} \omega\_{\frac{N}{2}}^{kn}
    + \omega\_N^k \sum\_{n=0}^{\frac{N}{2}-1} x\_{2n+1} \omega\_{\frac{N}{2}}^{kn},
    \\ X\_{k + \frac{N}{2}} &= \sum\_{n=0}^{\frac{N}{2}-1} x\_{2n} \omega\_{\frac{N}{2}}^{kn}
    - \omega\_N^k \sum\_{n=0}^{\frac{N}{2}-1} x\_{2n+1} \omega\_{\frac{N}{2}}^{kn}.
    \end{aligned} \tag{2}$$


    It can be expressed in a matrix form:


    $$\begin{split} X = \Omega\_N x &= \begin{bmatrix} \Omega\_{N/2} x\_{\text{even}}
    + D\_{N/2} \Omega\_{N/2} x\_{\text{odd}} \\ \Omega\_{N/2} x\_{\text{even}} - D\_{N/2}
    \Omega\_{N/2} x\_{\text{odd}} \end{bmatrix} \\ &= \begin{bmatrix} I\_{N/2} & D\_{N/2}
    \\ I\_{N/2} & -D\_{N/2} \end{bmatrix} \begin{bmatrix} \Omega\_{N/2} & 0 \\ 0 &
    \Omega\_{N/2} \end{bmatrix} \begin{bmatrix} x\_{\text{even}} \\ x\_{\text{odd}}
    \end{bmatrix} \\ &= B\_N \begin{bmatrix} \Omega\_{N/2} & 0 \\ 0 & \Omega\_{N/2}
    \end{bmatrix} P\_{N} x, \end{split} \tag{3}$$


    where DN/<sup>2</sup> is a diagonal matrix with entries 1, ω<sup>1</sup> <sup>N</sup>
    , ω<sup>2</sup> <sup>N</sup> , . . . , ω N/2−1 <sup>N</sup> . B<sup>N</sup> is
    a 2×2 block matrix consisting of diagonal matrices referred to as *"butterfly
    factors"* [\[7\]](#page-8-5). P<sup>N</sup> represents a permutation matrix responsible
    for performing a sequence reversion.


    The middle matrix Ω can be defined recursively, with the base case being ΩN/N
    = [ω 00 1 ] = [1]. Expanding the recursion as follows:


    <span id="page-2-1"></span>

    $$\begin{split} \Omega\_{N} &= B\_{N} \begin{bmatrix} \Omega\_{N/2} & 0 \\ 0 &
    \Omega\_{N/2} \end{bmatrix} P\_{N} \\ &= B\_{N} \begin{bmatrix} B\_{N/2} & 0 \\
    0 & B\_{N/2} \end{bmatrix} \begin{bmatrix} \Omega\_{N/4} & 0 & 0 & 0 \\ 0 & \Omega\_{N/4}
    & 0 & 0 \\ 0 & 0 & \Omega\_{N/4} & 0 \\ 0 & 0 & 0 & \Omega\_{N/4} \end{bmatrix}
    \begin{bmatrix} P\_{N/2} & 0 \\ 0 & P\_{N/2} \end{bmatrix} P\_{N} \\ &= \cdots
    \\ &= \begin{pmatrix} B\_{N} \dots \begin{bmatrix} B\_{2} & \dots & 0 \\ \vdots
    & \ddots & \vdots \\ 0 & \dots & B\_{2} \end{bmatrix} \end{pmatrix} \begin{pmatrix}
    \begin{bmatrix} P\_{2} & \dots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \dots &
    P\_{2} \end{bmatrix} \dots P\_{N} \\ &= \cdots \end{split} \tag{4}$$


    The consecutive multiplying of logN sparse matrices B<sup>i</sup> on the left
    in Eq [\(4\)](#page-2-1) shares the same regular pattern as BPMM in Fig. [4.](#page-1-1)
    But it is calculated in complex values which result in different *arithmetic density*
    (ratio between calculation and accessing) of vector elements, compared with the
    real-value BPMM.


    # *C. Butterfly Computation Acceleration*


    The parallelism of butterfly sparsity is not easy to exploit on block-oriented
    architectures like GPU, because of the incremental stride patterns in the consecutive
    matrices B<sup>i</sup> as


    <span id="page-2-2"></span>![](_page_2_Figure_13.jpeg)


    Fig. 6: Coarse-grained reconfigurable dataflow substrate mapped with GEMM DFG.


    shown in Fig. [4.](#page-1-1) Fan *et al*. proposed TensorFHE [\[20\]](#page-8-18)
    by using *Tensor Cores* to boost the overall performance for butterfly sparsity
    computing, but it didn''t make any compression on sparse elements and actually
    introduced a mass of *redundant computation* with poor energy efficiency. MaPU
    [\[21\]](#page-8-19) introduced multi-granularity parallel memory banks to implement
    flexible vector splicing and the diversity of accessing strides, but the data
    shuffling latency remains unsolved between different butterfly stages. The work
    [\[8\]](#page-8-6) proposed butterfly sparsity network with *comprehensive accuracy
    evaluations* as well as a software and hardware co-design to speed up the attention
    workloads, but it is fixed as a customized architecture with single concatenation
    on butterfly acceleration, lacking of *architectural reconfigurability*. Other
    customized designs using pipelinestage parallelism [\[22\]](#page-8-20) also *lack
    the flexibility* for other various tensor-based operations in neural networks
    (NNs) besides the butterfly computation.


    # III. DATAFLOW ARCHITECTURE ORCHESTRATION FOR BUTTERFLY SPARSITY


    In this section, we first introduce the substrate of our reconfigurable dataflow
    architecture as the basis of our orchestration method. Based on the accessing
    inefficiency in butterfly matrices computation as profiled in Fig. [2,](#page-0-1)
    we introduce layer-level flowing operation in DFG for butterfly element swapping
    and construct a multilayer DFG to better orchestrate the data reuse in consecutive
    butterfly matrices multiplications.


    # *A. The Substrate of Reconfigurable Dataflow Architecture*


    Reconfigurable spatial architecture [\[23\]](#page-8-21) with data-driven execution
    shows superiority for its *computational efficiency*


    <span id="page-3-0"></span>![](_page_3_Figure_0.jpeg)


    (a) A verification case of the butterfly swaps in 16 points. (b) Multilayer butterfly
    DFG execution with node streaming inside PE unit. (c) Sufficient data flowing
    between PEs on mesh NoC.


    Fig. 7: Multilayer dataflow orchestration exploiting data reuse on spatial PE
    array.


    and *flexibility*. As shown in Fig. [6,](#page-2-2) customized PE units are connected
    with a mesh-like network onto chip (NoC) to implement diverse data reuse. Dataflow
    graphs (DFG) compiled from workloads can be mapped on PE array with optimized
    strategies, which can flexiblely cover various operators in NNs [\[15\]](#page-8-13).
    Data dependence relationship between two nodes in DFG is explicitly *partial ordering*
    - from upstream (PE00) to downstream (PE03). Input vector flows from scratchpad
    memory (SPM) into the PE array by *LOAD* operations, and arrows indicate the data
    transfer of *COPY* operations to satisfy the data dependence of calculation -
    *CAL* inside PEs. PE units are simply designed with lightweight function units
    for the consideration of low energy consumption, while *single instruction multiple
    data* (SIMD) is introduced to improve calculation parallelism for boosting computation
    performance.


    #### *B. Butterfly Swap in Partial Ordering DFG*


    Fig. [5a](#page-2-3) depicts the dataflow of butterfly sparsity computation conducted
    through two matrices B<sup>i</sup> . Each butterfly calculation is encapsulated
    as a single DFG node mapped onto a PE. These nodes require two input elements
    of vectors X<sup>i</sup> and the prestored static weights of matrices B<sup>i</sup>
    , and they produce two output elements of vector Xi+1. Half of the output elements
    of a DFG node should be swapped with another node situated on the other PE. However,
    this mutual swap between DFG nodes in Bi''s multiplying is not in coordinate with
    the *partial ordering relation* defined in DFG, because the *dependence relation
    of upstream and downstream* is not explicit in PE00 and PE01.


    To tackle this incoordination of partial ordering relation in the butterfly DFG,
    we extend the graph nodes into multiple layers, rearranging the butterfly swaps
    as Fig. [5b.](#page-2-3) In the first layer, two elements needed for inner calculations
    are fetched from SPM. Subsequently, each node retains half of the elements through
    a local transfer *COPY I* inside PE unit, while the other elements flow to the
    node of the next layer via a remote transfer *COPY T* at the sequential distances
    of 1, 2, 4, 8, and so forth. Fig. [7a](#page-3-0) shows a simplified case of this
    flowing process. Vector elements from 0 to 15 are distributed among 8 PEs in order.
    In *stage 1*, swaps are applied to elements 1 and 2, 5 and 6, ..., 13 and 14.
    *In stage 3*, final swaps are performed on elements 4 and 8, 5 and 9, ..., 7 and
    11.


    # IV. MULTILAYER DATAFLOW ORCHESTRATION FOR BUTTERFLY SPARSITY


    #### *A. Mapping Multilayer Dataflow onto PE array*


    The principle of mapping the multilayer DFG of butterfly matrices multiplication
    onto PE array is to ensure workload balancing as well as sufficient data reusing.
    A computation example of a 32-point vector on a 4 × 4 array is illustrated. As
    the 6-layer DFG shown in Fig. [7b,](#page-3-0) graph nodes of different layer
    stages are distributed across the PE array in balance. Each PE accommodates one
    node per stage with a total of six. Colored arrows between nodes of different
    layers in Fig. [7b,](#page-3-0) manifest as the arrows with the same color between
    PEs connected by the mesh NoC in Fig. [7c.](#page-3-0) Therefore, this DFG mapping
    implements multilayer data flowing on the whole PE array with layer-level dependence
    between graph nodes, while sufficiently utilizing all the vertical and horizontal
    data paths of NoC in full throughput for efficient data reusing.


    Within each PE, the stacked nodes of all layers create a datadriven effect as
    streaming-like execution. For attention-based workloads, data dimensions of *batch
    size* and *head dimension* can provide data parallelism to pour adequate *graph
    iterations* into the multilayer DFG in a pipelined way, which can boost the overall
    computation throughput within the PE array. It should be noted that under this
    array mapping method with quadratic combination distance (1,2,4,...), the graph
    node in a PE paired with another node at a distance greater than 16 for butterfly
    swap is wrapped back to the same PE (e.g., PE1 pairs with PE17%16=PE1), as the
    black arrows from layer 5 to layer 6 in Fig. [7b.](#page-3-0) Therefore, butterfly
    swaps in later stages actually happen within the same PE, so further remote transfer
    is avoided. This careful orchestration of multilayer data dependence effectively
    avoids the massive concurrent data shuffling of different butterfly stages in
    shared SPM storage.


    #### V. PARALLELISM EXPLOITATION AND SCALABILITY


    #### *A. Coarse-grained Streaming with Decoupled Units*


    Unlike single-sequence scenarios found in *signal processing*, the additional
    data dimensions *(batch, head, hidden)* in attention workloads enable sufficient
    data parallelism and ensure nodes streaming with different iterations in a PE,
    as mentioned above. To manage this streaming parallelism, a *coarse-grained block-level*
    scheduler is introduced in the PE''s *controlUnit*. Since tensor-based attention
    workloads have explicitly computational certainty (with few branch control), their
    instructions can be securely arranged into sequential *Micro Code Blocks*, corresponding
    to the four function units as {*Load, Flow, Cal, Store*}, as the colored code
    blocks shown in Fig. [8.](#page-4-0)


    Each code block is scheduled, and it monopolizes the corresponding function unit
    until firing all the instructions. In Fig. [8,](#page-4-0) the indexes of *layer*
    and *iteration* belonging to each block are concatenated as a priority bit string
    - {*Layer idx, Iter idx*} to be judged by the block scheduling strategy in *controlUnit*.
    The function unit scheduler compares the bit strings of all the running blocks
    and picks out the one with the smallest string number. This priority strategy
    ensures more DFG iterations to stream in for better *exploiting block-level parallelism*
    as well as exhausting the utilization of all function units. Therefore, by scheduling
    coarse-grained code blocks instead of dispatching fine-grained instructions, four
    function units are decoupled from the instruction-level data dependence, and the
    arbitration logic inside *controlUnit* can be simplified, which pursues the light-weight
    scheduling design philosophy for the consideration towards *better energy efficiency*.


    #### *B. Butterfly Sparsity of Long Vector Using Multi-stage DFGs*


    In terms of the butterfly sparsity computation for longer vectors, on the basis
    of the *Cooley&Tukey algorithm* [\[19\]](#page-8-17), a multi-dimensional computation
    method with stage division is proposed to improve the data scalability of our
    design. The largest scale of the vector X<sup>i</sup> to be computed within a
    single multilayer DFG of BPMM or FFT is limited by the architecture resource in
    the dataflow substrate, like the SPM capacity or the register number in PE. In
    our design, the maximum DFG scale that can be mapped on the PE array is 256 for
    FFT (*complex number*) and 512 for BPMM (*real number*).


    Fig. [9](#page-4-1) illustrates the division process of an 8192-point vector example
    on our architecture. ❶ The 8192-point vector Xl=8192 is firstly reshaped as a
    128 × 64 matrix Ar=128,c=64. ❷ Secondly, the first-stage butterfly computation
    executed as a 128-scale DFG1 is applied on the columns of Ar,c. ❸ Next, a synchronization
    barrier waiting for the finish of all iterations in DFG1 is set, ensuring all
    the intermediate data flowing back to


    <span id="page-4-0"></span>![](_page_4_Figure_5.jpeg)


    Fig. 8: Coarse-grained scheduling with decoupled function units.


    the SPM under the resource constraint on PE array. ❹ Further, twiddle factors
    ω kn<sup>2</sup> <sup>N</sup> are multiplied on Ar,c in the form of an *element-wise
    computation* layer (which is necessary for FFT but needless for BPMM). ❺ Finally,
    the third stage computation of a 64-scale DFG2 is executed on the rows of Ar,c.


    To compute an even longer vector with the data scale so large that can not fit
    into the SPM storage (e.g., a 64K vector whose sparsity weights occupy 8.4MB storage,
    while the SPM capacity in our design is 4MB.), the multi-stage reshaping method
    still works. For example, the 64K vector can be reshaped as a 256 × 256 matrix.
    The two-stage 256-scale DFG1 and DFG2, each of which covers half of the butterfly
    sparsity weights, can be launched in two times on the PE array, with their weights
    or twiddle factors swapping between SPM storage and DDR.


    In terms of BPMM, the linear weight matrices of Q, K, V or *FFN* may have different
    input and output hidden sizes, so they should be sliced for applying butterfly
    sparsity decomposition. As shown in Fig. [10,](#page-5-0) if the hidden size of
    the input vector X<sup>i</sup> is larger than the size of the output vector Y<sup>i</sup>
    , both the weight matrix W<sup>i</sup> and the X<sup>i</sup> will be sliced into
    multiple pieces. Each sliced Wi,j makes decomposition as the butterfly matrices
    Bi,j and will be multiplied by the vector piece Xi,j . All these multiplied products
    will be summed up as the final result Y<sup>i</sup> . Otherwise, in the smaller
    situation, the product piece of Yi,j (produced by the decomposed Bi,j and the
    shorter-sized Xi) will be concatenated as the longer-sized output Y<sup>i</sup>
    .


    #### *C. Transpose-free SIMD Parallelism with Multi-line SPM*


    To utilize the adequate data parallelism provided by the data dimension of attention
    layers, the calculation units in our PEs are designed with SIMD to boost the overall
    performance. However, the mentioned multi-stage Cooley&Tukey method introduces
    SIMD alignment inconsistency in column and row stages. As the reshaped matrix
    shown in Fig. [9,](#page-4-1) the DFG1 com-


    <span id="page-4-1"></span>![](_page_4_Figure_12.jpeg)


    Fig. 9: Transpose-free 2D Cooley-Tukey factoring method using multiline SPM for
    larger scale butterfly computation.


    <span id="page-5-1"></span>


    | Platform Comparison           | Jetson Nano                      | SOTA Butterfly
    Acc [8]  | Jetson Xavier NX    | Multilayer Dataflow    |  |

    |-------------------------------|----------------------------------|-------------------------|---------------------|------------------------|--|

    | Frequency                     | 921MHz                           | 200MHz (FPGA)           |
    1.1GHz              | 1GHz                   |  |

    | Performance (fp16)            | 471.6 GFLOPs                     | 204.8 GFLOPS
    (512 MACs) | 1.69 TFLOPS (CUDA), | 1.02 TFLOPS (512 MACs) |  |

    |                               |                                  |                         |
    11 TFLOPS (Tensor)  | 256 GFLOPS (128 MACs)  |  |

    | Bandwidth Supply              | 25.6 GB/s                        | 21.3 GB/s               |
    59.71 GB/s          | 25.6x2 GB/s            |  |

    | Technology                    | 20nm                             | 28nm                    |
    12nm                | 12nm                   |  |

    | Power Consumption             | 10W                              | 11.355W                 |
    15W                 | 6.95W (DC Synthesized) |  |

    | Benchmark                     | Platform Selected for Comparison |                         |                     |                        |  |

    | VIT and BERT (BPMM and FFT)   |                                  |                         |
    ✔                   | ✔                      |  |

    | FABNet-Base Transformer       | ✔                                | ✔                       |                     |
    ✔                      |  |

    | One-layer Vanilla Transformer |                                  | ✔                       |                     |
    ✔                      |  |


    TABLE I: Platform Comparison and Benchmark.


    <span id="page-5-0"></span>![](_page_5_Figure_2.jpeg)


    Fig. 10: Weight matrix slicing with unequal input and output hidden size in BPMM
    computation.


    putes in columns and accesses SIMD in rows, while the DFG2 computes in rows and
    accesses SIMD in columns. To unify these two accessing patterns while avoiding
    redundant matrix transpose operation, a multi-line SPM design is introduced. Firstly,
    the entry width of SRAMs in SPM is set to SIMD16, matching the calculation width
    in the PE. And the address mapping of SRAM entries is interleaved among 4 banks
    in SPM unit, to utilize bank-level parallelism [\[24\]](#page-8-22). Secondly,
    each bank is architected with eight lines, so two banks can be accessed in parallel
    to give out SIMD16 from all lines. In Ar,c, elements of SIMD16 in rows can be
    accessed entry by entry from single SRAM, while elements in columns are scattered
    among 16 lines in banks in the form of {*element0*7→*bank0 line0, e1*7→*b0 l1,
    e2*7→*b0 l2, ..., e8*7→*b1 l0, e9*7→*b1 l1, ..., e15*7→*b1 l7*}.


    Under this data organization in SPM, SIMD alignments for butterfly sparsity in
    various data scales are achieved, as shown on the right of Fig. [9.](#page-4-1)
    A. The first alignment of the column butterfly in DFG1 is trivially satisfied
    by using a normal *rowwise* load operation in SRAM entries. B. The second alignment
    of the row butterfly in DFG2 requires a particular *column-wise* load to gather
    16 elements from lines. C. Moreover, in terms of the vectors in small scales (e.g.,
    shorter than 256 and no need for matrixing) but in multiple batches, their data
    layout is actually identical to the *column-wise* accessing in DFG2. These short
    vectors can be scattered among SPM lines so that the *batch dimension* can be
    aligned to the SIMD lanes. This multiline SPM architecture is friendly with the
    continuous burst data transfer of DMA and has high bandwidth utilization of DDR.


    #### VI. EXPERIMENTS


    #### *A. Experiment Settings*


    Methodology: We develop a parallel cycle-accurate simulator based on PDES [\[25\]](#page-8-23)
    framework for the performance statistic of running butterfly attentions on the
    dataflow substrate. An RTL fabric of the dataflow substrate equipped with coarsegrained
    scheduling is implemented and synthesized with Synopsys Design Compiler and 12nm
    TSMC standard library at 1 GHz frequency, for the evaluation of hardware resource
    and power consumption. The cycle error rate of the simulator is calibrated within
    7% over the RTL design. All butterfly sparsity kernels in various data scales
    are assembled with an optimized DFG template to generate the dependence relations
    and the micro codes of the graph blocks mapped onto the PE array.


    Baselines and benchmark: A. We first make performance comparison over NVIDIA -
    Jetson Xavier NX equipped with powerful tensor cores to evaluate the computation
    efficiency of butterfly sparsity running on multilayer dataflow design. We implement
    the butterfly kernels of FFT replacing sof tmax(qk)∗v as well as BPMM replacing
    *linear layers* in both VIT and BERT as the first benchmark. B. Next, we select
    the work [\[8\]](#page-8-6) as the state-of-the-art (SOTA) baseline to evaluate
    the performance and utilization improvement of our architectural superiority in
    attention computations, taking the original butterfly sparsity network - *FABNet-Base
    Transformer* in [\[8\]](#page-8-6) as the second benchmark, while using NVIDIA
    Jeston Nano as the normalized object for speedup ratio comparisons. Experiment
    results of the GPU platform are obtained from the NVIDIA profiling tool - Nsight
    Compute. Detailed parameter and configurations are listed in Table. [I.](#page-5-1)
    The butterfly kernels implemented on GPU are based on the *cuFFT library*, and
    all the attention workloads focus on *forward inference*.


    #### *B. Model Accuracy with Butterfly Sparsity*


    Accuracy estimations of model VIT with BPMM on linear weights q, k, v and *FFN*
    as well as FFT on *Attention matrices*


    <span id="page-5-2"></span>![](_page_5_Figure_12.jpeg)


    Fig. 11: VIT Model trained with butterfly sparsity after 1000 epochs.


    <span id="page-6-0"></span>TABLE II: Accuracy of bert and LLaMa-DeepSeekCoder
    using compression.


    | BERT, N=512 | Original | 1 Layer | 3 Layers | 6 Layers | 9 Layers | 12 Layers
    |

    |-------------|----------|---------|----------|----------|----------|-----------|

    | F1          | 86.93    | 87.04   | 86.52    | 86.6     | 85.77    | 84.93     |

    | Exact       | 78.37    | 78.24   | 77.81    | 77.87    | 76.96    | 75.92     |

    | LLaMa, N=4K | Original | L1-L3   | L3-L5    | L5-L7    | L7-L9    | L9-L12    |

    | Similarity  | 76.86    | 75.32   | 74.45    | 75.33    | 75.92    | 75.30     |

    | Exact Match | 42.18    | 39.42   | 38.56    | 39.07    | 40.62    | 39.41     |

    | Avg STDs    |          | 1.22    | 2.60     | 2.27     | 0.95     | 1.91      |


    are shown in Fig. [11.](#page-5-2) The overall accuracies of top-1 and top-5 have
    limited loss of less than 2.6% over the original VIT network, except for the case
    with all linear layers in q, k, v, and *FFN* replaced by BPMMs. An unexpected
    result is that the case of q, k, v in 6 attention layers replaced by BPMMs has
    accuracy improvements from 76.82% and 90.22% to 79.32% and 91.56%, after training
    with 1000 epochs. Also there is slight accuracy improvement (76.97% and 90.76%)
    in the case replaced with 6 FFT on attention layers. These results highlight the
    weight compression effect by introducing butterfly sparsity to speed up model
    convergence, and there can be a tradeoff between performance and accuracy in layer-level
    prunning. As shown in Table. [II.](#page-6-0) Applying comression on a certain
    amount of layers do not greatly affect accuracy. About the generative model LLaMa,
    the frontal layers from 1 to 12 of it are evaluated to have smaller STDs instead,
    compared with the remaining 20 layers. We apply consecutive 3-layer Fourier compressions
    on different layer segments with fine-tuning and evaluate the model metrics. The
    results basically consist with BERT, implying that it''s effective by using STD
    indication to help for selecting model layers and scaling truncation.


    ### *C. Accessing Inefficiency Alleviation with Dataflow*


    <span id="page-6-1"></span>![](_page_6_Figure_4.jpeg)


    Fig. 12: Data accessing requirement percentages of cache and SPM for the GPU and
    multilayer dataflow.


    Data-accessing throughput percentages of the cache in Jetson Xavier NX and the
    SPM in our multilayer dataflow are shown in Fig. [12.](#page-6-1) The accessing
    requirement of L1 cache in NX is over 20% (up to 53.80%), while the requirement
    is over 40% (up to 71.19%) for L2 cache. Both of them increase with the sequence
    scale at greater than 512 because of the more frequent data shuffling with larger
    accessing strides in the later butterfly stages on GPU. However, thanks to the
    multilayer DFG orchestration bringing sufficient data reuse on PE array and the
    multi-bank SPM design avoiding extra transpose operation, the overall accessing
    requirement from SPM is compressed to below 12.48%, which shows the alleviation
    effect of our dataflow method on the accessing bottleneck in butterfly sparsity
    computation.


    # *D. Effectiveness of Coarse-grained Decoupling*


    The utilization of four decoupled units is illustrated in Fig. [13.](#page-6-2)
    Although the unit executions are scheduled by code


    <span id="page-6-2"></span>![](_page_6_Figure_9.jpeg)


    (b) Decoupled units utilization of BPMM on linear layers.


    Fig. 13: The effectiveness of the coarse-grained decoupled units design for butterfly
    sparsity computation.


    <span id="page-6-3"></span>![](_page_6_Figure_12.jpeg)


    Fig. 14: CalUnit utilization in different sequence divisions.


    blocks in a coarse-grained manner, the multilayer DFG mapped onto the PE array
    provides adequate block-level parallelism to hide the accessing latency from SPM
    and the switching latency between iterations. Under this scheduling, the utilization
    of *calculation units* is over 64% for all butterfly kernels and above 89% for
    FFT in large scales. For the other data units, however, the utilization of *Load*
    is less than 6% for FFT and 8% for BPMM, which indicates the sufficient data reuse
    of our orchestration method that avoids remote data fetching. What differentiates
    BPMM from FFT kernels is the utilization of lower *Flow* and higher *Load* in
    the same sequence scales. The real-valued BPMM has lower arithmetic density and
    needs more element loading of input vectors and weights, while the complex-multiplication
    FFT requires twice *Flow* operations (20.45% on average) to swap the real and
    imaginary parts of the intermediate X<sup>i</sup> . This overall calculation efficiency
    among BPMM and FFT in different scales demonstrates the coarsegrained scheduling
    effectiveness in butterfly dataflow execution.


    # *E. Scalability with Different Stage Division*


    For long vectors, there are different stage division schemes to scale down the
    DFGs of butterfly kernels on PE array, as evaluated in Fig. [14.](#page-6-3) The
    best divisions respectively for BPMM-2k, 4k and 8k, with the highest *calUnit*
    utilization are 32\*64 (85.03%), 64\*64 (85.38%), 128\*64 (84.08%), which indicates
    the balancing tendency for the better performance in BPMM. This balanced division
    is also appropriate for FFT in large scales. These results make sense that since
    the multilayer DFG execution implements data reuse by data flowing on the mesh
    NoC, there will be inadequate reuse for the shallow stage (e.g.,


    <span id="page-7-0"></span>![](_page_7_Figure_0.jpeg)


    Fig. 15: Execution time comparison over Jetson Xavier NX with tensor (dense) and
    cuda (butterfly-sparse) cores.


    16 points) to overlap the data fetching latency in an unbalanced division, in
    which a marginal effect may happen to the other deep stage (e.g., 256 points).


    #### *F. Attention Performance over GPU*


    Execution times of attention kernels on Jetson Xavier NX and dataflow design are
    illustrated in Fig. [15.](#page-7-0) The *dense* prefix represents the attention
    kernel running without any sparsity on NX GPU. The *AT-to qkv* and *FFN-Lx* are
    the linear layers that are sparse with BPMM, while *AT-all* is the whole attention
    layer that is sparse with 2D-FFT. In comparison to Jetson Xavier NX with powerful
    *tensor cores* (11 TFLOPs) running dense attention kernels, our design running
    butterfly kernels achieves a speedup of up to 14.34× (11.13× on average) for VIT
    kernels and up to 8.42× (7.45× on average) for BERT. The speedup effects on the
    *AT-all* kernels with 2D-FFT sparsity are more obvious than others with BPMM,
    because the FFT replacement on attention layers has higher sparsity than BPMM
    while our design has equally excellent calculation efficiency on both FFT (over
    89%) and BPMM.


    In comparison to Jetson Xavier NX with pure *cuda cores* (1.69 TFLOPs) also running
    butterfly kernels, our design (1.02 TFLOPs) achieves a speedup of up to 1.82×
    (1.78× on average) for VIT under the 1.67× peak performance gap in between. The
    average speedup is 1.97× for BERT of larger 64k sequence scales, and the heaviest
    attention kernel - *BERT-AT-all (64K sequences, 1K hidden)* with FFT sparsity
    has the maximum speedup of 3.30× on the multilayer dataflow. This kernel is executed
    in 3-stage FFT of one-time 1K-point FFT on *hidden* and two-time 256-point FFT
    on *sequence*, on the basis of the scalable stage division method. This division
    ensures adequate point scale for sufficient flowing on PE array in each stage,
    while avoiding frequent data shuffling with huge strides in SPM. The speedup results
    show the excellent computational efficiency with dataflow orchestration for butterfly
    sparsity.


    #### *G. Hardware Overhead and Energy Efficiency*


    The hardware overhead of our dataflow design is shown in Table. [III.](#page-7-1)
    The PE unit takes up a synthesized area of 0.985mm<sup>2</sup> at 12nm process,
    and the overall power consumption of the 16 PE array evaluated by DC is 6.95W.
    Under this hardware configuration, energy efficiency comparison with Jetson Xavier
    NX is shown in Fig. [16.](#page-7-2) Dataflow design achieves energy efficiency
    increment from 6.38× to 12.32× for dense kernels running with tensor cores, and
    from 2.17× to 8.06× over the butterfly kernels with CUDA cores. The attention
    kernel of FFT with higher arithmetic density acquires higher


    <span id="page-7-1"></span>TABLE III: Synthesized area and power consumption of
    PE unit.


    | )<br>ActivePower(mW) | ActivePower(%)                                 |

    |----------------------|------------------------------------------------|

    | 6.37                 | 1.47                                           |

    | 62.21                | 14.31                                          |

    | 2.58                 | 0.59                                           |

    | 9.23                 | 2.12                                           |

    | 32.13                | 7.39                                           |

    | 322.16               | 74.11                                          |

    |                      | 100                                            |

    |                      | Cell Area(mm2<br>434.68 (6.95W for PE16 array) |


    <span id="page-7-4"></span>TABLE IV: Latency and energy comparisons over other
    architectures.


    | Accelerators         | SpAtten [26] | DOTA [10]   | SOTA Acc    | Our work          |

    |----------------------|--------------|-------------|-------------|-------------------|

    | Technology           | ASIC (40nm)  | ASIC (22nm) | FPGA (28nm) | ASIC (12nm)       |

    | Frequency            | 1 GHz        |             | 200 MHz     | 1 GHz             |

    | Number of MACs       | 128          |             | 640         | 128               |

    | Latency (ms)         | 48.8         | 34.1        | 2.4         | 2.06              |

    | Throughput(Pred./S)  | 20.49        | 29.32       | 416.66      | 485.43            |

    | Power (W)            | 1.06         | 0.858       | 11.355      | 3.94 (SIMD8
    PE16) |

    | Energy Eff.(Pred./J) | 19.33        | 34.18       | 36.69       | 123.21            |


    energy efficiency on our design. This efficiency advancement relies highly on
    the superb data reuse on PE array and the lightweight coarse-grained scheduling
    design inside PE.


    #### *H. Comparison with SOTA Works*


    Since the SOTA butterfly accelerator is evaluated on FPGA platform with limited
    performance (204.8 GFLOPs) and bandwidth supply (21.3 GB/s), we scale down the
    number of MACs from 512 to 128, and halve the DDR channel on our design to keep
    fair performance comparison while taking Jetson Nano as the normalized speedup
    object. We select the *FABNet transformer* [\[8\]](#page-8-6) as the workload
    which consists of 2D-FFT attention layers and BPMM FFN layers in sequence scales
    of 128, 256, 512 and 1K. As shown in Fig. [17,](#page-7-3) our dataflow design
    has speedup ratios from 5.27× to 11.13× in comparison to the speedups from 3.5×
    to 7.1× of the SOTA butterfly accelerator, gaining a performance increment from
    1.44× to 1.59× under the slight peak performance gap (256 versus 204.8 GFLOPs).
    The highest increment goes to the FABNet-512 whose working set size just fills
    up the SPM capacity (4MB) in our design scale, so all butterfly stages are executed
    in place, while extra DMA transfer from DDR is avoided and data reuse is exploited
    at most within PE array and SPM.


    We make overall end-to-end performance and energy efficiency comparison over SOTA
    and other accelerators that


    <span id="page-7-2"></span>![](_page_7_Figure_16.jpeg)


    ![](_page_7_Figure_17.jpeg)


    <span id="page-7-3"></span>![](_page_7_Figure_18.jpeg)


    Fig. 17: Speedup comparisons over SOTA butterfly sparsity accelerator with the
    normalized object - Jetson Nano.


    are designed towards other dynamic sparsity variants, under the premise of the
    same peak performance. Part of the evaluation results are quoted from the work
    [\[8\]](#page-8-6). We select *LAR-Image* dataset [\[27\]](#page-8-25) on a one-layer
    vanilla transformer [\[3\]](#page-8-2) (1K-sequence and 1K-hidden) which is applied
    butterfly sparsity with 2D-FFT on attention matrix and BPMMs on twolayer FFN,
    as our benchmark. Input sequences are supplied in batch-256 and streamed in one-by-one
    from DDR, which ensures the sufficient overlapping of DMA transfer and PE array
    computation. The *average execution time* of the sequence batch is estimated as
    the latency result. As shown in Table. [IV,](#page-7-4) owing to the computation
    reduction of butterfly sparsity and the dataflow orchestration, our design achieves
    23.69× and 16.56× latency reduction, as well as 6.37× and 3.60× energy efficiency
    over SpAtten [\[26\]](#page-8-24) and DOTA [\[10\]](#page-8-8). In terms of the
    SOTA butterfly work, our design achieves 1.17× speedup and 3.36× energy efficiency.
    It should be emphasized that our design is on the basis of a *reconfigurable dataflow
    architecture* that has the flexibility from programmability to cover other NN
    operators or attention computation with other structured sparsity, unlike the
    dedicated ASIC architectures evaluated above.


    #### VII. CONCLUSION


    In this paper, we proposed multilayer dataflow orchestration on reconfigurable
    dataflow architecture supported with energyefficient coarse-grained streaming
    parallelism, to accelerate butterfly sparsity computation for attention workloads.
    The experiments show that compared to NVIDIA Jetson Xavier NX with tensor cores,
    our design has a speedup of up to 14.34× (9.29× on average) as well as 12.3× energy
    efficiency advancement, owing to the computation efficiency of butterfly sparsity
    and the combined optimization of hardware architecture and dataflow mapping. In
    comparison with SOTA butterfly work, our design has 1.17× speedup and 3.36× energy
    efficiency improvement at the same peak performance.


    #### REFERENCES


    - <span id="page-8-0"></span>[1] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn,
    X. Zhai *et al.*, "An image is worth 16x16 words: Transformers for image recognition
    at scale," in *9th International Conference on Learning Representations, ICLR
    2021*, 2021.

    - <span id="page-8-1"></span>[2] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit
    *et al.*, "Attention is all you need," in *Advances in Neural Information Processing
    Systems 30: Annual Conference on Neural Information Processing Systems 2017*.

    - <span id="page-8-2"></span>[3] J. Devlin, M. Chang, K. Lee, and K. Toutanova,
    "BERT: pre-training of deep bidirectional transformers for language understanding,"
    in *NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short
    Papers)*, 2019.

    - <span id="page-8-3"></span>[4] M. Pagliardini, D. Paliotta, M. Jaggi, and F.
    Fleuret, "Fast attention over long sequences with dynamic sparse flash attention,"
    in *Advances in Neural Information Processing Systems 36: Annual Conference on
    Neural Information Processing Systems NeurIPS 2023*.

    - [5] J. Dass, S. Wu, H. Shi, C. Li, Z. Ye, Z. Wang, and Y. Lin, "Vitality: Unifying
    low-rank and sparse approximation for vision transformer acceleration with a linear
    taylor attention," in *IEEE International Symposium on High-Performance Computer
    Architecture, HPCA 2023*.

    - <span id="page-8-4"></span>[6] Z. Chen, Z. Qu, Y. Quan, L. Liu, Y. Ding, and
    Y. Xie, "Dynamic N: M fine-grained structured sparse attention mechanism," in
    *Proceedings of the 28th ACM SIGPLAN Annual Symposium on Principles and Practice
    of Parallel Programming, PPoPP 2023*.

    - <span id="page-8-5"></span>[7] T. Dao, B. Chen, N. S. Sohoni, A. D. Desai, M.
    Poli *et al.*, "Monarch: Expressive structured matrices for efficient and accurate
    training," in *International Conference on Machine Learning, ICML 2022*, ser.
    Proceedings of Machine Learning Research.

    - <span id="page-8-6"></span>[8] H. Fan, T. Chau, S. I. Venieris, R. Lee, A. Kouris
    *et al.*, "Adaptable butterfly accelerator for attention-based nns via hardware
    and algorithm co-design," in *55th IEEE/ACM International Symposium on Microarchitecture,
    MICRO*. IEEE, 2022, pp. 599–615.

    - <span id="page-8-7"></span>[9] N. Kitaev, L. Kaiser, and A. Levskaya, "Reformer:
    The efficient transformer," in *8th International Conference on Learning Representations,
    ICLR 2020*.

    - <span id="page-8-8"></span>[10] Z. Qu, L. Liu, F. Tu, Z. Chen, Y. Ding, and
    Y. Xie, "DOTA: detect and omit weak attentions for scalable transformer acceleration,"
    in *ASPLOS ''22: 27th ACM International Conference on Architectural Support for
    Programming Languages and Operating Systems*.

    - <span id="page-8-9"></span>[11] T. J. Ham, S. Jung, S. Kim, Y. H. Oh, Y. Park
    *et al.*, "A3: Accelerating attention mechanisms in neural networks with approximation,"
    in *IEEE International Symposium on High Performance Computer Architecture, HPCA
    2020*.

    - <span id="page-8-10"></span>[12] T. Dao, A. Gu, M. Eichhorn, A. Rudra, and C.
    Re, "Learning fast algo- ´ rithms for linear transforms using butterfly factorizations,"
    in *Proceedings of the 36th International Conference on Machine Learning, ICML
    2019*.

    - <span id="page-8-11"></span>[13] J. Lee-Thorp, J. Ainslie, I. Eckstein, and
    S. Ontan˜on, "Fnet: Mixing ´ tokens with fourier transforms," in *Proceedings
    of the 2022 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, NAACL*.

    - <span id="page-8-12"></span>[14] N. P. Jouppi, C. Young, N. Patil, D. A. Patterson,
    G. Agrawal, R. Bajwa *et al.*, "In-datacenter performance analysis of a tensor
    processing unit," in *Proceedings of the 44th Annual International Symposium on
    Computer Architecture, ISCA 2017*.

    - <span id="page-8-13"></span>[15] R. Prabhakar, Y. Zhang, D. Koeplinger, M. Feldman,
    T. Zhao *et al.*, "Plasticine: A reconfigurable architecture for parallel paterns,"
    in *Proceedings of the 44th Annual International Symposium on Computer Architecture*,
    ser. ISCA ''17.

    - <span id="page-8-14"></span>[16] R. Child, S. Gray, A. Radford, and I. Sutskever,
    "Generating long sequences with sparse transformers," in *ICLR*, 2019.

    - <span id="page-8-15"></span>[17] I. Beltagy *et al.*, "Longformer: The long-document
    transformer," *CoRR*, 2020.

    - <span id="page-8-16"></span>[18] Z. He, M. Yang *et al.*, "Fourier transformer:
    Fast long range modeling by removing sequence redundancy with FFT operator," in
    *ACL*, 2023.

    - <span id="page-8-17"></span>[19] J. W. Cooley and J. W. Tukey, "An algorithm
    for the machine calculation of complex fourier series," *Mathematics of Computation*,
    1965.

    - <span id="page-8-18"></span>[20] S. Fan, Z. Wang *et al.*, "Tensorfhe: Achieving
    practical computation on encrypted data using gpgpu," in *2023 (HPCA)*, 2023.

    - <span id="page-8-19"></span>[21] D. Wang, X. Du, L. Yin, C. Lin, H. Ma, W. Ren,
    L. Wang *et al.*, "MaPU: A novel mathematical computing architecture," in *2016
    IEEE International Symposium on High Performance Computer Architecture, HPCA*.
    IEEE Computer Society, 2016, pp. 457–468.

    - <span id="page-8-20"></span>[22] M. Garrido, "A survey on pipelined FFT hardware
    architectures," *J. Signal Process. Syst.*, 2022.

    - <span id="page-8-21"></span>[23] V. Govindaraju, C. Ho, T. Nowatzki, J. Chhugani,
    N. Satish, K. Sankaralingam, and C. Kim, "Dyser: Unifying functionality and parallelism
    specialization for energy-efficient computing," *IEEE Micro*, 2012.

    - <span id="page-8-22"></span>[24] X. Tang, M. T. Kandemir, P. Yedlapalli, and
    J. Kotra, "Improving banklevel parallelism for irregular applications," in *49th
    Annual IEEE/ACM International Symposium on Microarchitecture, MICRO 2016*.

    - <span id="page-8-23"></span>[25] J. Wang, D. Jagtap, N. B. Abu-Ghazaleh, and
    D. Ponomarev, "Parallel Discrete Event Simulation for Multi-Core Systems: Analysis
    and Optimization," *IEEE Trans. Parallel Distributed Syst.*, 2014.

    - <span id="page-8-24"></span>[26] H. Wang, Z. Zhang, and S. Han, "Spatten: Efficient
    sparse attention architecture with cascade token and head pruning," in *IEEE International
    Symposium on High-Performance Computer Architecture, HPCA 2021, Seoul, South Korea,
    February 27 - March 3, 2021*, 2021.

    - <span id="page-8-25"></span>[27] Y. Tay, M. Dehghani, S. Abnar, Y. Shen *et
    al.*, "Long range arena : A benchmark for efficient transformers," in *9th International
    Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May
    3-7, 2021*, 2021.'
- title: Hardware for converting floating-point to the microscaling (MX) format
  abstract: 'This paper proposes hardware converters for the microscaling format

    (MX-format), a reduced representation of floating-point numbers. We present an

    algorithm and a memory-free hardware model for converting 32 single-precision

    floating-point numbers to MX-format. The proposed model supports six different

    types of MX-format: E5M2, E4M3, E3M2, E2M3, E2M1, and INT8. The conversion

    process consists of three steps: calculating the maximum absolute value among

    32 inputs, generating a shared scale, and producing 32 outputs in the selected

    MX-format type. The hardware converters were implemented in FPGA, and

    experimental results demonstrate.'
  url: http://arxiv.org/abs/2411.03149v1
  keywords: floating-point formats, microscaling format (MX), FPGA.
  document: "# Hardware for converting floating-point to the microscaling (MX) format\n\
    \nDanila Gorodecky *Instituto Superior Tecnico, Universidade de Lisboa* Lisbon,\
    \ Portugal danila.gorodecky@gmail.com\n\n*Abstract*—This paper proposes hardware\
    \ converters for the microscaling format (MX-format), a reduced representation\
    \ of floating-point numbers. We present an algorithm and a memoryfree hardware\
    \ model for converting 32 single-precision floatingpoint numbers to MX-format.\
    \ The proposed model supports six different types of MX-format: E5M2, E4M3, E3M2,\
    \ E2M3, E2M1, and INT8. The conversion process consists of three steps: calculating\
    \ the maximum absolute value among 32 inputs, generating a shared scale, and producing\
    \ 32 outputs in the selected MX-format type. The hardware converters were implemented\
    \ in FPGA, and experimental results demonstrate.\n\n*Index Terms*—floating-point\
    \ formats, microscaling format (MX), FPGA.\n\n#### I. INTRODUCTION AND BACKGROUND\n\
    \nThe Microscaling (MX) number representation was proposed in 2023 [\\[1\\]](#page-5-0),\
    \ [\\[2\\]](#page-5-1) through a collaboration between AMD, Arm, Intel, Meta,\
    \ Microsoft, NVIDIA, and Qualcomm. This format aims to minimize hardware costs\
    \ and storage requirements via reduced bit-width, while maintaining the efficiency\
    \ and accuracy needed for machine learning training and inference tasks. MX-formats\
    \ provide a unified representation for half-precision (FP16), single-precision\
    \ (FP32), BFloat16, and others formats by using truncated binary formats with\
    \ a sign bit, up to 5 bits for the exponent, and up to 6 bits for the mantissa.\
    \ A key feature of the MX-format is its use of a shared scale factor that represents\
    \ the exponent of the floating-point format.\n\nMX-format is the modified floating-point\
    \ representation with w-bit shared scale X (represented in exponent format) and\
    \ privet n element P<sup>i</sup> , where i = 1, 2, . . . , n. Fig. [1](#page-0-0)\
    \ shows the mapping of 32-bits floating point numbers (FP32) V1, V2, . . . , V<sup>32</sup>\
    \ to MX-format X, P1, P2, . . . , P32.\n\nMX-format is characterized by the w-bit\
    \ shared scale X and private elements P1, P2, . . . , Pn. This paper considers\
    \ six types of MX-format. The bit-with of the shared scale depends on the type\
    \ of MX. The general form of a type is represented as EKMR, where \"K\" and \"\
    R\" are the number of bits of exponent (EK on Fig. [1\\)](#page-0-0) and mantissa\
    \ (MR on Fig. [1\\)](#page-0-0) respectively. The sign bit (S on Fig. [1\\)](#page-0-0)\
    \ is mentioned by default.\n\nThis paper considers the following types of MX-format:\
    \ E5M2, E4M3, E3M2, E2M3, E2M1, and INT8, which are the ones currently considered\
    \ for possible standardization. The bit range of the shared scale X for FP32 in\
    \ the mentioned\n\nLeonel Sousa *Instituto Superior Tecnico, Universidade de Lisboa*\
    \ Lisbon, Portugal las@inesc-id.pt\n\n<span id=\"page-0-0\"></span>![](_page_0_Figure_11.jpeg)\n\
    \nFig. 1: The ratio between FP32 and MX-format: S1, S2, . . . , S<sup>32</sup>\
    \ - sign bits, EV1, EV2, . . . , EV<sup>32</sup> - exponent and MV1, MV2, . .\
    \ . , MV<sup>32</sup> - mantissa of V1, V2, . . . , V32, X - shared scale, EK1,\
    \ EK2, . . . , EK<sup>32</sup> - exponent and MR1, MR2, . . . , MR<sup>32</sup>\
    \ - mantissa of P1, P2, . . . , P32, where K and R - nimber of bits of exponent\
    \ and mantissa, respectively, of MX-format\n\ntypes of MX-format is 8, i.e. w\
    \ = 8. For example, for n = 2 and E5M2, V1, V<sup>2</sup> in FP32 correspond to\
    \ two private elements P1, P<sup>2</sup> that consist of 8-bit, i.e. 1 sign bit,\
    \ 5-bit exponent, and 2-bit mantissa each. The bit-width of the shared scale,\
    \ sign bit, exponent, and mantissa for the considered formats are shown in Table\
    \ [I.](#page-0-1)\n\n<span id=\"page-0-1\"></span>TABLE I: Bit-width of the shared\
    \ scale (X), sign bit S, exponent EK, and mantissa MR for E5M2, E4M3, E3M2, E2M3,\
    \ E2M1, and INT8\n\n|      | X | S | EK | MR |\n|------|---|---|----|----|\n|\
    \ E5M2 |   |   | 5  | 2  |\n| E4M3 |   |   | 4  | 3  |\n| E3M2 | 8 | 1 | 3  |\
    \ 2  |\n| E2M3 |   |   | 2  | 3  |\n| E2M1 |   |   | 2  | 1  |\n| INT8 |   | \
    \  | 1  | 6  |\n\nAccording to the original mechanism [\\[1\\]](#page-5-0), [\\\
    [2\\]](#page-5-1), the converting consists of two steps:\n\n1) calculating X;\n\
    \n2) generating quantized outputs P1, P2, . . . , Pn.\n\nThe shared scale X can\
    \ represent NaN, but not infinity. X value is treated separately from the private\
    \ elements. Even when X is NaN, the private elements can be anything (normal number,\
    \ infinity, and NaN). However, in backward transformation, the resulting values\
    \ will all be NaN if X is NaN, because multiplication of NaN to a value equals\
    \ NaN.\n\nThis paper proposes an algorithm and a hardware architecture for converting\
    \ FP32 to MX formats. The proposed models were implemented on FPGA, with experimental\
    \ results obtained after synthesis and place-and-route.\n\nIn [\\[3\\]](#page-5-2),\
    \ a converter is described that processes input into blocks of MX values using\
    \ 'round to nearest even' for FP32 and BFloat16 and focuses on a pipelined (memory-based)\
    \ design. In contrast, the approach in this paper uses combinational logic (i.e.,\
    \ without memory) and applies standard IEEE 754 rounding.\n\nThe rest of the paper\
    \ is organized as follows. Section II will consider the converting algorithm from\
    \ FP32 to MXformat and quantization of the considered MX-formats with the examples.\
    \ Section III will represent the structure of the hardware converter. This will\
    \ be followed by the results of the synthesis in section IV. Finally, the conclusions\
    \ are presented in section V.\n\n#### II. CONVERTING ALGORITHM FP32 TO MX-FORMATS\n\
    \nThe procedure of the transformation reminds the original one, but it splits\
    \ the first step into two steps, where on the first step the largest power-of-two\
    \ among all inputs is calculated, and then the largest value is tis defined, thus\
    \ transformed to X (see Table [II\\)](#page-2-0). As a result, the procedure of\
    \ the transformation is the following:\n\n- 1) identifying the largest power of\
    \ two, denoted as max(|V<sup>i</sup> |), among all inputs V1, V2, . . . , Vn.\
    \ The largest power of two is determined by examining the exponents EV1, EV2,\
    \ . . . , EVn, so that max(|V<sup>i</sup> |) = max|{EVi}|. This maximum value\
    \ is calculated by comparing pairs of values in a hierarchical manner: first,\
    \ EV<sup>1</sup> with EV2, EV<sup>3</sup> with EV4, and so on, up to EV<sup>31</sup>\
    \ with EV32. At each level, the larger value in each pair is compared with the\
    \ larger value from the adjacent pair. This process continues until, on the final\
    \ (fifth) step, the largest value among all 32 inputs is determined;\n- 2) the\
    \ transformation of max(|EV<sup>i</sup> |) into X is achieved by converting max(|EV<sup>i</sup>\
    \ |) to the order Omax, then subtracting the value of the largest order of the\
    \ considered data type EK in the MX-format from Omax. For example, if max(|V<sup>i</sup>\
    \ |) = 0 10101011 | {z } 8-bit exponent 011 . . . , then\n\nmax(|EV<sup>i</sup>\
    \ |) = 10101011, which corresponds to EV<sup>i</sup> = 171 and Omax(EV ) = 44.\
    \ The value of the largest order of E5M2 is 15, with E5 = 11110; thus X = Omax\
    \ − E5 = 44 − 15 = 29. The ratio of exponent and order of numbers in format FP32\
    \ to the shared scale X for E5M2, E4M3, E3M2, E2M3, E2M1, and INT8 is shown in\
    \ Table [II;](#page-2-0)\n\n3) the calculation and quantization of P1, P2, . .\
    \ . , P<sup>n</sup> involve rounding r + 1 input bits to r output bits. For example,\
    \ the three most significant bits of the mantissa in FP32 are rounded to two bits\
    \ of the mantissa in the E5M2 format. The quantization details for E5M2, E4M3,\
    \ E3M2, E2M3, and E2M1 formats are presented in Tables [III,](#page-3-0) [IV,](#page-3-1)\
    \ [V,](#page-3-2) [VI,](#page-3-3) and [VII,](#page-3-4) respectively. The quantization\
    \ of INT8 is not considered here due to the extensive 65-line table required.\n\
    \nThe shared scale X can represent NaN, but it cannot represent infinity. The\
    \ value of X is handled separately from the private elements. Even if X is NaN,\
    \ the private elements can still be normal numbers, infinities, or NaNs. However,\
    \ if X is NaN, all resulting private values will also be NaN.\n\n# III. ARCHITECTURE\
    \ OF THE PROPOSED CONVERTER\n\nThe proposed procedure includes three steps (see\
    \ Fig. [2\\)](#page-2-1) by split calculating X into two steps, as mentioned in\
    \ the previous section[\\\\*](#page-1-0):\n\n- 1) defining the largest power-of-two\
    \ (max(|EV<sup>i</sup> |)) among all inputs V1, V2, . . . , Vn;\n- 2) transformation\
    \ of max(|EV<sup>i</sup> |) into X;\n- 3) calculating and quantization of P1,\
    \ P2, . . . , Pn.\n\n*A. Defining the largest among* V1, V2, . . . , V<sup>n</sup>\
    \ *(*max(|EV<sup>i</sup> |)*)*\n\nFor calculating the largest value max(|EV<sup>i</sup>\
    \ |) among V1, V2, . . . , V<sup>n</sup> is processed by comparing pairs of values\
    \ in a hierarchical manner in five steps as pictured on Fig. [2.](#page-2-1) The\
    \ core of the comparison is the module \"comp\" with two 32-bit inputs and one\
    \ 32-bit output.\n\nThe module \"comp\" calculates the following cases:\n\n- if\
    \ both exponents of FP32 x and y are equal 8-bit of 11111111, then 32-output equals\
    \ 32-bit of 0;\n- if one of exponents of FP32 x are equal 8-bit of 11111111, (i.e.\
    \ FP32 is \"0\" or ±∞) and the other y equals another value, then 32-output equals\
    \ y;\n- if both exponents of FP32 x and y are not equal 8-bit of 11111111, (i.e.\
    \ FP32 are not \"0\" or ±∞), then 32-output equals the largest of x and y.\n\n\
    *Example. Part 1.* Consider four FP32 inputs:\n\n$$\\begin{aligned} \\text{1)}\
    \ \\ V\\_1 &= \\underbrace{0 \\dots \\text{0}}\\_{V\\_2} \\underbrace{10101011}\\\
    _{\\text{8-bit exponent}} \\underbrace{011 \\dots}\\_{\\text{2-bit mantissa}},\\\
    \\ \\text{2)} \\ V\\_2 &= \\underbrace{0 \\dots \\text{0}}\\_{\\text{sign}} \\\
    underbrace{10101000}\\_{\\text{8-bit exponent}} \\underbrace{110 \\dots}\\_{\\\
    text{2-bit mantissa}},\\\\ \\text{3)} \\ V\\_3 &= \\underbrace{0 \\dots \\text{0}}\\\
    _{\\text{sign}} \\underbrace{00101011}\\_{\\text{8-bit exponent}} \\underbrace{001\
    \ \\dots}\\_{\\text{2-bit mantissa}},\\end{aligned}$$\n\n$$\\text{4) } V\\_4 =\
    \ \\underbrace{\\stackrel{\\text{sign 8-bit exponent 23-bit mantissa}}}\\_{\\\
    text{sign 8-bit exponent 23-bit mantissa}}.$$\n\nThe result of the comparison\
    \ of absolute values of V1, V2, V3, V<sup>4</sup> gives V<sup>1</sup> as largest\
    \ power-of-two result max(|EV<sup>i</sup> |) = EV<sup>1</sup> = 10101011.\n\n\
    #### *B. Transformation of* max(|EV<sup>i</sup> |) *into* X\n\nThe transformation\
    \ step consists of the module \"div\", which transforms the largest value max(|EV<sup>i</sup>\
    \ |) to the shared scale X (central part of Fig. [2](#page-2-1) b)). The 31-bit\
    \ input of this module represented as the least significant bits of FP32 (all\
    \ bits of FP32 without the \"sign\" bit) is transformed to 8-bit output X.\n\n\
    <span id=\"page-1-0\"></span><sup>\\*</sup>Verilog-files for the transformation\
    \ of FP32 to six types of MX: [https://github.com/ZeboZebo702/MX-formats/tree/main/FP32](https://github.com/ZeboZebo702/MX-formats/tree/main/FP32_to_MX)\
    \ to MX\n\n<span id=\"page-2-0\"></span>TABLE II: The ratio of exponent and order\
    \ of numbers in format FP32 to the shared scale X of E5M2, E4M3, E3M2, E2M3 E2M1,\
    \ and INT8\n\n| FP32 | order    | 0    | 1    | 2    | 3    | 4    | 5 | 6 | 7\
    \ | 8    | <br>15 | 16   | 17   | . | 254 | 255     |\n|------|----------|------|------|------|------|------|---|---|---|------|--------|------|------|---|-----|---------|\n\
    |      | exponent | -126 | -126 | -125 |      |      |   |   |   | .    |    \
    \    |      |      |   | 127 | ∞ / NaN |\n| E5M2 | order    | 0    | 0    |  \
    \    |      |      |   |   |   |      | 0      | 1    | 2    |   | 239 | 240 \
    \    |\n|      | exponent | 0    | 0    |      |      |      |   |   |   |   \
    \   | 0      | -126 | -125 |   | 112 | NaN     |\n| E4M3 | order    | 0    | 0\
    \    |      |      |      |   |   | 0 | 1    |        |      |      |   | 247\
    \ | 248     |\n|      | exponent | 0    | 0    |      |      |      |   |   |\
    \ 0 | -126 |        |      |      |   | 120 | NaN     |\n| E3M2 | order    | 0\
    \    | 0    | 0    | 0    | 1    |   |   |   |      |        |      |      | \
    \  | 251 | 252     |\n|      | exponent | 0    | 0    | 0    | 0    | -126 | \
    \  |   |   |      |        |      |      |   | 124 | NaN     |\n| E2M3 | order\
    \    | 0    | 0    | 1    | 2    |      |   |   |   |      |        |      | \
    \     |   | 253 | 254     |\n| E2M1 | exponent | 0    | 0    | -126 | -125 | \
    \     |   |   |   |      |        |      |      |   | 126 | / NaN   |\n| INT8\
    \ | order    | 0    | 1    | 2    |      |      |   |   |   |      |        |\
    \      |      |   | 254 | 255     |\n|      | exponent | 0    | -126 | -125 |\
    \      |      |   |   |   |      |        |      |      |   | 127 | NaN     |\n\
    \n<span id=\"page-2-1\"></span>![](_page_2_Figure_2.jpeg)\n\nFig. 2: Architecture\
    \ of the converter\n\na) The largest value calculation max(|Vi|) = EVi, where\
    \ a1, a2, · · · , a<sup>16</sup> are outputs of the first layer of the comparison,\
    \ b1, b2, · · · , b<sup>8</sup> are outputs of the second layer of the comparison,\
    \ c1, c2, c3, c<sup>4</sup> are outputs of the third layer of the comparison,\
    \ d1, d<sup>2</sup> are outputs of the fourth layer of the comparison\n\nb) The\
    \ shared scale computation block transforms 31-bit of the value EV<sup>i</sup>\
    \ to the shared scale value X c) The private elements computation block processes\
    \ the shred scale X and 9 + R most significant bits of FP32 inputs, where R is\
    \ the bit-range of the mantissa of the appropriate types of MX-format. The private\
    \ elements P1, P2, . . . , P<sup>32</sup> consist of 1 + K + R-bits, where K is\
    \ bit-range of the exponent of the appropriate types of MX-format\n\n8-bit exponent\
    \ EV<sup>i</sup> ([31 : 24] of E bit of EVi) is encoded to intermediate value\
    \ Xtemp according to the rule:\n\n$$X\\_{temp} = \\begin{cases} EV\\_i - 2^{K-1},\
    \ & \\text{if } EV\\_i > 2^{K-1} - 1; \\\\ 0, \\text{in other cases,} & \\end{cases}$$\n\
    \nwhere K is the bit-with of the exponent in MX-format type.\n\nFor example, if\
    \ EV<sup>i</sup> = 00001011, then for E5M2, where K = 5, Xtemp = 00000000; if\
    \ EV<sup>i</sup> = 11100010, then for E5M2, where K = 5, Xtemp = 11010011.\n\n\
    The 1-bit value N aN is calculated on this step.\n\n$$NaN = \\overline{V\\_i[1]}\
    \ \\text{ & } \\overline{V\\_i[2]} \\text{ & } \\dots \\text{ & } \\overline{V\\\
    _i[23]},$$\n\nwhere V<sup>i</sup> [1], V<sup>i</sup> [2], . . . , V<sup>i</sup>\
    \ [23] are 23 mantissa bits of EV<sup>i</sup> .\n\nThe output X of the module\
    \ \"div\" is calculated in the following way:\n\n- Xtemp = 11110000 for E5M2,\
    \ Xtemp = 11111000 for E4M2, Xtemp = 11111100 for E3M2, Xtemp = 1111110 for E2M3\
    \ and E2M1, (i.e. EV<sup>i</sup> = 11111111), and NAN = 0, then X = 11111111.\
    \ i.e. X is considered to the value NAN;\n- Xtemp = 11110000 for E5M2, Xtemp =\
    \ 11111000 for E4M2, Xtemp = 11111100 for E3M2, Xtemp = 1111110 for E2M3 and E2M1,\
    \ (i.e. EV<sup>i</sup> ̸= 11111111), and NAN = 1, then X = 11111110. i.e. X is\
    \ considered as infinity without sign identification;\n- Xtemp ̸= 11110000 for\
    \ E5M2, Xtemp ̸= 11111000 for E4M2, Xtemp ̸= 11111100 for E3M2, Xtemp ̸= 1111110\
    \ for E2M3 and E2M1, (i.e. EV<sup>i</sup> ̸= 11111111), then X = Xtemp.\n\n*Example.\
    \ Part 2.* As defined above, max(|V<sup>i</sup> |) = EV<sup>1</sup> =\n\nTABLE\
    \ III: Quantization of E5M2\n\n<span id=\"page-3-0\"></span>\n\n| E5         \
    \                   |   | V [23 : 21] |                                | M2 |\
    \   |  |\n|-------------------------------|---|-------------|--------------------------------|----|---|--|\n\
    | E[5] E[4] E[3] E[2] E[1]      |   |             | V [23] V [22] V [21] M[2]\
    \ M[1] |    |   |  |\n| E[5]&E[4]&E[3]&E[2]&E[1] ̸= 1 | 0 | 0           | 0  \
    \                            | 0  | 0 |  |\n| E[5]&E[4]&E[3]&E[2]&E[1] ̸= 1 |\
    \ 0 | 0           | 1                              | 0  | 1 |  |\n| E[5]&E[4]&E[3]&E[2]&E[1]\
    \ ̸= 1 | 0 | 1           | 0                              | 1  | 1 |  |\n| E[5]&E[4]&E[3]&E[2]&E[1]\
    \ ̸= 1 | 0 | 1           | 1                              | 1  | 0 |  |\n| E[5]&E[4]&E[3]&E[2]&E[1]\
    \ ̸= 1 | 1 | 0           | 0                              | 1  | 0 |  |\n| E[5]&E[4]&E[3]&E[2]&E[1]\
    \ ̸= 1 | 1 | 0           | 1                              | 1  | 1 |  |\n| E[5]&E[4]&E[3]&E[2]&E[1]\
    \ ̸= 1 | 1 | 1           | 0                              | 1  | 1 |  |\n| E[5]&E[4]&E[3]&E[2]&E[1]\
    \ ̸= 1 | 1 | 1           | 1                              | 0  | 0 |  |\n| E[5]&E[4]&E[3]&E[2]&E[1]\
    \ = 1  | 1 | 1           | 1                              | 1  | 1 |  |\n\nTABLE\
    \ IV: Quantization of E4M3\n\n<span id=\"page-3-1\"></span>\n\n| E4          \
    \             |   |                                            | V [23 : 20] |\
    \   |   | M3 |   |\n|--------------------------|---|--------------------------------------------|-------------|---|---|----|---|\n\
    | E[4] E[3] E[2] E[1]      |   | V [23] V [22] V [21] V [20] M[3] M[2] M[1] |\
    \             |   |   |    |   |\n| E[4]&E[3]&E[2]&E[1] ̸= 1 | 0 | 0         \
    \                                 | 0           | 0 | 0 | 0  | 0 |\n| E[4]&E[3]&E[2]&E[1]\
    \ ̸= 1 | 0 | 0                                          | 0           | 1 | 0\
    \ | 0  | 1 |\n| E[4]&E[3]&E[2]&E[1] ̸= 1 | 0 | 0                             \
    \             | 1           | 0 | 0 | 0  | 1 |\n| E[4]&E[3]&E[2]&E[1] ̸= 1 | 0\
    \ | 0                                          | 1           | 1 | 0 | 1  | 0\
    \ |\n| E[4]&E[3]&E[2]&E[1] ̸= 1 | 0 | 1                                      \
    \    | 0           | 0 | 0 | 1  | 0 |\n| E[4]&E[3]&E[2]&E[1] ̸= 1 | 0 | 1    \
    \                                      | 0           | 1 | 0 | 1  | 1 |\n| E[4]&E[3]&E[2]&E[1]\
    \ ̸= 1 | 0 | 1                                          | 1           | 0 | 0\
    \ | 1  | 1 |\n| E[4]&E[3]&E[2]&E[1] ̸= 1 | 0 | 1                             \
    \             | 1           | 1 | 1 | 0  | 0 |\n| E[4]&E[3]&E[2]&E[1] ̸= 1 | 1\
    \ | 0                                          | 0           | 0 | 1 | 0  | 0\
    \ |\n| E[4]&E[3]&E[2]&E[1] ̸= 1 | 1 | 0                                      \
    \    | 0           | 1 | 1 | 0  | 1 |\n| E[4]&E[3]&E[2]&E[1] ̸= 1 | 1 | 0    \
    \                                      | 1           | 0 | 1 | 0  | 1 |\n| E[4]&E[3]&E[2]&E[1]\
    \ ̸= 1 | 1 | 0                                          | 1           | 1 | 1\
    \ | 1  | 0 |\n| E[4]&E[3]&E[2]&E[1] ̸= 1 | 1 | 1                             \
    \             | 0           | 0 | 1 | 1  | 0 |\n| E[4]&E[3]&E[2]&E[1] ̸= 1 | 1\
    \ | 1                                          | 0           | 1 | 1 | 1  | 1\
    \ |\n| E[4]&E[3]&E[2]&E[1] ̸= 1 | 1 | 1                                      \
    \    | 1           | 0 | 1 | 1  | 1 |\n| E[4]&E[3]&E[2]&E[1] ̸= 1 | 1 | 1    \
    \                                      | 1           | 1 | 0 | 0  | 0 |\n| E[4]&E[3]&E[2]&E[1]\
    \ = 1  | 1 | 1                                          | 1           | 1 | 1\
    \ | 1  | 1 |\n\n10101011 and EV<sup>1</sup> = 10101011 > 2 <sup>5</sup>−<sup>1</sup>\
    \ − 1 = 1111. Hence, Xtemp = 10101011−1111 = 10011100. Xtemp ̸= 11110000, then\
    \ X = Xtemp = 10011100.\n\n# *C. Calculating and quantization of* P1, P2, . .\
    \ . , P<sup>n</sup>\n\nIn this section, we provide rules of the quantization of\
    \ the considered types of MX-format according to [\\[1\\]](#page-5-0), [\\[2\\\
    ]](#page-5-1). Hence, the third step of the conversion scales and quantilizas\
    \ V<sup>i</sup> to the appropriate type of MX-format in the module \"Pi\" (right\
    \ part of Fig. [2](#page-2-1) c)). The module is equipped with 8-bit input X,\n\
    \n#### TABLE V: Quantization of E3M2\n\n<span id=\"page-3-2\"></span>\n\n| E3\
    \                  |   | V [23 : 21]                    | M2 |   |   |\n|---------------------|---|--------------------------------|----|---|---|\n\
    | E[3] E[2] E[1]      |   | V [23] V [22] V [21] M[2] M[1] |    |   |   |\n| E[3]&E[2]&E[1]\
    \ ̸= 1 | 0 | 0                              | 0  | 0 | 0 |\n| E[3]&E[2]&E[1] ̸=\
    \ 1 | 0 | 0                              | 1  | 0 | 1 |\n| E[3]&E[2]&E[1] ̸= 1\
    \ | 0 | 1                              | 0  | 1 | 1 |\n| E[3]&E[2]&E[1] ̸= 1 |\
    \ 0 | 1                              | 1  | 1 | 0 |\n| E[3]&E[2]&E[1] ̸= 1 | 1\
    \ | 0                              | 0  | 1 | 0 |\n| E[3]&E[2]&E[1] ̸= 1 | 1 |\
    \ 0                              | 1  | 1 | 1 |\n| E[3]&E[2]&E[1] ̸= 1 | 1 | 1\
    \                              | 0  | 1 | 1 |\n| E[3]&E[2]&E[1] ̸= 1 | 1 | 1 \
    \                             | 1  | 0 | 0 |\n| E[3]&E[2]&E[1] = 1  | 1 | 1  \
    \                            | 1  | 1 | 1 |\n\nTABLE VI: Quantization of E2M3\n\
    \n<span id=\"page-3-3\"></span>\n\n| E2             |   | V [23 : 20]        \
    \                        | M2 |   |   |   |   |\n|----------------|---|--------------------------------------------|----|---|---|---|---|\n\
    | E[2] E[1]      |   | V [23] V [22] V [21] V [20] M[3] M[2] M[1] |    |   | \
    \  |   |   |\n| E[2]&E[1] ̸= 1 | 0 | 0                                       \
    \   | 0  | 0 | 0 | 0 | 0 |\n| E[2]&E[1] ̸= 1 | 0 | 0                         \
    \                 | 0  | 1 | 0 | 0 | 1 |\n| E[2]&E[1] ̸= 1 | 0 | 0           \
    \                               | 1  | 0 | 0 | 0 | 1 |\n| E[2]&E[1] ̸= 1 | 0 |\
    \ 0                                          | 1  | 1 | 0 | 1 | 0 |\n| E[2]&E[1]\
    \ ̸= 1 | 0 | 1                                          | 0  | 0 | 0 | 1 | 0 |\n\
    | E[2]&E[1] ̸= 1 | 0 | 1                                          | 0  | 1 | 0\
    \ | 1 | 1 |\n| E[2]&E[1] ̸= 1 | 0 | 1                                        \
    \  | 1  | 0 | 0 | 1 | 1 |\n| E[2]&E[1] ̸= 1 | 0 | 1                          \
    \                | 1  | 1 | 1 | 0 | 0 |\n| E[2]&E[1] ̸= 1 | 1 | 0            \
    \                              | 0  | 0 | 1 | 0 | 0 |\n| E[2]&E[1] ̸= 1 | 1 |\
    \ 0                                          | 0  | 1 | 1 | 0 | 1 |\n| E[2]&E[1]\
    \ ̸= 1 | 1 | 0                                          | 1  | 0 | 1 | 0 | 1 |\n\
    | E[2]&E[1] ̸= 1 | 1 | 0                                          | 1  | 1 | 1\
    \ | 1 | 0 |\n| E[2]&E[1] ̸= 1 | 1 | 1                                        \
    \  | 0  | 0 | 1 | 1 | 0 |\n| E[2]&E[1] ̸= 1 | 1 | 1                          \
    \                | 0  | 1 | 1 | 1 | 1 |\n| E[2]&E[1] ̸= 1 | 1 | 1            \
    \                              | 1  | 0 | 1 | 1 | 1 |\n| E[2]&E[1] ̸= 1 | 1 |\
    \ 1                                          | 1  | 1 | 0 | 0 | 0 |\n| E[2]&E[1]\
    \ = 1  | 1 | 1                                          | 1  | 1 | 1 | 1 | 1 |\n\
    \n<span id=\"page-3-4\"></span>TABLE VII: Quantization of E2M1\n\n| E2       \
    \      |   | V [23 : 22]        | M1 |\n|----------------|---|--------------------|----|\n\
    | E[2] E[1]      |   | V [23] V [22] M[1] |    |\n| E[2]&E[1] ̸= 1 | 0 | 0   \
    \               | 0  |\n| E[2]&E[1] ̸= 1 | 0 | 1                  | 1  |\n| E[2]&E[1]\
    \ ̸= 1 | 1 | 0                  | 0  |\n| E[2]&E[1] ̸= 1 | 1 | 1             \
    \     | 0  |\n| E[2]&E[1] = 1  | 1 | 1                  | 1  |\n\n- 12-bit input,\
    \ where the most significant bit of FP32 V<sup>i</sup> , [11 : 4] bits are the\
    \ exponent of V<sup>i</sup> , and [3 : 1] bits correspond to [23 : 21] and 8-bit\
    \ output P<sup>i</sup> for E5M2 MX-format;\n- 13-bit input, where the most significant\
    \ bit of FP32 V<sup>i</sup> , [12 : 5] bits are the exponent of V<sup>i</sup>\
    \ , and [4 : 1] bits correspond to [23 : 20] and 8-bit output P<sup>i</sup> for\
    \ E4M3 MX-format;\n- 12-bit input, where the most significant bit of FP32 V<sup>i</sup>\
    \ , [11 : 4] bits are the exponent of V<sup>i</sup> , and [3 : 1] bits correspond\
    \ to [23 : 21] and 6-bit output P<sup>i</sup> for E3M2 MX-format;\n- 13-bit input,\
    \ where the most significant bit of FP32 V<sup>i</sup> , [12 : 5] bits are the\
    \ exponent of V<sup>i</sup> , and [4 : 1] bits correspond to [23 : 20] and 6-bit\
    \ output P<sup>i</sup> for E2M3 MX-format;\n- 11-bit input, where the most significant\
    \ bit of FP32 V<sup>i</sup> , [10 : 3] bits are the exponent of V<sup>i</sup>\
    \ , and [2 : 1] bits correspond to [23 : 22] and 4-bit output P<sup>i</sup> for\
    \ E2M1 MX-format.\n\nThe result P<sup>i</sup> calculates according to the following\
    \ conditions:\n\n- if X = 11111111 (i.e. X is NaN), then\n\t- P<sup>i</sup> =\
    \ V [12]1111110 for E5M2;\n\t- P<sup>i</sup> = V [12]1111110 for E4M3;\n\t- P<sup>i</sup>\
    \ = V [12]11110 for E3M2;\n\t- P<sup>i</sup> = V [12]11110 for E2M3;\n\t- P<sup>i</sup>\
    \ = V [12]111 for E2M1;\n- if X = 11111110 (i.e. X is infinity), then\n\t- P<sup>i</sup>\
    \ = V [12]1111100 for E5M2;\n\t- P<sup>i</sup> = V [12]1111000 for E4M3;\n\t-\
    \ P<sup>i</sup> = V [12]11100 for E3M2;\n\t- P<sup>i</sup> = V [12]1100 for E2M3;\n\
    \t- P<sup>i</sup> = V [12]110 for E2M1;\n- if X < 11110000, then EK = X + 2K−<sup>1</sup>\
    \ −1±E, where E is the exponent of V<sup>i</sup> and V<sup>i</sup> [32] = 0 corresponds\
    \ to \"+\" and V<sup>i</sup> [32] = 1 corresponds to \"−\" in ±E; then after if\
    \ EK > 2 <sup>K</sup>, then EK := 0 (with the same number of bits as K) and MR\
    \ := 0 (with the same number of bits as R)), else EK := 2<sup>K</sup> − 2 − EK,\
    \ then\n\t- if EK = 2<sup>K</sup> − 1 and\n\t\t- ∗ V<sup>i</sup> [3 : 1] = 111,\
    \ then P<sup>i</sup> = V [12] ∗ 1111011 for E5M2, i.e. there is no quatilization;\n\
    \t\t- ∗ V<sup>i</sup> [4 : 1] = 1111, then P<sup>i</sup> = V [12] ∗ 1110111 for\
    \ E4M3, i.e. there is no quatilization;\n\t\t- ∗ V<sup>i</sup> [3 : 1] = 111,\
    \ then P<sup>i</sup> = V [12] ∗ 11011 for E3M2, i.e. there is no quatilization;\n\
    \t\t- ∗ V<sup>i</sup> [4 : 1] = 1111, then P<sup>i</sup> = V [12] ∗ 10111 for\
    \ E2M3, i.e. there is no quatilization;\n\t\t- ∗ V<sup>i</sup> [1] = 1, then P<sup>i</sup>\
    \ = V [12] ∗ 101 for E2M1, i.e. there is no quatilization;\n\t- if EK ̸= 2<sup>K</sup>\
    \ − 1 and\n\t\t- ∗ V<sup>i</sup> [3 : 1] = 111, then P<sup>i</sup> = V [12] ∗\
    \ (EK + 1) ∗ 00 for E5M2;\n- ∗ V<sup>i</sup> [4 : 1] = 1111, then P<sup>i</sup>\
    \ = V [12]∗(EK+1)∗000 for E4M3;\n- ∗ V<sup>i</sup> [3 : 1] = 111, then P<sup>i</sup>\
    \ = V [12] ∗ (EK + 1) ∗ 00 for E3M2;\n- ∗ V<sup>i</sup> [4 : 1] = 1111, then P<sup>i</sup>\
    \ = V [12]∗(EK+1)∗000 for E2M3;\n- ∗ V<sup>i</sup> [1] = 1, then P<sup>i</sup>\
    \ = V [12] ∗ (EK + 1) ∗ 0 for E2M1;\n- if V<sup>i</sup> [3 : 1] = 110 or V<sup>i</sup>\
    \ [3 : 1] = 101, then P<sup>i</sup> = V [12] ∗ EK ∗ 11 for E5M2;\n- if V<sup>i</sup>\
    \ [3 : 1] = 010 or V<sup>i</sup> [3 : 1] = 001, then P<sup>i</sup> = V [12] ∗\
    \ EK ∗ 01 for E5M2;\n- if V<sup>i</sup> [3 : 1] = 011, then P<sup>i</sup> = V\
    \ [12] ∗ EK ∗ 10 for E5M2;\n- if V<sup>i</sup> [4 : 1] = 1111 orV<sup>i</sup>\
    \ [4 : 1] = 1110 or V<sup>i</sup> [4 : 1] = 1101, then P<sup>i</sup> = V [12]\
    \ ∗ EK ∗ 111 for E4M3;\n- if V<sup>i</sup> [4 : 1] = 1011 or V<sup>i</sup> [4\
    \ : 1] = 1100, then P<sup>i</sup> = V [12] ∗ EK ∗ 110 for E4M3;\n- if V<sup>i</sup>\
    \ [4 : 1] = 1001 or V<sup>i</sup> [4 : 1] = 1010, then P<sup>i</sup> = V [12]\
    \ ∗ EK ∗ 101 for E4M3;\n- if V<sup>i</sup> [4 : 1] = 0111 or V<sup>i</sup> [4\
    \ : 1] = 1000, then P<sup>i</sup> = V [12] ∗ EK ∗ 100 for E4M3;\n- if V<sup>i</sup>\
    \ [4 : 1] = 0101 or V<sup>i</sup> [4 : 1] = 0110, then P<sup>i</sup> = V [12]\
    \ ∗ EK ∗ 011 for E4M3;\n- if V<sup>i</sup> [4 : 1] = 0010 or V<sup>i</sup> [4\
    \ : 1] = 0011 or V<sup>i</sup> [4 : 1] = 0100, then P<sup>i</sup> = V [12] ∗ EK\
    \ ∗ 010 for E4M3;\n- if V<sup>i</sup> [4 : 1] = 0001, then P<sup>i</sup> = V [12]\
    \ ∗ EK ∗ 001 for E4M3;\n- if V<sup>i</sup> [3 : 1] = 110 or V<sup>i</sup> [3 :\
    \ 1] = 101, then P<sup>i</sup> = V [12] ∗ EK ∗ 11 for E3M2;\n- if V<sup>i</sup>\
    \ [3 : 1] = 010 or V<sup>i</sup> [3 : 1] = 001, then P<sup>i</sup> = V [12] ∗\
    \ EK ∗ 01 for E3M2;\n- if V<sup>i</sup> [3 : 1] = 011, then P<sup>i</sup> = V\
    \ [12] ∗ EK ∗ 10 for E3M2;\n- if V<sup>i</sup> [4 : 1] = 1111 orV<sup>i</sup>\
    \ [4 : 1] = 1110 or V<sup>i</sup> [4 : 1] = 1101, then P<sup>i</sup> = V [12]\
    \ ∗ EK ∗ 111 for E2M3;\n- if V<sup>i</sup> [4 : 1] = 1011 or V<sup>i</sup> [4\
    \ : 1] = 1100, then P<sup>i</sup> = V [12] ∗ EK ∗ 110 for E2M3;\n- if V<sup>i</sup>\
    \ [4 : 1] = 1001 or V<sup>i</sup> [4 : 1] = 1010, then P<sup>i</sup> = V [12]\
    \ ∗ EK ∗ 101 for E2M3;\n- if V<sup>i</sup> [4 : 1] = 0111 or V<sup>i</sup> [4\
    \ : 1] = 1000, then P<sup>i</sup> = V [12] ∗ EK ∗ 100 for E2M3;\n- if V<sup>i</sup>\
    \ [4 : 1] = 0101 or V<sup>i</sup> [4 : 1] = 0110, then P<sup>i</sup> = V [12]\
    \ ∗ EK ∗ 011 for E2M3;\n- if V<sup>i</sup> [4 : 1] = 0010 or V<sup>i</sup> [4\
    \ : 1] = 0011 or V<sup>i</sup> [4 : 1] = 0100, then P<sup>i</sup> = V [12] ∗ EK\
    \ ∗ 010 for E4M3;\n- if V<sup>i</sup> [4 : 1] = 0001, then P<sup>i</sup> = V [12]\
    \ ∗ EK ∗ 001 for E4M3;\n- if V<sup>i</sup> [3 : 1] = 10 or V<sup>i</sup> [3 :\
    \ 1] = 01 or or V<sup>i</sup> [3 : 1] = 11, then P<sup>i</sup> = V [12] ∗ EK ∗\
    \ 1 for E2M1.\n\n*Example. Part 3.* Convert four input FP32 to E5M2. As far as\
    \ X < 11110000, then\n\n1) for V<sup>1</sup> = 0 10101011 | {z } E 011 . . . :\
    \ EK = 156 + 2<sup>5</sup>−<sup>1</sup> − 1 − 171, then EK := 2<sup>K</sup> −\
    \ 2 − 0 = 30; V1[23 : 21] = 011, then MK = 10. Hence, P<sup>1</sup> = V [32] ∗\
    \ 11110 ∗ 10 = 01111010;\n\n- 2) for V<sup>2</sup> = 0 10101000 | {z } E 110 .\
    \ . . : EK = 156 + 25−<sup>1</sup> − 1 − 168, then EK := 2<sup>K</sup> − 2 − 3\
    \ = 27; V2[23 : 21] = 110, then MK = 11. Hence, P<sup>2</sup> = V [32] ∗ 11011\
    \ ∗ 11 = 01101111;\n- 3) for V<sup>3</sup> = 0 00101011 | {z } E 001 . . . : EK\
    \ = 156+25−<sup>1</sup>−1−43. As far as EK > 2 5 , then EK := 0 and MR := 0.\n\
    \nHence, P<sup>3</sup> = V [32] ∗ 00000 ∗ 00 = 00000000; 4) for V<sup>4</sup>\
    \ = 1 10001111 001 . . . : EK = 156 + 25−<sup>1</sup> − 1 +\n\n| {z } E 143. As\
    \ far as EK > 2 5 , then EK := 0 and MR := 0. Hence, P<sup>4</sup> = V [32] ∗\
    \ 00000 ∗ 00 = 10000000.\n\n# IV. EXPERIMENTAL RESULTS\n\nThe paper considers\
    \ the conversion of 32-bit of FP32 to 32 EKMR of MX-format. It means that converter\
    \ has\n\n- 32 · 32 | {z } inputs of V1, V<sup>2</sup> . . . , V<sup>32</sup> +\
    \ 8 |{z} output X + 32 · 8 | {z } output P1, P<sup>2</sup> . . . , P<sup>32</sup>\
    \ = 1288 input/output blocks for E5M2;\n- 32 · 32 | {z } inputs of V1, V<sup>2</sup>\
    \ . . . , V<sup>32</sup> + 8 |{z} output X + 32 · 8 | {z } output P1, P2·, P<sup>32</sup>\
    \ = 1288 input/output blocks for E4M3;\n- 32 · 32 | {z } inputs of V1, V<sup>2</sup>\
    \ . . . , V<sup>32</sup> + 8 |{z} output X + 32 · 6 | {z } output P1, P2·, P<sup>32</sup>\
    \ = 1224 input/output blocks for E3M2;\n- 32 · 32 | {z } inputs of V1, V<sup>2</sup>\
    \ . . . , V<sup>32</sup> + 8 |{z} output X + 32 · 6 | {z } output P1, P2·, P<sup>32</sup>\
    \ = 1224 input/output blocks for E2M3;\n- 32 · 32 | {z } inputs of V1, V<sup>2</sup>\
    \ . . . , V<sup>32</sup> + 8 |{z} output X + 32 · 4 | {z } output P1, P2·, P<sup>32</sup>\
    \ = 1160 input/output blocks for E2M1;\n- 32 · 32 | {z } inputs of V1, V<sup>2</sup>\
    \ . . . , V<sup>32</sup> + 8 |{z} output X + 32 · 4 | {z } output P1, P2·, P<sup>32</sup>\
    \ = 1288 input/output blocks for INT8.\n\nThe implementation design was conducted\
    \ on Virtex Ultra-Scale (xcvu440-flga2892-1-i), which is equipped with 1456 input/output\
    \ blocks. Table [VIII](#page-5-3) provides the results of the Xilinx Vivado 2019.1.\
    \ The proposed design is based on latches implementation (elements with feed-back\
    \ wires), hence the critical path evaluation is meaningless. The experimental\
    \ results include the place and routing process.\n\nThe architecture of the proposed\
    \ converter of 32 FP32 numbers consists of three sort of blocks. The first step\
    \ calculates the greater exponent among 32 FP32 inputs and it costs about 55%\
    \ of all hardware resources. The second step transforms the greater exponent to\
    \ the shared exponent \"X\" and it costs up to 1%. The third step generates private\
    \ elements P1, P2, . . . , P<sup>32</sup> and costs around 44% of hardware costs.\n\
    \n# V. CONCLUSION\n\nThis paper presents the hardware architecture of a converter\
    \ that transforms 32 single-precision floating-point numbers (FP32) into six types\
    \ of MX-format. FPGA implementation results are provided for the six MX-format\
    \ types: E5M2, E4M3,\n\n<span id=\"page-5-3\"></span>TABLE VIII: Results of the\
    \ implementation Virtex UltraScale (xcvu440-flga2892-1-i)\n\n| MX-format     \
    \     |      |      |      | E5M2 E4M3 E3M2 E2M3 E2M1 INT8 |      |      |\n|--------------------|------|------|------|-------------------------------|------|------|\n\
    | LUTs               | 2319 | 2776 | 2230 | 2039                          | 1896\
    \ | 1614 |\n| critical path (ns) | 51.4 | 80.2 | 52.8 | 62.6                 \
    \         | 65.2 | 50.6 |\n\nE3M2, E2M3, E2M1, and INT8. The proposed converter\
    \ architecture utilizes a three-step algorithm and is implemented using combinational\
    \ logic.\n\n### ACKNOWLEDGMENT\n\nThe authors gratitude to Ritchie Zhao for his\
    \ valuable assistance in explaining the rounding and quantization of numbers in\
    \ the MX-format.\n\n# REFERENCES\n\n- <span id=\"page-5-0\"></span>[1] B.D. Rouhani,\
    \ R. Zhao, A. More, M.Hall. A. Khodamoradi, S. Deng, D. Choudhary, M. Cornea,\
    \ E. Dellinger, K. Denolf, S. Dusan, V. Elango, M. Golub, A. Heinecke, P. James-Roxby,\
    \ D. Jani, G. Kolhe, M. Langhammer, A. Li, L. Melnick, M. Mesmakhosroshahi, A.\
    \ Rodriguez, M. Schulte, R. Shafipour, L. Shao, M Siu, P. Dubey, P. Micikevicius,\
    \ M. Naumov, C. Verrilli, R. Wittig, D. Burger, E. Chung \"Microscaling Data Formats\
    \ for Deep Learning\", arXiv: 2310.10537, 2023.\n- <span id=\"page-5-1\"></span>[2]\
    \ B.D. Rouhani, N. Garegrat, T. Savell, A. More, K.-N. Han, R. Zhao, M. Hall,\
    \ J. Klar, E. Chung, Y. Yu, M. Schulte, R. Wittig, I. Bratt, N. Stephens, J. Milanovic,\
    \ J. Brothers, P. Dubey, M. Cornea, A. Heinecke, A. Rodriguez, M. Langhammer,\
    \ S. Deng, M. Naumov, P. Micikevicius, M. Siu, C. Verrilli \"OCP Microscaling\
    \ (MX) Specification\", Open Compute Project, 2023.\n- <span id=\"page-5-2\"></span>[3]\
    \ E. Samson, N. Mellempudi, W. Luk, G.A. Constantinides, \"Exploring FPGA designs\
    \ for MX and beyond\", 34th International Conference on Field-Programmable Logic\
    \ and Applications (FPL), Turin, Italy, 2024."
- title: "TATAA: Programmable Mixed-Precision Transformer Acceleration with a\n  Transformable\
    \ Arithmetic Architecture"
  abstract: 'Modern transformer-based deep neural networks present unique technical

    challenges for effective acceleration in real-world applications. Apart from

    the vast amount of linear operations needed due to their sizes, modern

    transformer models are increasingly reliance on precise non-linear computations

    that make traditional low-bitwidth quantization methods and fixed-dataflow

    matrix accelerators ineffective for end-to-end acceleration. To address this

    need to accelerate both linear and non-linear operations in a unified and

    programmable framework, this paper introduces TATAA. TATAA employs 8-bit

    integer (int8) arithmetic for quantized linear layer operations through

    post-training quantization, while it relies on bfloat16 floating-point

    arithmetic to approximate non-linear layers of a transformer model. TATAA

    hardware features a transformable arithmetic architecture that supports both

    formats during runtime with minimal overhead, enabling it to switch between a

    systolic array mode for int8 matrix multiplications and a SIMD mode for

    vectorized bfloat16 operations. An end-to-end compiler is presented to enable

    flexible mapping from emerging transformer models to the proposed hardware.

    Experimental results indicate that our mixed-precision design incurs only 0.14%

    to 1.16% accuracy drop when compared with the pre-trained single-precision

    transformer models across a range of vision, language, and generative text

    applications. Our prototype implementation on the Alveo U280 FPGA currently

    achieves 2935.2 GOPS throughput on linear layers and a maximum of 189.5 GFLOPS

    for non-linear operations, outperforming related works by up to 1.45x in

    end-to-end throughput and 2.29x in DSP efficiency, while achieving 2.19x higher

    power efficiency than modern NVIDIA RTX4090 GPU.'
  url: http://arxiv.org/abs/2411.03697v1
  keywords: ''
  document: '# TATAA: PROGRAMMABLE MIXED-PRECISION TRANSFORMER ACCELERATION WITH A
    TRANSFORMABLE ARITHMETIC ARCHITECTURE


    Jiajun Wu<sup>∗</sup> Department of Electrical and Electronic Engineering University
    of Hong Kong Hong Kong jjwu@eee.hku.hk


    Jingmin Zhao Department of Electrical and Electronic Engineering University of
    Hong Kong Hong Kong jmzhao@eee.hku.hk


    Jia Li Department of Electrical and Electronic Engineering University of Hong
    Kong Hong Kong lijia@eee.hku.hk


    Mo Song<sup>∗</sup> Department of Electrical and Electronic Engineering University
    of Hong Kong Hong Kong songmo@eee.hku.hk


    Yizhao Gao Department of Electrical and Electronic Engineering University of Hong
    Kong Hong Kong yzgao@eee.hku.hk


    #### Hayden Kwok-Hay So


    Department of Electrical and Electronic Engineering University of Hong Kong Hong
    Kong hso@eee.hku.hk


    November 7, 2024


    ### ABSTRACT


    Modern transformer-based deep neural networks present unique technical challenges
    for effective acceleration in real-world applications. Apart from the vast amount
    of linear operations needed due to their sizes, modern transformer models are
    increasingly reliance on precise non-linear computations that make traditional
    low-bitwidth quantization methods and fixed-dataflow matrix accelerators ineffective
    for end-to-end acceleration. To address this need to accelerate both linear and
    non-linear operations in a unified and programmable framework, this paper introduces
    TATAA. TATAA employs 8-bit integer (int8) arithmetic for quantized linear layer
    operations through post-training quantization, while it relies on bfloat16 floating-point
    arithmetic to approximate non-linear layers of a transformer model. TATAA hardware
    features a transformable arithmetic architecture that supports both formats during
    runtime with minimal overhead, enabling it to switch between a systolic array
    mode for int8 matrix multiplications and a SIMD mode for vectorized bfloat16 operations.
    An end-to-end compiler is presented to enable flexible mapping from emerging transformer
    models to the proposed hardware. Experimental results indicate that our mixed-precision
    design incurs only 0.14 % to 1.16 % accuracy drop when compared with the pre-trained
    single-precision transformer models across a range of vision, language, and generative
    text applications. Our prototype implementation on the Alveo U280 FPGA currently
    achieves 2935.2 GOPS throughput on linear layers and a maximum of 189.5 GFLOPS
    for non-linear operations, outperforming related works by up to 1.45× in end-to-end
    throughput and 2.29× in DSP efficiency, while achieving 2.19× higher power efficiency
    than modern NVIDIA RTX4090 GPU.


    <sup>\*</sup>These authors contributed equally to this work


    <span id="page-1-0"></span>![](_page_1_Figure_1.jpeg)


    Figure 1: Illustration of a typical Transformer block, and how TATAA maps different
    operations in Transformers including linear *MatMul* and a variety of non-linear
    functions into transformable architecture, based on a general end-to-end framework
    design.


    # 1 Introduction


    Since its introduction in 2017, the Transformer model [\[1\]](#page-21-0) and
    its variations have rapidly risen to the forefront of modern deep learning architectures.
    Unlike previous-generation convolutional neural networks (CNNs) that were based
    predominantly on linear operations, modern transformer models are increasingly
    reliance on *high-precision non-linear operations* in their designs. For instance,
    the self-attention mechanism of a transformer model is typically based on the
    SoftMax function, which has been demonstrated to require high precision computation
    in order to achieve a model''s accuracy [\[2\]](#page-21-1). Normalization layers
    such as LayerNorm [\[3\]](#page-21-2) or root mean square normalization (RMSNorm)
    [\[4\]](#page-21-3), require complex nonlinear operations on data that cannot
    easily be fused into preceding linear layers. Finally, sophisticated activation
    functions such as GELU [\[5\]](#page-21-4), SiLU [\[6\]](#page-21-5) and SwiGLU
    [\[7\]](#page-21-6) are often used in transformer models which require precise
    computation, unlike in CNNs.


    To address the need to approximate these non-linear functions with high precision
    and high performance, specialized hardware modules have previously been extensively
    explored [\[8,](#page-21-7) [9,](#page-21-8) [10,](#page-21-9) [11\]](#page-21-10).
    Yet, customized hardware must be designed for every non-linear function that is
    being employed in a model, which is impractical when new non-linear operations
    for transformers are still actively being developed in this rapidly-evolving field
    [\[12,](#page-21-11) [13\]](#page-21-12). Other researchers have focused on quantizing
    such non-linear functions into low-bitwidth fixed point formats in order to reduce
    the computation complexity [\[14,](#page-21-13) [15,](#page-22-0) [16,](#page-22-1)
    [17\]](#page-22-2). Due to the outliers in transformers [\[18,](#page-22-3) [19\]](#page-22-4),
    retraining is generally required for such quantization to maintain good accuracy.
    However, the large size of modern transformer models, data availability and privacy
    concerns, have all made such retraining method impractical in most real-world
    scenarios. Besides, existing accelerators either rely on individual and specific
    non-linear function units [\[20,](#page-22-5) [21,](#page-22-6) [22\]](#page-22-7),
    or attempt to handle non-linear functions with general arithmetic logic units
    [\[17\]](#page-22-2). Both strategies often lead to increased hardware overhead,
    reduced hardware efficiency, and it complicates the workload balance between linear
    and non-linear layers. Instead, to improve future compatibility, a general-purpose
    transformer accelerator that can be reprogrammed to support new non-linear functions
    in floating-point arithmetic with low hardware overhead is highly desirable.


    In this paper, we present TATAA, a novel end-to-end framework for flexible and
    quantized Transformer Acceleration using a Transformable Arithmetic Architecture
    that supports both floating-point and integer operations (Figure [1\)](#page-1-0).
    In TATAA, static post-training quantization (PTQ) is performed on the linear layers
    of a transformer model to facilitate 8-bit integer (int8) operations. On the other
    hand, non-linear layers are performed using high-precision bfloat16 operations.
    In contrast to certain previous efforts that reduced data bitwidths in nonlinear
    layers, we alleviate the need for retraining by maintaining high bitwidth data
    formats in these layers, all the while ensuring high efficiency during execution.
    To support both int8 and bfloat16 operations efficiently, TATAA hardware architecture
    consists of dual-mode processing units (DMPUs) that feature configurable arrays
    of integer processing elements (PE). The proposed architecture can be transformed
    between a systolic array for int8 matrix multiplications and a SIMD-like vectorized
    bfloat16 computing unit. In particular, the proposed TATAA architecture employs
    a single type of processing unit, which is reused for all run-time operations,
    leveraging the bit field patterns of bfloat16. This design choice minimizes hardware
    overhead and maximizes flexibility compared to previous studies. By minimizing
    the overhead for run-time reconfiguration, the proposed transformable architecture
    ensures the high hardware processing density necessary to deliver the highest
    performance on FPGAs with limited resources. Finally, a compilation framework
    is developed that maps the user-provided transformer models to the custom instruction
    set architecture (ISA) of the TATAA processor cores to facilitate all operations
    in both linear and non-linear layers.


    To the best of our knowledge, TATAA is the first FPGA-based acceleration framework
    for transformer inference that integrates floating-point non-linear functions
    into integer-based linear processing units. It is programmable and is ready to
    support emerging transformer models with potentially new non-linear functions.
    Our experimental results indicate that when simulating model performance with
    a hybrid data format for transformer inference, TATAA achieves only a minimal
    accuracy reduction, with 0.14 % to 1.16 % decrease across all evaluated models
    compared to the original pretrained model in single-precision floating point (fp32).
    Additionally, the FPGA accelerator reaches a peak throughput of 2935.2 giga-operations-per-second
    (GOPS) for int8 linear operations and a peak throughput of 169.8 giga-floatingpoint-operations-per-second
    (GFLOPS) when the processor is configured for bfloat16 non-linear operations at
    a clock frequency of 225 MHz. Compared to related studies, TATAA achieves up to
    1.45× higher throughput and 2.29× higher throughput efficiency on DSP blocks.
    With the transformable architecture for non-linear functions, our implementation
    achieves 4.25× lower latency for these complex bfloat16 operations compared with
    other works, while supporting flexible and general compilation for emerging functions.
    Our end-to-end compilation framework also presents optimal mapping from non-linear
    functions to hardware ISA by appropriate approximation schemes and efficient dataflow
    control. Moreover, compared to state-of-the-art GPUs, our TATAA architecture outperforms
    a maximum 2.19× higher power efficiency over a variety of transformer models.
    This prototype underscores the potential of the TATAA approach for expediting
    transformer models and sets the stage for future optimization at the microarchitectural
    level, while our extensive compilation flow opens up a significant optimization
    space for non-linear functions and to quickly adapt to future transformer-based
    model as they are being developed.


    ### 2 Background & Related Works


    #### 2.1 Transformer and Quantization


    The Transformer architecture [\[1\]](#page-21-0), along with its various derivatives,
    such as the Vision Transformer (ViT) [\[23,](#page-22-8) [24,](#page-22-9) [25\]](#page-22-10),
    and language models including BERT [\[26\]](#page-22-11), OPT [\[27\]](#page-22-12),
    GPT [\[27\]](#page-22-12), and Llama [\[12\]](#page-21-11), have been extensively
    utilized in numerous applications. Regardless of the overall topology, the fundamental
    unit in these models, the transformer block, typically includes components such
    as MLP, activation, etc. Figure [1](#page-1-0) illustrates a block in ViT, where
    the green components represent linear matrix multiplications (*MatMul*), and the
    yellow components denote non-linear functions or residual adders. In detail, linear
    *MatMul* layers encompass the generation of self-attention entries (QKV-GEN),
    multiplication of QK<sup>T</sup> to calculate attention weights (QK-MUL), the
    *MatMul* operation between the SoftMax-applied attention scores and V entries
    (SV-MUL), followed by three feed-forward networks (ATT-PROJ, FFN1, and FFN2).
    Accordingly, the non-linear functions incorporate normalization, SoftMax, and
    activation, which differ across various transformer-based models, as illustrated
    in Figure [1.](#page-1-0)


    Due to the efficiency of integer *MatMul* operations on hardware, linear quantization
    has been widely applied in modern deep neural networks (DNNs), including transformer
    models, to reduce memory footprint and computational complexity. Equation [\(1\)](#page-2-0)
    presents the switching between floating-point (f p) format and quantized integer
    number (q), in terms of a basic multiplication z = x · y. Such a quantized basic
    operation can be extended to any linear operations (e.g., *MatMul*) by giving
    a sufficient intermediate integer bitwidth to avoid overflow.


    <span id="page-2-0"></span>

    $$\begin{aligned} q\_x &= \lfloor x\_{fp}/S\_x \rfloor, q\_y = \lfloor y\_{fp}/S\_y
    \rfloor \\ z\_{fp} &= x\_{fp} \cdot y\_{fp} = q\_x S\_x \cdot q\_y S\_y \\ q\_z
    &= \lfloor q\_x S\_x \cdot q\_y S\_y/S\_z \rfloor = \lfloor (q\_x \cdot q\_y)
    S\_x S\_y/S\_z \rfloor \end{aligned} \tag{1}$$


    According to Equation [\(1\)](#page-2-0), the key components to deploy quantized
    operations are scaling factors. To determine the scaling factors for each layer
    in transformer models, the primary quantization approaches are post-training quantization
    (PTQ) [\[28,](#page-22-13) [29,](#page-22-14) [30,](#page-22-15) [18\]](#page-22-3)
    and quantization-aware training (QAT) [\[14,](#page-21-13) [15,](#page-22-0) [31\]](#page-22-16).
    Since QAT requires fine-tuning and retraining with expensive overhead [\[32\]](#page-22-17),
    exploring the static PTQ approach is more practical in transformer applications
    and is applied in our quantization framework [\[33,](#page-22-18) [30\]](#page-22-15).
    In TATAA, we develop the quantization emulator based on a hardware matching style
    instead of *fake quantization* to get more convincing results, following the HAWQ
    setups [\[34\]](#page-23-0). Besides, TATAA can integrate existing PTQ schemes
    like FQ-ViT [\[30\]](#page-22-15) and SmoothQuant [\[33\]](#page-22-18). The static
    PTQ scheme only requires to access a relatively small part of dataset for calibration
    and getting all the scaling factors (i.e., Sx, Sy, Sz) before deploying inference.
    Once we have the scaling factors, our mixed-precision quantization can be done
    through Equation [\(1\)](#page-2-0), switching between floating-point and integer
    numbers for different kinds of layers.


    #### 2.2 Non-linear Functions in Transformer


    Beyond integer-based *MatMul* layers, transformers require non-linear functions
    to achieve high performance. For example, SoftMax [\[35\]](#page-23-1), Normalization
    (e.g., LayerNorm, RMSNorm) [\[3,](#page-21-2) [4\]](#page-21-3), and activation
    functions (e.g., GELU[1](#page-3-0) , SiLU, SwiGLU) [\[5,](#page-21-4) [6,](#page-21-5)
    [7\]](#page-21-6) in Equation [\(2\)](#page-3-1), are commonly used in transformers
    to extract self-attention features, activate the feed-forward block, and normalize
    the output of each block, respectively. These non-linear operations and their
    variants are essential yet costly building basis of transformer models and can
    be difficult to implement directly or efficiently on hardware. The linear quantization
    methods described in Equation [\(1\)](#page-2-0) no longer suit non-linear functions
    due to more complex operations and higher range & precision requirement during
    runtime.


    <span id="page-3-1"></span>

    $$\text{SoftMax} \left( \mathbf{x} \right) = \frac{\exp \left( \mathbf{x} \right)}{\sum\_{i}
    \exp \left( x\_{i} \right)}$$


    $$\text{LayerNorm} (\mathbf{x}) = \frac{\mathbf{x} - \mathbf{E}[\mathbf{x}]}{\sqrt{\text{Var}[\mathbf{x}]
    + \epsilon}} \cdot \gamma + \beta, \quad \text{RMSNorm}(\mathbf{x}) = \frac{\mathbf{x}}{\sqrt{\text{E}[\mathbf{x}^{2}]}}
    \cdot \gamma$$


    $$\text{GELU}(\mathbf{x}) = 0.5 \cdot \mathbf{x} \cdot \left( 1 + \tanh \left(
    \sqrt{2/\pi} \left( \mathbf{x} + 0.044715 \mathbf{x}^{3} \right) \right) \right)$$


    $$\text{SILU}(\mathbf{x}) = \mathbf{x} \cdot \sigma(\mathbf{x}) = \mathbf{x} \cdot
    \frac{1}{1 + \exp(-\mathbf{x})}, \quad \text{SwidU}(\mathbf{x}) = (\mathbf{x}
    \cdot \sigma(\mathbf{x})) \cdot (\mathbf{x} \cdot \sigma(W \mathbf{x} + b))$$


    To alleviate hardware inefficiency, several works proposed approximation techniques
    based on integer-only arithmetic [\[15\]](#page-22-0) or reduced precision computation
    [\[36\]](#page-23-2), and several researchers argued that LUT-based methods [\[37,](#page-23-3)
    [38\]](#page-23-4) demonstrated both negligible model accuracy degradation and
    higher computational efficiency. Especially in [\[11\]](#page-21-10), the miscellaneous
    non-linear operations are element-wise handled by a special function unit, which
    requires special breakpoints of vectors to perform in fine granularity to hide
    computation latency and wire resources. LogicNets[\[39\]](#page-23-5) and NullaNet
    [\[40\]](#page-23-6) act as general architectures that encapsulate all the operations
    embedded in linear or non-linear layers by enumerating the truth table values
    and can be further optimized following logic optimization algorithms. All these
    implementations of non-linear functions necessitate additional hardware units
    beyond the linear processing units with larger bitwidth support. In TATAA, we
    opt to utilize the same hardware processing units for both types of layers and
    comprehensive support.


    #### 2.3 Transformer Accelerators


    Various transformer acceleration frameworks for efficient inference have been
    proposed based on GPU [\[45,](#page-23-7) [50\]](#page-23-8), ASIC [\[41,](#page-23-9)
    [18,](#page-22-3) [16\]](#page-22-1), and FPGA [\[20,](#page-22-5) [43,](#page-23-10)
    [42,](#page-23-11) [17,](#page-22-2) [51,](#page-23-12) [11,](#page-21-10) [49,](#page-23-13)
    [21,](#page-22-6) [48,](#page-23-14) [52,](#page-23-15) [53,](#page-24-0) [54,](#page-24-1)
    [55,](#page-24-2) [46,](#page-23-16) [56,](#page-24-3) [47\]](#page-23-17). Unlike
    GPU and ASIC design, FPGA has attracted much attention recently, thanks to the
    configurable and flexible nature of FPGA devices, which has released the low hardware
    utilization rate issue in fixed architecture GPU or ASIC designs [\[47\]](#page-23-17).
    In terms of hardware architecture, part of existing accelerators focus on linear
    *MatMul* only, without full support for transformer models [\[18,](#page-22-3)
    [43,](#page-23-10) [42,](#page-23-11) [56\]](#page-24-3). In addition, all other
    designs with full support for transformer implement individual float-point units
    [\[17\]](#page-22-2) or specific modules for non-linear functions [\[20,](#page-22-5)
    [51,](#page-23-12) [16,](#page-22-1) [57\]](#page-24-4). Among them, spatial architecture
    that allows a deep pipeline between different layers has been selected in many
    previous works [\[49,](#page-23-13) [48,](#page-23-14) [20\]](#page-22-5), to
    reduce off-chip memory I/O. The limited on-chip resources on FPGA challenge such
    a design choice, especially when the transformer models have a larger and larger
    scale. On the contrary, TATAA utilizes a transferable architecture, allowing full
    support for all operations in transformer models by compiling non-linear functions
    into basic operations. Our proposed design reuses integer processing elements
    for all operations within transformer models, thereby avoiding additional hardware
    costs for the small-workload non-linear functions, as shown in Figure [1.](#page-1-0)
    Table [1](#page-4-0) presents the qualitative comparison between TATAA and the
    relative works. Note that TATAA presents a new but orthogonal angle for transformer
    accelerators compared to the spatial architecture, and it also has the potential
    to achieve an efficient pipeline in TATAA design.


    ### 3 Motivation


    As shown earlier, while linear layers such as self-attention and MLP can be easily
    quantized to integers and deployed on matrix multiplication (*MatMul*) processing
    units, quantizing other non-linear layers without sacrificing model performance
    is challenging unless one applies quantization-aware training (QAT). Furthermore,
    maintaining non-linear functions in higher precision (such as floating point)
    and creating specialized processing units for these less dominating functions
    results in significant hardware overhead and low hardware utilization. Thus, our
    primary motivation is:


    <span id="page-3-0"></span><sup>1</sup>We use tanh approximation of GELU function
    in this work


    | Work                    | End-to-End<br>Support | Retrain/<br>Fine-Tuning |
    Hardware<br>Platform | MatMul<br>Data Format | Non-linear<br>Data Format | Non-linear<br>Implementation         |

    |-------------------------|-----------------------|-------------------------|----------------------|-----------------------|---------------------------|--------------------------------------|

    | A3<br>[41]              | No                    | N/A                     |
    ASIC                 | int8                  | Integer                   | N/A                                  |

    | Mokey [18]              | No                    | No                      |
    ASIC                 | fxp                   | N/A                       | N/A                                  |

    | Auto-ViT-Acc [42]       | No                    | Yes                     |
    FPGA                 | Mixed Integer         | fp32                      | Host
    CPU                             |

    | Zhang et al. [43]       | No                    | Yes                     |
    FPGA                 | int8                  | fp32                      | N/A                                  |

    | FQ-BERT [44]            | Yes                   | Yes                     |
    FPGA                 | int8                  | fxp                       | Special
    Units                        |

    | I-ViT [14]              | Yes                   | Yes                     |
    GPU                  | int8                  | Integer                   | GPU
    Vector Units                     |

    | I-BERT [15]             | Yes                   | Yes                     |
    GPU                  | int8                  | Integer                   | GPU
    Vector Units                     |

    | Transformer Engine [45] | Yes                   | N/A                     |
    GPU                  | fp8                   | fp16/fp32                 | GPU
    Vector Units                     |

    | ViA [20]                | Yes                   | No                      |
    FPGA                 | fp16                  | fp16                      | Special
    Units                        |

    | SwiftTron [16]          | Yes                   | Yes                     |
    ASIC                 | int8                  | fxp                       | Special
    Units                        |

    | FTRANS [46]             | Yes                   | No                      |
    FPGA                 | fp16                  | fp32                      | Special
    Units                        |

    | Huang et al. [21]       | Yes                   | Yes                     |
    FPGA                 | int8                  | int8                      | Special
    Units                        |

    | FlexRun [47]            | Yes                   | Yes                     |
    FPGA                 | int8                  | fp32                      | Vector
    Units                         |

    | SSR [48]                | Yes                   | N/A                     |
    FPGA                 | int8                  | fp32                      | Special
    Units                        |

    | FlightLLM [11]          | Yes                   | N/A                     |
    FPGA                 | int4                  | fp16                      | Special
    Units                        |

    | Chen et al. [49]        | Yes                   | N/A                     |
    FPGA                 | int8                  | fp16                      | Special
    Units<br>(spatial pipeline)  |

    | TATA (Ours)             | Yes                   | No                      |
    FPGA                 | int8                  | bfloat16                  | Reuse
    MatMul<br>Hardware for Vectors |


    <span id="page-4-0"></span>Table 1: Qualitative Comparison with Related Software-Hardware
    Co-Design Transformer Acceleration Frameworks


    *Can we develop a unified processing unit that efficiently supports linear layers
    in integer and non-linear layers in floating-point?*


    Given a floating-point number x with its significant bit sx, exponent e<sup>x</sup>
    and mantissa mx, the real value of x can be represented as (−1)s<sup>x</sup> ·
    2 ex−e<sup>b</sup> · m<sup>x</sup> (the exponent bias is eb). Then, floating-point
    multiplication (*fpmul*) of two numbers x and y is:


    <span id="page-4-2"></span>

    $$x \cdot y = (-1)^{s\_x \wedge s\_y} \cdot 2^{e\_x + e\_y - e\_b} \cdot (m\_x
    \cdot m\_y) \tag{3}$$


    In this context, e<sup>x</sup> + e<sup>y</sup> − e<sup>b</sup> and m<sup>x</sup>
    · m<sup>y</sup> are operations on integers (specifically, unsigned integers) with
    a small bitwidth. Consequently, we can implement floating point multiplication
    using integer processing units with minimal overhead for the significant bit s.
    Standard floating-point addition (*fpadd*), as another basic operation, can be
    represented as:


    <span id="page-4-1"></span>

    $$x + y = (-1)^{s\_x} \cdot 2^{e\_x - e\_b} \cdot m\_z$$


    $$e\_z = \begin{cases} e\_x, & e\_x > e\_y \\ e\_y, & e\_y \ge e\_x \end{cases},
    \quad \Delta e = \begin{cases} e\_x - e\_y, & e\_x > e\_y \\ e\_y - e\_x, & e\_y
    \ge e\_x \end{cases} \tag{4}$$


    $$\{s\_z, m\_z\} = \begin{cases} \{s\_x, m\_x\} + \left(\{s\_y, m\_y\} \gg \Delta
    e\right), & e\_x > e\_y \\ \{s\_y, m\_y\} + \left(\{s\_x, m\_x\} \gg \Delta e\right),
    & e\_y > e\_x \end{cases}$$


    In Equation [\(4\)](#page-4-1), we have already merged the significant bit and
    mantissa field and transformed this fixed-point number (fxp) into 2''s complement
    for integer operations. It can be seen that *fpadd* is more complex than *fpmul*
    due to the alignment of the mantissa. However, after converting to 2''s complement,
    this series of operations becomes integer addition and multiplication. Specifically,
    the right shift can be performed using *fpmul* with a small lookup table (LUT).
    The only additional overhead is the conversion between signed digital and 2''s
    complement, as well as the small LUT for right shift.


    Since floating-point division is inherently costly, we aim to speed it up using
    integer operations. To achieve this, we employ the fast inverse square root algorithm
    [\[58\]](#page-24-5), which decomposes division into integer operations, as shown
    in Equation [\(5\)](#page-5-0) and Algorithm 1. Observe that the t 2 computation
    within this algorithm is distinct from the basic *fpmul* and


    | Algorithm 1 Fast Inverse Square Root                                            |                                                                                                                      |

    |---------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------|

    | Input: Input bfloat16 number y<br>1√y<br>Output: The inverse square root result
    |                                                                                                                      |

    | 1: yint<br>= y.view(int16)<br>2: tint<br>= 0x5f37 − (yint<br>>> 1)              |
    ▷ Does not change data bits, only changes the data format it refers to<br>▷ 0x5f37
    is the magic number in int16 [58] |

    | 3: t = tint.view(bf loat16)<br>1√y<br>2<br>= y · (1.5 − (y · 0.5 · t<br>)<br>4:
    | 2<br>▷ Define t<br>as fpapp operation. In TATAA, fpapp is one of the basic operations                                |


    *fpadd* operations. Hence, we designate it as an *approximated* calculation for
    the square root and division, abbreviated as *fpapp*. This term will be utilized
    in the subsequent discussion in this paper.


    <span id="page-5-0"></span>

    $$\frac{x}{y} = x \cdot \frac{1}{y} = \begin{cases} \ x \cdot \frac{1}{\sqrt{y}}
    \cdot \frac{1}{\sqrt{y}}, & y > 0\\ x \cdot \frac{-1}{\sqrt{-y}} \cdot \frac{-1}{\sqrt{-y}},
    y < 0 \end{cases} \tag{5}$$


    Based on transformable arithmetic, all basic floating-point operations can be
    transformed to a series of integer atom operations. As int8 has been the most
    commonly used format for linear layers quantization, we choose bfloat16 as the
    high-precision format for non-linear functions, featuring an 8-bit exponent and
    an 8-bit mantissa. The bfloat16 format has been extensively employed in the deep
    learning field for many years and has developed into a well-established standard
    for both training and inference [\[59\]](#page-24-6). The bitwidth of this unique
    floating-point format is perfectly aligned with the widely used int8, for both
    the exponent and the mantissa. Consequently, based on the analysis aforementioned,
    it is feasible to repurpose standard int8 processing units for fundamental bfloat16
    operations, such as *fpmul*, *fpadd*, and *fpdiv* discussed in this section. We
    also find that the most commonly used architecture for *MatMul*, systolic array,
    can actually match the vectorized floating-point execution in terms of computation
    and data layout. We will further explain the details of hardware design, ISA support,
    and workload mapping in the following sections.


    ## 4 Hardware Design


    #### 4.1 System Architecture


    Figure [2](#page-6-0) illustrates the proposed hardware architecture for the TATAA
    inference scheme. The on-chip accelerator comprises K processing cores that function
    independently with their own run-time instructions, while only one TATAA core
    is presented for simplification. Given the prevalent use of high-bandwidth memory
    (HBM) today, we have configured the processing cores with individual memory interfaces
    to communicate with external memory, thereby optimizing the memory bandwidth utilization
    rate. To prevent data synchronization issues between cores, we have deliberately
    divided the workloads among different cores without data dependency, which will
    be further detailed in our compilation framework. As shown in Figure [2](#page-6-0)
    (a), each TATAA core contains N dual-mode processing units (DMPU) with the integer
    PE design presented in Figure [2](#page-6-0) (b). The multi-DMPU architecture
    can be configured for two types of workload in transformer models, as shown in
    Figure [2](#page-6-0) (c). In int8 *MatMul* mode, all N DMPUs are connected to
    form a single systolic array. In contrast, in bfloat16 mode, the DMPUs function
    independently in a SIMD-like manner to execute vectors. The Mode MUX depicted
    in Figure [2](#page-6-0) (a) manages the run-time configuration that controls
    the connections between DMPUs with a shared controller. We abstract the on-chip
    data memory as register files for better high-level abstract and compilation.
    The input data register files are separated to the X and Y directions (RFX and
    RFY), corresponding to the horizon and vertical directions in a common systolic
    array for *MatMul*. They can be configured in different modes and store different
    formats of data during run-time. Additionally, we incorporate a quantization unit
    and an on-chip layout conversion module to quantize output results across layers
    and handle various data layouts between different operations.


    #### 4.2 Dual-Mode Processing Unit


    The key component in the TATAA architecture is DMPU which is configurable for
    two data formats. As shown in Figure [2](#page-6-0) (b), each DMPU comprises W
    columns by 4 rows of the processing element (PE) array. The PE is designed for
    integer MAC operations, with an integer multiplier (MUL) and a large bitwidth
    adder (ADD), as standard setups in int8 *MatMul*. In the int8 *MatMul* mode, all
    the N arrays in the DMPUs are connected to function as a unified W by 4N systolic
    array, and the results of the bottom PEs in one DMPU will be fed to the next DMPU
    as the top input, controlled by the mode MUX in Figure [2](#page-6-0) (a). Based
    on the conventional integer MAC PE design, we expand it by adding low-overhead
    top and bottom logic units and some extra MUX to support mapping bfloat16 operations
    into


    <span id="page-6-0"></span>![](_page_6_Figure_1.jpeg)


    Figure 2: TATAA hardware architecture and dual-mode processing unit (DMPU) design.
    The TATAA core can be configured for two kinds of workload during runtime, to
    support both linear *MatMul* and non-linear functions.


    it. To enhance the throughput of *MatMul* and address the memory-bound challenge
    in transformer models, we have implemented two loading ports per TATAA core to
    interact with external memory, given the presence of two matrices in *MatMul*.
    It''s important to highlight that these memory ports are not fixed to specific
    register files, RFX or RFY; instead, they are dynamically managed by a crossbar
    which routes the register files to the appropriate port.


    When TATAA works in the bfloat16 mode, each DMPU functions autonomously following
    a SIMD-like process, receiving data from the corresponding RFY instead of the
    previous DMPU. Each column of the PE array is considered a floating point unit
    (FPU), and the 4 rows become 4 pipeline stages in a floating point, naming PE-S0
    (the first stage), PE-S1, PE-S2, and PE-S3 (the last stage). The results are buffered
    in dual-mode buffers (DMB) in both modes before being stored back to external
    memory. Note that the intermediate bfloat16 results can also be written to RFY
    for further computation, avoiding frequent I/O access. Consequently, the core
    utilizes W · N parallel SIMD lanes in the bfloat16 mode, allowing the software
    stack to specify vectorized operations in bfloat16 with a maximum vector length
    of W · N. The switch of execution modes is completely online without reconfiguring
    the hardware, as presented in Figure [2](#page-6-0) (c), thanks to the custom
    ISA design in TATAA.


    ### 4.3 Processing Element Design on FPGA


    Figure [3](#page-7-0) shows the central component of DMPU in TATAA, specifically,
    the processing element (PE). Within this PE, the multiplier (MUL) and adder (ADD)
    perform multiply-accumulation (MAC) tasks for *MatMul* layers, alongside executing
    basic integer multiplication or addition in the bfloat16 mode according to the
    breakdown described from Equation [\(3\)](#page-4-2) to Equation [\(5\)](#page-5-0).
    As TATAA aims to reuse the same hardware units for these diverse functions, multiple
    MUX are incorporated within the PE to manage data pathways in different modes.
    Additionally, top- and bottom-logic are executed through look-up tables (LUTs)
    on FPGA for additional operations with minimal overhead, such as normalization
    and overflow or underflow clamping, vital for all floating-point calculations.
    Moreover, since DSP48E2 block in modern AMD FPGAs has large multiplier & adder
    bitwidth (27 × 18 for multiplier and 48-bit adder), a combined MAC optimization
    is implemented in each PE to enhance runtime throughput, a strategy commonly adopted
    in various FPGA accelerators [\[60,](#page-24-7) [61\]](#page-24-8). When functioning
    in MatMul mode, the PEs are organized as a large systolic array, integrating multiple
    DMPUs to maximize throughput.


    As demonstrated previously, bfloat16 operations are perceived as a four-stage
    pipeline, with each stage assigned to one processing element (PE) within a column.
    To utilize the same hardware, operations in bfloat16 format must be


    <span id="page-7-0"></span>![](_page_7_Figure_1.jpeg)


    *…*


    <span id="page-7-1"></span>Figure 3: Processing element (PE) design in the proposed
    DMPUs when deployed on FPGA devices. The converter from 2''s complement to signed
    digit (SD) logic that locates in PE-S3''s bottom logic is not depicted in this
    figure since it is similar to the SD to 2''s complement circuit in PE-S1.


    ![](_page_7_Figure_3.jpeg)


    Figure 4: Execution dataflow in int8 *MatMul* mode, in which all the PEs are connected
    as a whole systolic array and deploy output stationary dataflow.


    transformed from the original signed-digit format of the floating-point standard
    into the 2''s complement format used in DSP blocks, with conversions back to signed
    digits required before storing the results in memory. Consequently, the PE must
    include additional processing units specifically for bfloat16 mode, identified
    as top-logic and bottom-logic in Figure [3.](#page-7-0) It is important to highlight
    that the top-logic architecture varies across different PE stages, and the extra
    circuits have minimal overhead. For example, the converter from signed digit (SD)
    to 2''s complement is implemented in PE-S1 (second stage), while the logic handling
    overflow and underflow clamping is placed in PE-S2, and only PE-S3 contains the
    bottom logic needed for final normalization prior to outputting the bfloat16 result.
    In detail, the SD to 2''s complement converter in Figure [3](#page-7-0) concludes
    with a bitwise inverter, a +1 adder, and a MUX to select positive or negative
    data as the output. In addition, since the exponent in bfloat16 is 8-bit, the
    corresponding PE needs to clamp exponent from 0 to 255, thus implementing such
    a unit in top-logic. The overflow & underflow The normalization unit is the same
    as the standard normalization design in common floating-point units, with a leading
    one detector to align the mantissa and hide the hidden ''1'' and an adder to adjust
    exponent after shifting mantissa. In addition, each PE contains several constant
    registers for bfloat16 mode in each stage, as the input of multiplier. The detailed
    data flow of *MatMul* mode and bfloat16 mode will be thoroughly discussed in the
    following sections, explaining how these topand bottom-logic places in different
    stages of PEs.


    <span id="page-8-0"></span>![](_page_8_Figure_1.jpeg)


    Figure 5: bfloat16 mode dataflow and how to reuse the integer processing units.
    The Top-Logic, MUL & ADD, and Bottom-Logic processing are depicted in specific
    colors in this figure.


    # 4.4 Dataflow


    The architecture proposed in TATAA can be configured for int8 *MatMul* mode and
    bfloat16 mode during runtime. Figure [4](#page-7-1) shows the dataflow in the
    int8 *MatMul* mode, where all PEs are connected as a systolic array, and intermediate
    results accumulate across DMPUs. We choose to deploy the output stationary dataflow
    for matrix multiplication. In such an execution flow, the X and Y matrices go
    through the systolic array in the X (horizontal) and Y (vertical) directions,
    respectively. Registers L and R are responsible for horizontal and vertical data
    passing, while the bottom register directly accepts the data from the top and
    sends them to the next PE. After *MatMul* finishes, the results stored in register
    P will be sent to the corresponding dual-mode buffer (DMB). The intermediate sums
    are accumulated in the int16 format and subsequently quantized to either int8
    or bfloat16 before being saved to external memory, depending on the format required
    by the subsequent layer. The static scaling factors are pre-loaded to the quantization
    unit in the TATAA core before *MatMul* starts.


    When the TATAA architecture is set in bfloat16 mode, it can execute three basic
    operations: multiplication (*fpmul*), addition (*fpadd*) and the approximation
    step for the inverse square root in Equation [\(5\)](#page-5-0) and Algorithm
    1 to support (0x5f37 − (yint >> 1))<sup>2</sup> (*fpapp*) operation. These operations
    can be assigned directly to the 4 pipeline stages in the 4 rows of integer-based
    PE, as illustrated in Figure [5.](#page-8-0) Thanks to the arithmetic analysis
    from Equation [\(3\)](#page-4-2) to Equation [\(5\)](#page-5-0), we can convert
    bfloat16 operations into a sequence of integer operations. The integer multiplier
    and adder (MUL


    <span id="page-9-0"></span>![](_page_9_Figure_1.jpeg)


    Figure 6: Register files (RF) design in TATAA, and the connections between DMPU,
    RF and Dual-mode Buffers in different modes.


    & ADD in Figure [5\)](#page-8-0) are reused in the bfloat16 mode for higher resource
    efficiency. The floating-point pipeline not only adapts to integer arithmetic
    but also shares similar extra top- and bottom-logic, significantly reducing hardware
    overhead. The overall overhead encompasses converters for signed digits and 2''s
    complement, a compact 2''s power LUT, typical overflow and underflow management,
    and a normalization unit, all of which are standard components in conventional
    floating-point units. Specifically, the special *fpapp* first treats the input
    bfloat16 binary number as int16, performs integer subtraction (addition) in the
    first stage, and converts the integer binary number back to bfloat16 with the
    remaining 3 stages for square. Using this arithmetic mapping, the proposed DMPU
    can execute one operation per column in parallel, thus improving the SIMD execution
    of bfloat16 operations from a global perspective. This type of sharing scheme
    between two modes significantly reduces the consumption of hardware resources
    so that TATAA can map more parallel cores when resources are limited.


    #### 4.5 Register Files and Buffers


    Since the proposed TATAA framework is for flexible acceleration, abstract register
    files are required for efficient instruction set architecture (ISA) and compiler
    design. As shown in Figure [6](#page-9-0) (a), the RFX is available only in the
    int8 *MatMul* mode since the bfloat16 data only go through the Y direction. Therefore,
    the abstract concept *"registers"* in RFX is actually matrix buffers, and we set
    two registers inside (RMX0 and RMX1) to apply double-buffer optimization, hiding
    the memory I/O latency. The RFX has N ports to send data to corresponding N DMPUs.
    In TATAA, we define that each of the X matrix buffers has Dmat depth for matrix
    multiplication, so the total depth of RFX is 2 · Dmat.


    In terms of RFY, we set up two memory banks named RFYa and RFYb, to support the
    two input operators in the bfloat16 mode, and each address stores one part of
    the parallel vector, as shown in Figure [6](#page-9-0) (b). The data layout becomes
    more complex because only the RFY for DMPU0 (Dual-mode RFY, DMRFY0) needs to store
    both int8 matrices and bfloat16 vectors. When it works in int8 *MatMul* mode,
    the DMPU only needs one specific part of RFYa and RFYb. The two-bank design naturally
    supports double-buffer optimization (selected by the MUX), so each of them only
    costs Dmat depth. Like RFX, we abstract the matrix buffers as RMY0 and RMY1 physically
    corresponding to RFYa and RFYb. For the other RFY, they can only be used in the
    bfloat16 mode. Hence, the depth of these register files is set to Dfpv (Dfpv ≪
    Dmat), so the extra memory overhead of these bfloat16 vectors is relatively small.
    There is a natural data layout conflict between int8 and bfloat16. Since each
    bfloat16 number takes 16 bits, the output of one DMRFY0 bank (RFYa or RFYb) should
    have W · 16 bits in bfloat16 mode. However, DMRFY0 also needs to store the int8
    matrix, and the W columns of the PE array only need W · 8 bits. Thanks to the
    combined MAC optimization introduced before, the bitwidth of the DMPU input in
    int8 *MatMul* becomes W · 16-bit, matching the bitwidth of one bank in DMRFY0.
    As for non-FPGA implementation, designers can also deploy such optimization with
    larger multiplier and accumulator, to fit bitwidth in bfloat16, as well as benefits
    from higher throughput in *MatMul* operations.


    In TATAA architecture, the dual-mode buffers (DMB) serve the function of storing
    results temporarily before they are returned to the external memory. Each DMPU
    has a corresponding DMB in the bottom output direction. Importantly, DMBs have
    varying execution procedures in *MatMul* and bfloat16 modes. In *MatMul* mode,
    all DMPUs form a single systolic array, leading to inactivity in the DMBs linked
    to DMPU 0 through DMPU 6, with only the last DMPU 7 receiving *MatMul* intermediate
    results. In contrast, in bfloat16 mode, all DMPUs with W columns (essentially,
    W


    <span id="page-10-0"></span>![](_page_10_Figure_1.jpeg)


    <span id="page-10-1"></span>Figure 7: Illustration of the on-chip quantization
    and layout conversion module.


    | Table 2: TATAA ISA |                                                             |  |  |  |  |  |  |  |

    |--------------------|-------------------------------------------------------------|--|--|--|--|--|--|--|

    | Instruction Type   | Description                                                 |  |  |  |  |  |  |  |

    | CONFIG             | Set up static parameters (e.g., scaling factors, constants)
    |  |  |  |  |  |  |  |

    | LOAD.M             | Load a matrix from memory                                   |  |  |  |  |  |  |  |

    | LOAD.V             | Load a vector from memory                                   |  |  |  |  |  |  |  |

    | MATMUL             | Execute matrix multiplication Z = XY                        |  |  |  |  |  |  |  |

    | MUL.V              | Execute fpmul of two vectors                                |  |  |  |  |  |  |  |

    | ADD.V              | Execute fpadd of two vectors                                |  |  |  |  |  |  |  |

    | APP.V              | Execute fpapp of one vector                                 |  |  |  |  |  |  |  |

    | STORE.M            | Store a matrix (executed results) to memory                 |  |  |  |  |  |  |  |

    | STORE.V            | Store a vector (executed results) to memory                 |  |  |  |  |  |  |  |

    |                    |                                                             |  |  |  |  |  |  |  |


    #### FPUs) function independently following a SIMD approach, necessitating all
    DMBs to store bfloat16 vector results. A MUX also dictates the data path between
    the two modes, as illustrated in Figure [6](#page-9-0) (c). In *MatMul* mode,
    both the input Y and output Z traverse all DMPUs, with the bottom DMPU receiving
    data transmitted from the top DMPU as determined by the MUX. Conversely, in bfloat16
    mode, each DMPU obtains its input from RFY selected by the MUX.


    #### 4.6 Layout Conversion and On-chip Quantization


    Before writing the calculated int8 matrix or bfloat16 vector results back to external
    memory, the quantization unit dynamically quantizes the activations according
    to the current configuration, as shown in Figure [7.](#page-10-0) TATAA architecture
    supports data format switching between int8 and bfloat16, with four types of configuration
    in the quantization unit. All the conversions here can be handled based on Equation
    [\(1\)](#page-2-0) with preloaded floating point scaling factors, except the bfloat16
    → bfloat16 conversion does not require any specific quantization. Besides, if
    the subsequent workload cannot be deployed directly on the current data layout,
    TATAA handles on-chip layout conversion. On the one hand, the QK-MUL layer requires
    the matrix K from the previous layer to be transposed to match the layout. On
    the other hand, the bfloat16 mode is aimed at a vector-based layout, which differs
    from matrix workloads, which requires hardware with a row-by-row storage scheme
    for both int8 → bfloat16 and bfloat16 → int8 conversion. The only hardware overhead
    is to transpose the submatrix from DMPU, since all other conversions can be done
    by delicately controlling the write-back addresses.


    # 5 Compilation


    Before introducing the proposed TATAA compilation framework, we define the terminology
    related to layers, nodes, and operations. A layer is a concept at the model level,
    with its definition detailed in Figure [1.](#page-1-0) The nodes operate at the
    graph level and are derived from a specific transformer model. For example, the
    non-linear SoftMax function can be broken down into a sequence of sub-functions,
    such as exponentiation, summation, and division, that become nodes in the computational
    graph. These nodes can be amalgamated or subdivided into additional nodes. In
    matrix multiplication (*MatMul*), a node with a large *MatMul* size can be divided
    into smaller tiled *MatMul* to better align


    <span id="page-11-0"></span>![](_page_11_Figure_1.jpeg)


    Figure 8: Top-Down workflow of TATAA compiler. Note that TATAA supports various
    non-linear functions. The depicted LayerNorm, SoftMax and GELU are used as examples.


    with hardware structures. Operations reflect a hardware-level concept derived
    from nodes, implemented in the TATAA architecture, as discussed in Section 4.


    #### 5.1 Instruction Set Architecture (ISA)


    To better decouple hardware and software, we have developed a customized Instruction
    Set Architecture (ISA). Our software system can map linear layers in int8 and
    non-linear operations with a high-precision approximation in bfloat16. Table [2](#page-10-1)
    presents the simple ISA design in TATAA. In an ISA-level perspective, the controller
    is able to detect data dependencies and exploit instruction-level parallelism
    (ILP) to improve throughput performance. As an example, the double buffer optimization
    allows the parallel execution of the LOAD.M and MATMUL instructions. Furthermore,
    the previously mentioned data layout conversion with specific write-back addresses
    is incorporated into the STORE.M and STORE.V instructions, offering sufficient
    flexibility and comprehensive support for inference runtime. After compilation,
    all the runtime instructions are stored in the external memory, and TATAA accelerator
    acts as a processor that fetches instructions from external memory, removing the
    requirement overhead of host-based scheduling.


    #### 5.2 End-to-End Transformer Mapping


    Figure [8](#page-11-0) illustrates the top-down compilation process from an input
    transformer-based model to the TATAA hardware runtime. The compilation framework
    first parses and converts nonlinear functions into a series of basic operations
    (e.g., summation, squaring, multiplication, etc.) by examining the computation
    graph. For example, the LayerNorm function in Equation [\(2\)](#page-3-1) is parsed
    to summation (sum up vectors in-between), power of 2 (calculate x 2 for variantion),
    division (calculate E[x] based on summation), etc., as a series of operations.
    This parsing process has been mature in existing machine learning frameworks like
    ONNX [\[62\]](#page-24-9).


    Next, the compiler applies node fusion and mixed-precision quantization by integrating
    data layout conversion and quantization into the previous node, because the hardware
    supports on-chip quantization & layout conversion at runtime. With this intermediate
    representation (IR), the compiler then schedules linear *MatMul* operations into
    a sequence of tiled *MatMul* operations, with each tile conforming to the size
    of the TATAA systolic array. Currently, the basic operations of nonlinear functions
    are approximated and compiled into TATAA-supported operations (i.e., *fpmul*,
    *fpadd*, *fpapp*). Details of how to approximate these operations are shown in
    Algorithm 2. Upon completing these conversions, the compiler can analyze bfloat16
    workloads and vectorize them for SIMD-like instructions MUL.V and ADD.V


    |  | Algorithm 2 Approximation examples for non-linear functions |  |  |

    |--|-------------------------------------------------------------|--|--|

    |  |                                                             |  |  |


    Input: Input activation x Output: Exponent value of x, exp\_x 1: exp\_x = 2⌊(x/
    ln 2)⌋ Output: Inverse square root of x, isqrt\_x 2: y = 0x5f37 − (short(x) >>
    1) ▷ Similar to Algorithm 1 3: isqrt\_x = 1.5y − 0.5x · y 3 Output: Padé approximation
    of tanh (x) 4: tanh\_x = clamp( 27x+x 3 27+9x<sup>2</sup> , min = −1, max = 1)


    ▷ 2 ⌊·⌋ is fused into output quantization process by a small LUT


    <span id="page-12-0"></span>![](_page_12_Figure_5.jpeg)


    Figure 9: Linear *MatMul* scheduling in TATAA compilation.


    as shown in Table [2.](#page-10-1) Finally, the compiler assigns addresses to
    each atomic operation for the hardware runtime and generates binary instructions
    for the TATAA processor.


    # 5.2.1 MatMul Schedule


    The scheduler first analyzes the shape of the output matrix and distributes it
    evenly across batches to avoid data dependency between parallel cores. As activations
    are independent across various batches, the *MatMul* scheduler and the non-linear
    functions compiler can concentrate solely on the batches of a single core, incrementally
    updating the starting address to determine the activation addresses for other
    cores. Each TATAA core employs *MatMul* based on an output-stationary dataflow
    with a fixed output tile size, W by 4N. Based on this tiling, each output tile
    corresponds to a tile of matrices X and Y. After determining the addresses for
    X, Y, and Z, the scheduler generates a series of instructions LOAD.M, MATMUL,
    and STORE.M as assembly codes for runtime inference and applies double buffer
    optimization by reordering the three types of instruction, similar to the instruction-level
    parallelism (ILP) strategy. Figure [9](#page-12-0) illustrates the scheduler in
    terms of dataflow design and an example of ILP optimization using assambly codes.
    In this example, the MATMUL,RMX0,RMY0,Xw, LOAD.M,RMX1,0100H,Xw and LOAD.M,RMY1M,1100H,Xw
    can be executed in parallel since 1) there is no register files index conflict
    2) we design two I/O ports for loading. Besides, the scheduler needs to decide
    how to map the two operands of *MatMul* into X and Y, since the data layout of
    the two input ports in systolic array are different. For normal MLP, we map the
    weights and activations into X and Y, respectively, while for QK-MUL and SV-MUL
    without weights, we use another mapping scheme, as the layout conversion between
    X and Y can be done on hardware.


    ## 5.2.2 Non-Linear Functions Optimization


    The fundamental process of mapping non-linear functions to TATAA hardware runtime
    is illustrated in Figure [10.](#page-13-0) To optimize the performance of executing
    non-linear functions, TATAA focuses on reducing memory access and


    <span id="page-13-0"></span>![](_page_13_Figure_1.jpeg)


    Figure 10: Non-linear functions compilation in TATAA. We only present three examples
    which are commonly applied in most transformer models.


    maintaining computation on the chip by consistently reloading computation results
    into local registers. As shown in the yellow dashed box with the reload operation
    in Figure [10,](#page-13-0) when two vectors perform a MUL.V or ADD.V operation,
    the result is stored (reloaded) in one of the X or Y registers. By consistently
    performing this reloading and computation process, most computational operations
    are executed together without requiring additional memory access. All operations
    that do not involve memory access employ this on-chip computation method to maximize
    execution throughput. Since this on-chip computation necessitates loading as much
    data as possible into the register files before executing computation instructions,
    load instructions are assembled at the beginning of each non-linear function.
    To mitigate the sluggish memory access times for these load instructions, the
    outstanding transaction features of AXI are utilized to minimize the total load
    duration.


    Additionally, we emphasize two types of node-level optimization for non-linear
    functions, as depicted in Figure [10,](#page-13-0) with the aim of significantly
    reducing memory I/O costs while maximizing computational efficiency. We optimize
    the compiler to reuse input x vectors in register files by rearranging the computation
    nodes within the graph. This method prevents redundant loading of x onto the hardware.
    Given that the LayerNorm function involves both variance and mean calculations,
    as shown in Figure [10,](#page-13-0) AMA (accumulate-multiply-accumulate) facilitates
    the computation of E[x], x 2 , and E[x 2 ] by splitting both the X and Y registers
    into two groups. The input vector x is first loaded into the first group of the
    X and Y registers and then multiplied by itself to store the result x 2 into the
    second group of registers. As demonstrated in the assembly code, after loading
    the input vectors, the AMA process initiates with a series of MUL.V instructions
    to compute x 2 . The initial and subsequent two-by-two accumulation operations
    are executed without encountering data hazards, thus avoiding additional delays
    between computation instructions. In the first accumulation example, the values
    in the X registers 0 to 3 are added to the corresponding values in the Y registers.
    The sums are then stored back into registers 0 to 1 of both X and Y registers
    in a crosswise manner. This approach effectively reduces the total number of registers
    holding partial sums by half. The values in registers 0 to 1 are further consolidated
    into another partial sum, which is reloaded into register 0 in both X and Y registers.
    This partial sum is subsequently added together to produce the final accumulation
    result. Since the values from the first and second accumulation processes are
    stored in separate sections of the X and Y register files, the second two-by-two
    accumulation can occur concurrently with the first accumulation process. This
    approach ensures that the maximal amount of input vector data that both register
    files can hold is loaded only once from memory, leaving a substantial portion
    of the computation to be performed on-chip, thereby maximizing performance. Furthermore,
    to mitigate potential data hazards during this consistent computation, a two-by-two
    accumulation method is employed, which adds every two lines of input vectors and
    stores the results


    <span id="page-14-0"></span>


    | Model     | Type    | # Blocks   | # Heads      | Hidden Size       | MLP Size                           |
    Non-linear Functions     |

    |-----------|---------|------------|--------------|-------------------|------------------------------------|--------------------------|

    | Deit-S    | Encoder | 12         | 6            | 384               | 1536                               |                          |

    | Deit-B    | Encoder | 12         | 12           | 768               | 3072                               |
    SoftMax                  |

    | Swin-T    | Encoder | †{2,2,6,2} | †{3,6,12,24} | †{96,192,384,768} | †{562<br>,
    282<br>,142<br>2}<br>,7 | LayerNorm                |

    | BERT      | Encoder | 12         | 12           | 768               | 3072                               |
    GELU                     |

    | GPT2      | Decoder | 24         | 16           | 1024              | 4096                               |                          |

    | OPT-1.3B# | Decoder | 24         | 16           | 2048              | 8192                               |
    SoftMax, LayerNorm, ReLU |

    | Llama-7B# | Decoder | 32         | 32           | 4096              | 11008                              |
    SoftMax, RMSNorm, SwiGLU |

    | ChatGLM2# | Decoder | 28         | 32           | 4096              | 13696                              |
    SoftMax, RMSNorm, SiLU   |


    Table 3: Selected Transformer Models or Non-linear Functions in the Experiments


    † {. . . } shows dimension variance of each stage in a Swin-T [\[25\]](#page-22-10).


    # These large language models are not evaluated on hardware runtime. We only select
    them for various non-linear functions test.


    crosswise into the X and Y registers. Such an optimization compilation works for
    other normalization functions, e.g., RMSNorm as well.


    Furthermore, it is important to note that the GELU function maintains a consistent
    tensor shape across all nodes, enabling segmentation of all nodes into uniform
    tile shapes and allowing sequential execution of these tiles from start to finish.
    As illustrated in Figure [10,](#page-13-0) each iteration corresponds to a tile,
    and within every iteration, a much longer sequence of computation instructions
    is executed between a load and a store instruction. The yellow arrows indicate
    this extended sequence of computations. Although the number of memory accesses
    increases with the number of tiles, the memory overhead remains minimal compared
    to the lengthy computation sequence. Upon completion of each iteration, the results
    are written back to the address from which the input vector was initially read.
    Simultaneously, all registers used in the current iteration are cleared and ready
    to receive the next tile for the subsequent iteration. This multi-iterative method
    is optimized for GELU scheduling to achieve two main objectives: 1) avoiding intermediate
    I/O communication with external memory and 2) accommodating the limited register
    file space. Given that TATAA employs a layer-by-layer execution method, these
    I/O optimizations are crucial for improving throughput efficiency. Note that as
    long as the activation functions have the same tensor shape patterns across all
    nodes (e.g., SiLU), this tile-based compilation can be applied.


    # 6 Evaluation


    #### 6.1 Experiments Setup


    We implemented and prototyped TATAA on Alveo U280 FPGA platform using Verilog
    HDL and Vitis 2021.1 tools under 225 MHz frequency, to measure resource utilization,
    power consumption, and end-to-end runtime throughput. The hyperparametes mentioned
    in Figure [2](#page-6-0) are set as K = 8, N = 8, W = 16, with a corresponding
    32 by 32 systolic array and 128-lane SIMD FPUs in each core. In Alveo U280, we
    set up 16 AXI channels for the 8 TATAA cores and each AXI channel has 256-bit
    memory bitwidth.


    We have chosen a range of transformer models to evaluate the accuracy of quantization
    and their runtime performance in tasks such as image classification, text classification,
    and text generation. The selected models are listed below and their feature dimensions
    are shown in Table [3.](#page-14-0)


    - For Vision Transformers (ViT), we select DeiT[\[24\]](#page-22-9) and Swin Transformer
    (Swin) [\[25\]](#page-22-10) with ImageNet-1k [\[63\]](#page-24-10) dataset for
    the image classification.

    - BERT [\[26\]](#page-22-11), as a widely used language model, is also selected
    for evaluation based on the GLUE benchmark including different tasks [\[64\]](#page-24-11).

    - We also evaluate other popular language models, GPT-2 [\[65\]](#page-24-12)
    and OPT [\[27\]](#page-22-12), for the text generation task with LAMBADA [\[66\]](#page-24-13)
    and WikiText-2 datasets.

    - To evaluate the general support of TATAA for non-linear functions in transformer
    models, we also select state-of-the-art Llama [\[12\]](#page-21-11) and ChatGLM2
    [\[67\]](#page-24-14) where some extra functions like RMSNorm [\[4\]](#page-21-3),
    SwiGLU [\[7\]](#page-21-6) and SiLU [\[6\]](#page-21-5) are deployed. Note that
    the two models are not evaluated end-to-end, as we only tested the non-linear
    functions part on hardware.


    <span id="page-15-0"></span>![](_page_15_Figure_1.jpeg)


    Figure 11: The nonlinear function precision measures between our approximated
    and PyTorch''s built-in functions. We selected two approximated sub-operations
    (inverse square root and pade tanh), and a function-level GELU function for error
    evaluation.


    <span id="page-15-1"></span>Table 4: Quantization Evaluation for Various Transformer
    Models Based on TATAA Setups with int8 + bfloat16


    | Method                                                                                              |
    PTQ.              | ViT Classification Accuracy (%) |                  |                  |                  |                  |
    BERT on GLUE (%) |                  |                  | GPT-2 Medium     | OPT-1.3B†        |                  |                   |

    |-----------------------------------------------------------------------------------------------------|-------------------|---------------------------------|------------------|------------------|------------------|------------------|------------------|------------------|------------------|------------------|------------------|------------------|-------------------|

    |                                                                                                     |
    Format            | DeiT-T                          | DeiT-S           | DeiT-B           |
    Swin-T           | Swin-S           | Swin-B           | QQP              | SST-2            |
    MRPC             | WikiText2<br>PPL | WikiText2<br>PPL | Lambada<br>Acc(%) |

    | Baseline                                                                                            |
    fp32              | 72.14                           | 79.83            | 81.79            |
    80.99            | 83.21            | 83.60            | 90.98            | 92.90            |
    86.03            | 15.94            | 14.62            | 75.41             |

    | TATAA                                                                                               |
    int8+<br>bfloat16 | 70.98<br>(-1.16)                | 79.35<br>(-0.48) | 81.65<br>(-0.34)
    | 79.98<br>(-1.01) | 82.44<br>(-0.77) | 82.70<br>(-0.90) | 90.15<br>(-0.83) |
    92.32<br>(-0.58) | 85.54<br>(-0.49) | 16.41<br>(+0.47) | 15.18<br>(+0.56) | 74.96<br>(-0.45)  |

    | *Baseline models with pretrained fp32 parameters are loaded from PyTorch or
    Hugging Face model hub. |                   |                                 |                  |                  |                  |                  |                  |                  |                  |                  |                  |                  |                   |


    †SmoothQuant [\[33\]](#page-22-18) is applied for OPT-1.3B quantization.


    #### 6.2 Model Accuracy


    Firstly, we evaluate the approximation techniques outlined in Section 5.2.2 to
    demonstrate that our bfloat16 implementation of non-linear functions is precise
    and can therefore be used for complete model inference. Figure [11](#page-15-0)
    shows the errors for the inversed square root, pade tanh, as well as the funtion-level
    GELU approximation. We did not evaluate the power of 2 approximation as it has
    been widely utilized in SoftMax hardware and demonstrated to produce negligible
    errors [\[36\]](#page-23-2). In the given input range, the overall RMSEs of approximations
    are 1.90 × 10<sup>−</sup><sup>3</sup> , 1.52 × 10<sup>−</sup><sup>2</sup> , and
    1.97 × 10<sup>−</sup><sup>3</sup> for the two methods and the GELU activation
    function, respectively, demonstrating our selected approximations for non-linear
    functions are sufficient.


    Using the int8 + bfloat16 post-training quantization (PTQ) method, we evaluate
    several transformer models on a range of tasks, simulating model accuracy through
    PyTorch-based quantization codes. The calibration dataset is generated by randomly
    sampled a very small size of training set (16 ∼ 128 in our setups). Table [4](#page-15-1)
    presents the inference performance for ViT[2](#page-15-2) , BERT, and GPT-2 models,
    with classification and text generation tasks. The drop in accuracy among all
    evaluation tasks is negligible from 0.34% to 1.16%, demonstrating that the TATAA
    PTQ scheme is available for flexible transformer acceleration without the need
    for retraining overhead. As illustrated above, static PTQ is applied in TATAA,
    and in the current work we can deploy other existing PTQ approaches like SmoothQuant
    [\[33\]](#page-22-18) and FQ-ViT [\[30\]](#page-22-15), thanks to the general
    support in TATAA framework. With the on-chip quantization & layout conversion
    module, TATAA can efficiently deploy quantized matrix multiplication and non-linear
    functions with appropriate data format and layout, as long as the framework gets
    static quantization scaling factors, according to Equation [\(1\)](#page-2-0).


    # 6.3 Hardware Utilization


    Table [5](#page-16-0) presents the hardware utilization on FPGA based on the selected
    configuration. It can be concluded that the DMPUs dominate the resources cost,
    and other overhead units like quantization and transpose are relatively small.
    In detail, the proposed DMPUs cost 86.8% LUTs, 85.4% FFs, and 94.1% DSPs for FPGA
    resources. Due to the SIMD approach in the bfloat16 mode, the controller overhead
    is minimal because the dataflow is shared between all FPUs.


    We provide a detailed breakdown of utilization, separating integer linear units
    from the overhead needed to support non-linear operations, and the experiment
    setups are illustrated in Figure [12,](#page-16-1) named as SA+FPU Indiv. design.
    For TATAA and SA+FPU Indiv. setup, Figure [13](#page-16-2) presents the normalized
    utilization in terms of hardware units for both linear layers (*MatMul*) and non-linear
    functions (indicated with shadows). We compare our TATAA architecture


    <span id="page-15-2"></span><sup>2</sup>Three scales of DeiT and Swin Transformer,
    -T, -S, -B refer to Tiny, Small and Base, respectively.


    <span id="page-16-0"></span>


    | Components                  | FPGA Utilization |               |              |             |  |  |  |  |

    |-----------------------------|------------------|---------------|--------------|-------------|--|--|--|--|

    |                             | LUT              | FF            | BRAM         |
    DSP         |  |  |  |  |

    | DMPUs                       | 60117 (86.8%)    | 85035 (85.4%) | 0 (0.0%)     |
    512 (94.1%) |  |  |  |  |

    | Register Files              | 2240 (3.2%)      | 4333 (4.4%)   | 78.5 (54.0%)
    | 0 (0.0%)    |  |  |  |  |

    | Dual-mode Buffers           | 280 (0.4%)       | 224 (0.2%)    | 60 (41.2%)   |
    0 (0.0%)    |  |  |  |  |

    | Quantization Layout Convert | 6558 (9.5%)      | 9899 (9.9%)   | 7 (4.8%)     |
    32 (5.9%)   |  |  |  |  |

    | Controller Misc             | 87 (0.1%)        | 80 (0.1%)     | 0 (0.0%)     |
    0 (0.0%)    |  |  |  |  |

    | One TATAA Core Total        | 69282            | 99571         | 145.5        |
    544         |  |  |  |  |


    Table 5: Hardware Utilization of the Proposed TATAA Processing Core


    <span id="page-16-1"></span>![](_page_16_Figure_3.jpeg)


    Figure 12: Experiment setup of comparing TATAA (reusing same hardware for *MatMul*
    and bfloat16 operations) and traditional implementation (individual systolic array
    and FPU, SA+FPU Indiv. in abbreviation). The systolic array scale and the number
    of FPUs are the same for fair comparison.


    with a design that utilizes individual integer systolic arrays and FPUs without
    reuse, on the same scales (32 by 32 array and 128 lane SIMD FPUs), as illustrated
    on the left side of Figure [13](#page-16-2) (SA+FPU Indiv.). Such a comparison
    indicates that the reuse scheme drastically reduces hardware costs for non-linear
    functions. Additionally, we present other related FPGA-based accelerators utilization
    of linear versus non-linear functions based on their reported results [\[21,](#page-22-6)
    [51,](#page-23-12) [55,](#page-24-2) [52,](#page-23-15) [68\]](#page-24-15). Our
    TATAA architecture exhibits comparable overhead across three resource types, with
    only 10.5% FFs, and no DSPs overhead especially. As exceptions, EFA-Trans [\[51\]](#page-23-12)
    reports linear operation units including SoftMax, and in the work by Lu et al.
    [\[68\]](#page-24-15), where LUTs are employed for linear computations and the
    DSP overhead for non-linear operations reaches 100%.


    #### 6.4 TATAA Runtime Analysis


    <span id="page-16-2"></span>We choose DeiT-Small (DeiT-S) with batch size 16 and
    BERT with sequence length 128 & batch size 32 to measure layer latency with its
    workload size Figure [14.](#page-17-0) The linear *MatMul* layers (such as QKV-GEN,
    QK-MUL, SV-MUL, etc.) are the primary contributors to the transformer workload
    when measured in terms of GOP (giga operations) or GFLOP (giga floating-point
    operations), and therefore heavily influence total latency. Among these linear
    layers, QK-MUL and SV-MUL are slightly less efficient, exhibiting a smaller workload-latency
    ratio compared to other MLP layers such


    ![](_page_16_Figure_8.jpeg)


    Figure 13: Normalized hardware utilization on FPGA for the proposed TATAA and
    other related works, in terms of linear *MatMul* units (-L) and non-linear functions
    overhead (-NL) based on three types of resources (LUT, FF, DSP).


    <span id="page-17-0"></span>![](_page_17_Figure_1.jpeg)


    <span id="page-17-1"></span>Figure 14: Layerwise latency & computation workload
    size breakdown in DeiT-Small and BERT, during TATAA runtime.


    ![](_page_17_Figure_3.jpeg)


    Figure 15: Evaluation of the selected non-linear functions in various transformer
    models. The throughput (GFLOPS) is measured on hardware runtime. The SoftMax,
    LayerNorm and GELU functions are based on BERT model, while the SiLU is used in
    ChatGLM and RMSNorm is based on Llama-7B.


    as QKV-GEN. This inefficiency is attributed to the fact that the multi-head attention
    mechanism reduces the matrix sizes in each *MatMul*, which in turn leads to less
    data reuse in the output-stationary dataflow. Such an analysis shows the optimization
    space in the future to dynamically optimize the different size of workloads. In
    addition, despite the non-linear functions having significantly smaller workloads
    compared to the linear *MatMul* layers, they nevertheless contribute significantly
    to latency, accounting for approximately 25% of the total end-to-end inference
    time. Hence, the implementation of non-linear functions is crucial for both model
    performance and hardware efficiency, as is also proved in some previous studies
    [\[2\]](#page-21-1). Our flexible and adaptable framework for compiling these
    functions offers an optimization potential for new transformer models that utilize
    various non-linear functions.


    Figure [15](#page-17-1) presents the non-linear functions throughput based on
    bfloat16 basic operations (giga floating-point operations per second, GFLOPS),
    over several selected transformer models. Since the proposed TATAA architecture
    supports full pipeline between basic operations (*fpmul*, *fpadd*, and *fpapp*),
    the theoretical maximum throughput considering the computation resources (i.e.,
    FPU number in TATAA architecture), can be calculated by Equation [\(6\)](#page-17-2):


    <span id="page-17-2"></span>

    $$GFLOPS\_{theo} = K \cdot N \cdot W \cdot freq \tag{6}$$


    where the K · N · W refer to the number of FPUs (128 in our setup). In our evaluation
    setup, the throughput GF LOP Stheo = 230.40. Among all the test functions, our
    TATAA can reach to maximum 189.45 GFLOPS in GELU function of BERT model. This
    is because the memory-bound nature of these bfloat16-based non-linear functions.
    Still, our compilation framework can reach 82.2% maximum throughput and leave
    an optimization space for compiler in the future. For instance, the SoftMax function
    requires accessing external memory several times due to the data dependency as
    illustrated in the compilation steps Figure [10,](#page-13-0) thus causing lower
    throughput as the compiled operations are highly memory-bound. The users can deploy
    more efficient SoftMax schemes to improve it, like Flash-Attention [\[69\]](#page-24-16)
    which significantly reduces the memory I/O.


    We further compare the latency of non-linear functions in TATAA with various related
    studies that have documented their evaluations of nonlinear functions, as shown
    in Table [6.](#page-18-0) Since the token lengths and model scales in these acceleration
    works are different, we normalize the latency to cycles per element, as the same
    setup in [\[17\]](#page-22-2). The proposed TATAA achieves significantly lower
    latency in terms of SoftMax and LayerNorm function, because the transformable
    architecture is able to utilize all the processing units for non-linear functions,
    boosting the theoretical floating-point operations throughput. In terms of GELU,
    since TATAA only applies naive approximation to demonstrate


    ![](_page_18_Figure_1.jpeg)


    <span id="page-18-0"></span>Table 6: Normalized non-linear functions latency for
    TATAA and related works for non-linear functions implementation.


    Latency (cycles per element) DSP


    Non-linear


    <span id="page-18-1"></span>Figure 16: Comparison of normalized throughput and
    normalized area efficiency between TATAA and related works. The area efficiency
    is measured by DSP utilization.


    our flexibility, the final latency is not good as Chen et al. [\[49\]](#page-23-13).
    But in general, our total latency for non-linear functions still outperforms Chen
    et al. by 4.25×, without any computational resources overhead. In addition, we
    compare the proposed implementation with previous works in terms of throughput
    and area efficiency (throughput/DSP blocks), as shown in Figure [16.](#page-18-1)
    The results show that TATAA also achieves higher throughput and comparable area
    efficiency in DSPs (reach 19.6× and 9.1× higher than the baseline NPE-1024 design
    [\[17\]](#page-22-2)), while other works may still cost more LUTs or FFs for non-linear
    functions. Compared to our previous study [\[70\]](#page-24-17) which fuses fp32
    and 8-bit block floating point (bfp8) with the similar idea, TATAA achieves higher
    throughput improvement since we are targeting a cheaper data format (int8 and
    bfloat16) and a more efficient hardware reuse scheme. All in all, the key benefit
    of TATAA is the potential for further optimization of efficiency through compilation
    of emerging non-linear functions, a feature that is absent in those accelerators
    with fixed and specific units.


    #### 6.5 Resource Efficiency


    <span id="page-18-2"></span>As one of the key contributions in TATAA, we reuse
    the same integer hardware units for non-linear functions deployment, saving the
    hardware overhead and thus improving the resource/area efficiency. Firstly, we
    evaluate end-to-end throughput (giga operations per second, GOPS) of TATAA and
    normalize the throughput into resource to obtain resource efficiency,


    ![](_page_18_Figure_7.jpeg)


    Figure 17: Resource efficiency in terms of GOPS/DSP and GOPS/kLUT, comparing TATAA
    with several FPGA-based acceleration frameworks


    <span id="page-19-0"></span>


    | Work                 | Data              | End2end | FPGA<br>Platform | FPGA
    Utilization |        |       | Freq. | Power | Eval. | Throughput               |
    Throughput             | DSP                        |                               |

    |----------------------|-------------------|---------|------------------|------------------|--------|-------|-------|-------|-------|--------------------------|------------------------|----------------------------|-------------------------------|

    |                      | Formats‡          | Support |                  | LUT(k)           |
    FF(k)  | BRAM  | DSP   | (MHz) | (W)   | Models                   | Inf./sec§              |
    (GOPS)                     | Efficiency                    |

    | Auto-ViT<br>Acc [42] | fxp, fp32         | No      | ZCU102           | 185.0            |
    -      | -     | 1552  | 150   | 9.6   | DeiT-S<br>DeiT-B         | 99.7<br>34.0           |
    907.8<br>1181.5            | 0.585<br>0.761                |

    | Huang<br>et al. [21] | int8, int8        | Yes     | ZCU102           | 144.5            |
    168.0  | 648   | 1268  | 300   | 29.6  | ViT-S<br>ViT-T           | 89.7<br>245.3          |
    762.7<br>616.1             | 0.601<br>0.486                |

    | HPTA[71]             | int8, int8        | Yes     | ZCU102           | 209.9            |
    368.4  | 345   | 2307  | 200   | 20.0  | BERT<br>Swin-T           | 81.9<br>148.8          |
    -<br>-                     | 0.035†<br>0.065†              |

    | NPE [17]             | int16, fxp        | Yes     | VCU118           | 192.4            |
    351.1  | 369   | 2020  | 200   | 20.0  | BERT                     | 36.8                   |
    -                          | 0.018†                        |

    | FTRANS [46]          | fp16, fp32        | Yes     | VCU118           | 451.1            |
    506.6  | -     | 6531  | -     | -     | RoBERTa                  | 94.25                  |
    -                          | 0.014                         |

    | ViA [20]             | fp16, fp16        | Yes     | Alveo U50        | 258.0            |
    257.0  | 1022  | 2420  | 300   | 39.0  | Swin-T                   | -                      |
    309.6                      | 0.128                         |

    | SWAT [52]            | int8, fxp         | Yes     | Alveo U50        | 271.0            |
    -      | 609.5 | 1863  | 200   | 14.4  | Swin-T                   | -                      |
    301.9                      | 0.162                         |

    | ME-ViT[53]           | int8, fxp         | Yes     | Alveo U200       | 192.0            |
    132.0  | 288   | 1024  | 300   | 9.3   | DeiT-B<br>DeiT-S         | 23.9<br>41.7           |
    -<br>-                     | 0.0233†<br>0.0407†            |

    | DFX [54]             | fp16, fp16        | Yes     | Alveo U280       | 520.0            |
    1107.0 | 1192  | 3533  | 200   | -     | GPT-2                    | 0.361                  |
    185.6                      | 0.0001†                       |

    | Ye et al.[56]        | int8, fxp         | No      | Alveo U250       | 736.0            |
    -      | 1781  | 4189  | 300   | -     | -                        | -                      |
    1800.0                     | 0.430                         |

    | FET-OPU[22]          | int8, fxp         | Yes     | Alveo U280       | 886.8            |
    716.6  | 1357  | 4864  | 200   | 7.4   | DeiT-B<br>BERT<br>Swin-T | 71.8<br>146.6<br>124.1
    | 1264.6<br>1635.8<br>1070.1 | 0.0148†<br>0.0301†<br>0.0256† |

    | TATAA                | int8,<br>bfloat16 | Yes     | Alveo U280       | 724.9            |
    1154.9 | 1472  | 4352  | 225   | 10.8  | DeiT-S                   | 218.6                  |
    2836.2*                    | 0.626<br>0.0502†              |

    |                      |                   |         |                  |                  |        |       |       |       |       |
    DeiT-B                   | 67.6                   | 2796.5                     |
    0.643<br>0.0156†              |

    |                      |                   |         |                  |                  |        |       |       |       |       |
    BERT                     | 116.8                  | 2935.2                     |
    0.674<br>0.0269†              |

    |                      |                   |         |                  |                  |        |       |       |       |       |
    Swin-T                   | 179.7                  | 2512.3                     |
    0.685<br>0.0587†              |

    |                      |                   |         |                  |                  |        |       |       |       |       |
    GPT-2                    | 7.9                    | 2579.4                     |
    0.593<br>0.0018†              |


    Table 7: Hardware Performance Comparison with Relative FPGA-based Accelerators
    for Transformer Models


    ‡ Data formats for linear *MatMul* (the former one) and non-linear functions (the
    latter one). fxp refers to fixed-point numbers.


    § Inference per second (Inf./sec) measures how many end-to-end images or sequences
    can be processed through hardware in one second.


    <sup>∗</sup> We clarify that in our work, the total operation is obtained by doubling
    the MAC operation. Some previous work may directly report MACs as throughput.


    † Results with inference/sec/DSP are marked with symbols †, while those based
    on GOPS/DSP are indicated separately. We provide both results for TATAA.


    in terms of both LUT and DSP. For comparison, we selected related FPGA-based accelerators
    for transformers, calculating their efficiency based on the utilization results.
    The selected models have various data format setups, e.g., DFX [\[54\]](#page-24-1)
    implements fp16 for all operations while Auto-ViT-Acc [\[42\]](#page-23-11) only
    targets linear *MatMul* in integer. To benchmark our transformable architecture,
    we also implement the SA+FPU Indiv. design, the opposite setup compared to TATAA,
    as shown in Figure [12.](#page-16-1)


    Figure [17](#page-18-2) presents the resource efficiency results based on the
    selected implementations, in terms of end-to-end GOPS per DSP (GOPS/DSP) and GOPS
    per kilo-LUT (GOPS/kLUT). Compared with our own baseline (SA+FPU Indiv.), TATAA
    achieves 1.28× LUT efficiency and 1.18× DSP efficiency, due to the hardware reusing
    scheme in the proposed transformable architecture. Compared to other related works,
    our int8 + bfloat16 approach is still compatible with smaller workload in linear
    layers. Although some previous works demonstrate superior resource efficiency
    in terms of LUT or DSP, it is important to mention that they either do not support
    full inference on hardware (Auto-ViT-Acc [\[42\]](#page-23-11)) or employ more
    aggressive approximations of non-linear functions using fixed-point format (Huang
    et al. [\[21\]](#page-22-6)), lacking the PTQ retrain-free benefit that is implemented
    in our TATAA. Besides, Wu et. al [\[70\]](#page-24-17) proposed to fusing block
    floating-point bfp8 and fp32 formats in the same architecture which is similar
    to TATAA. However, due to the higher hardware cost for block-wise operation and
    fp32, our TATAA achieves around 2.25× and 1.31× higher efficiency for GOPS/kLUT
    and GOPS/DSP.


    #### 6.6 Systematic Comparison with Related Studies


    We summarize and compare other related FPGA-based transformer accelerators with
    our TATAA FPGA prototype with respect to throughput and resource efficiency, as
    illustrated in Table [7.](#page-19-0) In our setup, the ViT models (DeiT, Swin)
    are evaluated on ImageNet with a batch size of 16, the BERT sequence length is
    fixed at 128, and the GPT-2 with 345M parameters is under 512 sequence length.
    We only measure the pre-fill stage for the GPT-2 inference. For other related
    works, we scaled their results to match ours if they set different sequence lengths
    or batch sizes, for a fair comparison. Our TATAA achieves up to 2836.2 GOPS with
    an end-to-end acceleration rate of maximum 218.6 inference per second (Inf./sec)
    for vision models, while reaching 2579.4 ∼ 2935.2 GOPS in language models. We
    also report throughput


    <span id="page-20-0"></span>![](_page_20_Figure_1.jpeg)


    Figure 18: Nomalized power efficiency (Inf./sec/W) comparison between TATAA and
    GPU implementations.


    efficiency by normalizing the total throughput with computational resources (DSPs
    in FPGA), offering results in terms of GOPS/DSP or Inf./sec/DSP to facilitate
    comparisons across all related works. Compared to other accelerators, TATAA obtains
    higher resource efficiency by factors ranging from 1.13× to 2.29× in terms of
    Inf./sec/DSP among all small-scale models, except in certain cases where end-to-end
    support is lacking (e.g., Auto-ViT-Acc [\[42\]](#page-23-11)), or in some dedicated
    optimization work (e.g., FET-OPU [\[22\]](#page-22-7) achieves ∼ 0.003 higher
    efficiency than TATAA). Nevertheless, the most important benefit of TATAA is to
    support general and flexible non-linear functions, unlike other works deploying
    specific support for these functions.


    We acknowledge that there are other leading acceleration frameworks that surpass
    TATAA in terms of throughput and area efficiency. For instance, the work by Chen
    et al. [\[49\]](#page-23-13) introduced a complete pipeline model using a spatial
    accelerator on FPGAs, with each block layer allocated to a separate hardware module,
    maintaining adequate buffer capacities for the pipeline. This configuration significantly
    enhances throughput by minimizing memory I/O operations, but as transformer models
    expand, mapping and compiling such a comprehensive pipeline approach becomes increasingly
    complex. Our strategy, in contrast, focuses on developing a general-purpose and
    broadly applicable accelerator. SSR''s proposal by Zhuang et al. [\[48\]](#page-23-14)
    involves implementing a spatial and temporal hybrid design on Versal ACAP devices,
    where MatMul operations utilize the AI Engine of AMD FPGAs. Directly comparing
    their framework''s efficiency with ours wouldn''t be equitable. Nonetheless, the
    primary innovation in TATAA is presenting a novel methodology for reusing the
    same hardware across varied operations within transformer models. Thus, TATAA
    can operate independently of spatial or temporal architectures in transformer
    accelerators. There is significant potential to further refine pipeline strategies
    among different TATAA cores to facilitate the MatMul and bfloat16 pipeline, and
    we intend to explore this potential in the future, inspired by these related works.


    ### 6.7 Power Efficiency versus GPUs


    Power efficiency, as an important highlight for the TATAA to be deployed on U280,
    shows prominent features of FPGA over modern GPUs. We used Xilinx RunTime(XRT)
    for hardware execution, and Vitis embeded power profile with Xilinx Board Utility
    (xbutil) for computing power measurements. NVIDIA system management interface
    (nvidia-smi) is used for measuring GPU power on Quadro RTX8000, GeForce RTX3090,
    and GeForce RTX4090.


    We evaluate the internal power consumption of the TATAA framework, assessing the
    power efficiency (inferences per watt) for each model. Subsequently, we measured
    the model execution latency on various GPUs using consistent batch sizes and sequence
    lengths as in the TATAA framework, thereby computing the power efficiency (Inf./sec/W).
    As depicted in Figure [18,](#page-20-0) our TATAA FPGA surpasses the performance
    of the RTX3090 and RTX8000 across all models, demonstrating a 1.10× improvement
    over the RTX4090 in normalized power efficiency for GPT2. For smaller models,
    such as the small DeiT transformer, TATAA remains competitive, achieving comparable
    power efficiency in end-to-end throughput. While TATAA shows a slight degradation
    in efficiency on Deit-B compared to the RTX4090, it significantly outperforms
    other devices on the BERT model (2.19× more than 4090) when the sequence length
    of inputs is properly adapted to the dataflow requirements of the TATAA hardware.
    This emphasizes one of the critical advantages of deploying TATAA on FPGAs compared
    to GPUs, highlighting its potential for processing large models with superior
    power efficiency.


    # 7 Conclusions


    In this work, we have presented TATAA, a programmable accelerators on FPGA for
    transformer models by using a novel transformable arithmetic architecture. Using
    TATAA, we demonstrate that both low-bitwidth integer (int8) and floating-point
    (bfloat16) operations can be implemented efficiently using the same underlying
    processing array hardware. By transforming the array from systolic mode for int8
    matrix multiplication to SIMD-mode for vectorized bfloat16 operations, we show
    that end-to-end acceleration of modern transformer models including both linear
    and non-linear functions can be achieved with state-of-the-art performance and
    efficiency. In the future, we plan to explore more general FPGA implementations
    of TATAA with more devices support (i.e., with or without HBM) and to enhance
    the flexibility of our compilation framework to accelerate future transformer
    models as they are being rapidly developed.


    # 8 Acknowledgements


    This work was supported in part by the Research Grants Council (RGC) of Hong Kong
    under the Research Impact Fund project R7003-21 and the Theme-based Research Scheme
    (TRS) Project T45-701-22-R. This work was supported by AI Chip Center for Emerging
    Smart Systems (ACCESS), sponsored by InnoHK funding, Hong Kong SAR.


    # References


    - <span id="page-21-0"></span>[1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
    Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention
    Is All You Need. *Advances in Neural Information Processing Systems*, 30, 2017.

    - <span id="page-21-1"></span>[2] Jacob R. Stevens, Rangharajan Venkatesan, Steve
    Dai, Brucek Khailany, and Anand Raghunathan. Softermax: Hardware/software co-design
    of an efficient softmax for transformers. In *2021 58th ACM/IEEE Design Automation
    Conference (DAC)*, page 469–474. IEEE Press, 2021.

    - <span id="page-21-2"></span>[3] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey
    E Hinton. Layer normalization. *arXiv preprint arXiv:1607.06450*, 2016.

    - <span id="page-21-3"></span>[4] Biao Zhang and Rico Sennrich. Root Mean Square
    Layer Normalization. In *Advances in Neural Information Processing Systems 32*,
    Vancouver, Canada, 2019.

    - <span id="page-21-4"></span>[5] Dan Hendrycks and Kevin Gimpel. Gaussian error
    linear units (gelus). *arXiv preprint arXiv:1606.08415*, 2016.

    - <span id="page-21-5"></span>[6] Prajit Ramachandran, Barret Zoph, and Quoc V
    Le. Searching for activation functions. *arXiv preprint arXiv:1710.05941*, 2017.

    - <span id="page-21-6"></span>[7] Noam Shazeer. Glu variants improve transformer.
    *arXiv preprint arXiv:2002.05202*, 2020.

    - <span id="page-21-7"></span>[8] Vivienne Sze, Yu-Hsin Chen, Tien-Ju Yang, and
    Joel S Emer. Efficient processing of deep neural networks: A tutorial and survey.
    *Proceedings of the IEEE*, 105(12):2295–2329, 2017.

    - <span id="page-21-8"></span>[9] Dexu Lin, LIAO Edward, Somdeb Majumdar, Aaron
    Lamb, and Karamvir Chatha. Approximation of non-linear functions in fixed point
    using look-up tables, July 31 2018. US Patent 10,037,306.

    - <span id="page-21-9"></span>[10] Xue Geng, Jie Lin, Bin Zhao, Anmin Kong, Mohamed
    M Sabry Aly, and Vijay Chandrasekhar. Hardware-aware softmax approximation for
    deep neural networks. In *Computer Vision–ACCV 2018: 14th Asian Conference on
    Computer Vision, Perth, Australia, December 2–6, 2018, Revised Selected Papers,
    Part IV 14*, pages 107–122. Springer, 2019.

    - <span id="page-21-10"></span>[11] Shulin Zeng, Jun Liu, Guohao Dai, Xinhao Yang,
    Tianyu Fu, Hongyi Wang, Wenheng Ma, Hanbo Sun, Shiyao Li, Zixiao Huang, Yadong
    Dai, Jintao Li, Zehao Wang, Ruoyu Zhang, Kairui Wen, Xuefei Ning, and Yu Wang.
    Flightllm: Efficient large language model inference with a complete mapping flow
    on fpgas. *Proceedings of the 2024 ACM/SIGDA International Symposium on Field
    Programmable Gate Arrays*, 2024.

    - <span id="page-21-11"></span>[12] Hugo Touvron, Thibaut Lavril, Gautier Izacard,
    Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman
    Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation
    language models. *arXiv preprint arXiv:2302.13971*, 2023.

    - <span id="page-21-12"></span>[13] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng
    Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position
    embedding. *Neurocomputing*, 568:127063, 2024.

    - <span id="page-21-13"></span>[14] Zhikai Li and Qingyi Gu. I-vit: Integer-only
    quantization for efficient vision transformer inference. In *Proceedings of the
    IEEE/CVF International Conference on Computer Vision*, pages 17065–17075, 2023.

    - <span id="page-22-0"></span>[15] Sehoon Kim, Amir Gholami, Zhewei Yao, Michael
    W Mahoney, and Kurt Keutzer. I-bert: Integer-only bert quantization. *International
    Conference on Machine Learning (Accepted)*, 2021.

    - <span id="page-22-1"></span>[16] A. Marchisio, D. Dura, M. Capra, M. Martina,
    G. Masera, and M. Shafique. Swifttron: An efficient hardware accelerator for quantized
    transformers. In *2023 International Joint Conference on Neural Networks (IJCNN)*,
    2023.

    - <span id="page-22-2"></span>[17] Hamza Khan, Asma Khan, Zainab Khan, Lun Bin
    Huang, Kun Wang, and Lei He. Npe: An fpga-based overlay processor for natural
    language processing. In *The 2021 ACM/SIGDA International Symposium on Field-Programmable
    Gate Arrays*, FPGA ''21, page 227, New York, NY, USA, 2021. Association for Computing
    Machinery.

    - <span id="page-22-3"></span>[18] Ali Hadi Zadeh, Mostafa Mahmoud, Ameer Abdelhadi,
    and Andreas Moshovos. Mokey: Enabling narrow fixed-point inference for out-of-the-box
    floating-point transformer models. In *Proceedings of the 49th Annual International
    Symposium on Computer Architecture*, pages 888–901, 2022.

    - <span id="page-22-4"></span>[19] Xiuying Wei, Yunchen Zhang, Xiangguo Zhang,
    Ruihao Gong, Shanghang Zhang, Qi Zhang, Fengwei Yu, and Xianglong Liu. Outlier
    suppression: Pushing the limit of low-bit transformer language models. *Advances
    in Neural Information Processing Systems*, 35:17402–17414, 2022.

    - <span id="page-22-5"></span>[20] Teng Wang, Lei Gong, Chao Wang, Yang Yang,
    Yingxue Gao, Xuehai Zhou, and Huaping Chen. Via: A novel vision-transformer accelerator
    based on fpga. *IEEE Transactions on Computer-Aided Design of Integrated Circuits
    and Systems*, 41(11):4088–4099, 2022.

    - <span id="page-22-6"></span>[21] Mingqiang Huang, Junyi Luo, Chenchen Ding,
    Zikun Wei, Sixiao Huang, and Hao Yu. An integer-only and group-vector systolic
    accelerator for efficiently mapping vision transformer on edge. *IEEE Transactions
    on Circuits and Systems I: Regular Papers*, 2023.

    - <span id="page-22-7"></span>[22] Yueyin Bai, Hao Zhou, Keqing Zhao, Hongji Wang,
    Jianli Chen, Jun Yu, and Kun Wang. Fet-opu: A flexible and efficient fpga-based
    overlay processor for transformer networks. In *2023 IEEE/ACM International Conference
    on Computer Aided Design (ICCAD)*, pages 1–9. IEEE, 2023.

    - <span id="page-22-8"></span>[23] Alexey Dosovitskiy, Lucas Beyer, Alexander
    Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani,
    Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16
    words: Transformers for image recognition at scale. *arXiv preprint arXiv:2010.11929*,
    2020.

    - <span id="page-22-9"></span>[24] Hugo Touvron, Matthieu Cord, Matthijs Douze,
    Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou. Training data-efficient
    image transformers & distillation through attention. In *International Conference
    on Machine Learning*, pages 10347–10357. PMLR, 2021.

    - <span id="page-22-10"></span>[25] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan
    Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical
    vision transformer using shifted windows. In *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, pages 10012–10022, 2021.

    - <span id="page-22-11"></span>[26] Jacob Devlin, Ming-Wei Chang, Kenton Lee,
    and Kristina Toutanova. BERT: Pre-training of Deep Bidirectional Transformers
    for Language Understanding. *arXiv preprint arXiv:1810.04805*, 2018.

    - <span id="page-22-12"></span>[27] Susan Zhang, Stephen Roller, Naman Goyal,
    Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li,
    Xi Victoria Lin, et al. OPT: Open Pre-trained Transformer Language Models. *arXiv
    preprint arXiv:2205.01068*, 2022.

    - <span id="page-22-13"></span>[28] Haoli Bai, Lu Hou, Lifeng Shang, Xin Jiang,
    Irwin King, and Michael R Lyu. Towards efficient post-training quantization of
    pre-trained language models. *Advances in Neural Information Processing Systems*,
    35:1405–1418, 2022.

    - <span id="page-22-14"></span>[29] Zhenhua Liu, Yunhe Wang, Kai Han, Wei Zhang,
    Siwei Ma, and Wen Gao. Post-training quantization for vision transformer. *Advances
    in Neural Information Processing Systems*, 34:28092–28103, 2021.

    - <span id="page-22-15"></span>[30] Yang Lin, Tianyu Zhang, Peiqin Sun, Zheng
    Li, and Shuchang Zhou. Fq-vit: Post-training quantization for fully quantized
    vision transformer. In *Proceedings of the Thirty-First International Joint Conference
    on Artificial Intelligence, IJCAI-22*, pages 1173–1179, 2022.

    - <span id="page-22-16"></span>[31] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman,
    and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. *arXiv preprint
    arXiv:2305.14314*, 2023.

    - <span id="page-22-17"></span>[32] Markus Nagel, Marios Fournarakis, Rana Ali
    Amjad, Yelysei Bondarenko, Mart Van Baalen, and Tijmen Blankevoort. A white paper
    on neural network quantization. *arXiv preprint arXiv:2106.08295*, 2021.

    - <span id="page-22-18"></span>[33] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao
    Wu, Julien Demouth, and Song Han. SmoothQuant: Accurate and efficient post-training
    quantization for large language models. In *Proceedings of the 40th International
    Conference on Machine Learning*, 2023.

    - <span id="page-23-0"></span>[34] Zhewei Yao, Zhen Dong, Zhangcheng Zheng, Amir
    Gholami, Jiali Yu, Eric Tan, Leyuan Wang, Qijing Huang, Yida Wang, Michael Mahoney,
    et al. Hawq-v3: Dyadic neural network quantization. In *International Conference
    on Machine Learning*, pages 11875–11886. PMLR, 2021.

    - <span id="page-23-1"></span>[35] John Bridle. Training stochastic model recognition
    algorithms as networks can lead to maximum mutual information estimation of parameters.
    *Advances in Neural Information Processing Systems*, 2, 1989.

    - <span id="page-23-2"></span>[36] Jacob R. Stevens, Rangharajan Venkatesan, Steve
    Dai, Brucek Khailany, and Anand Raghunathan. Softermax: Hardware/software co-design
    of an efficient softmax for transformers. *CoRR*, abs/2103.09301, 2021.

    - <span id="page-23-3"></span>[37] Song Han, Junlong Kang, Huizi Mao, Yiming Hu,
    Xin Li, Yubin Li, Dongliang Xie, Hong Luo, Song Yao, Yu Wang, et al. Ese: Efficient
    speech recognition engine with sparse lstm on fpga. In *Proceedings of the 2017
    ACM/SIGDA International Symposium on Field-Programmable Gate Arrays*, pages 75–84,
    2017.

    - <span id="page-23-4"></span>[38] Xiao Dong, Xiaolei Zhu, and De Ma. Hardware
    implementation of softmax function based on piecewise lut. In *2019 IEEE International
    Workshop on Future Computing (IWOFC*, pages 1–3. IEEE, 2019.

    - <span id="page-23-5"></span>[39] Yaman Umuroglu, Yash Akhauri, Nicholas J. Fraser,
    and Michaela Blott. Logicnets: Co-designed neural networks and circuits for extreme-throughput
    applications, 2020.

    - <span id="page-23-6"></span>[40] Mahdi Nazemi, Ghasem Pasandi, and Massoud Pedram.
    Nullanet: Training deep neural networks for reducedmemory-access inference, 2018.

    - <span id="page-23-9"></span>[41] Tae Jun Ham, Sung Jun Jung, Seonghak Kim, Young
    H Oh, Yeonhong Park, Yoonho Song, Jung-Hun Park, Sanghee Lee, Kyoung Park, Jae
    W Lee, et al. Aˆ 3: Accelerating attention mechanisms in neural networks with
    approximation. In *2020 IEEE International Symposium on High Performance Computer
    Architecture (HPCA)*, pages 328–341. IEEE, 2020.

    - <span id="page-23-11"></span>[42] Zhengang Lit, Mengshu Sun, Alec Lu, Haoyu
    Ma, Geng Yuan, Yanyue Xie, Hao Tang, Yanyu Li, Miriam Leeser, Zhangyang Wang,
    et al. Auto-vit-acc: An fpga-aware automatic acceleration framework for vision
    transformer with mixed-scheme quantization. In *2022 32nd International Conference
    on Field-Programmable Logic and Applications (FPL)*, pages 109–116. IEEE, 2022.

    - <span id="page-23-10"></span>[43] Xinyi Zhang, Yawen Wu, Peipei Zhou, Xulong
    Tang, and Jingtong Hu. Algorithm-hardware co-design of attention mechanism on
    fpga devices. *ACM Transactions on Embedded Computing Systems (TECS)*, 20(5s):1–24,
    2021.

    - <span id="page-23-18"></span>[44] Zejian Liu, Gang Li, and Jian Cheng. Hardware
    acceleration of fully quantized bert for efficient natural language processing.
    In *2021 Design, Automation & Test in Europe Conference & Exhibition (DATE)*,
    pages 513–516. IEEE, 2021.

    - <span id="page-23-7"></span>[45] NVIDIA. Transformer engine documentation. [https://docs.nvidia.com/deeplearning/](https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/)
    [transformer-engine/user-guide/](https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/).

    - <span id="page-23-16"></span>[46] Bingbing Li, Santosh Pandey, Haowen Fang,
    Yanjun Lyv, Ji Li, Jieyang Chen, Mimi Xie, Lipeng Wan, Hang Liu, and Caiwen Ding.
    Ftrans: Energy-efficient acceleration of transformers using fpga. In *Proceedings
    of the ACM/IEEE International Symposium on Low Power Electronics and Design*,
    ISLPED ''20, page 175–180, New York, NY, USA, 2020. Association for Computing
    Machinery.

    - <span id="page-23-17"></span>[47] Suyeon Hur, Seongmin Na, Dongup Kwon, Joonsung
    Kim, Andrew Boutros, Eriko Nurvitadhi, and Jangwoo Kim. A fast and flexible fpga-based
    accelerator for natural language processing neural networks. *ACM Trans. Archit.
    Code Optim.*, 20(1), February 2023.

    - <span id="page-23-14"></span>[48] Jinming Zhuang, Zhuoping Yang, Shixin Ji,
    Heng Huang, Alex K. Jones, Jingtong Hu, Yiyu Shi, and Peipei Zhou. Ssr: Spatial
    sequential hybrid architecture for latency throughput tradeoff in transformer
    acceleration. In *Proceedings of the 2024 ACM/SIGDA International Symposium on
    Field Programmable Gate Arrays*, FPGA ''24, page 55–66, New York, NY, USA, 2024.
    Association for Computing Machinery.

    - <span id="page-23-13"></span>[49] Hongzheng Chen, Jiahao Zhang, Yixiao Du, Shaojie
    Xiang, Zichao Yue, Niansong Zhang, Yaohui Cai, and Zhiru Zhang. Understanding
    the potential of fpga-based spatial acceleration for large language model inference.
    *ACM Trans. Reconfigurable Technol. Syst.*, apr 2024. Just Accepted.

    - <span id="page-23-8"></span>[50] Ramya Prabhu, Ajay Nayak, Jayashree Mohan,
    Ramachandran Ramjee, and Ashish Panwar. vattention: Dynamic memory management
    for serving llms without pagedattention, 2024.

    - <span id="page-23-12"></span>[51] Xin Yang and Tao Su. Efa-trans: An efficient
    and flexible acceleration architecture for transformers. *Electronics*, 11(21):3550,
    2022.

    - <span id="page-23-15"></span>[52] Qiwei Dong, Xiaoru Xie, and Zhongfeng Wang.
    Swat: An efficient swin transformer accelerator based on fpga. In *Proceedings
    of the 29th Asia and South Pacific Design Automation Conference*, ASPDAC ''24,
    page 515–520. IEEE Press, 2024.

    - <span id="page-24-0"></span>[53] Kyle Marino, Pengmiao Zhang, and Viktor K Prasanna.
    Me-vit: A single-load memory-efficient fpga accelerator for vision transformers.
    In *2023 IEEE 30th International Conference on High Performance Computing, Data,
    and Analytics (HiPC)*, pages 213–223. IEEE, 2023.

    - <span id="page-24-1"></span>[54] Seongmin Hong, Seungjae Moon, Junsoo Kim, Sungjae
    Lee, Minsub Kim, Dongsoo Lee, and Joo-Young Kim. Dfx: A low-latency multi-fpga
    appliance for accelerating transformer-based text generation. In *2022 55th IEEE/ACM
    International Symposium on Microarchitecture (MICRO)*, pages 616–630. IEEE, 2022.

    - <span id="page-24-2"></span>[55] Yueyin Bai, Hao Zhou, Keqing Zhao, Manting
    Zhang, Jianli Chen, Jun Yu, and Kun Wang. Ltrans-opu: A low-latency fpga-based
    overlay processor for transformer networks. In *2023 33rd International Conference
    on Field-Programmable Logic and Applications (FPL)*, pages 283–287, 2023.

    - <span id="page-24-3"></span>[56] Wenhua Ye, Xu Zhou, Joey Zhou, Cen Chen, and
    Kenli Li. Accelerating attention mechanism on fpgas based on efficient reconfigurable
    systolic array. *ACM Trans. Embed. Comput. Syst.*, 22(6), nov 2023.

    - <span id="page-24-4"></span>[57] Stefano Markidis, Steven Wei Der Chien, Erwin
    Laure, Ivy Bo Peng, and Jeffrey S Vetter. Nvidia tensor core programmability,
    performance & precision. In *2018 IEEE International Parallel and Distributed
    Processing Symposium Workshops (IPDPSW)*, pages 522–531. IEEE, 2018.

    - <span id="page-24-5"></span>[58] Chris Lomont. Fast inverse square root. *Tech-315
    Nical Report*, 32, 2003.

    - <span id="page-24-6"></span>[59] Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen
    Mellempudi, Dipankar Das, Kunal Banerjee, Sasikanth Avancha, Dharma Teja Vooturi,
    Nataraj Jammalamadaka, Jianyu Huang, Hector Yuen, Jiyan Yang, Jongsoo Park, Alexander
    Heinecke, Evangelos Georganas, Sudarshan Srinivasan, Abhisek Kundu, Misha Smelyanskiy,
    Bharat Kaul, and Pradeep Dubey. A study of bfloat16 for deep learning training,
    2019.

    - <span id="page-24-7"></span>[60] Yao Fu, Ephrem Wu, Ashish Sirasao, Sedny Attia,
    Kamran Khan, and Ralph Wittig. Deep learning with int8 optimization on Xilinx
    devices. *Xilinx White Paper*, 2016.

    - <span id="page-24-8"></span>[61] Xinheng Liu, Yao Chen, Prakhar Ganesh, Junhao
    Pan, Jinjun Xiong, and Deming Chen. Hikonv: High throughput quantized convolution
    with novel bit-wise management and computation. In *2022 27th Asia and South Pacific
    Design Automation Conference (ASP-DAC)*, pages 140–146, 2022.

    - <span id="page-24-9"></span>[62] Junjie Bai, Fang Lu, Ke Zhang, et al. Onnx:
    Open neural network exchange. <https://github.com/onnx/onnx>, 2019.

    - <span id="page-24-10"></span>[63] Jia Deng, Wei Dong, Richard Socher, Li-Jia
    Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database.
    In *2009 IEEE Conference on Computer Vision and Pattern Recognition*, pages 248–255.
    IEEE, 2009.

    - <span id="page-24-11"></span>[64] Alex Wang, Amanpreet Singh, Julian Michael,
    Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis
    platform for natural language understanding. *arXiv preprint arXiv:1804.07461*,
    2018.

    - <span id="page-24-12"></span>[65] Alec Radford, Jeffrey Wu, Rewon Child, David
    Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask
    learners. *OpenAI blog*, 1(8):9, 2019.

    - <span id="page-24-13"></span>[66] Denis Paperno, Germán Kruszewski, Angeliki
    Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni,
    Gemma Boleda, and Raquel Fernández. The lambada dataset: Word prediction requiring
    a broad discourse context. *arXiv preprint arXiv:1606.06031*, 2016.

    - <span id="page-24-14"></span>[67] Team GLM, :, Aohan Zeng, Bin Xu, Bowen Wang,
    Chenhui Zhang, Da Yin, Dan Zhang, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu
    Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui,
    Jie Tang, Jing Zhang, Jingyu Sun, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong,
    Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan
    Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia,
    Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan
    Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi
    Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou,
    and Zihan Wang. Chatglm: A family of large language models from glm-130b to glm-4
    all tools, 2024.

    - <span id="page-24-15"></span>[68] Siyuan Lu, Meiqi Wang, Shuang Liang, Jun Lin,
    and Zhongfeng Wang. Hardware accelerator for multi-head attention and position-wise
    feed-forward in the transformer. In *2020 IEEE 33rd International System-on-Chip
    Conference (SOCC)*, pages 84–89. IEEE, 2020.

    - <span id="page-24-16"></span>[69] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri
    Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention
    with io-awareness, 2022.

    - <span id="page-24-17"></span>[70] Jiajun Wu, Mo Song, Jingmin Zhao, and Hayden
    Kwok-Hay So. A case for low bitwidth floating point arithmetic on fpga for transformer
    based dnn inference. In *2024 IEEE International Parallel and Distributed Processing
    Symposium Workshops (IPDPSW)*, pages 178–185. IEEE, 2024.

    - <span id="page-24-18"></span>[71] Yuntao Han and Qiang Liu. Hpta: A high performance
    transformer accelerator based on fpga. In *2023 33rd International Conference
    on Field-Programmable Logic and Applications (FPL)*, pages 27–33, 2023.'
- title: 'The Survey of Chiplet-based Integrated Architecture: An EDA perspective'
  abstract: 'Enhancing performance while reducing costs is the fundamental design

    philosophy of integrated circuits (ICs). With advancements in packaging

    technology, interposer-based chiplet architecture has emerged as a promising

    solution. Chiplet integration, often referred to as 2.5D IC, offers significant

    benefits, including cost-effectiveness, reusability, and improved performance.

    However, realizing these advantages heavily relies on effective electronic

    design automation (EDA) processes. EDA plays a crucial role in optimizing

    architecture design, partitioning, combination, physical design, reliability

    analysis, etc. Currently, optimizing the automation methodologies for chiplet

    architecture is a popular focus; therefore, we propose a survey to summarize

    current methods and discuss future directions. This paper will review the

    research literature on design automation methods for chiplet-based

    architectures, highlighting current challenges and exploring opportunities in

    2.5D IC from an EDA perspective. We expect this survey will provide valuable

    insights for the future development of EDA tools for chiplet-based integrated

    architectures.'
  url: http://arxiv.org/abs/2411.04410v1
  keywords: ''
  document: '# The Survey of Chiplet-based Integrated Architecture: An EDA perspective


    Shixin Chen<sup>1</sup> , Hengyuan Zhang<sup>2</sup> , Zichao Ling<sup>2</sup>
    , Jianwang Zhai2,† , Bei Yu1,† <sup>1</sup>The Chinese University of Hong Kong
    <sup>2</sup>Beijing University of Posts and Telecommunications


    {sxchen22, byu}@cse.cuhk.edu.hk, {zhy679, lingzichao, zhaijw}@bupt.edu.cn


    ## Abstract


    Enhancing performance while reducing costs is the fundamental design philosophy
    of integrated circuits (ICs). With advancements in packaging technology, interposer-based
    chiplet architecture has emerged as a promising solution. Chiplet integration,
    often referred to as 2.5D IC, offers significant benefits, including cost-effectiveness,
    reusability, and improved performance. However, realizing these advantages heavily
    relies on effective electronic design automation (EDA) processes. EDA plays a
    crucial role in optimizing architecture design, partitioning, combination, physical
    design, reliability analysis, etc. Currently, optimizing the automation methodologies
    for chiplet architecture is a popular focus; therefore, we propose a survey to
    summarize current methods and discuss future directions. This paper will review
    the research literature on design automation methods for chiplet-based architectures,
    highlighting current challenges and exploring opportunities in 2.5D IC from an
    EDA perspective. We expect this survey will provide valuable insights for the
    future development of EDA tools chiplet-based integrated architectures.


    ## 1 Introduction


    The challenge of designing complex electronic systems that deliver high performance
    while keeping costs low has been a driving force in integrated circuit (IC) design.
    The concept of Moore''s law [\[1\]](#page-7-0) has historically guided the industry,
    illustrating how technological advancements enable the integration of more transistors
    on a single die, thereby enhancing performance and reducing costs. However, chip
    scaling improvements resulting from the decreasing size of transistors are no
    longer effective today. Firstly, physical limitations make it increasingly difficult
    to reduce transistor sizes further. This results in a bottleneck in improving
    performance by placing more transistors in a unit area of the silicon wafer. Secondly,
    manufacturing costs continue to rise with the size reduction of transistors. Smaller
    transistors require more sophisticated and complex lithography systems, which
    significantly increase manufacturing costs for the chip industry. Moreover, the
    yield from silicon wafers also decreases, making it impossible to maintain cost
    benefits while improving overall performance.


    © 2025 Copyright held by the owner/author(s).


    ACM ISBN 979-8-4007-0635-6/25/01.


    <https://doi.org/10.1145/3658617.3703134>


    <span id="page-0-0"></span>![](_page_0_Figure_12.jpeg)


    Figure 1: The architecture of chiplet-based 2.5D IC.


    In response, many leading foundries, such as TSMC, Samsung, and Intel, are actively
    investigating alternative strategies to lower wafer costs and improve production
    yields [\[2\]](#page-7-1). One promising avenue is the adoption of advanced heterogeneous
    integration and multi-chiplet architectures. This design philosophy involves creating
    separate hardware modules with specific functions, which are then combined through
    an interposer to form a comprehensive system [\[3\]](#page-7-2). Figure [1](#page-0-0)
    illustrates the structure of a chiplet-based system fabricated using the advanced
    chip-on-wafer-on-substrate (CoWoS) technology [\[4\]](#page-7-3). The interposer-based
    2.5D architecture has been used in products such as the Xilinx Virtex-7 2000T
    FPGA [\[5\]](#page-7-4) and the AMD ZEN2 Processor [\[6\]](#page-7-5).


    In the design and implementation of 2.5D architectures, electronic design automation
    (EDA) plays a crucial role, starting from the front end with architectural design
    and performance simulation, to the back end with physical design and package design.
    Figure [2](#page-1-0) shows the typical stages in the EDA flow for 2.5D architecture.
    At the design stage, EDA tools facilitate the simulation and exploration of various
    chiplet configurations, enabling designers to evaluate different architectural
    choices and their potential impacts on performance, power consumption, cost, etc.
    This early-stage analysis is essential for identifying potential bottlenecks and
    ensuring that the design meets the desired specifications. For physical design
    and package design, EDA assists in the precise arrangement and interconnection
    of chiplets on a silicon interposer, accounting for factors such as thermal management
    and communication latency. EDA tools also support reliability analysis and multi-physics
    modeling to ensure the final assembly is efficient and manufacturable.


    This paper reviews the critical role of EDA in the design flow of 2.5D IC and
    explores future directions to address current challenges. Section Section [2](#page-1-1)
    highlights the benefits of 2.5D architecture, summarizing research in architectural
    design, functional simulation, and other issues encountered in the early design
    stages. In Section Section [3,](#page-3-0) we analyze strategies for partitioning
    chiplets and their interconnection mechanisms. Section Section [4](#page-5-0)
    discusses the physical design workflow for 2.5D IC, emphasizing the importance
    of


    <sup>†</sup>Corresponding authors.


    Permission to make digital or hard copies of part or all of this work for personal
    or classroom use is granted without fee provided that copies are not made or distributed
    for profit or commercial advantage and that copies bear this notice and the full
    citation on the first page. Copyrights for third-party components of this work
    must be honored. For all other uses, contact the owner/author(s). ASPDAC ''25,
    January 20–23, 2025, Tokyo, Japan


    ASPDAC ''25, January 20–23, 2025, Tokyo, Japan Shixin Chen, Hengyuan Zhang, Zichao
    Ling, Jianwang Zhai, and Bei Yu


    <span id="page-1-0"></span>![](_page_1_Figure_2.jpeg)


    Figure 2: The EDA flow of the chiplet-based architecture.


    floorplan & placement, Routing, multiphysics modeling, and other topics in physics
    design. Next, Section Section [5](#page-7-6) explores current challenges and potential
    advancements in EDA methodology for 2.5D integration. Finally, Section Section
    [6](#page-7-7) concludes the paper.


    ## <span id="page-1-1"></span>2 Architecture Design of Chiplet


    ## 2.1 Chiplet-based 2.5D Architecture


    2.5D integrated architecture is a design paradigm that bridges the gap between
    traditional 2D and advanced 3D architectures. In a 2D architecture, all components
    of the circuit system are integrated on a single silicon die, known as a system-on-chip
    (SoC). However, this design can restrict performance, scalability, and yield.
    In contrast, 3D architectures [\[7\]](#page-8-0) stack multiple dies vertically,
    allowing for greater integration but often introducing complexities related to
    thermal management and manufacturing challenges. The 2.5D approach employs a silicon
    interposer to place multiple chiplets side by side in a single package. In Figure
    [1,](#page-0-0) at the top of the architecture are the chiplets, each serving
    various functions and bonded to the interposer, where wires connecting different
    chiplets can be routed. The interposer is attached to the package substrate with
    silicon vias, exposing the pins to peripheral hardware such as SRAM or DRAM.


    The 2.5D architecture presents several key advantages over traditional 2D and
    3D designs. Firstly, it enhances performance by providing high-bandwidth communication
    between chiplets via the interposer, significantly reducing latency and improving
    data transfer speeds. Secondly, it improves power efficiency, as less critical
    components can be manufactured using more cost-effective technology nodes, while
    high-performance components can leverage advanced nodes. This strategic separation
    not only optimizes power consumption but also enhances area and cost efficiency,
    allowing for better utilization of silicon real estate. Moreover, chiplets can
    be reused and combined to quickly meet a wide variety of demands. Overall, these
    factors contribute to a more efficient and powerful integrated circuit design,
    making 2.5D architecture a compelling choice for modern applications.


    ## 2.2 Emerging Applications of Chiplet


    Apart from mature applications in personal CPU, server CPU, GPU, and FPGA products.
    2.5D IC technology is particularly advantageous for applications requiring high
    performance and efficiency, such as large-scale deep neural network (DNN) accelerators
    and inference engines for large language models (LLMs). For DNN accelerators,
    the architecture''s capacity to facilitate massive parallel processing and rapid
    data transfer between chiplets is crucial for handling the computational demands
    of contemporary AI models. Wang et al. [\[8\]](#page-8-1) explores the advantages
    and limitations of 2.5D and 3D heterogeneous integration on representative AI
    algorithms, such as DNNs, transformers, and graph neural networks. Ankur et al.
    [\[9\]](#page-8-2) proposes the synthesizer to enable no-human-in-the-loop generation
    and design space exploration of the chiplets for highly specialized artificial
    intelligence accelerator.


    Similarly, LLM inference benefits from the high bandwidth and low latency afforded
    by 2.5D designs, enabling quicker processing and enhanced responsiveness. Chiplet
    Cloud [\[10\]](#page-8-3) proposes a chiplet-based architecture for LLM supercomputers,
    and the analysis demonstrates that this architecture offers significant cost benefits
    for serving large generative language models compared to traditional GPU and Tensor
    Processor Unit solutions. Yang et al. [\[11\]](#page-8-4) discusses how can chiplet
    bring better cost efficiency and shorter time to market by orchestrating heterogeneous
    chiplets to support LLM computing workloads. As the demand for advanced AI applications
    continues to surge, the adoption of 2.5D architecture is expected to grow, promoting
    innovation and efficiency across various computational domains.


    To conclude, there are several reasons to support 2.5D architecture for various
    applications. Higher Yield: The chiplet only needs to support a sub-function of
    the original large system, which also decreases the area of each chip. A smaller
    chip area will lead to a higher yield in manufacturing technology, meaning that
    given the same silicon wafer budget, the number of chips will be higher. Modular
    Design: Since each chiplet has its own function and performance, we can use different
    combinations of chiplets to create new configurations for various scenarios, like
    AMD''s approach [\[6\]](#page-7-5), i.e., more computing chiplets for computation-intensive
    applications in server clusters, while fewer computing chiplets are used for personal
    computers. The interconnection philosophy also makes it possible to use chiplets
    with different technology nodes, where the core modules utilize more advanced
    technology and the less critical modules use more mature technology to provide
    greater cost benefits. Design Efficiency: A mature chiplet that has been validated
    can be seen as hardware intellectual property (IP), allowing the design house
    to use these chiplets to design new products more efficiently. For example, a
    Lego-like design method [\[12\]](#page-8-5) utilizes a heterogeneous chiplet-based
    architecture to make each chiplet portable to support diverse scenarios, like
    server CPU, AI processor, and processor CPU. Such design methods will save costs
    in functional testing, shorten the time from design to market, and therefore bring
    more benefits.


    ## 2.3 Simulation Methods of Chiplet Architecture


    Simulation plays a critical role in the design of 2.5D integrated architectures,
    particularly in the early evaluations of system performance and functional correctness.
    Given the complexity introduced by chiplet architectures, interposers, and communication
    interfaces, effective simulation is essential for identifying potential issues
    prior to manufacturing. The structural characteristics of chiplets and interposers
    can lead to additional overhead, such as increased latency and power consumption,
    which must be accurately modeled to ensure that the integrated system meets its
    performance specifications. The simulations of the 2.5D system allow designers
    to explore various configurations and assess the impact of different design choices
    on overall system efficiency.


    However, there is currently no customized simulator specifically designed for
    chiplet-based architectures. In the research literature, performance-driven chiplet
    designs [\[13,](#page-8-6) [14\]](#page-8-7) often utilize simulation frameworks
    built for NoC [\[15](#page-8-8)[–17\]](#page-8-9). Zhi et al. [\[18\]](#page-8-10)
    propose an intersimulator process communication and synchronization protocol to
    leverage single-core simulators for modeling the interposerlevel network. Additionally,
    Floorplet [\[19\]](#page-8-11) employs the system-level simulator Gem5 [\[20\]](#page-8-12)
    to incorporate chiplet latency information introduced by floorplanning, evaluating
    the overall performance of chiplet-based architectures.


    While these tools provide valuable insights, there are still limitations to the
    simulation of 2.5D systems, which can be categorized into several key areas:


    - Accurate Modeling of Chiplet Interactions: In 2.5D architectures, inter-chip
    communication is a critical factor, and accurately modeling these interactions
    is essential for performance evaluation. However, many existing simulation tools
    fail to adequately consider the complex interactions between chiplets, leading
    to inaccurate performance predictions.

    - Integrate Different Technology Nodes: One of the primary advantages of 2.5D
    architecture is the ability to integrate chiplets from different technology nodes.
    However, existing simulation tools often lack support for these heterogeneous
    components, limiting design flexibility and optimization capabilities.

    - Simulate Complex Communication Protocols: As the complexity of multi-chip systems
    increases, simulation tools must be able to model a variety of communication protocols
    and interfaces. However, current tools often have limited capabilities in this
    regard, which may fail to capture the true dynamics of interchip communication.


    Developing specialized simulation tools tailored for chiplet performance evaluation
    requires addressing these critical challenges to enhance the effectiveness and
    applicability of simulation tools in evaluating 2.5D architectures.


    ## 2.4 Power Analysis of Chiplet Architecture


    Power simulation requires careful consideration of the additional complexity introduced
    by chiplet-based designs. Unlike traditional monolithic architectures, chiplet
    designs often comprise heterogeneous components that operate and communicate under
    varying workloads, which complicates power estimation.


    To address these challenges, existing research typically employ methods such as
    static power analysis and dynamic power modeling. For instance, McPAT [\[21\]](#page-8-13)
    is widely used to model the power of SoCs and has been calibrated using machine
    learning (ML) to analyze chip power in the advanced technology node [\[22\]](#page-8-14).
    Zhai et al. [\[23\]](#page-8-15) use transfer learning to improve the transferability
    of power models across designs. PANDA [\[24\]](#page-8-16) combines the advantages
    of analytical and ML power models and further supports power prediction for unknown
    new technology nodes. However,


    ![](_page_2_Figure_11.jpeg)


    <span id="page-2-0"></span>![](_page_2_Figure_12.jpeg)


    Figure 3: The relationship between cost, yield, and chip area across various manufacturing
    technology nodes.


    these approaches may not fully account for the unique thermal and operational
    characteristics of individual chiplets or the interposer. Consequently, there
    is a pressing need for new methodologies that can accurately assess power consumption
    across different chiplets and their interactions. Recent research has begun to
    tackle this issue. For example, Kim et al. [\[25\]](#page-8-17) present an effective
    methodology for co-designing the system-level optimization of chiplet power delivery
    network (PDN) in 2.5D IC designs using commercial EDA tools. Some AI-based methods
    are also proposed to manage the power supply system for the chiplet-based architecture.
    For example, Li et al. [\[26\]](#page-8-18) utilizes the deep reinforcement learning
    method to maximum power efficiency for the online control stage.


    Looking ahead, the dynamic nature of workloads in applications such as AI and
    machine learning necessitates the incorporation of real-time workload models into
    power simulations. This approach would more effectively capture the impact of
    workload variations on power consumption. By addressing these additional considerations
    and refining existing tools or developing new ones, designers can achieve more
    accurate power simulations, ultimately leading to more efficient and reliable
    chiplet-based architectures.


    ## 2.5 Cost Analysis of Chiplet Architecture


    The circuit systems incur two main types of costs: non-recurring engineering (NRE)
    costs and recurring engineering (RE) costs. NRE costs are the initial expenses
    associated with designing a VLSI system, which encompass software development,
    intellectual property licensing, module or chip design, verification processes,
    and mask production. On the other hand, RE costs pertain to the ongoing expenses
    related to large-scale manufacturing, including wafer fabrication, packaging,
    and testing. The basic concept is that if the production quantity is small, the
    NRE cost is dominant; otherwise, the NRE cost becomes negligible if the quantity
    is large enough.


    In the context of modern architectures, such as chiplets, understanding these
    cost dynamics is crucial for optimizing design and manufacturing strategies. The
    cost analysis of 2.5D architecture differs from that of traditional monolithic
    SoCs, with the core idea being to build a specific cost model that considers all
    factors in design and manufacturing. The yield model serves as the foundation
    for 2.5D architecture, with Seed''s model and the Negative Binomial model being
    the most widely used [\[27\]](#page-8-19). Figure [3](#page-2-0) shows the cost
    and yield relationship between the manufacturing technology node, which further
    demonstrate that smaller area of chiplet will bring more cost benefit. Literature
    [\[28–](#page-8-20)[30\]](#page-8-21) propose different cost models


    from various perspectives. Chiplet Actuary [\[29\]](#page-8-22) presents a quantitative
    cost model tailored for multi-chip systems, leveraging three representative multi-chip
    integration technologies. Ahmad et al. [\[28\]](#page-8-20) introduce an open-source
    model that provides details on costs—such as material, testing, Know-Good-Die,
    and operations—for engineering and supply chain personnel to make early trade-off
    decisions regarding chiplet architecture. Additionally, Tang et al. [\[30\]](#page-8-21)
    develop an analytical cost model for the 2.5D chiplet system, considering various
    interconnection options and technology nodes. This study conducts a series of
    case analyses to explore cost characteristics under both homogeneous and heterogeneous
    scenarios.


    In the cost analysis of chiplet systems, technical complexity presents significant
    challenges, especially with heterogeneous integration. Fortunately, AI technology
    offers valuable opportunities to enhance cost evaluations. By utilizing historical
    data, AI can create predictive models for rapid assessments of costs related to
    various design options, improving decision-making accuracy. Additionally, real-time
    monitoring of cost factors during manufacturing allows for quick identification
    of potential overruns and optimized resource allocation. While these advancements
    offer substantial benefits, challenges regarding system complexity and data quality
    still exist. Overall, integrating AI technology will significantly enhance the
    efficiency and accuracy of cost evaluations in future chiplet development.


    ## 2.6 Design Exploration of Chiplet Architecture


    Designing efficient 2.5D chiplet systems poses significant challenges due to the
    vast design space, which differs greatly from traditional monolithic architectures.
    Key challenges arise from the modular and heterogeneous nature of these systems,
    where factors such as chiplet partitioning, interconnect topology, and packaging
    must be carefully balanced. For example, finer chiplet granularity may improve
    yield and flexibility, but it can introduce substantial die-to-die communication
    latency and power overhead. The necessity to simultaneously optimize performance,
    power, cost, and thermal management complicates this process, rendering it both
    critical and challenging.


    To tackle these challenges, various research efforts have introduced tools and
    methods for design space exploration (DSE). DSE methodologies are broadly divided
    into analytical and machine learning-based approaches. Analytical methods aim
    to create lightweight, interpretable models that describe the relationship between
    hardware design configurations and PPA (performance, power, area), aiding in design
    space pruning and rapid exploration. However, these methods often require extensive
    domain knowledge and face limitations in accuracy and scalability. To overcome
    these limitations, a series of studies [\[31](#page-8-23)[–37\]](#page-8-24) have
    proposed utilizing learning-based approaches to systematically explore chip configurations
    for microarchitecture design or DNN accelerators. These techniques provide scalable
    solutions for exploring the design space of 2.5D chiplet architectures.


    Building on this foundation, Pal et al. [\[38\]](#page-8-25) developed a framework
    specifically for the design space exploration of 2.5D architectures, focusing
    on trade-offs in chiplet partitioning and interconnect strategies to optimize
    performance and cost. Similarly, Iff et al. [\[13\]](#page-8-6) introduced RapidChiplet,
    which enables early evaluations before register-transfer-level design, allowing
    for quick estimates of power, performance, and cost.


    Additionally, workload mapping plays a crucial role in influencing performance
    and power consumption in 2.5D systems. Cai et al. [\[39\]](#page-8-26)proposed
    the Gemini framework, which integrates layer mapping with architectural adjustments
    to optimize DNN pipelining and reduce inter-chiplet communication overhead. Similarly,
    Tan et al. [\[40\]](#page-8-27) introduced NN-Baton, a hierarchical framework
    that employs the critical-capacity and critical-position method to efficiently
    manage workload distribution, thereby enhancing energy efficiency. Collectively,
    these tools equip researchers and engineers with essential techniques for navigating
    the complex 2.5D chiplet design space, effectively addressing both architectural
    and workload-level challenges.


    Significant advancements have been made in the DSE of 2.5D chiplet systems, yet
    many current approaches focus on isolated aspects, such as microarchitecture configurations
    or workload mapping. There is a need for an integrated framework that considers
    multiple design factors—like thermal management, power delivery, and mechanical
    stress. Future efforts should aim to create unified DSE methods that incorporate
    these elements for more accurate design optimization. As chiplet architectures
    evolve, integrating AI-driven techniques like reinforcement learning can automate
    the exploration process and adjust design parameters based on real-time performance
    data.


    ## <span id="page-3-0"></span>3 Partition and Interconnection of Chiplet


    In 2.5D chiplet systems, partitioning and combining chiplets are critical steps
    that significantly influence system performance, power consumption, and overall
    cost. Figure [4](#page-4-0) illustrates these processes. By utilizing chiplets
    from existing designs and thirdparty IPs, designers can create new chip systems
    with optimized interconnects and communication protocols.


    ## 3.1 Partition and Combination


    The primary objective of partitioning is to divide traditional monolithic SoC
    designs into smaller, independent chiplet modules. This modularization enhances
    flexibility and adaptability to various manufacturing processes, allowing for
    greater scalability. However, achieving an optimal partition involves challenges
    such as managing coupling between functional modules, minimizing communication
    overhead, and ensuring efficient resource allocation. Once the system is partitioned,
    the combination phase focuses on integrating these chiplet modules into a cohesive
    system. This stage requires careful optimization of topology and interconnects
    to meet performance targets while minimizing costs. Balancing these factors is
    crucial for the success of a chiplet-based system.


    To tackle the challenges of partitioning, various approaches have been developed.
    KaHyPar [\[41\]](#page-8-28) is a hypergraph partitioning tool that minimizes
    communication overhead and resource overlap. Similarly, Chen et al. [\[19\]](#page-8-11)
    proposed the parChiplet algorithm, which uses a recursive tree structure to optimize
    the division of SoC modules and packaging layout, addressing communication delays
    and resource imbalances. For complex systems, Chipletizer [\[42\]](#page-8-29)
    employs multi-layer partitioning and simulated annealing to enhance core reuse
    and reduce costs. Zhuang et al. [\[2\]](#page-7-1) introduced a multi-package
    co-design method that integrates hierarchical programming to distribute dies across
    multiple packages, improving reliability and scalability.


    <span id="page-4-0"></span>![](_page_4_Figure_2.jpeg)


    Figure 4: Illustration of partitioning and combining in chiplet-based architecture.


    For the combination, optimizing chiplet selection and interconnect topology is
    essential for effective communication and system performance. Li et al. [\[43\]](#page-8-30)
    proposed GIA that supports adaptive interconnect configurations and provides an
    automated framework for chiplet selection, topology generation, and layout evaluation,
    enabling efficient integration of heterogeneous chiplets. Pal et al. [\[38\]](#page-8-25)
    introduced a framework for exploring chiplet configurations across varied applications,
    reducing costs while meeting performance constraints and enhancing chiplet reusability.


    For network topology, Bharadwaj et al. [\[44\]](#page-8-31) proposed the Kite
    topology, optimizing chiplet placement to improve throughput, while Patrick et
    al.''s HexaMesh[\[45\]](#page-8-32) adopts a hexagonal layout to minimize communication
    paths and maximize bandwidth. GIA [\[43\]](#page-8-30) further includes high-performance
    interconnects with bypass paths and configurable routers for adaptive communication
    flows. Similarly, Zheng et al. [\[46\]](#page-8-33)proposed the Adapt-NoC framework,
    which dynamically allocates resources to form tailored sub-networks for specific
    communication needs. Murali et al. [\[47\]](#page-8-34) enhanced NoC design by
    integrating floorplan information to enable early timing and power optimization
    in large networks.


    Overall, advancements in partitioning and combining chiplets have significantly
    improved the adaptability and efficiency of 2.5D systems. However, increasing
    heterogeneity presents challenges like managing diverse functionalities and power
    profiles across chiplets. Future research should explore adaptive partitioning
    methods that dynamically adjust to the specific needs of different chiplets, optimizing
    the balance between performance, power, and area. Moreover, as chiplet system
    complexity grows, integrating thermal management and mechanical stress considerations
    into the partitioning process will be crucial for stability and reliability. Developing
    automated tools to manage these factors early in the design phase will be vital
    for seamless integration. Addressing these challenges will enhance the flexibility
    of future 2.5D systems, meeting diverse application needs with improved efficiency.


    ## 3.2 Interconnection and Communication


    Effective communication is fundamental to the success of 2.5D chiplet systems,
    where the modular architecture introduces unique challenges in maintaining system
    performance, scalability, and power efficiency. As chiplets replace traditional
    monolithic designs, ensuring seamless data flow between these components becomes
    increasingly complex. These systems must support high data transfer


    rates, low latency, and dynamic routing while managing heterogeneous chiplets
    that differ in performance, power requirements, and communication protocols. The
    need for high-speed, low-latency communication across chiplets further complicates
    system integration and configuration, making the design of interconnects and protocols
    a critical factor in system efficiency.


    To address these challenges, various standardized communication protocols have
    been developed. UCIe (unified chiplet interconnect express) [\[48\]](#page-8-35)
    simplifies the integration of diverse chiplets by offering a unified interface
    that supports a wide range of chip types, ensuring compatibility across different
    manufacturing processes and technology nodes. USR (ultra-short reach) [\[49\]](#page-8-36)
    and AIB (advanced interface bus) are two additional protocols that enable high-speed
    die-to-die communication, providing low-latency links that are essential for efficient
    data transfers between chiplets. Moreover, protocols like BoW (bunch-of-wires)
    [\[50\]](#page-8-37) offer flexible packaging solutions that enhance communication
    between chiplets in different systems, supporting various application-specific
    needs.


    Recent innovations have introduced further advancements in the interconnect space
    based on these protocols. For example, Feng et al. [\[51\]](#page-8-38) introduced
    a heterogeneous interface architecture that integrates parallel and serial interfaces,
    allowing for flexible communication between chiplets with varying requirements.
    This architecture is particularly advantageous in systems with mixed workloads,
    where the ability to dynamically switch between communication modes can significantly
    improve performance and efficiency. Additionally, optical communication technologies
    can offer significantly higher data transfer rates compared to traditional electrical
    interconnects, enabling large-scale multi-chip systems to handle higher bandwidth
    demands [\[52\]](#page-8-39). Li et al. [\[53\]](#page-8-40) proposed HPPI, a
    reconfigurable photonic interconnect optimized for deep learning accelerators,
    which enhances dataflow efficiency and reduces energy consumption.


    Communication routing algorithms have also evolved to handle the complexities
    of dynamic traffic and heterogeneous workloads. Feng et al. [\[54\]](#page-8-41)
    developed the Minus-First Routing algorithm, which combines secure and non-secure
    flow control strategies to optimize bandwidth utilization and reduce latency.
    Similarly, DeFT [\[55\]](#page-8-42) offers a deadlock-free and fault-tolerant
    routing approach that employs virtual-network separation and dynamic vertical
    link selection to ensure reliable communication in 2.5D systems.


    Despite recent advances, there remain challenges in optimizing communication pathways
    to accommodate diverse workloads and traffic patterns. Future research should
    focus on adaptive communication protocols capable of real-time adjustments, ensuring
    system responsiveness and minimizing latency. With expanding system scales, addressing
    power management and maintaining data integrity across communication layers will
    become increasingly important. Real-time analytics and AI-driven methods will
    offer promising ways to manage these complexities, enabling dynamic optimization
    of communication strategies.


    ## <span id="page-5-0"></span>4 Physical Design of Chiplet-based Architecture


    ## 4.1 Physical Design Flow


    The physical design flow of 2.5D integration encompasses several critical steps,
    including floorplanning, placement, routing, multiphysics modeling, and reliability
    analysis. Figure [2](#page-1-0) demonstrates the EDA flow for physical designs.


    Initially, during the floorplanning stage, designers establish the preliminary
    layout of various functional blocks on the chip and the interposer, optimizing
    spatial utilization and performance. For instance, in the design of high-performance
    computing systems, careful floorplanning can significantly reduce the distances
    between processing units, thereby minimizing latency. Following floorplanning,
    the placement phase involves the precise positioning of these functional blocks
    to achieve optimal data transfer rates and minimal delays. The routing stage is
    then executed, which involves connecting the signal lines between the various
    modules to ensure rapid data transmission. In 2.5D architectures, reliability
    is critically important due to the complexities of heterogeneous integration.
    This integration introduces various factors—such as thermal effects, mechanical
    stress, and electronic interactions—that can significantly impact overall system
    performance.


    Table [1](#page-5-1) lists the research works that optimize the steps in the physical
    design flow from an EDA perspective. While EDA tools have made substantial strides
    in facilitating 2.5D physical design, further research and innovation are needed
    to effectively address the increasing complexity and diversity of modern integrated
    circuit designs. Recent progress in EDA for 2.5D architecture will be introduced
    in the following aspects: floorplanning and placement, redistributed layer (RDL)
    design & routing strategies, multiphysics modeling, and reliability analysis.


    ## 4.2 Floorplan and Placement


    In this section, we primarily discuss the chiplet floorplan and placement in 2.5D
    systems. In the traditional EDA flow for SoCs, the floorplan provides a high-level
    layout for the functional blocks and macro cells of the chip, considering their
    size, orientation, and position, along with factors such as power distribution
    and thermal management. The placement follows floorplanning and determines the
    specific locations of standard cells or macros within the chip, aiming to optimize
    connections while satisfying constraints like signal delay, power consumption,
    and routing resource utilization. However, in chiplet-based architectures, placement
    usually refers to determining the locations of chiplets on the interposer. Therefore,
    in this survey, the terms placement and floorplan will be used interchangeably.


    Table 1: The Research Works of Physical Design


    <span id="page-5-1"></span>


    | Design Stages         | Works | Descriptions                               |

    |-----------------------|-------|--------------------------------------------|

    | Floorplan & Placement | [19]  | Performance-aware floorplan                |

    |                       | [56]  | Sequence-pair tree placement               |

    |                       | [57]  | Thermal-aware SA algorithm                 |

    |                       | [58]  | Non-matrix asymmetric SA algorithm         |

    |                       | [59]  | Transitive closure graph SA algorithm      |

    |                       | [60]  | Chiplet-oriented RL method                 |

    |                       | [61]  | Thermal-aware Bayesian optimization        |

    |                       | [62]  | Learning-to-rank based on GNN              |

    | RDL & Routing         | [63]  | Impact of inductive effects on timing      |

    |                       | [64]  | Simultaneous routing and geometry patterns |

    |                       | [65]  | Router design in interposer routing        |

    |                       | [66]  | Any-angle routing for multi-RDL            |

    | Multi-Physics         | [67]  | Accurate and time-consuming FEA method     |

    |                       | [68]  | Compact thermal model from power trace     |

    |                       | [69]  | Models for TSV, bumps, and RDL             |

    |                       | [70]  | Thermal map estimation using GNN           |

    | Other Topics          | [71]  | Signal and power integrity analysis        |

    |                       | [72]  | PDN analysis with deep RL                  |


    Regarding the placement of chiplet systems, two key issues arise. Firstly, there
    is a need for an evaluation framework that precisely reflects the impact of the
    layout design, considering metrics such as thermal performance, communication
    latency, and warpage. Secondly, the layout optimization algorithms must achieve
    high solution quality and fast solving speed. Generally, search algorithms can
    be classified into three types: i.e., heuristic methods, mathematical analytic
    optimization, and machine learning approaches.


    Heuristic methods, such as enumeration-based algorithms [\[73\]](#page-8-60),
    branch-and-bound (B&B) algorithms [\[74\]](#page-8-61), B\* Tree [\[75\]](#page-8-62),
    sequence pair (SP) [\[76\]](#page-8-63), and corner block list (CBL) [\[77\]](#page-8-64),
    have been widely used in the placement for SoC design. These methods use a specified
    data structure to represent the placement solution and utilize the simulated annealing
    (SA) algorithm to optimize objectives such as area, wirelength, and stress. Some
    works have customized these methods for chiplet-based architectures. For example,
    Chiou et al. [\[56\]](#page-8-43) construct an SP-Tree structure that includes
    representations of rotated and partial/full sequence pairs and apply the B&B method
    to the SP-Tree to seek the optimal layout solution. TAP-2.5D [\[57\]](#page-8-44)
    employs a thermal-aware placer based on the SA algorithm, relying on a new placement
    description data structure to explore the physical design space. Coskun et al.
    [\[58\]](#page-8-45) also use the SA algorithm to support arbitrary placements
    that consider non-matrix and asymmetric chiplet organizations. This method benefits
    the overall 2.5D cost by incorporating a comprehensive micro-bump cost and yield
    analysis. At the same time, Yang et al. [\[59\]](#page-8-46) consider warping
    effects and use a transitive closure graph-based SA algorithm to perturb and optimize
    these effects more directly.


    Another important method for placement optimization is mathematical analytic optimization,
    which formulates placement or floorplanning as an optimization problem with specific
    constraints and objectives. For example, in traditional placement for SoCs, NTUPlace3
    [\[78\]](#page-8-65) introduces an analytical placement algorithm that uniquely
    integrates wirelength, preplaced blocks, and density control through a two-stage
    smoothing technique and advanced optimization methods. In chiplet-based floorplanning,
    analytic methods are also utilized to handle more complicated constraints, particularly
    those related to reliability. For instance, Chen et al. [\[19\]](#page-8-11) propose
    optChiplet, a floorplan optimization framework that considers


    reliability, cost, area, and performance metrics. While this framework accurately
    accounts for many factors, using the solver to solve the problem can be quite
    time-consuming (taking several hours for multiple chiplets). Zhuang et al. [\[2\]](#page-7-1)
    present a novel approach to solving the multi-package co-design problem using
    mathematical programming methods, aiming to optimize the trade-offs between inter-package
    and intra-package costs.


    With the development of AI, placement algorithms also benefit from ML-based methods
    [\[60–](#page-8-47)[62,](#page-8-49) [79\]](#page-8-66). For example, Duan et
    al. introduce RLPlanner [\[60\]](#page-8-47), a layout tool based on reinforcement
    learning suitable for the planning of early chiplet-based systems. It utilizes
    advanced RL technology combined with a novel rapid thermal assessment method.
    Molter et al. [\[61\]](#page-8-48) employ Bayesian optimization for thermal-aware
    chip layout optimization. They propose an input space kernel function that includes
    arrangement and continuous variables, evaluating it using a multi-objective function
    composed of area, temperature, and temperature gradient, through finite element
    method thermal simulations. Deng et al. [\[62\]](#page-8-49) propose a chiplet
    placement order ranking framework based on graph neural networks, utilizing the
    learning-to-rank method to select the optimal chiplet placement order. A network
    architecture primarily based on RankNet [\[80\]](#page-8-67) was designed, combining
    graph neural networks for feature extraction.


    Overall, the existing methods can handle small-scale 2.5D chiplet sets in accept
    time with good performance. However, the scale of future 2.5D chiplet systems
    may increase to dozens or even more small chips, and the algorithms mentioned
    above require several hours to process systems containing more small chips. Therefore,
    finding more efficient chip layout algorithms and tools to shorten optimization
    time is crucial research direction in the physical design of large-scale 2.5D
    IC.


    ## 4.3 RDL and Routing


    The interposer, also known as a redistributed layer (RDL), is a crucial component
    in the physical design of chiplets. Its design and the associated wiring issues
    are important topics of study. The wires on the interposer facilitate communication
    between chiplets and provide interfaces for peripheral devices. However, the routing
    of these wires introduces additional latency, and parasitic parameters can significantly
    affect overall performance and reliability. In the advanced packaging technologies
    currently promoted by the industry, such as CoWoS, an intermediary interposer
    design method is commonly employed. This intermediary layer uses wiring within
    the interposer and through-silicon-via (TSV) technology to connect chiplets and
    establish connections to the packaging substrate.


    Researchers are increasingly focusing on large-scale redistributed layer design
    and the associated interconnection wiring challenges. For example, Kabir et al.
    [\[63\]](#page-8-50) propose a new collaborative optimization process that takes
    into account the inductive effects of redistributed layer wires on timing during
    the design of 2.5D systems. They also create an RLC delay model based on the properties
    of redistributed layer interconnections to analyze input/output drivers. Cai et
    al. [\[64\]](#page-8-51) develop an innovative simultaneous routing framework
    that employs a chord-based tile model and a netsequence list to search for global
    routing guides. Their approach includes both free-assignment and pre-assignment
    on the same routing graph. Additionally, they introduce a geometry-based pattern
    routing algorithm to find routing solutions, followed by a net


    padding method to address any failed routes. Lee et al. [\[65\]](#page-8-52) introduce
    a router designed for interposer routing, which optimizes the integrity of high-speed
    signal transmission by protecting signal lines and adjusting their width and spacing.
    This router adheres to complex design rules to maximize signal integrity and interconnect
    reliability. Chung et al. [\[66\]](#page-8-53) tackle the challenge of any-angle
    routing for multi-redistributed layer systems. They develop an optimization routing
    algorithm and propose a method for generating routing guides after dynamic triangulation,
    extending the access point adjustment technique to support multiple lines.


    As the chiplet architecture continues to evolve, routing tools will play a pivotal
    role in addressing both opportunities and challenges in this field. The increasing
    complexity of interconnections among numerous chiplets demands advanced routing
    techniques that can efficiently manage the associated latency and performance
    issues. Future routing tools must not only optimize wire routing but also incorporate
    machine learning algorithms to predict and mitigate potential reliability concerns,
    such as signal integrity and thermal effects. The development of robust, scalable
    routing solutions that can accommodate the unique requirements of chiplet-based
    systems will be crucial for realizing the full potential of this architecture


    ## 4.4 Multiphysics Simulation


    The complexity of the 2.5D package can introduce significant thermal stress risks
    due to varying material properties, particularly in chiplet technology applications
    where high-power electromagnetic pulses and self-heating effects are prominent.
    The multiphysics effects in advanced integrated packaging are primarily described
    by the continuity equation for current, heat conduction equations, and elasticity
    equations. Factors such as Joule heating can elevate temperatures, causing thermal
    deformation and stress, which can alter simulation mesh and affect electric and
    thermal field distributions. Consequently, multiphysics simulation is crucial
    for analyzing the interactions among electrical, thermal, and mechanical effects.
    By integrating these simulations, it is possible to predict and optimize chip
    performance in complex packaging environments.


    In 2.5D chip design, various multiphysics simulation tools are widely used to
    address complex design challenges and multiple physical effects, often employing
    time-consuming finite element analysis (FEA) methods. For example, Ansys Icepak
    [\[67\]](#page-8-54) focuses on thermal management, helping engineers optimize
    heat dissipation by accurately simulating heat flow and temperature distribution.
    COMSOL Multiphysics provides coupled simulation capabilities for multiple physical
    fields, enabling users to analyze electrical, thermal, and mechanical effects
    simultaneously [\[81\]](#page-8-68). However, these commercial tools also have
    some shortcomings, including high costs, steep learning curves, and limitations
    in accuracy under extreme conditions.


    In contrast to commercial tools, researchers tend to develop fast, relatively
    accurate, and lightweight approaches to reduce computation time and quickly predict
    metrics to accelerate the development process. HotSpot [\[68\]](#page-8-55) is
    a simulation tool used in the early stages of 2.5D architecture design to model
    thermal effects using the compact thermal model (CTM). It predicts temperature
    distributions by analyzing power consumption and heat dissipation, allowing for
    the creation of detailed thermal models that incorporate various components and
    environmental factors. Ma et al. [\[69\]](#page-8-56) present an


    electrical-thermal coupling model that simultaneously predicts temperature distribution
    and voltage drop in 2.5D chip heterogeneous integration systems, taking into account
    the Joule heating effect. Its advantages lie in high accuracy and practicality,
    making it useful for optimizing thermal management and electrical performance
    in packaging structures.


    For future direction, machine learning-based methods are being utilized in multiphysics
    simulation to speed up the simulation. For example, Chen et al. [\[70\]](#page-8-57)
    present a novel graph convolutional network architecture for estimating thermal
    maps in 2.5D chiplet-based systems, leveraging global power features and advanced
    techniques such as skip connections and edge-based attention.


    ## 4.5 Other Topics


    In addition to the physical design considerations discussed, there are several
    other critical topics that influence the overall performance and reliability of
    chiplet systems.


    One such topic is Signal Integrity (SI) and Power Integrity (PI), which plays
    a vital role in electronic design, particularly concerning the quality and reliability
    of signal transmission in high-speed digital circuits. In chiplet systems, the
    complexity of interconnect channels, compounded by varying operating frequencies
    and layouts, makes signal integrity issues increasingly significant.


    To address these challenges, the SPIRAL framework [\[71\]](#page-8-58) has been
    introduced for the integrated analysis of SI and PI in high-speed chiplet interconnects.
    By employing impulse response models and leveraging machine learning techniques,
    SPIRAL achieves notable enhancements in processing speed while ensuring low error
    rates.


    Additionally, the design of the power distribution network (PDN) within chiplet
    systems is another essential consideration. Miao et al. [\[72\]](#page-8-59) present
    a framework utilizing deep reinforcement learning to optimize the PDN design for
    2.5D packaged multi-chiplet systems. This innovative approach notably reduces
    computation time and uncovers superior solutions within a vast design space, effectively
    mitigating issues related to lengthy manual design processes and the scarcity
    of open-source data.


    ## <span id="page-7-6"></span>5 Challenges and Future Directions


    Currently, most commercial EDA tools designed specifically for single-die applications
    do not adequately support chiplet-based architectures. The entire design process
    of 2.5D IC is quite complex, with each step presenting new demands and challenges
    for EDA tools. In a heterogeneous integration system of 2.5D IC, the function
    and reliability of circuits are influenced by various factors, including electrical,
    magnetic, thermal, and mechanical stresses. There is currently no comprehensive
    platform that considers all relevant factors; instead, many single-point tools
    that cannot provide a holistic view or integrated feedback, potentially reducing
    design efficiency. Moreover, early-stage multi-physics modeling tools rely on
    numerous mathematical and physical models, and the computation-intensive nature
    of this modeling can slow down the design process, hindering overall design efficiency.


    From the perspective of placement and routing, traditional EDA tools often struggle
    to manage the increased complexity introduced by chiplets. The interactions between
    multiple chiplets necessitate advanced algorithms that can optimize not only for
    distance but also for signal integrity and power distribution. Current tools typically
    lack the capability to effectively model these inter-chiplet connections, leading
    to suboptimal layouts.


    Additionally, design rule checks for 2.5D IC require more sophisticated approaches,
    as the rules governing interactions between various chiplets can differ significantly
    from those of single-die designs. Existing tools may not be equipped to handle
    the nuances of multi-chiplet interactions, increasing the risk of design errors
    that could compromise performance and manufacturability.


    Moreover, the verification processes for 2.5D IC face unique challenges. The traditional
    methodologies for functional verification may not suffice, as they often overlook
    the complexities of chiplet communications and shared resources. This gap can
    lead to undetected issues that only manifest during later stages of production,
    thus increasing costs and time-to-market.


    To address these challenges, it is essential to develop EDA tools that support
    multi-domain simulations and provide integrated solutions that align with the
    unique requirements of 2.5D IC designs. This includes enhancing the interoperability
    of tools, allowing seamless data exchange, and implementing machine learning algorithms
    to optimize the design workflow. Additionally, the incorporation of real-time
    feedback mechanisms during the design phase could significantly improve reliability
    and performance, ultimately leading to more efficient and robust 2.5D IC solutions.


    ## <span id="page-7-7"></span>6 Conclusion


    In this article, we explored the application and advantages of chiplets in 2.5D
    integrated architectures from an EDA perspective. We highlighted the improvements
    in performance, power consumption, and area efficiency that 2.5D architectures
    offer compared to traditional 2D and 3D designs, emphasizing the importance of
    simulation, physical design, communication mechanisms, and modeling in the design
    process. By effectively leveraging EDA workflows, design teams can better tackle
    the complexities involved and achieve efficient 2.5D integrated solutions. Looking
    ahead, with the advancement of technologies such as AI and machine learning, 2.5D
    architectures are poised to demonstrate even greater potential in high-performance
    computing and other fields.


    ## Acknowledgment


    This work is supported by the National Key R&D Program of China (2022YFB2901100),
    the National Natural Science Foundation of China (No. 62404021), the Beijing Natural
    Science Foundation (No. 4244107, QY24216), the MIND project (MINDXZ202404), and
    AI Chip Center for Emerging Smart Systems (ACCESS), Hong Kong.


    ## References


    - <span id="page-7-0"></span>[1] G. E. Moore, "Cramming more components onto integrated
    circuits," Proc. IEEE, vol. 86, no. 1, pp. 82–85, 1998.

    - <span id="page-7-1"></span>[2] Z. Zhuang, B. Yu, K.-Y. Chao, and T.-Y. Ho, "Multi-package
    co-design for chiplet integration," in Proc. ICCAD, 2022, pp. 1–9.

    - <span id="page-7-2"></span>[3] M. Sunohara, T. Tokunaga et al., "Silicon interposer
    with TSVs (Through Silicon Vias) and fine multilayer wiring," in Proc. ECTC, 2008,
    pp. 847–852.

    - <span id="page-7-3"></span>[4] S. Y. Hou, W. C. Chen, C. Hu et al., "Wafer-level
    integration of an advanced logic-memory system through the second-generation CoWoS
    technology," IEEE TED, vol. 64, no. 10, pp. 4071–4077, 2017.

    - <span id="page-7-4"></span>[5] T. G. Lenihan, L. Matthew, and E. J. Vardaman,
    "Developments in 2.5 D: The role of silicon interposers," in Proc. EPTC, 2013,
    pp. 53–55.

    - <span id="page-7-5"></span>[6] S. Naffziger, K. Lepak, M. Paraschou et al.,
    "AMD chiplet architecture for highperformance server and desktop products," in
    Proc. ISSCC, 2020, pp. 44–45.


    The Survey of Chiplet-based Integrated Architecture: An EDA perspective ASPDAC
    ''25, January 20–23, 2025, Tokyo, Japan


    - <span id="page-8-0"></span>[7] S. J. Souri, K. Banerjee, A. Mehrotra et al.,
    "Multiple Si layer ICs: Motivation, performance analysis, and design implications,"
    in Proc. DAC, 2000, pp. 213–220.

    - <span id="page-8-1"></span>[8] Z. Wang, J. Sun, A. Goksoy et al., "Exploiting
    2.5 D/3D heterogeneous integration for AI computing," in Proc. ASPDAC, 2024, pp.
    758–764.

    - <span id="page-8-2"></span>[9] A. Limaye, C. Barone, N. B. Agostini et al.,
    "Towards automated generation of chiplet-based systems invited paper," in Proc.
    ASPDAC, 2024, pp. 771–776.

    - <span id="page-8-3"></span>[10] H. Peng, S. Davidson, R. Shi et al., "Chiplet
    Cloud: Building AI supercomputers for serving large generative language models,"
    arXiv preprint, 2024.

    - <span id="page-8-4"></span>[11] Z. Yang, S. Ji, X. Chen et al., "Challenges
    and Opportunities to Enable Large-Scale Computing via Heterogeneous Chiplets,"
    in Proc. ASPDAC, 2024, pp. 765–770.

    - <span id="page-8-5"></span>[12] T. Wang et al., "Application defined on-chip
    networks for heterogeneous chiplets: An implementation perspective," in Proc.
    HPCA, 2022, pp. 1198–1210.

    - <span id="page-8-6"></span>[13] P. Iff, B. Bruggmann, M. Besta et al., "RapidChiplet:
    A toolchain for rapid design space exploration of chiplet architectures," arXiv
    preprint, 2023.

    - <span id="page-8-7"></span>[14] S. Pal, D. Petrisko, R. Kumar, and P. Gupta,
    "Design space exploration for chipletassembly-based processors," IEEE TVLSI, vol.
    28, no. 4, pp. 1062–1073, 2020.

    - <span id="page-8-8"></span>[15] N. Jiang, D. U. Becker et al., "A detailed and
    flexible cycle-accurate network-onchip simulator," in Proc. ISPASS, 2013, pp.
    86–96.

    - [16] V. Catania, A. Mineo, S. Monteleone et al., "Cycle-accurate network on
    chip simulation with noxim," ACM TOMACS, vol. 27, no. 1, pp. 1–25, 2016.

    - <span id="page-8-9"></span>[17] T. E. Carlson, W. Heirman et al., "Sniper: Exploring
    the level of abstraction for scalable and accurate parallel multi-core simulation,"
    in Proc. SC, 2011, pp. 1–12.

    - <span id="page-8-10"></span>[18] H. Zhi, X. Xu, W. Han et al., "A methodology
    for simulating multi-chiplet systems using open-source simulators," in Proc. NANOCOM,
    2021, pp. 1–6.

    - <span id="page-8-11"></span>[19] S. Chen, S. Li, Z. Zhuang et al., "Floorplet:
    Performance-aware floorplan framework for chiplet integration," IEEE TCAD, vol.
    43, no. 6, pp. 1638–1649, 2024.

    - <span id="page-8-13"></span><span id="page-8-12"></span>[20] The gem5 simulator.
    [Online]. Available: <https://resources.gem5.org/> [21] S. Li, J. H. Ahn, R. D.
    Strong et al., "The McPAT Framework for Multicore and Manycore Architectures:
    Simultaneously Modeling Power, Area, and Timing," ACM TACO, vol. 10, no. 1, 2013.

    - <span id="page-8-14"></span>[22] J. Zhai, C. Bai, B. Zhu et al., "McPAT-Calib:
    A RISC-V BOOM Microarchitecture Power Modeling Framework," IEEE TCAD, vol. 42,
    no. 1, pp. 243–256, 2023.

    - <span id="page-8-15"></span>[23] J. Zhai, Y. Cai, and B. Yu, "Microarchitecture
    Power Modeling via Artificial Neural Network and Transfer Learning," in Proc.
    ASPDAC, 2023, p. 302–307.

    - <span id="page-8-16"></span>[24] Q. Zhang, S. Li et al., "PANDA: Architecture-Level
    Power Evaluation by Unifying Analytical and Machine Learning Solutions," in Proc.
    ICCAD, 2023, pp. 1–9.

    - <span id="page-8-17"></span>[25] J. Kim, V. C. K. Chekuri, N. M. Rahman et al.,
    "Chiplet/interposer co-design for power delivery network optimization in heterogeneous
    2.5-D ICs," IEEE TCPMT, vol. 11, no. 12, pp. 2148–2157, 2021.

    - <span id="page-8-18"></span>[26] X. Li, L. Chen, S. Chen et al., "Power management
    for chiplet-based multicore systems using deep reinforcement learning," in Proc.
    ISVLSI, 2022, pp. 164–169.

    - <span id="page-8-19"></span>[27] J. A. Cunningham, "The use and evaluation of
    yield models in integrated circuit manufacturing," IEEE TSM, vol. 3, no. 2, pp.
    60–71, 1990.

    - <span id="page-8-20"></span>[28] M. Ahmad, J. DeLaCruz, and A. Ramamurthy, "Heterogeneous
    integration of chiplets: cost and yield tradeoff analysis," in Proc. EuroSimE,
    2022, pp. 1–9.

    - <span id="page-8-22"></span>[29] Y. Feng and K. Ma, "Chiplet actuary: a quantitative
    cost model and multi-chiplet architecture exploration," in Proc. DAC, 2022, pp.
    121–126.

    - <span id="page-8-21"></span>[30] T. Tang and Y. Xie, "Cost-aware exploration
    for chiplet-based architecture with advanced packaging technologies," arXiv preprint,
    2022.

    - <span id="page-8-23"></span>[31] C. Bai, Q. Sun, J. Zhai et al., "Boom-explorer:
    Risc-v boom microarchitecture design space exploration framework," in Proc. ICCAD,
    2021, pp. 1–9.

    - [32] J. Zhai and Y. Cai, "Microarchitecture design space exploration via pareto-driven
    active learning," IEEE TVLSI, vol. 31, no. 11, pp. 1727–1739, 2023.

    - [33] C. Bai, J. Zhai, Y. Ma et al., "Towards Automated RISC-V Microarchitecture
    Design with Reinforcement Learning," in Proc. AAAI, vol. 38, no. 1, 2024, pp.
    12–20.

    - [34] X. Yi, J. Lu, X. Xiong et al., "Graph Representation Learning for Microarchitecture
    Design Space Exploration," in Proc. DAC, 2023, pp. 1–6.

    - [35] S. Chen, S. Zheng, C. Bai et al., "SoC-Tuner: An importance-guided exploration
    framework for DNN-targeting SoC design," in Proc. ASPDAC, 2024, pp. 207–212. [36]
    Z. Wang, J. Wang, et al., "A hierarchical adaptive multi-task reinforcement learn-

    - ing framework for multiplier circuit design," in Proc. ICML, 2024.

    - <span id="page-8-24"></span>[37] W. Zhihai, W. Jie, Y. Qingyue, B. Yinqi et
    al., "Towards next-generation logic synthesis: A scalable neural circuit generation
    framework," in Proc. NeurIPS, 2024.

    - <span id="page-8-25"></span>[38] S. Pal, D. Petrisko, R. Kumar, and P. Gupta,
    "Design space exploration for chipletassembly-based processors," IEEE TVLSI, vol.
    28, no. 4, pp. 1062–1073, 2020.

    - <span id="page-8-26"></span>[39] J. Cai, Z. Wu, S. Peng et al., "Gemini: Mapping
    and Architecture Co-exploration for Large-scale DNN Chiplet Accelerators," in
    Proc. HPCA, 2024, pp. 156–171.

    - <span id="page-8-27"></span>[40] Z. Tan et al., "NN-Baton: DNN Workload Orchestration
    and Chiplet Granularity Exploration for Multichip Accelerators," in Proc. ISCA,
    2021, pp. 1013–1026.

    - <span id="page-8-28"></span>[41] S. Schlag, T. Heuer, L. Gottesbüren, Y. Akhremtsev,
    C. Schulz, and P. Sanders, "High-quality hypergraph partitioning," ACM J. Exp.
    Algorithmics, vol. 27, 2023.

    - <span id="page-8-29"></span>[42] F. Li, Y. Wang, Y. Wang et al., "Chipletizer:
    Repartitioning SoCs for Cost-Effective Chiplet Integration," in Proc. ASPDAC,
    2024, pp. 58–64.

    - <span id="page-8-30"></span>[43] F. Li, Y. Wang, Y. Cheng et al., "GIA: A Reusable
    General Interposer Architecture for Agile Chiplet Integration," in Proc. ICCAD,
    2022, pp. 1–9.

    - <span id="page-8-31"></span>[44] S. Bharadwaj, J. Yin, B. Beckmann, and T. Krishna,
    "Kite: A family of heterogeneous interposer topologies enabled via accurate interconnect
    modeling," in Proc. DAC. IEEE, 2020, pp. 1–6.

    - <span id="page-8-32"></span>[45] P. Iff, M. Besta, M. Cavalcante, T. Fischer,
    L. Benini, and T. Hoefler, "HexaMesh: Scaling to Hundreds of Chiplets with an
    Optimized Chiplet Arrangement," in Proc. DAC, 2023, pp. 1–6.

    - <span id="page-8-33"></span>[46] H. Zheng, K. Wang, and A. Louri, "A versatile
    and flexible chiplet-based system design for heterogeneous manycore architectures,"
    in Proc. DAC, 2020, pp. 1–6.

    - <span id="page-8-34"></span>[47] S. Murali, P. Meloni, F. Angiolini et al.,
    "Designing Application-Specific Networks on Chips with Floorplan Information,"
    in Proc. ICCAD, 2006, pp. 355–362.

    - <span id="page-8-35"></span>[48] D. D. Sharma, G. Pasdast, Z. Qian, and K. Aygun,
    "Universal chiplet interconnect express (UCIe): An open industry standard for
    innovations with chiplets at package level," IEEE TCPMT, vol. 12, no. 9, pp. 1423–1431,
    2022.

    - <span id="page-8-36"></span>[49] B. Dehlaghi, N. Wary, and T. C. Carusone, "Ultra-short-reach
    interconnects for dieto-die links: Global bandwidth demands in microcosm," IEEE
    Solid-State Circuits Magazine, vol. 11, no. 2, pp. 42–53, 2019.

    - <span id="page-8-37"></span>[50] S. Ardalan, H. Cirit, R. Farjad et al., "Bunch
    of wires: An open die-to-die interface," in Proc. HOTI, 2020, pp. 9–16.

    - <span id="page-8-38"></span>[51] Y. Feng, D. Xiang, and K. Ma, "Heterogeneous
    die-to-die interfaces: Enabling more flexible chiplet interconnection systems,"
    in Proc. MICRO, 2023, p. 930–943.

    - <span id="page-8-39"></span>[52] Y. Thonnart et al., "POPSTAR: A robust modular
    optical NoC architecture for chiplet-based 3D integrated systems," in Proc. DATE,
    2020, pp. 1456–1461.

    - <span id="page-8-40"></span>[53] G. Li and Y. Ye, "HPPI: A high-performance
    photonic interconnect design for chiplet-based DNN accelerators," IEEE TCAD, vol.
    43, no. 3, pp. 812–825, 2024.

    - <span id="page-8-41"></span>[54] Y. Feng, D. Xiang, and K. Ma, "A scalable methodology
    for designing efficient interconnection network of chiplets," in Proc. HPCA, 2023,
    pp. 1059–1071. [55] E. Taheri, S. Pasricha et al., "DeFT: A Deadlock-Free and
    Fault-Tolerant Routing

    - <span id="page-8-42"></span>Algorithm for 2.5D Chiplet Networks," in Proc. DATE,
    2022, pp. 1047–1052. [56] H.-W. Chiou, J.-H. Jiang et al., "Chiplet placement
    for 2.5 D IC with sequence

    - <span id="page-8-43"></span>pair based tree and thermal consideration," in Proc.
    ASPDAC, 2023, pp. 7–12.

    - <span id="page-8-44"></span>[57] Y. Ma, L. Delshadtehrani et al., "TAP-2.5D:
    A Thermally-Aware Chiplet Placement Methodology for 2.5D Systems," in Proc. DATE,
    2021, pp. 1246–1251.

    - <span id="page-8-45"></span>[58] A. Coskun et al., "Cross-layer co-optimization
    of network design and chiplet placement in 2.5-D systems," IEEE TCAD, vol. 39,
    no. 12, pp. 5183–5196, 2020.

    - <span id="page-8-46"></span>[59] Y. Hsu, M.-H. Chung, Y.-W. Chang et al., "Transitive
    Closure Graph-Based Warpage-Aware Floorplanning for Package Designs," in Proc.
    ICCAD, 2022.

    - <span id="page-8-47"></span>[60] Y. Duan, X. Liu, Z. Yu et al., "RLPlanner:
    Reinforcement learning based floorplanning for chiplets with fast thermal analysis,"
    arXiv preprint, 2024.

    - <span id="page-8-48"></span>[61] M. Molter, R. Kumar, S. Koller et al., "Thermal-Aware
    SoC Macro Placement and Multi-chip Module Design Optimization with Bayesian Optimization,"
    in Proc. ECTC, 2023, pp. 935–942.

    - <span id="page-8-49"></span>[62] Z. Deng, Y. Duan, L. Shao et al., "Chiplet
    Placement Order Exploration based on Learning to Rank with Graph Representation,"
    in Proc. ISEDA, 2024, pp. 605–610.

    - <span id="page-8-50"></span>[63] M. A. Kabir, D. Petranovic, and Y. Peng, "Cross-boundary
    inductive timing optimization for 2.5 D chiplet-package co-design," in Proc. GLSVLSI,
    2021.

    - <span id="page-8-51"></span>[64] Y.-J. Cai, Y. Hsu et al., "Simultaneous pre-and
    free-assignment routing for multiple redistribution layers with irregular vias,"
    in Proc. DAC, 2021, pp. 1147–1152.

    - <span id="page-8-52"></span>[65] S.-Y. Lee, D. Kim, K. Min et al., "Signal-Integrity-Aware
    Interposer Bus Routing in 2.5 D Heterogeneous Integration," in Proc. ASPDAC, 2022,
    pp. 178–183.

    - <span id="page-8-53"></span>[66] M.-H. Chung, J.-W. Chuang, and Y.-W. Chang,
    "Any-angle routing for redistribution layers in 2.5 D IC packages," in Proc. DAC,
    2023, pp. 1–6.

    - <span id="page-8-54"></span>[67] A. Inc. (2024) Ansys icepak. [Online]. Available:
    [https://www.ansys.com/](https://www.ansys.com/products/thermal/ansys-icepak)
    [products/thermal/ansys-icepak](https://www.ansys.com/products/thermal/ansys-icepak)

    - <span id="page-8-55"></span>[68] (2024) Hotspot7.0. [Online]. Available: <https://github.com/uvahotspot/HotSpot>

    - <span id="page-8-56"></span>[69] X. Ma, Q. Xu, C. Wang et al., "An Electrical–Thermal
    Co-Simulation Model of Chiplet Heterogeneous Integration Systems," IEEE TVLSI,
    2024.

    - <span id="page-8-57"></span>[70] L. Chen, W. Jin, and S. X.-D. Tan, "Fast thermal
    analysis for chiplet design based on graph convolution networks," in Proc. ASPDAC,
    2022.

    - <span id="page-8-58"></span>[71] X. Dong, S. Sun, Y. Jiang et al., "SPIRAL:
    Signal-Power Integrity Co-Analysis for High-Speed Inter-Chiplet Serial Links Validation,"
    in Proc. ASPDAC, 2024.

    - <span id="page-8-59"></span>[72] W. Miao, Z. Xie, C. S. Tan et al., "Deep reinforcement
    learning-based power distribution network design optimization for multi-chiplet
    system," in Proc. ECTC, 2024, pp. 1716–1723.

    - <span id="page-8-60"></span>[73] W.-H. Liu, M.-S. Chang, and T.-C. Wang, "Floorplanning
    and signal assignment for silicon interposer-based 3D ICs," in Proc. DAC, 2014,
    pp. 1–6.

    - <span id="page-8-61"></span>[74] S. Osmolovskyi, J. Knechtel, I. L. Markov,
    and J. Lienig, "Optimal die placement for interposer-based 3D ICs," in Proc. ASPDAC,
    2018, pp. 513–520.

    - <span id="page-8-62"></span>[75] Y.-C. Chang, Y.-W. Chang, G.-M. Wu et al.,
    "B\*-trees: A new representation for non-slicing floorplans," in Proc. DAC, 2000,
    p. 458–463.

    - <span id="page-8-63"></span>[76] H. Murata, K. Fujiyoshi, S. Nakatake, and Y.
    Kajitani, "Rectangle-packing-based module placement," in Proc. ICCAD, 1995, pp.
    472–479.

    - <span id="page-8-64"></span>[77] X. Hong, G. Huang et al., "Corner block list:
    an effective and efficient topological representation of non-slicing floorplan,"
    in Proc. ICCAD, 2000, pp. 8–12.

    - <span id="page-8-65"></span>[78] T.-C. Chen, Z.-W. Jiang, T.-C. Hsu et al.,
    "Ntuplace3: An analytical placer for large-scale mixed-size designs with preplaced
    blocks and density constraints," IEEE TCAD, vol. 27, no. 7, pp. 1228–1240, 2008.

    - <span id="page-8-66"></span>[79] Z. Wang, Z. Geng, Z. Tu, J. Wang et al., "Benchmarking
    end-to-end performance of ai-based chip placement algorithms," arXiv preprint,
    2024.

    - <span id="page-8-67"></span>[80] C. Burges, T. Shaked, E. Renshaw et al., "Learning
    to rank using gradient descent," in Proc. ICML, 2005, pp. 89–96.

    - <span id="page-8-68"></span>[81] C. AB. (2024) Comsol multiphysics. [Online].
    Available: [https://www.comsol.](https://www.comsol.com/comsol-multiphysics) [com/comsol-multiphysics](https://www.comsol.com/comsol-multiphysics)'
- title: Further Evaluations of a Didactic CPU Visual Simulator (CPUVSIM)
  abstract: 'This paper discusses further evaluations of the educational effectiveness
    of

    an existing CPU visual simulator (CPUVSIM). The CPUVSIM, as an Open Educational

    Resource, has been iteratively improved over a number of years following an

    Open Pedagogy approach, and was designed to enhance novices understanding of

    computer operation and mapping from high-level code to assembly language. The

    literature reports previous evaluations of the simulator, at K12 and

    undergraduate level, conducted from the perspectives of both developers and

    students, albeit with a limited sample size and primarily through qualitative

    methods. This paper describes additional evaluation activities designed to

    provide a more comprehensive assessment, across diverse educational settings:

    an action research pilot study recently carried out in Singapore and the

    planning of a more quantitative-oriented study in Dubai, with a larger sample

    size. Results from the pilot study in Singapore confirm the effectiveness and

    high level of appreciation of the tool, alongside a few identified challenges,

    which inform the planning of the more comprehensive evaluation in Dubai.'
  url: http://arxiv.org/abs/2411.05229v1
  keywords: ''
  document: '| Renato Cortinovis    | Tamer Mohamed       | Devender Goyal   | Luiz
    Fernando      |

    |----------------------|---------------------|------------------|--------------------|

    | Freelance Researcher | Abdellatif          | Raytheon         | Capretz            |

    | Italy                | Canadian University | Technologies,    | Western University
    |

    | rmcortinovis@gmail.c | Dubai, United Arab  | USA              | Canada             |

    | om                   | Emirates            | dg1998@gmail.com | lcapretz@uwo.ca    |

    |                      | tamer.mohamed@cu    |                  |                    |

    |                      | d.ac.ae             |                  |                    |


    # **Further Evaluations of a Didactic CPU Visual Simulator (CPUVSIM)**


    ## **Abstract**


    This paper discusses further evaluations of the educational effectiveness of an
    existing CPU visual simulator (CPUVSIM). The CPUVSIM, as an Open Educational Resource,
    has been iteratively improved over a number of years following an Open Pedagogy
    approach, and was designed to enhance novices'' understanding of computer operation
    and mapping from high-level code to assembly language. The literature reports
    previous evaluations of the simulator, at K12 and undergraduate level, conducted
    from the perspectives of both developers and students, albeit with a limited sample
    size and primarily through qualitative methods. This paper describes additional
    evaluation activities designed to provide a more comprehensive assessment, across
    diverse educational settings: an action research pilot study recently carried
    out in Singapore and the planning of a more quantitative-oriented study in Dubai,
    with a larger sample size. Results from the pilot study in Singapore confirm the
    effectiveness and high level of appreciation of the tool, alongside a few identified
    challenges, which inform the planning of the more comprehensive evaluation in
    Dubai.


    ### **1. Introduction**


    Numerous CPU visual simulators have emerged over time with the primary objective
    of enhancing the understanding of computer operation (Nikolic et al., 2009). These
    simulators cater to various levels of expertise and often specialize in specific
    facets of computer science, such as computer security (Imai et al., 2013) or pipelining
    (Zhang and Adams III, 1997). Among this diverse array of simulators, a small subset
    addresses a well-recognized issue: that students – despite studying both high-level
    programming languages and computer architecture fundamentals – frequently struggle
    to grasp how high-level code actually executes on computer hardware (Evangelidis
    et al., 2021; Miura et al., 2003). These concepts are considered fundamental in
    computer science and software engineering education and training: the Software
    Engineering Body of Knowledge (Bourque and Fairley, 2022), for example, reports
    that "software engineers are expected to know how high-level programming languages
    are translated into machine languages".


    In this context, the CPUVSIM (Cortinovis, 2021) – an Open Educational Resource
    available from Merlot and OER Commons – supports novices in comprehending the
    fundamental components of a simplified CPU, and in understanding the mapping from
    high-level control structures to low-level code, i.e. assembly and machine code.
    This is achieved through detailed animations that illustrate the execution of
    instructions, and empowering learners to write meaningful programs using a minimalist
    yet representative assembly language.


    Figure 1 shows a screenshot of the CPUVSIM running in a browser, with a simple
    program loaded in RAM. The user can execute the program one instruction or micro-instruction
    at a time, at the desired speed. The user can interactively modify, at any time,
    the content of the RAM, or any register in the CPU. While the execution is animated,
    a voice over explains what is happening – in English, Spanish, or Italian.


    As detailed by Cortinovis (2021), the development of CPUVSIM sought to address
    limitations observed in existing applications. While some simulators, such as
    LMC (Higginson, 2014), were deemed overly simplistic, others were considered unnecessarily
    complex. CPUVSIM, on the other hand, was honed through iterative improvements
    and extensions, building upon the foundation of an already popular visual simulator
    known as PIPPIN (Decker and Hirshfield, 1998). Its development process followed
    a sustainable Open Pedagogy approach (Wiley and Hilton, 2018) in the form of non-disposable
    assignments to computer science students over multiple years.


    ![](_page_1_Figure_2.jpeg)


    *Figure 1 - A screenshot of the CPUVSIM*


    The CPUVSIM has previously undergone evaluations of its educational effectiveness,
    albeit with limited sample size and primarily through qualitative methods. These
    evaluations were conducted from the perspectives of both its developers and students
    who engaged with it in different contexts. In this paper, we describe an evaluation
    recently conducted in Singapore and also describe an evaluation planned to be
    conducted in Dubai. These studies outline additional evaluation activities designed
    to provide a more comprehensive assessment of the CPUVSIM. Our efforts encompass
    diverse settings, employing as far as possible complementary approaches.


    Following the general introduction in this section, Section 2 reports on the evaluation
    of CPU simulators in the literature, Section 3 describes our Action Research pilot
    evaluation recently carried out in Singapore and its findings. Section 4 describes
    the planning of the evaluation to be conducted at a University in Dubai with a
    larger number of students, utilizing again Action Research but complemented with
    elements of a quantitative-oriented experimental approach. Finally, Section 5
    presents our conclusions.


    # **2. CPU simulators evaluation strategies**


    As mentioned earlier, the CPUVSIM has already undergone some limited and mainly
    qualitative evaluations. Cortinovis (2021) describes its informal qualitative
    evaluation from the developers'' point of view, who deeply appreciated, in particular,
    the opportunity to work on a real problem in a real context, and the opportunity
    to contribute to the common good. Cortinovis and Rajan (2022) describe the evaluation
    from the students'' point of view, both in two specialized technical schools in
    Italy (K12 and lifelong adult education) and in a first and a second-year undergraduate
    computer architecture courses in Colorado (USA). The students who used the simulator
    provided very positive feedback, which was analysed with a qualitative thematic
    content analysis, and was then used to further improve and extend the latest version
    of the simulator.


    Nikolic et al. (2009), evaluate a rich set of existing CPU simulators, but only
    on the basis of characteristics identified from the documentation. Some of the
    criteria they used, such as level of coverage of the topics included in standard
    curricula, are not considered fully relevant in this context: the CPUVSIM is meant
    to support a firm grasp of the fundamental mechanisms, but at a relatively high
    level of abstraction, without dwelling too much in details.


    Imai et al. (2013) evaluated the correlation between tests carried out on their
    simulator and the course final exam results. The strong correlation they reported
    is interesting, but to demonstrate the effectiveness of their simulator, it is
    necessary to compare it with an alternative tool or a comparable teaching method.
    Indeed, in subsequent works (Imai et al., 2018), they adopted a qualitative approach
    alone with a simple questionnaire.


    Chalk (2002) and Mustafa (2010) both used a mixed qualitative and quantitative
    strategy. The quantitative approach, in particular, makes use of a quasi-experimental
    schema, with experimental and control groups, and pre and post knowledge tests.
    Although the experimental strategy is instrumental in collecting supporting evidence
    about the effectiveness of the tool, and while pre- and post- knowledge tests
    can demonstrate the improvements of students'' knowledge using the simulator,
    this approach does not provide information about its effectiveness against alternative
    strategies. Chalk (2002) demonstrates, in particular, the importance of referring
    to precisely formulated learning objectives, to test results against.


    # **3. CPUVSIM pilot evaluation in Singapore**


    We planned and executed a first pilot evaluation of the simulator, on a small
    scale (13 students in total), in two undergraduate courses at the Yale-NUS College
    in Singapore: a course on C Programming and a course on Software Verification
    and Validation. The simulator was used in the first course to help students understand
    the mapping between C control structures and assembly code. It was used in the
    second course to test programs at machine language level. Considering the limited
    number of students, and the limited possibilities to control the many factors
    involved (different classes, different teachers, etc.), we considered it appropriate
    to adopt a socially-oriented, situational Action Research methodology, preferring
    a more postmodernist-oriented approach over a strictly positivist one (Kemmis
    and McTaggart, 2000).


    Given the overall goal of grasping how code written in high-level language is
    actually executed on the hardware of a computer, we outlined first, as recommended
    by Chalk (2002), the learning objectives:


    - Understand the role of the key components of a CPU.

    - Understand the mapping from high-level to low-level control structures (assembly
    and machine) code.

    - Code meaningful high-level programs with a minimalist but representative assembly
    language.


    More specifically:


    - Describe typical assembly instructions supported by a CPU.

    - Explain the fundamental steps carried out by the main subcomponents of a CPU,
    to execute a given assembly instruction.

    - Identify the information transferred on the Data bus, Address bus, and Control
    bus during each step of every instruction.

    - Apply the suitable numeric/immediate and direct addressing modalities.

    - Exemplify the use of the CPU flags through simple examples.

    - Translate a program in C with a single control structure to assembly code.


    According to the adopted research methodology, we defined an action plan for the
    proposed intervention, including specific pedagogical activities as well as "Data
    analysis and critical reflection", and "Refinement of the planned intervention
    for future courses".


    In particular, we foresaw a first activity to present in class the CPUVSIM and
    its associated e-book (1.5 hours), follow-up students'' activities to be started
    in class and completed at home (a couple of hours to familiarize individually
    with the CPUVSIM and related educational material), plus an additional hour to
    complete the graded activities. These included:


    Briefly explain the differences between conditional and non-conditional jump.


    Briefly list/describe the steps carried out by the main sub-components of a CPU,
    to execute the instructions ADD #20 and ADD 20 (immediate and direct addressing).


    Identify the missing instruction in the following translation of an IF-THEN-ELSE
    control structure to assembly:


    | IF SUM == 2 | LOD SUM          |  |

    |-------------|------------------|--|

    | THEN SUM=3  | CMP #2           |  |

    | ELSE SUM=5  | JNZ ELSE         |  |

    | ENDIF       | LOD #3           |  |

    |             | // MISSING CODE? |  |

    |             | ELSE:<br>LOD #5  |  |

    |             | ENDIF: STO SUM   |  |

    |             | HALT             |  |

    |             | SUM:<br>0        |  |


    The second course on software verification and validation included the following
    additional assignments:


    Use the Simulator to test if the translation of the following high-level control
    structures to Assembler are correct or not […]; explain your answer.


    Discuss Specific Testing Strategies for Assembly code.


    We finally specified a survey with Likert type questions and open questions (Mustafa,
    2010; Cortinovis and Rajan, 2022), to collect feedback about the CPUVSIM and the
    learning experience, such as:


    What ameliorations could be made to the simulator and/or related e-book to improve
    your learning experience?


    The final assignment was graded and analysed with psychometric Classical Test
    Theory (Novick, 1996). Taking into account the limited number of students, the
    Likert-type questions in the survey were analysed with basic descriptive statistics
    (Mustafa, 2010), the open questions in the survey were analysed with thematic
    content analysis (Cortinovis and Rajan, 2022).


    Finally, we carried out a critical reflection on the effectiveness of the intervention
    to derive the recommendations for planning the subsequent intervention – according
    to the iterative nature of Action Research, and to the goal of a pilot.


    #### 3.1 Pilot Results


    The data extracted from the survey (Table 1) on 13 students shows that the CPUVSIM
    was definitely appreciated, especially for understanding how C control structures
    actually get executed on a computer, which was the main goal.


    | Ques�ons                                                                                                                              |
    Strongly<br>Agree +<br>Agree (%) | Strongly<br>Disagree +<br>Disagree (%) | Neutral
    (%) |

    |---------------------------------------------------------------------------------------------------------------------------------------|----------------------------------|----------------------------------------|-------------|

    | The simulator and related e-book<br>were mo�va�ng and interes�ng.                                                                     |
    77% (10)                         | 0%                                     | 23%
    (3)     |

    | The simulator and related e-book<br>were useful for understanding how C<br>control
    structures actually get<br>executed on a computer. | 85% (11)                         |
    0%                                     | 15% (2)     |

    | I found the simulator too complicated<br>to understand and use effec�vely.                                                            |
    23% (3)                          | 46% (6)                                | 31%
    (4)     |


    *Table 1 – Sample extracted from the survey in Singapore.*


    Interestingly, a relevant number (23%) of students stated that the simulator was
    not easy to understand and use effectively: this was probably due to the limited
    time devoted to its presentation (just 1.5 hours in total). Indeed, a first student
    who found the simulator too complicated suggested having "more hand holding in
    class"; a second student considered that "the lecture included too much". A student
    found that the simulator was too fast: "It was challenging keeping up with its
    fast-pace while still understanding newly introduced concepts". Obviously, this
    student did not notice the possibility to control the speed, which was explicitly
    appreciated by other students.


    Despite these problems, the students'' answers on the assignments demonstrated
    a remarkable grasp of the targeted key concepts. There were no incorrect solutions
    to the assignments, even if there were omissions of relevant details in a few
    of them – notably from a student who found the use of the simulator somewhat complicated.


    These overall positive outcomes were strongly correlated with the students'' self-perceptions:
    one of them, for example, stated that she reached "a solid understanding of how
    high-level code runs on hardware", while another one found it "really eye-opening
    to see how it actually works at the base level". The number of students involved
    in this pilot was limited, yet the evaluation results confirm previous results
    available in the literature (Cortinovis and Rajan, 2022). The main lesson learned
    for future deliveries of the course, is the need to dedicate more time to coaching
    the students in the use of the simulator, so that all of them can get the most
    from it.


    # **4. CPUVSIM planned evaluation in Dubai**


    In Dubai we aim to improve the previous evaluations addressing two potentially
    weak aspects of action research: generalizability and rigor. Concerning generalizability,
    we are evaluating the simulator in different contexts, that is, different courses,
    multiple classes, and different countries. To improve the rigor of the evaluation
    process, we take advantage of the larger sample size available (120+ students),
    enriching the qualitative-oriented action research design used in Singapore, with
    a quantitative-oriented experimental approach.


    Drawing lessons from the pilot evaluation conducted in Singapore, we will allocate
    additional time to ensure that every student gains complete mastery over the utilization
    of CPUVSIM and its accompanying documentation. First, we will dedicate a decent
    time to our lab instructors to train on and master the simulator. This is planned
    to take place during the pre-semester preparation period of two weeks. During
    this period, the course instructors, with the support of the lab instructors,
    will work on integrating the simulator within the course syllabi and preparing
    the simulator-based assessment tasks. Accordingly, full two lab sessions (2 hours
    each) will be dedicated to the students'' training on the simulator. The first
    lab session (2 hours) will be dedicated to introducing the simulator''s built-in
    CPU instructions in addition to the education supporting features, such as the
    instructions execution simulator and the simulator e-book. After finishing this
    lab session, the students will be left with a simulatorbased homework. In the
    second lab session (2 hours), the students will be provided with time dedicated
    to homework discussion and one-by-one coaching on the answer of each of the homework
    tasks using the simulator. This way, the students will acquire a fair understanding
    level of the simulator''s functionality before proceeding with more challenging
    tasks.


    Furthermore, we will include the following additional learning objective:


    • Translate a program in a high-level language with multiple control structures,
    both sequenced and nested, to assembly code.


    Therefore, beyond the exercises proposed in Singapore, we will conduct a more
    comprehensive assessment of students'' proficiency in translating high-level constructs,
    such as loops, logic operators, and arithmetic operations, into assembly language.
    For example:


    - write an assembly program that determines whether the value stored in a variable
    "var1" is odd or even;

    - write an assembly program that performs a comparison between two signed variables.
    Var1=7Fh and Var2=80h. Then saves the highest and the lowest variables in HIGH
    and LOW variables respectively;

    - write an assembly program that determines whether a given positive integer number
    satisfies the Collatz conjecture.


    ### 4.1 Experimental Design and Population


    We have a total of 120+ students that will be partitioned into two groups. The
    first one will follow the traditional educational path of the previous years,
    which did not use a simulator but was based on theoretical lessons and paper-based
    exercises. The second one will follow an educational path modified


    with the new intervention, using the CPUVSIM. The students, who have Python and
    Java programming background, will have different lab instructors and teaching
    assistants.


    ### 4.2 Quantitative Data Collection and Analysis


    We will gather quantitative data through the following assessment setups:


    - Exercises under an invigilated environment: for this setup, students will be
    asked to answer the assessments at the lab using our learning management system
    (LMS). The students will have no access to the internet. The assessment will have
    a specified time limit and the LMS system will record the time taken by the student
    to complete each task (completing the task in less time will lead to collecting
    more marks). We plan to conduct this assessment setup twice within the course
    timeline.

    - Group-based exercises carried out in the lab: this kind of assessment will allow
    the students in small sub-groups (3-4 students) to hone their assembly level programming
    and benefit from each other. In case of any needed support from the instructor,
    the students will be asked to submit their inquiries via the LMS system.

    - Written exams: this includes both graded mid-term and final exams. Both students''
    grades and specific mistakes will be gathered for this kind of assessment setup.


    Therefore, in addition to the students'' grades, we will gather other quantitative
    data such as the time needed to finish the assessment, number of students'' mistakes,
    types/categories of the students'' mistakes (for instance, whether due to incorrect
    understanding of the logic of jump instructions or to incorrect understanding
    of the mapping between high level programming and assembly code logic), and the
    Grade Point Average of each group member. Furthermore, each assessment''s activity/question
    will be mapped to a certain learning objective. This allows us to evaluate the
    effectiveness of using the simulator on improving the students'' knowledge for
    each of our learning objectives, in addition to the overall impact on the students''
    performance along course(s).


    We will adopt ANOVA variance analysis statistical test to verify whether the results
    for the two main student groups show an overall statistically significant difference
    based on the use of CPUVSIM according to the sample size at hand. In addition,
    we plan to apply the Tukey''s Honestly Significant Difference (HSD) statistical
    test to figure out which group of data parameters is impacted the most by using
    the CPUVSIM based on the sample at hand. Examples of data parameters that can
    be studied by HSD are: time needed to finish the task, number of mistakes.


    #### 4.3 Qualitative Data Collection and Analysis


    Qualitative data will be collected using a survey with both Likert-type questions
    and open questions similar to the ones adopted in the pilot study. Here, however,
    we plan to extend our qualitative data by collecting feedback from teachers too.
    Therefore, the survey questions for the teachers will include, for example: How
    does the CPUVSIM impact on the students'' understanding of the assembly language
    structure? How does it impact related explanations? Would you recommend making
    use of the CPUVSIM, and how?


    We plan to interview all the course instructors as well as one student from each
    student-focused subgroup. To analyse these qualitative data, both thematic and
    narrative analysis will be adopted. Thematic analysis will help in identifying
    and interpreting the patterns from our survey results, while narrative analysis
    will provide a better understanding of the motivation behind the feedback provided
    by the interviewees.


    Finally, we will also run a longitudinal study, because we suspect that the students
    with hands-on experience with the simulator might better retain over time the
    competences acquired, compared to the students who followed the more traditional
    path. Therefore, we will retest the students of both groups after 12 months, to
    assess the possible different levels of retention of key concepts and competencies.


    #### 4.4 Threats to Validity


    Generalizability and rigor are the two main weak aspects of the situational nature
    of action research. Yet, the variety of contexts where these evaluations are carried
    out (Singapore) and planned (Dubai), in addition to the evaluations previously
    reported in the literature (Italy and USA), should contribute to support the generalization
    of the outcomes. Additionally, we integrated in the methodology for the planned
    evaluation in Dubai, a quantitative-oriented experimental component to improve
    rigor: the use of complementary research methodologies, selected to better fit
    the particular contexts, should help compensate for their weaknesses.


    More importantly, we acknowledge our limited control over numerous variables,
    particularly the inevitably diverse approaches employed by various educators when
    utilizing the tool. The challenge lies in discerning the tool''s impact amidst
    the multitude of factors influencing student learning. It is imperative to recognize
    the tool as just a component within a broader socio-technical system, as highlighted
    by Mulholland (personal communication, 2023). To tackle this challenge, our evaluation
    strategy in Dubai includes qualitative assessments from educators'' perspectives.
    These assessments aim to glean insights into how teachers perceive the CPUVSIM
    and its effects on their educational endeavours. Additionally, we leverage the
    recently developed CPUVSIM accompanying e-book titled "A Gentle Introduction to
    the Central Processing Unit (CPU) and Assembly Language". This Open Educational
    Resource, available via Merlot or OER Commons, offers some pedagogical support
    for utilizing the CPUVSIM. It provides explanations and practical activities to
    support both teachers as well as self-learners. Notably, this interactive e-book
    integrates the CPUVSIM seamlessly. Each programming example or exercise within
    the e-book features live images, embedding the fully functional CPUVSIM. Users
    can execute, modify, and re-execute these "images" at will, enhancing the interactive
    learning experience.


    ## 4.5 Ethical Considerations


    In our plan, only one of the two groups of students will have the opportunity
    to reap the potential advantages of using the CPUVSIM, which can be questioned
    from the ethical point of view. To overcome this problem, in case the evaluation
    would show that a group was considerably disadvantaged compared to the other,
    we plan to offer, at the end of the evaluation, some extra educational activities
    to level their competences. These extra activities would target specifically the
    topics where the evaluation might have identified significant differences.


    ## **5. Conclusions**


    We have outlined our plans for assessing the educational effectiveness of the
    CPUVSIM simulator. The feedback from the pilot in Singapore, using Action Research,
    confirms a positive effect on students'' ability to grasp important concepts and
    good appreciation for the tool, as reported in the literature, together with the
    identification of some challenges. This provided useful indications for the wider
    evaluation planned in Dubai, where we will enrich the qualitative Action Research
    methodology with a more quantitative-oriented study, aiming to address concerns
    about generalizability and rigor.


    Through these activities, we are collecting feedback from students and teachers
    in diverse geographical regions, broadening the perspective on how the CPUVSIM
    resonates with stakeholders from different cultural backgrounds and educational
    systems.


    ## **6. References**


    - Bourque, P., & Fairley, R. E. (2022). Guide to the Software Engineering Body
    of Knowledge, Version 4.0 beta. IEEE Computer Society.

    - Chalk, B. (2002). Evaluation of a Simulator to Support the Teaching of Computer
    Architecture. In 3rd Annual LTSN-ICS Conference, Loughborough University.

    - Cortinovis, R. (2021). An educational CPU visual simulator. 32nd Annual Workshop
    of the Psychology of Programming Interest Group.

    - Cortinovis, R., & Rajan, R. (2022). Evaluating and improving the educational
    CPU visual simulator: a sustainable open pedagogy approach. In Proceedings of
    the 33rd Annual Workshop of the Psychology of Programming Interest Group, 189-196.

    - Decker, R., & Hirshfield, S. (1998). The Analytical Engine: An Introduction
    to Computer Science Using the Internet. PWS Publishing, Boston.

    - Evangelidis, G., Dagdilelis, V., Satratzemi, M., & Efopoulos, V. (2021). X-compiler:
    yet another integrated novice programming environment. In Proceedings of the IEEE
    International Conference on Advanced Learning Technologies, 166-169.

    - Higginson, P. (2014). Little Man Computer [Javascript application]. Retrieved
    September 2023, from https://peterhigginson.co.uk/LMC/.

    - Imai, Y., Hara, S., Doi, S., Kagawa, K., Ando, K., & Hattori, T. (2018). Application
    and evaluation of visual CPU simulator to support information security education.
    IEEJ Transactions on Electronics, Information and Systems. 138, 9, 1116-1122.

    - Imai, Y., Imai, M., & Moritoh, Y. (2013). Evaluation of visual computer simulator
    for computer architecture education. International Association for Development
    of the Information Society.

    - Kemmis, S., & McTaggart, R. (2000). Participatory action research. In Handbook
    of Qualitative Research, edited by Norman K. Denzin & Yvonna S. Lincoln, 2nd ed.,
    567-605.

    - Miura, Y., Keiichi, K., & Masaki, N. (2003). Development of an educational computer
    system simulator equipped with a compilation browser. In Proceedings of the International
    Conference of Computers in Education, 140-143.

    - Mustafa, B. (2010). Evaluating a system simulator for computer architecture
    teaching and learning support. Innovation in Teaching and Learning in Information
    and Computer Sciences. 9, 1, 100- 104.

    - Nikolic, B., Radivojevic, Z., Djordjevic, J., & Milutinovic, V. (2009). A survey
    and evaluation of simulators suitable for teaching courses in computer architecture
    and organization. IEEE Transactions on Education. 52, 4, 449-458.

    - Novick, M. R. (1996). The axioms and principal results of classical test theory.
    Journal of Mathematical Psychology. 3, 1, 1-18.

    - Wiley, D., & Hilton, J. (2018). Defining OER-enabled pedagogy. International
    Review of Research in Open and Distance Learning. 19, 4.

    - Zhang, Y., & Adams III, G. B. (1997). An interactive, visual simulator for the
    DLX pipeline. IEEE Computer Society Technical Committee on Computer Architecture
    Newsletter. 9-12.'
- title: A Review of SRAM-based Compute-in-Memory Circuits
  abstract: 'This paper presents a tutorial and review of SRAM-based Compute-in-Memory

    (CIM) circuits, with a focus on both Digital CIM (DCIM) and Analog CIM (ACIM)

    implementations. We explore the fundamental concepts, architectures, and

    operational principles of CIM technology. The review compares DCIM and ACIM

    approaches, examining their respective advantages and challenges. DCIM offers

    high computational precision and process scaling benefits, while ACIM provides

    superior power and area efficiency, particularly for medium-precision

    applications. We analyze various ACIM implementations, including current-based,

    time-based, and charge-based approaches, with a detailed look at charge-based

    ACIMs. The paper also discusses emerging hybrid CIM architectures that combine

    DCIM and ACIM to leverage the strengths of both approaches.'
  url: http://arxiv.org/abs/2411.06079v2
  keywords: ''
  document: '# A Review of SRAM-based Compute-in-Memory Circuits


    ## Kentaro Yoshioka\*, Shimpei Ando, Satomi Miyagi, Yung-Chin Chen, Wenlun Zhang


    Keio University Department of Electrical and Electronics Engineering Yagami Campus
    3-14-1 Hiyoshi, Kohoku-ku, Yokohama, Kanagawa 223-8522, Japan


    E-mail: kyoshioka47@keio.jp


    October 2024


    #### Abstract


    This paper presents a tutorial and review of SRAM-based Compute-in-Memory (CIM)
    circuits, with a focus on both Digital CIM (DCIM) and Analog CIM (ACIM) implementations.
    We explore the fundamental concepts, architectures, and operational principles
    of CIM technology.


    The review compares DCIM and ACIM approaches, examining their respective advantages
    and challenges. DCIM offers high computational precision and process scaling benefits,
    while ACIM provides superior power and area efficiency, particularly for medium-precision
    applications. We analyze various ACIM implementations, including current-based,
    time-based, and charge-based approaches, with a detailed look at charge-based
    ACIMs. The paper also discusses emerging hybrid CIM architectures that combine
    DCIM and ACIM to leverage the strengths of both approaches.


    #### 1. Introduction


    Recent advancements in Artificial Intelligence (AI) and Machine Learning (ML)
    have led to a dramatic increase in computational demands, particularly in the
    field of Deep Neural Networks (DNNs). The evolution of DNNs, from the breakthrough
    of AlexNet in 2012 [\[1\]](#page-19-0) to modern architectures like ResNet [\[2\]](#page-19-1)
    and Transformers [\[3\]](#page-19-2), has resulted in exponential growth in model
    complexity and scale. While these advancements have enabled remarkable achievements
    in various applications, including surpassing humanlevel performance in image
    recognition [\[4\]](#page-19-3) and demonstrating impressive language understanding
    capabilities [\[5\]](#page-19-4), they have also pushed traditional computing
    architectures to their limits.


    Traditional von Neumann architectures, which physically separate processors and
    memory, have become a significant bottleneck in processing AI/ML workloads [\[6\]](#page-19-5).
    Specifically, the memory wall problem limits performance and power efficiency
    due to data transfer between processors and memory. Moreover, while GPUs are defacto
    standard for DNN computing hardware, there is an increasing demand for improved
    power efficiency in AI/ML processing across all scenarios, from edge devices to
    data centers. To address these challenges, researchers are exploring new computing
    paradigms, with Compute-In-Memory (CIM) technology [\[7–](#page-20-0)[33\]](#page-21-0)
    gaining particular attention. CIM aims to minimize data movement and significantly
    enhance parallel processing capabilities and power efficiency by physically integrating
    memory and computational units.


    Compared to the previous SSDM2024 Extended Abstract [\[34\]](#page-21-1), this
    paper aims to provide a comprehensive review on recent advancements in the field
    of CIM macros, analyzing various DCIM, ACIM and hybrid architectures and their
    implementations. SRAM is well-suited for CIM macro implementation due to its high-speed
    access, low power consumption, and widespread use as on-chip memory in CMOS processes,
    requiring no additional process options and thus being cost-effective. We will
    particularly emphasize the comparative analysis of Digital CIM (DCIM) [\[27–](#page-21-2)[33\]](#page-21-0)
    and Analog CIM (ACIM) [\[8,](#page-20-1) [10–](#page-20-2)[21\]](#page-20-3),
    examine the advantages and challenges of ACIM approaches, and explore the potential
    of hybrid DCIM and ACIM approaches [\[22–](#page-20-4)[26\]](#page-21-3).


    The paper is structured as follows:


    - Section 2 details the fundamental concepts of CIM, including its basic architecture
    and operation principles.

    - Section 3 discusses Digital CIM (DCIM), examining its characteristics, advantages,
    and challenges.

    - Section 4 focuses on Analog CIM (ACIM), providing an in-depth analysis of when
    to use analog computing, various ACIM implementations, and a detailed look at
    charge-based ACIMs.

    - Section 5 reviews the designs of Hybrid CIMs and its advantages.

    - Section 6 concludes the paper, summarizing key points and discussing future
    directions in CIM research.


    By examining the current state-of-the-art, we seek to offer insights into the
    challenges, opportunities, and potential future directions of the CIM technology.


    #### 2. Basic Concepts and Architecture of Compute-In-Memory (CIM)


    Compute-In-Memory (CIM) is an innovative architecture that performs computations
    within or in close proximity to memory. This approach fundamentally rethinks the
    separation of memory and processor in traditional von Neumann architectures, aiming
    to significantly reduce the overhead associated with data movement. As shown in
    Fig[.1,](#page-2-0) the most crucial feature of CIM is its specialization in calculating
    the dot product of input vectors (IN) and weight vectors (W), which is the fundamental
    operation in neural networks. A typical CIM macro consists of an SRAM cell array,
    multiplication circuits, accumulator circuits, input/output interfaces, and control
    logic. The SRAM cell array stores the weight vector (W), while the input vector
    (IN) is


    ![](_page_2_Figure_0.jpeg)


    <span id="page-2-0"></span>Figure 1. Conceptional block diagram of a Compute-in-Memory
    Macro.


    supplied through bitlines. This structure enables parallel execution of multiple
    weightinput multiplications, allowing for fast and efficient dot product calculations.


    Here, the CIM operation with data mapping is explained. Mathematically, the dot
    product operation in CIM can be expressed as:


    $$y = \sum\_{i=1}^{n} IN\_i \cdot W\_i \tag{1}$$


    where IN<sup>i</sup> and W<sup>i</sup> are the i-th elements of the input and
    weight vectors, respectively, and n is the vector length. Specifically, the CIM
    architecture executes dot product calculations as follows:


    - (i) Mapping of the Weight vector (W): The weight vector W is stored in the memory
    array, with each memory cell representing one bit of a weight value. For CIM macro
    with n rows, each bit cell stores the weight value of W<sup>i</sup> .

    - (ii) Mapping of the Input Vector (IN): Each IN<sup>i</sup> is simultaneously
    applied to the corresponding bitline of the SRAM array. This means that n inputs
    are processed in parallel, realizing the parallel processing from i = 1 to n in
    the formula.

    - (iii) Multiplication Operation: The weight W<sup>i</sup> stored in each SRAM
    cell is multiplied by the corresponding input value IN<sup>i</sup> . In CIM architecture,
    this multiplication operation occurs simultaneously in each memory cell, using
    methods such as analog domain current multiplication or digital domain AND operations.

    - (iv) Addition (Accumulation) Operation: The multiplication results IN<sup>i</sup>
    · W<sup>i</sup> are summed by the accumulator circuit. In ACIMs, this addition
    is performed along the wordline and is realized as a sum of currents or voltages.
    In DCIMs, this is realized through n-input digital adder trees.

    - (v) Final Dot Product Output: The result of the addition operation becomes the
    final output y. In ACIMs, the analog summed value is converted to a digital value
    through an analog-to-digital converter (ADC).


    This entire process occurs in parallel within the memory array, minimizing data
    movement compared to conventional architectures and enabling high-speed, low-power
    computations.


    While the above explained the CIM operation for binary dot products, we can extend
    this to multi-bit representations [\[35\]](#page-21-4). For multi-bit representation,
    we express IN<sup>i</sup> and W<sup>i</sup> as:


    $$IN\_i = \sum\_{j=0}^{m-1} IN\_{i,j} \cdot 2^j, \quad W\_i = \sum\_{k=0}^{p-1}
    W\_{i,k} \cdot 2^k \tag{2}$$


    where m and p are the number of bits for input and weight, respectively. The multi-bit
    dot product operation then becomes:


    <span id="page-3-0"></span>

    $$y = \sum\_{j=0}^{m-1} \sum\_{k=0}^{p-1} \left( \sum\_{i=1}^{n} IN\_{i,j} \cdot
    W\_{i,k} \right) \cdot 2^{j+k} \tag{3}$$


    Importantly, this equation shows that the core CIM circuit for computing dot products
    ( P<sup>n</sup> <sup>i</sup>=1 IN<sup>i</sup> · Wi) remains unchanged. The multi-bit
    dot product result y is obtained by appropriately weighting (simply bit-shifting)
    and accumulating the results from the CIM circuit in the digital domain. This
    approach maintains the efficiency of the CIM architecture while extending its
    capability to handle multi-bit operations.


    To summarize, the characteristic of CIM architecture is particularly effective
    for operations requiring large-scale matrix-vector multiplications, such as neural
    network forward propagation, self-attention mechanisms and convolution operations.
    For instance, calculations in fully connected layers or filter applications in
    convolutional layers can be efficiently executed using CIM.


    #### 2.1. Digital and Analog CIMs


    ![](_page_3_Figure_8.jpeg)


    Figure 2. Conceptional block diagram of a Compute-in-Memory Macro.


    The implementation methods of CIM are primarily classified based on the design
    of the accumulator. This classification leads to three main categories: Digital
    CIM (DCIM), Analog CIM (ACIM), and Hybrid CIM, each with its own set of characteristics,
    advantages, and challenges.


    Digital CIM (DCIM) [\[27](#page-21-2)[–30\]](#page-21-5) employs digital adder
    trees to perform accumulation. This approach benefits from high precision, ease
    of design, and the ability to leverage existing digital circuit design techniques.
    However, DCIM tends to have lower area and power efficiency compared to its analog
    counterpart.


    Analog CIM (ACIM) [\[8,](#page-20-1)[10–](#page-20-2)[18,](#page-20-5)[20,](#page-20-6)[21\]](#page-20-3),
    on the other hand, performs accumulation in the analog domain. Various methods
    have been proposed for ACIM, including operations in current [\[21\]](#page-20-3),
    charge [\[10–](#page-20-2)[18\]](#page-20-5), or time domains [\[20\]](#page-20-6).
    ACIM offers high power and area efficiency, as well as the potential for high-speed
    computation. Nevertheless, it faces challenges in terms of precision limitations,
    susceptibility to process variations, and scalability issues.


    A third category, Hybrid CIM [\[22–](#page-20-4)[24\]](#page-21-6), aims to combine
    the strengths of both digital and analog designs. One example of this approach
    is processing the Most Significant Bits (MSB) digitally while handling the Least
    Significant Bits (LSB) in the analog domain. This hybrid method often allows for
    a better balance between precision and efficiency.


    The selection among these implementation methods is determined by the required
    precision, power efficiency, area efficiency, and the characteristics of the target
    application. In the following chapters, we will delve deeper into DCIM and ACIM,
    exploring their features, advantages, and challenges in greater detail.


    ### 3. Digital Compute-In-Memory (DCIM)


    Digital Compute-In-Memory (DCIM) is a variant of CIM architecture that implements
    the accumulator using digital circuits, specifically a Digital Adder Tree (DAT).
    DCIM aims to combine the advantages of digital circuit precision and design ease
    with the benefits of in-memory computation.


    One of the primary strengths of DCIM is its high computational precision. The
    inherent nature of digital operations allows for maintaining error-free, high-accuracy
    calculations.


    Another significant of DCIM is its excellent scalability. As semiconductor manufacturing
    processes continue to advance, DCIM architectures can leverage these improvements
    to enhance both performance and energy efficiency. Recent reports have demonstrated
    successful DCIM implementations at process nodes as small as 3nm, indicating that
    DCIM can continue to progress in tandem with semiconductor technology advancements
    [\[31\]](#page-21-7).


    DCIM, despite its advantages, faces some design challenges. Integrating digital
    circuits within memory cells requires adherence to memory design rules rather
    than logic design rules. This constraint limits the use of automatic placement
    and routing tools post logic synthesis, necessitating manual circuit design by
    skilled layout designers. Consequently, this approach increases design complexity
    and potentially reduces productivity compared to conventional digital circuit
    design methodologies.


    Power and area efficiency remain significant challenges for DCIM. Digital adders,
    essential components in DCIM, consume substantial power and occupy considerable
    area.


    ![](_page_5_Figure_2.jpeg)


    <span id="page-5-0"></span>Figure 3. Floating point (BF16) enabled DCIM macro
    (Figure adapted from [\[32\]](#page-21-8) ©IEEE)


    Recent research has demonstrated the implementation of floating-point (BF16) operations
    in DCIM [\[32\]](#page-21-8). This high precision makes DCIM particularly suitable
    for tasks demanding exceptional accuracy. To enable floating-point operations,
    as illustrated in Fig. [3,](#page-5-0) exponent and mantissa computing units are
    integrated within the CIM cell. While floating-point support significantly expands
    the range of potential applications, it''s important to note that the increased
    complexity of computational mechanisms within the bit cell may lead to concerns
    about reduced area efficiency.


    ![](_page_6_Figure_1.jpeg)


    <span id="page-6-0"></span>Figure 4. INT12xINT12 DCIM macro (Figure adapted from
    [\[31\]](#page-21-7) ©IEEE)


    The DCIM macro presented at ISSCC''24 [\[31\]](#page-21-7) (Fig. [4\)](#page-6-0)
    supports high-precision INT12xINT12 MAC operations. Notably, it separates data
    storage from computational circuitry, resembling a register and ALU unit architecture
    rather than traditional in-cell arithmetic units. This approach allows independent
    design of memory and arithmetic units, enhancing flexibility.


    ![](_page_6_Figure_4.jpeg)


    <span id="page-6-1"></span>Figure 5. DCIM macro with approximate DAT circuits.
    (Figure adapted from [\[36\]](#page-21-9) ©IEEE)


    To improve power and area efficiency in DCIMs, researchers have developed approximate
    digital CIM architectures. One approach approximates the DAT circuit [\[36\]](#page-21-9),
    exploiting the noise tolerance of many DNN applications to trade accuracy for
    efficiency. Fig[.5](#page-6-1) illustrates three DCIM designs with varying DAT
    approximation levels, where (a) is the typical error-free DAT implementation using
    a full array of full adders (FAs). [\[36\]](#page-21-9) shows that by replacing
    the FAs for OR and AND gates for error-induced summation, DAT can trade accuracy
    for reduced transistor count. By accepting a RMSE of 6.7%, [\[36\]](#page-21-9)
    achieves about 50% less transistor count, achieving both power and area efficiency
    improvements.


    ![](_page_7_Figure_1.jpeg)


    <span id="page-7-0"></span>Figure 6. Approximating dot products with probabilistic
    computation. Figure adapted from [\[28\]](#page-21-10) CC-BY-NC.


    Another innovative design [\[28\]](#page-21-10) employs probabilistic computing
    techniques to approximate computations in DCIMs, as illustrated in Fig. [6.](#page-7-0)
    This approach models dot product (DP) calculations as a series of Bernoulli trials,
    counting ''1''s in both input and weight vectors. Each element-wise multiplication
    is treated as a trial, with the probability of success determined by the proportion
    of ''1''s in the vectors. The dot product is then approximated by the number of
    successful trials in n attempts, where n is the vector length. This method exploits
    the law of large numbers, suggesting that for sufficiently large vector sizes,
    the approximation converges to the true value with high probability. The figure
    demonstrates this convergence, showing how the actual MAC result approaches the
    expected MAC as the DP length increases. The bottom graphs illustrate the error
    distribution and how RMSE decreases with increasing DP length, outperforming existing
    digital and analog approaches.


    #### 4. Analog Compute-In-Memory (ACIM)


    Analog Compute-In-Memory (ACIM) is a CIM technique that leverages analog domain
    computations within the memory array to achieve high power and area efficiency
    [\[8,](#page-20-1)[10–](#page-20-2) [21\]](#page-20-3). The fundamental architecture
    of ACIM eliminates the need for a DAT, which is typically required in DCIMs, by
    summing the multiplication results of IN and W in the analog domain. This approach
    follows a streamlined processing flow: (1) conversion of digital inputs to analog
    signals, (2) multiplication in the analog domain (IN × W), (3) summation in the
    analog domain (Σ(IN × W)), and (4) conversion of analog results to digital via
    Analog-to-Digital Conversion (ADC).


    #### 4.1. When should we use analog computing?


    ![](_page_8_Figure_4.jpeg)


    <span id="page-8-0"></span>Figure 7. Digital and analog computing energy estimates
    (Figure adapted from [\[37\]](#page-22-0) ©IEEE). The "Sweet spot" annotation
    is made by the author.


    The efficacy of analog computation is intricately tied to the required precision
    of the task at hand. Drawing from Murmann''s research [\[37\]](#page-22-0) (Fig.
    [7\)](#page-8-0), we can delineate the characteristics of analog and digital computing
    across different precision ranges:


    - Low Precision (1-2 bits): Digital circuits demonstrate superior efficiency due
    to their inherent simplicity and speed advantages in this range.

    - Medium Precision (3-8 bits): Analog circuits excel, offering optimal power consumption
    and area efficiency without being significantly constrained by noise limitations.
    This range can be seen as the analog computing''s "sweet spot".

    - High Precision (10+ bits): The efficiency of analog computation diminishes due
    to thermal noise constraints, making digital implementations more favorable.


    Recent advancements in low-precision DNNs have demonstrated the viability of operating
    at bit-widths that fall within the optimal range for analog computing. For instance,
    cutting-edge research has shown successful quantization of CNNs to INT2 precision
    [\[38\]](#page-22-1), while Vision Transformers (ViT), based on the Transformer
    architecture, have been effectively quantized to INT4 [\[39\]](#page-22-2). These
    precision levels align remarkably well with the range where analog circuits exhibit
    superior efficiency, as illustrated in Figure [7.](#page-8-0)


    |         | Pros                       | Cons                       |

    |---------|----------------------------|----------------------------|

    | Current | Excellent area, power      | Current susceptible to     |

    |         | efficiency                 | mismatch+PVT.              |

    | Time    | Excellent area, power      | Delay<br>susceptible<br>to |

    |         | efficiency                 | mismatch+PVT.              |

    | Charge  | Good area,<br>power ef     | Require<br>area/energy     |

    |         | ficiency.<br>Potential for | hungry<br>high-precision   |

    |         | high-precision.            | ADC.                       |


    4.2. Review of existing analog CIM implementations


    <span id="page-9-0"></span>Table 1. Comparison of Different ACIM Implementations


    As summarized in Table [1,](#page-9-0) ACIM implementations can be broadly categorized
    into three main types: current-based, time-based, and charge-based. Each of these
    approaches offers unique advantages and challenges.


    Current-based ACIM [\[21\]](#page-20-3) converts multiplication results into currents
    and performs accumulation via current integration. This approach enables highly
    area and power-efficient implementations, allowing for compact 7T cell designs.
    The simplicity of current conversion circuits, achievable with a single transistor,
    underlies this efficiency. However, the method faces a significant challenge:
    the inherent non-linearity of transistor currents. When using basic transconductance
    (gm) cells, achieving the linearity required for accurate computations across
    PVTs becomes difficult due to the non-linear voltage-current relationship in MOSFETs.


    Time-based ACIM [\[20\]](#page-20-6) converts multiplication results into delays,
    performing computations in the time domain. This approach utilizes delay-controlled
    cells for computation, which are essentially simple digital circuits, making them
    extremely compact and well-suited for implementation with deep-scaled transistors.
    Consequently, the design achieves high area and power efficiency. However, the
    primary challenge lies in the inherent non-linearity and PVT sensitivity of transistor
    delays.


    Charge-based ACIM, the focus of our paper, converts multiplication results into
    charges and performs integration through charge redistribution. This approach
    leverages charge redistribution operations, extensively studied in successive
    approximation register (SAR) ADCs. The process can be implemented almost entirely
    with digital circuits, making it highly compatible with deep-scaled transistors
    and enabling highspeed, low-power execution. A key advantage is the use of metal-to-metal
    capacitors for charge storage, resulting in computing elements that are inherently
    robust against PVT variations. Recent research in SAR ADCs has demonstrated 12-bit
    linearity even in 3nm processes [\[40\]](#page-22-3), showing the potential for
    high-precision operations. This technique has shown promising results in complex
    applications like ImageNet-class image recognition as well [\[16\]](#page-20-7).


    #### 4.3. Charge-based ACIM operation


    ![](_page_10_Figure_2.jpeg)


    <span id="page-10-0"></span>Figure 8. Structure and operation of the Multiplying
    Bit Cell (M-BC) in charge-based ACIM. Bit-cell figure adapted from [\[10\]](#page-20-2)
    ©IEEE


    Charge-based ACIMs operate on a principle of charge manipulation and redistribution,
    as depicted in Figure [8.](#page-10-0) The process begins with recharging all
    capacitors to a reference voltage. Weights are set by adjusting charge quantities
    in corresponding capacitors based on SRAM cell states. Input data modulates bitline
    voltages, initiating charge redistribution between bitlines and capacitors—effectively
    performing multiplication. Row-wise charge integration yields matrix multiplication
    results, which are then digitized via ADC conversion.


    The Multiplying Bit Cell (M-BC) structure [\[10\]](#page-20-2) is a key innovation
    in this domain. It combines weight storage and multiplication within a compact
    layout, featuring a standard 6T SRAM cell enhanced with two pMOS transistors and
    a metal-oxide-metal (MOM) capacitor. This design efficiently executes XNOR operations
    between stored weights and input activations, accumulating results as charge on
    MOM capacitors.


    MOM capacitors offer significant advantages: they''re robust against PVT variations,
    maintaining consistent capacitance across diverse operating conditions. They also
    achieve high linearity up to 12-bit levels, contributing to the superior computational
    accuracy of charge-based ACIMs compared to current or time-domain approaches.
    Furthermore, MOM capacitors can be constructed entirely in metal layers above
    the 8T transistor structure, effectively eliminating area overhead and enhancing
    the M-BC''s density and scalability.


    Compute SNR (CSNR) is a critical metric for evaluating the performance of ACIM
    systems, enabling fair comparisons with digital circuits and among different ACIM
    implementations. While traditional metrics such as INL and average ADC code noise
    have been used to assess ACIM performance, they are challenging to incorporate
    into software simulations and are not well-suited for comparing ACIMs. Here, we
    adopt the CSNR metric, as advocated in [\[8,](#page-20-1) [13\]](#page-20-8),
    which provides a direct evaluation of the noise characteristics of analog circuits
    and can be easily integrated into software simulations.


    Although [\[8\]](#page-20-1) initially introduced the theoretical foundation of
    CSNR, it did not provide a specific methodology for calculating CSNR from practical
    circuit measurement results. Here, we extend the analysis of [\[8\]](#page-20-1)
    by proposing a practical CSNR measurement method, enabling the effective quantification
    of analog noise and fair comparisons between high-precision and low-precision
    ACIMs as described in [\[13\]](#page-20-8).


    CSNR is conceptually similar to the SNR used in analog circuits and is defined
    as follows:


    $$\text{CSNR} = 20 \log\_{10} \left( \frac{\text{Signal}}{\text{Noise}} \right)
    \tag{4}$$


    The noise model of ACIMs can be expressed using the ideal digital output yD, the
    ACIM output yA, the ADC quantization noise Q, and the analog noise γ:


    <span id="page-11-0"></span>

    $$y\_{\mathcal{A}} = y\_{\mathcal{D}} + Q + \gamma \tag{5}$$


    The analog noise γ encompasses all non-idealities introduced by analog computation,
    with the primary sources being the nonlinearity of the MAC unit, thermal noise
    of transistors, device mismatch, and power supply noise. Intuitively, CSNR quantifies
    the ratio between the desired signal power and the power of the noise introduced
    by the analog computation process. A higher CSNR indicates that the analog computation
    is more accurate and closer to the ideal digital output, and vice versa.


    To calculate CSNR from the computation results of a bit-serial ACIM, we consider
    an example where an 8-bit MAC output is obtained from 4-bit inputs and weights.
    In this scenario, the bit-serial ACIM performs 8 MAC operations, each involving
    a 1-bit×1 bit multiplication and accumulation. For each column of the bit-serial
    ACIM, a single 1b×1b MAC output AMACBS is mapped. AMACBS is computed using the
    input bit IN[i, N] (the N-th bit of the input) and the weight bit Weight[i, M]
    (the M-th bit of the weight):


    $$\text{AMAC}\_{\text{BS}} = \sum \text{IN}[i, N] \times \text{Weight}[i, M] \tag{6}$$


    The 8 AMACBS values are then appropriately bit-shifted and accumulated to obtain
    the 8-bit MAC result AMAC, which is intended to be equivalent to the digital 4b×4b
    MAC output (DMAC):


    $$\text{AMAC} = \sum\_{k=0}^{7} \text{AMAC}\_{\text{BS}}[k] \times 2^k \tag{7}$$


    where AMACBS[k] represents the k-th bit-serial computation result.


    In general, bit-serial ACIMs are arranged in an array structure, allowing parallel
    processing of multiple columns. For example, a 4×4 array can simultaneously process
    the dot product of 4 input vectors and 4 weight vectors. In this case, the AMAC
    calculation described above is performed independently for each column, yielding
    4 8 bit MAC results. As AMAC contains the error shown in Eq. [5,](#page-11-0)
    CSNR can be calculated by treating the DMAC result (DMAC) as the ideal signal
    and the absolute difference between DMAC and AMAC as noise:


    $$\text{CSNR} = 20 \log\_{10} \left( \frac{\text{DMAC}}{|\text{DMAC} - \text{AMAC}|}
    \right) \tag{8}$$


    However, when performing multiple trials, it is common to calculate CSNR using
    the average of the signal and noise powers (squared values):


    $$\text{CSNR} = 10 \log\_{10} \left( \frac{\text{mean}(\text{DMAC}^2)}{\text{mean}(|\text{DMAC}
    - \text{AMAC}|^2)} \right) \tag{9}$$


    Since a single computation result may not accurately capture the impact of Gaussiandistributed
    noise, it is recommended to use the average CSNR value obtained from 1,000 to
    10,000 computation results. In this paper, we calculate CSNR using 10,000 trials,
    balancing the trade-off between convergence and computation time. [‡](#page-12-0)


    ![](_page_12_Figure_6.jpeg)


    <span id="page-12-1"></span>Figure 9. CIFAR10 accuracy changes when varying CSNR.


    Using this metric, [\[13\]](#page-20-8) conducted tests in PyTorch to simulate
    the addition of noise to DNN layers during inference, examining how CIFAR10 image
    recognition accuracy changes with CSNR variations (Fig[.9\)](#page-12-1). The
    results showed that CNN (ResNet18) maintained nearly constant CIFAR10 recognition
    accuracy even as CSNR fluctuated between 15 and 30 dB. In contrast, Transformer
    (Vision Transformer) exhibited significant accuracy degradation with only slight
    CSNR deterioration. These findings reveal that Transformers require higher precision
    computations compared to CNNs, while CNNs are well-suited for operation at low
    CSNR. Leveraging this characteristic, [\[13\]](#page-20-8) optimizes ACIM at the
    algorithm level by operating CNNs at low CSNR to improve power efficiency, while
    running Transformers at high CSNR to enhance computational accuracy.


    <span id="page-12-0"></span>‡ Open sourced in: <https://github.com/Keio-CSG/AnalogCIM-CSNR-Sim>


    ![](_page_13_Figure_1.jpeg)


    Figure 10. Bit-parallel operation in charge-based CIM. Figure adapted from [\[17\]](#page-20-9)
    ©IEEE


    Bit-parallel analog CIMs: While Section 2 assumed 1-bit input signals for CIM
    operations, charge-based CIM leverages the linear relationship between charge
    and voltage (Q = CV ). This property allows for multi-bit analog voltage inputs,
    reducing the number of dot product operations related to IN by a factor of n in
    equation [3,](#page-3-0) thereby improving throughput and power efficiency. Ref.
    [\[17\]](#page-20-9) proposes this "bitparallel" operation that generates IN signals
    from 16 reference voltages, achieving a 5-bit DAC through Dynamic-range Doubling
    technology. This approach yields a 16-fold improvement in power efficiency compared
    to 1-bit bit-serial CIM operations. However, n-bit bit-parallel operation requires
    increased ADC resolution by n bits to maintain the same readout precision as bit-serial
    operations, presenting a trade-off between efficiency and ADC complexity.


    Multi-bit analog CIMs: Conventional charge-based ACIMs place an ADC for each column,
    converting the accumulated analog charge per column and then applying weights
    in the digital domain. In contrast, multi-bit ACIMs (Fig[.11\)](#page-14-0) use
    weighted capacitors to apply weights in the analog domain before accumulation,
    and then convert the result using a single ADC. By accumulating n columns together
    in this manner, the number of ADCs required for computation can be reduced to
    1/n, promising significant efficiency improvements. However, similar to the bit-parallel
    method, achieving high accuracy requires improving ADC precision. Additionally,
    the precision of the analog domain weighting must be sufficiently high, which
    may pose a design challenge.


    ![](_page_14_Figure_0.jpeg)


    <span id="page-14-0"></span>Figure 11. Multi-bit charge-based CIM. Figure adapted
    from [\[41\]](#page-22-4) ©IEEE


    ![](_page_14_Figure_2.jpeg)


    Figure 12. ACIM with sparsity sensing mechanism that dynamically adjusts ADC resolution
    by counting input ''1''s. Figure adapted from [\[11\]](#page-20-10). ©IEEE


    Sparcity analog CIMs: In ACIM systems, ADC power consumption becomes a significant
    concern, necessitating effective power reduction strategies. Ref. [\[11\]](#page-20-10)
    achieves substantial ADC power reduction by leveraging input sparsity. The approach
    focuses on counting the number of ''1''s in the IN vector to estimate the maximum
    possible value of the dot product. For instance, in a 256-element dot product
    operation, if only 60 ''1''s are present in the input, the maximum dot product
    output is constrained to 60. This insight allows the system to deduce that the
    two most significant bits of an 8-bit SAR ADC result will be ''0'', effectively
    enabling the ADC to operate as a 6 bit converter. This dynamic adjustment effectively
    reducing ADC power consumption without compromising accuracy.


    Transformer-capable analog CIMs: Algorithms like Transformers require higher computational
    precision compared to CNNs. To achieve such precision with ACIM, the resolution
    of the ADC must be increased, which in SAR ADCs requires a


    ![](_page_15_Figure_0.jpeg)


    <span id="page-15-0"></span>Figure 13. Capacitor-reconfigured CIM structure integrating
    computation and the capacitor array of an SAR ADC. Figure adapted from [\[13\]](#page-20-8)
    CC-BY-NC.


    larger capacitor array, leading to significant area overhead. In Fig[.13](#page-15-0)
    [\[13\]](#page-20-8), a capacitorreconfigured CIM structure is proposed, where
    the capacitor array used for SAR ADCs is integrated with computation, reusing
    the capacitors in the CIM bit-cell for AD conversion. This approach eliminates
    the area overhead associated with high-resolution ADCs, achieving 10-bit ADC conversion
    accuracy and providing a foundation for Transformer operations. However, improving
    precision beyond 10 bits is challenging due to thermal noise in analog circuits
    (Fig[.7\)](#page-8-0), necessitating alternative approaches for further accuracy
    enhancement.


    #### 5. Hybrid CIMs


    So far, we''ve considered computation in either the analog or digital domain within
    the accumulator. A method combining both domains is referred to as Hybrid CIM.
    In [\[23\]](#page-21-11), the MSB portion of the dot product is computed using
    DCIM, while the LSB portion is handled by ACIM (Fig[.14\)](#page-16-0). Since
    the MSB side is error-free, this approach maintains computation accuracy compared
    to ACIM while achieving lower power consumption than DCIM. Anpther recent hybrid
    CIM design [\[42\]](#page-22-5) splits the DCIM and ACIM boundary in a "lightning"
    style, further balancing compute precision and energy efficiency (Fig[.15\)](#page-16-1).


    From an ACIM perspective, hybridization offers enhanced MSB accuracy "for free."
    This allows for higher computational precision while exceeding the thermal noise
    limits shown in Fig[.7,](#page-8-0) unlocking the potential for further balance
    in power reduction and compute precision. As DNN algorithms become more complex
    and demand higher


    ![](_page_16_Figure_0.jpeg)


    <span id="page-16-0"></span>Figure 14. Hybrid CIM incorporating both ACIM and
    DCIM. Figure adapted from [\[23\]](#page-21-11) ©IEEE.


    ![](_page_16_Figure_2.jpeg)


    <span id="page-16-1"></span>Figure 15. "Lightning" style Hybrid CIM. Figure adapted
    from [\[42\]](#page-22-5) ©IEEE.


    precision, hybrid computation will become a crucial approach.


    To further enhance the power efficiency of Hybrid CIM, an innovative approach
    has been proposed that dynamically adjusts the balance between DCIM and ACIM based
    on the data''s importance (saliency) [\[24\]](#page-21-6). As shown in Fig[.16](#page-17-0)
    and Fig[.17,](#page-17-1) the region of the cat is highly salient, where accuracy
    is critical. In such areas, DCIM is primarily used for its precision. Conversely,
    in less salient regions (e.g., the background), where computational precision
    is less crucial, ACIM is employed to reduce


    ![](_page_17_Figure_0.jpeg)


    <span id="page-17-0"></span>Figure 16. Data importance (Saliency). In image recognition,
    the cat is considered salient, significantly impacting algorithm outcomes, whereas
    the background is nonsalient. Figure adapted from [\[43\]](#page-22-6) CC-BY-NC.


    ![](_page_17_Figure_2.jpeg)


    <span id="page-17-1"></span>Figure 17. Saliency-based Hybrid CIM, dynamically
    adjusting the ratio of DCIM and ACIM based on saliency. Figure adapted from [\[43\]](#page-22-6)
    CC-BY-NC.


    power consumption. Unlike conventional CIM sparsity that focuses on input data,
    the saliency-based computation determines algorithmic importance by evaluating
    MAC output values (in both FC and CNN layers) - higher output values indicate
    higher saliency and thus greater algorithmic significance. To evaluate the saliency
    without any prior knowledge of the input or weights, our system uses the On-the-fly
    Saliency Evaluator (OSE). The key idea of OSE is to compute the s highest-order
    1-bit MACs (s=2 in our implementation) using precise DCIM operations, and use
    these results to estimate the saliency of the entire MAC operation. These values
    are summed across channels to generate a saliency score. The OSE then uses this
    calculated saliency score to determine the Hybrid CIM MAC''s digital-to-analog
    boundary (BD/A) by comparing it against pre-trained thresholds. Based on this
    dynamic evaluation, our system can dynamically optimize the CIM''s precision and
    power based on the saliency: high-saliency data are processed using more precise
    digital computation (DCIM), while low-saliency inputs use more analog computation
    (ACIM).


    #### 6. System level implementations


    While our previous discussion focused primarily on macro-level implementations,
    this section addresses system-level implementations at the processor level. It''s
    important to note that system-level power efficiency varies significantly depending
    on the algorithm size (which affects the number of external memory accesses),
    making direct comparisons challenging. Therefore, our analysis focuses more on
    the features achieved by these chips rather than purely comparing their efficiencies.


    The research presented in [\[44\]](#page-22-7) proposes a highly integrated CIM
    processor that achieves system-level efficiency of 30 TOPS/W. This implementation
    goes beyond basic CIM macros by incorporating pipelining and other processor-level
    optimizations. Additionally, it supports diverse computational precisions, including
    BF16 and INT8 formats.


    Mediatek''s reported work [\[45\]](#page-22-8) presents a CIM processor designed
    for mobile accelerator applications, bringing this technology closer to commercial
    products. Their implementation is particularly interesting as it builds upon the
    CIM macro architecture from [\[31\]](#page-21-7), refining it for practical processor
    applications. The processor is optimized for super-resolution applications and
    relatively small network sizes, allowing for an efficient system-level implementation
    without weight updates. They report approximately 5x improvement in system-level
    efficiency compared to processors using equivalent digital MAC architectures.


    In these system-level implementations, DCIM has generally taken the lead over
    ACIM architectures. This advantage can be attributed to DCIM''s deterministic
    operation, which makes it more manageable from a system perspective and simplifies
    compiler design. However, research into large-scale processor-level ACIM integration
    is also progressing, as demonstrated in [\[16\]](#page-20-7). While these ACIM
    implementations achieve good efficiency at the macro level, they require additional
    algorithmic considerations to address accuracy degradation, necessitating software-level
    compensation for ACIM characteristics. This challenge, however, presents an interesting
    research direction, as it could foster valuable collaboration between algorithm
    developers and analog circuit designers, potentially leading to significant advancements
    in the field.


    #### 7. Conclusion and Future remarks


    In this paper, we have reviewed the key advancements in SRAM-based CIM circuits,
    focusing on both DCIM and ACIM implementations. While DCIM offers high computational
    precision and process scaling, ACIM presents benefits in power and area efficiency,
    particularly for applications requiring medium precision.


    Current primary limitations of CIM techniques can be categorized into accuracy
    concerns and weight-stationary characteristics. Regarding accuracy, DCIM demonstrates
    high performance, while ACIM can achieve improved accuracy through hybridization,
    making it applicable to CNN and ViT inference tasks at ImageNet scale. Also, advanced
    image processing tasks like semantic segmentation, which are fundamentally built
    on CNN layers, can theoretically be implemented using either CIM approach.


    While LLMs share algorithmic similarities with ViT due to their transformerbased
    architecture, they face a fundamental challenge in implementing CIM''s core principle.
    The basic concept of CIM aims to minimize data movement by keeping weights stationary
    within memory. However, as models grow larger, maintaining stationary weights
    within on-chip SRAM becomes problematic. Since the gigantic weight matrices must
    be read from off-chip memory, this often requires weight data to be rewritten
    between layers. When this occurs, CIM''s distinctive advantage over conventional
    PEs diminishes, as frequent weight updates effectively nullify the benefits of
    in-memory computation. This may represent a critical bottleneck in scaling CIM
    applications to larger models.


    #### Acknowledgements


    This research was supported in part by the JST CREST JPMJCR21D2 and JSPS Kakenhi
    23H00467.


    #### References


    - <span id="page-19-0"></span>[1] A. Krizhevsky, I. Sutskever, and G. E. Hinton,
    "Imagenet classification with deep convolutional neural networks," Advances in
    neural information processing systems, vol. 25, 2012.

    - <span id="page-19-1"></span>[2] K. He, X. Zhang, S. Ren, and J. Sun, "Deep residual
    learning for image recognition," in CVPR, 2016, pp. 770–778.

    - <span id="page-19-2"></span>[3] A. Vaswani, "Attention is all you need," Advances
    in Neural Information Processing Systems, 2017.

    - <span id="page-19-3"></span>[4] K. He, X. Zhang, S. Ren, and J. Sun, "Delving
    deep into rectifiers: Surpassing human-level performance on imagenet classification,"
    in Proceedings of the IEEE international conference on computer vision, 2015,
    pp. 1026–1034.

    - <span id="page-19-4"></span>[5] T. B. Brown, "Language models are few-shot learners,"
    arXiv preprint arXiv:2005.14165, 2020.

    - <span id="page-19-5"></span>[6] M. Horowitz, "1.1 computing''s energy problem
    (and what we can do about it)," in 2014 IEEE international solid-state circuits
    conference digest of technical papers (ISSCC). IEEE, 2014, pp. 10–14.

    - <span id="page-20-0"></span>[7] A. Sebastian, M. Le Gallo, R. Khaddam-Aljameh,
    and E. Eleftheriou, "Memory devices and applications for in-memory computing,"
    Nature nanotechnology, vol. 15, no. 7, pp. 529–544, 2020.

    - <span id="page-20-1"></span>[8] S. K. Gonugondla, C. Sakr, H. Dbouk, and N.
    R. Shanbhag, "Fundamental limits on the precision of in-memory architectures,"
    in Proceedings of the 39th International Conference on Computer-Aided Design,
    2020, pp. 1–9.

    - [9] N. Verma, H. Jia, H. Valavi, Y. Tang, M. Ozatay, L.-Y. Chen, B. Zhang, and
    P. Deaville, "Inmemory computing: Advances and prospects," IEEE Solid-State Circuits
    Magazine, vol. 11, no. 3, pp. 43–55, 2019.

    - <span id="page-20-2"></span>[10] H. Valavi, P. J. Ramadge, E. Nestler, and N.
    Verma, "A 64-tile 2.4-Mb in-memory-computing CNN accelerator employing charge-domain
    compute," IEEE Journal of Solid-State Circuits, vol. 54, no. 6, pp. 1789–1799,
    2019.

    - <span id="page-20-10"></span>[11] C.-Y. Yao, T.-Y. Wu, H.-C. Liang, Y.-K. Chen,
    and T.-T. Liu, "A Fully Bit-Flexible Computation in Memory Macro Using Multi-Functional
    Computing Bit Cell and Embedded Input Sparsity Sensing," IEEE Journal of Solid-State
    Circuits, 2023.

    - [12] K. Yoshioka, "A 818-4094TOPS/W Capacitor-Reconfigured CIM Macro for Unified
    Acceleration of CNNs and Transformers," in International Solid-State Circuits
    Conference (ISSCC), vol. 67. IEEE, 2024, pp. 574–576.

    - <span id="page-20-8"></span>[13] ——, "A 818–4094 TOPS/W Capacitor-Reconfigured
    Analog CIM for Unified Acceleration of CNNs and Transformers," IEEE Journal of
    Solid-State Circuits, 2024.

    - [14] S. Ando, S. Miyagi, Y. Chen, W. Zhang, and K. Yoshioka, "A Saliency-Aware
    Analog Computing-In-Memory Macro with SAR-Embedded Saliency Detection Technique,"
    in 2024 SSDM. JJAP, 2024, pp. 1–2.

    - [15] H. Jia, H. Valavi, Y. Tang, J. Zhang, and N. Verma, "A programmable heterogeneous
    microprocessor based on bit-scalable in-memory computing," IEEE Journal of Solid-State
    Circuits, vol. 55, no. 9, pp. 2609–2621, 2020.

    - <span id="page-20-7"></span>[16] H. Jia, M. Ozatay, Y. Tang, H. Valavi, R. Pathak,
    J. Lee, and N. Verma, "A Programmable Neural-Network Inference Accelerator based
    on Scalable In-Memory Computing," in International Solid-State Circuits Conference
    (ISSCC), vol. 64. IEEE, 2021, pp. 236–238.

    - <span id="page-20-9"></span>[17] J. Lee, H. Valavi, Y. Tang, and N. Verma, "Fully
    row/column-parallel in-memory computing SRAM macro employing capacitor-based mixed-signal
    computation with 5-b inputs," in 2021 Symposium on VLSI Circuits. IEEE, 2021,
    pp. 1–2.

    - <span id="page-20-5"></span>[18] K. Lee, J. Kim, and J. Park, "Low-Cost 7T-SRAM
    Compute-In-Memory Design based on Bit-Line Charge-Sharing based Analog-To-Digital
    Conversion," in Proceedings of the 41st IEEE/ACM International Conference on Computer-Aided
    Design, 2022, pp. 1–8.

    - [19] Z. Chen, Z. Wen, W. Wan, A. R. Pakala, Y. Zou, W.-C. Wei, Z. Li, Y. Chen,
    and K. Yang, "PICO-RAM: A PVT-Insensitive Analog Compute-In-Memory SRAM Macro
    With In Situ Multi-Bit Charge Computing and 6T Thin-Cell-Compatible Layout," IEEE
    Journal of Solid-State Circuits, 2024.

    - <span id="page-20-6"></span>[20] P.-C. Wu, J.-W. Su, Y.-L. Chung, L.-Y. Hong,
    J.-S. Ren, F.-C. Chang, Y. Wu, H.-Y. Chen, C.-H. Lin, H.-M. Hsiao et al., "A 28nm
    1Mb time-domain computing-in-memory 6T-SRAM macro with a 6.6 ns latency, 1241GOPS
    and 37.01 TOPS/W for 8b-MAC operations for edge-AI devices," in International
    Solid-State Circuits Conference (ISSCC), vol. 65. IEEE, 2022, pp. 1–3.

    - <span id="page-20-3"></span>[21] Q. Dong, M. E. Sinangil, B. Erbagci, D. Sun,
    W.-S. Khwa, H.-J. Liao, Y. Wang, and J. Chang, "A 351TOPS/W and 372.4 GOPS compute-in-memory
    SRAM macro in 7nm FinFET CMOS for machine-learning applications," in International
    Solid-State Circuits Conference-(ISSCC). IEEE, 2020, pp. 242–244.

    - <span id="page-20-4"></span>[22] P. Houshmand, G. M. Sarda, V. Jain, K. Ueyoshi,
    I. A. Papistas, M. Shi, Q. Zheng, D. Bhattacharjee, A. Mallik, P. Debacker et
    al., "Diana: An end-to-end hybrid digital and analog neural network soc for the
    edge," IEEE Journal of Solid-State Circuits, vol. 58, no. 1, pp. 203–215, 2022.

    - <span id="page-21-11"></span>[23] J. Chen, T. Xiong, and X. Si, "A Charge-Digital
    Hybrid Compute-In-Memory Macro with full precision 8-bit Multiply-Accumulation
    for Edge Computing Devices," in International Symposium on Embedded Multicore/Many-core
    Systems-on-Chip (MCSoC). IEEE, 2022, pp. 153–158.

    - <span id="page-21-6"></span>[24] Y.-C. Chen, S. Ando, D. Fujiki, S. Takamaeda-Yamazaki,
    and K. Yoshioka, "OSA-HCIM: On-The-Fly Saliency-Aware Hybrid SRAM CIM with Dynamic
    Precision Configuration," IEEE ASP-DAC, 2024.

    - [25] Y. Yuan, Y. Yang, X. Wang, X. Li, C. Ma, Q. Chen, M. Tang, X. Wei, Z. Hou,
    J. Zhu et al., "34.6 A 28nm 72.12 TFLOPS/W Hybrid-Domain Outer-Product Based Floating-Point
    SRAM Computing-in-Memory Macro with Logarithm Bit-Width Residual ADC," in 2024
    IEEE International Solid-State Circuits Conference (ISSCC), vol. 67. IEEE, 2024,
    pp. 576–578.

    - <span id="page-21-3"></span>[26] A. Guo, X. Chen, F. Dong, J. Chen, Z. Yuan,
    X. Hu, Y. Zhang, J. Zhang, Y. Tang, Z. Zhang et al., "34.3 A 22nm 64kb Lightning-Like
    Hybrid Computing-in-Memory Macro with a Compressed Adder Tree and Analog-Storage
    Quantizers for Transformer and CNNs," in 2024 IEEE International Solid-State Circuits
    Conference (ISSCC), vol. 67. IEEE, 2024, pp. 570–572.

    - <span id="page-21-2"></span>[27] Y.-D. Chih, P.-H. Lee, H. Fujiwara, Y.-C. Shih,
    C.-F. Lee, R. Naous, Y.-L. Chen, C.-P. Lo, C.-H. Lu, H. Mori et al., "An 89TOPS/W
    and 16.3 TOPS/mm 2 all-digital SRAM-based full-precision compute-in memory macro
    in 22nm for machine-learning edge applications," in International Solid-State
    Circuits Conference (ISSCC), vol. 64. IEEE, 2021, pp. 252–254.

    - <span id="page-21-10"></span>[28] W. Zhang, S. Ando, Y.-C. Chen, S. Miyagi,
    S. Takamaeda-Yamazaki, and K. Yoshioka, "Pacim: A sparsity-centric hybrid compute-in-memory
    architecture via probabilistic approximation," arXiv preprint arXiv:2408.16246,
    2024.

    - [29] F. Tu, Z. Wu, Y. Wang, L. Liang, L. Liu, Y. Ding, L. Liu, S. Wei, Y. Xie,
    and S. Yin, "TranCIM: Full-digital bitline-transpose CIM-based sparse transformer
    accelerator with pipeline/parallel reconfigurable modes," IEEE Journal of Solid-State
    Circuits, 2022.

    - <span id="page-21-5"></span>[30] F. Tu, Z. Wu, Y. Wang, W. Wu, L. Liu, Y. Hu,
    S. Wei, and S. Yin, "MulTCIM: Digital Computing-in-Memory-Based Multimodal Transformer
    Accelerator With Attention-Token-Bit Hybrid Sparsity," IEEE Journal of Solid-State
    Circuits, 2023.

    - <span id="page-21-7"></span>[31] H. Fujiwara, H. Mori, W.-C. Zhao, K. Khare,
    C.-E. Lee, X. Peng, V. Joshi, C.-K. Chuang, S.-H. Hsu, T. Hashizume et al., "34.4
    A 3nm, 32.5 TOPS/W, 55.0 TOPS/mm 2 and 3.78 Mb/mm 2 Fully-Digital Compute-in-Memory
    Macro Supporting INT12× INT12 with a Parallel-MAC Architecture and Foundry 6T-SRAM
    Bit Cell," in 2024 IEEE International Solid-State Circuits Conference (ISSCC),
    vol. 67. IEEE, 2024, pp. 572–574.

    - <span id="page-21-8"></span>[32] W.-S. Khwa, P.-C. Wu, J.-J. Wu, J.-W. Su, H.-Y.
    Chen, Z.-E. Ke, T.-C. Chiu, J.-M. Hsu, C.-Y. Cheng, Y.-C. Chen et al., "34.2 A
    16nm 96Kb Integer/Floating-Point Dual-Mode-Gain-Cell-Computing-in-Memory Macro
    Achieving 73.3-163.3 TOPS/W and 33.2-91.2 TFLOPS/W for AI-Edge Devices," in 2024
    IEEE International Solid-State Circuits Conference (ISSCC), vol. 67. IEEE, 2024,
    pp. 568–570.

    - <span id="page-21-0"></span>[33] M.-E. Shih, S.-W. Hsieh, P.-Y. Tsai, M.-H.
    Lin, P.-K. Tsung, E.-J. Chang, J. Liang, S.-H. Chang, C.-L. Huang, Y.-Y. Nian
    et al., "20.1 NVE: A 3nm 23.2 TOPS/W 12b-Digital-CIM-Based Neural Engine for High-Resolution
    Visual-Quality Enhancement on Smart Devices," in 2024 IEEE International Solid-State
    Circuits Conference (ISSCC), vol. 67. IEEE, 2024, pp. 360– 362.

    - <span id="page-21-1"></span>[34] K. Yoshioka, S. Ando, S. Miyagi, Y. Chen, and
    W. Zhang, "Towards Efficient and Precise Analog Compute-in-Memory Circuits," in
    2024 SSDM. JJAP, 2024, pp. 1–2.

    - <span id="page-21-4"></span>[35] A. Shafiee, A. Nag, N. Muralimanohar, R. Balasubramonian,
    J. P. Strachan, M. Hu, R. S. Williams, and V. Srikumar, "ISAAC: A convolutional
    neural network accelerator with in-situ analog arithmetic in crossbars," ACM SIGARCH
    Computer Architecture News, vol. 44, no. 3, pp. 14–26, 2016.

    - <span id="page-21-9"></span>[36] C.-T. Lin, D. Wang, B. Zhang, G. K. Chen, P.
    C. Knag, R. K. Krishnamurthy, and M. Seok, "Dimca: An area-efficient digital in-memory
    computing macro featuring approximate arithmetic


    hardware in 28 nm," IEEE Journal of Solid-State Circuits, 2023.


    - <span id="page-22-0"></span>[37] B. Murmann, "Mixed-signal computing for deep
    neural network inference," IEEE Transactions on Very Large Scale Integration (VLSI)
    Systems, vol. 29, no. 1, pp. 3–13, 2020.

    - <span id="page-22-1"></span>[38] J. Choi, Z. Wang, S. Venkataramani, P. I.-J.
    Chuang, V. Srinivasan, and K. Gopalakrishnan, "Pact: Parameterized clipping activation
    for quantized neural networks," arXiv preprint arXiv:1805.06085, 2018.

    - <span id="page-22-2"></span>[39] Z. Li, J. Xiao, L. Yang, and Q. Gu, "Repq-vit:
    Scale reparameterization for post-training quantization of vision transformers,"
    in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023,
    pp. 17 227–17 236.

    - <span id="page-22-3"></span>[40] S. Lee, J. Park, J. Park, S. Lee, J. Lee, Y.
    Cho, M. Choi, and J. Shin, "3.9 A 1.2 V High-Voltage-Tolerant Bootstrapped Analog
    Sampler in 12-bit SAR ADC Using 3nm GAA''s 0.7 V Thin-Gate-Oxide Transistor,"
    in 2024 IEEE International Solid-State Circuits Conference (ISSCC), vol. 67. IEEE,
    2024, pp. 70–72.

    - <span id="page-22-4"></span>[41] X. Yang and N. Sun, "A 4-bit mixed-signal MAC
    macro with one-shot ADC conversion," IEEE Journal of Solid-State Circuits, vol.
    58, no. 9, pp. 2648–2658, 2023.

    - <span id="page-22-5"></span>[42] A. Guo, X. Chen, F. Dong, J. Chen, Z. Yuan,
    X. Hu, Y. Zhang, J. Zhang, Y. Tang, Z. Zhang et al., "A 22nm 64kb Lightning-Like
    Hybrid Computing-in-Memory Macro with a Compressed Adder Tree and Analog-Storage
    Quantizers for Transformer and CNNs," in International Solid-State Circuits Conference
    (ISSCC), vol. 67. IEEE, 2024, pp. 570–572.

    - <span id="page-22-6"></span>[43] Y.-C. Chen, S. Ando, D. Fujiki, S. Takamaeda-Yamazaki,
    and K. Yoshioka, "OSA-HCIM: On-The-Fly Saliency-Aware Hybrid SRAM CIM with Dynamic
    Precision Configuration," arXiv preprint arXiv:2308.15040, 2023.

    - <span id="page-22-7"></span>[44] F. Tu, Y. Wang, Z. Wu, L. Liang, Y. Ding, B.
    Kim, L. Liu, S. Wei, Y. Xie, and S. Yin, "A 28nm 29.2 TFLOPS/W BF16 and 36.5 TOPS/W
    INT8 reconfigurable digital CIM processor with unified FP/INT pipeline and bitwise
    in-memory booth multiplication for cloud deep learning acceleration," in 2022
    IEEE International Solid-State Circuits Conference (ISSCC), vol. 65. IEEE, 2022,
    pp. 1–3.

    - <span id="page-22-8"></span>[45] M.-E. Shih, S.-W. Hsieh, P.-Y. Tsai, M.-H.
    Lin, P.-K. Tsung, E.-J. Chang, J. Liang, S.-H. Chang, C.-L. Huang, Y.-Y. Nian
    et al., "20.1 NVE: A 3nm 23.2 TOPS/W 12b-Digital-CIM-Based Neural Engine for High-Resolution
    Visual-Quality Enhancement on Smart Devices," in 2024 IEEE International Solid-State
    Circuits Conference (ISSCC), vol. 67. IEEE, 2024, pp. 360– 362.'
- title: '16 Years of SPEC Power: An Analysis of x86 Energy Efficiency Trends'
  abstract: 'The SPEC Power benchmark offers valuable insights into the energy efficiency

    of server systems, allowing comparisons across various hardware and software

    configurations. Benchmark results are publicly available for hundreds of

    systems from different vendors, published since 2007. We leverage this data to

    perform an analysis of trends in x86 server systems, focusing on power

    consumption, energy efficiency, energy proportionality and idle power

    consumption. Through this analysis, we aim to provide a clearer understanding

    of how server energy efficiency has evolved and the factors influencing these

    changes.'
  url: http://arxiv.org/abs/2411.07062v1
  keywords: Computer architecture, Performance analysis, High performance computing,
    Processor energy efficiency
  document: '# 16 Years of SPEC Power: An Analysis of x86 Energy Efficiency Trends


    Hannes Tröpgen, Robert Schöne, Thomas Ilsche, Daniel Hackenberg ZIH, CIDS, TU
    Dresden, 01062 Dresden, Germany {hannes.troepgen, robert.schoene, thomas.ilsche,
    daniel.hackenberg}@tu-dresden.de


    *Abstract*—The SPEC Power benchmark offers valuable insights into the energy efficiency
    of server systems, allowing comparisons across various hardware and software configurations.
    Benchmark results are publicly available for hundreds of systems from different
    vendors, published since 2007. We leverage this data to perform an analysis of
    trends in x86 server systems, focusing on power consumption, energy efficiency,
    energy proportionality and idle power consumption. Through this analysis, we aim
    to provide a clearer understanding of how server energy efficiency has evolved
    and the factors influencing these changes.


    *Index Terms*—Computer architecture, Performance analysis, High performance computing,
    Processor energy efficiency


    #### I. INTRODUCTION


    SPECpower\_ssj 2008[1](#page-0-0) is the most prominent server energy efficiency
    benchmark. Its rigorous methodology and healthy benchmark submission review process
    have led to 16 years of continuous benchmark submissions and corresponding published
    data. These results allow hardware vendors to rank and promote their systems with
    respect to energy efficiency, measured in ssj\_ops/W. This metric gives customers
    an idea of how much computing they get for each invested Joule of energy, where
    a lower power consumption can increase it as well as a higher processing performance.
    Figure [1](#page-0-1) illustrates some strengths of the benchmark: Due to the
    simplicity and scalability of the benchmark, server systems with multiple sockets
    and/or nodes can be measured. The workload can also be executed on different operating
    systems (OS) and different hardware, even though non-x86 processors are rare,
    and up to 2017, more than 97 % of results use Windows.


    Based on the benchmark results available via the SPEC Power website, we track
    the performance and power efficiency of x86 processors over the previous 16 years.
    We analyze data for different load levels to evaluate energy proportionality,
    as well as active idle power consumption trends.


    #### II. BACKGROUND AND RELATED WORK


    SPECpower\_ssj 2008 [\[1\]](#page-4-0), [\[2\]](#page-4-1) is designed to measure
    "the performance and power consumption of servers". It consists of an integer-heavy
    transactional Java-based client/server workload with six differently weighted
    transaction types. A calibration phase is used to determine the maximum throughput
    of the system under test (SUT), which runs the server side. Partial


    <span id="page-0-0"></span><sup>1</sup>*SPECpower\_ssj 2008* is the first and
    so far only release of the *SPEC Power benchmark suite* released by the *Standard
    Performance Evaluation Corporation* (SPEC).


    loads of 10 %, 20 %, . . . , 90 % are created by scaling down the number of transactions
    proportionally. This allows the SUT to apply power-saving mechanisms [\[3,](#page-4-2)
    Section Power-Saving Techniques] and can be used to analyze energy proportionality
    [\[4\]](#page-4-3), [\[5\]](#page-4-4). The test regime also includes a 0% load
    point, which greatly helps to record, track, and optimize *active idle* power
    consumption.


    Based on the SPEC Power methodology, vendors and performance engineers generated
    hundreds of reports with 1017 being published on the SPEC website[2](#page-0-2)
    at the time of writing. This long history of vendor-submitted data distinguishes
    it from other energy efficiency benchmarks, e.g., Green500 [\[6\]](#page-4-5)
    or SPEC OMP 2012 [\[7\]](#page-4-6). The SPECpower committee[3](#page-0-3) is
    not only responsible for the SPECpower\_ssj 2008 benchmark, but also for the definitions
    and tool infrastructures for power measurements [\[2\]](#page-4-1), e.g., the
    ptdaemon interface, SERT suite, and the Chauffeur


    <span id="page-0-3"></span><span id="page-0-2"></span>![](_page_0_Figure_12.jpeg)


    <span id="page-0-1"></span>![](_page_0_Figure_13.jpeg)


    Figure 1: Share of features on all 960 successfully parsed (unfiltered) SPECpower\_ssj
    2008 results (as of June 2024)


    <span id="page-1-2"></span>![](_page_1_Figure_0.jpeg)


    Figure 2: Power consumption (per socket) at full load trend


    Worklet Development Kit, which are also used for other benchmarks and certifications
    [\[8\]](#page-4-7). Recently, members of SPEC concluded that the workload of SPECpower\_ssj
    2008 does not represent the current demands and described the next version of
    the benchmark, the still unreleased SPECpowerNext [\[9\]](#page-4-8), [\[10\]](#page-4-9).
    It will use different technologies for interfaces and measurement handling, but
    it will also use different workloads targeted at accelerators and CPUs.


    As dataset for this paper, we download all 1017 .txt result files,[4](#page-1-0)
    and extract information for hardware and software stack, as well as performance
    and power measurement results. We check the consistency, filtering runs that have
    not been accepted by SPEC (40), runs with ambiguous (3) or implausible (4) dates,
    ambiguous CPU names (3), or missing node count (1), as well as submissions where
    reported core/thread counts are inconsistent (5) or implausible (1). This leaves
    a dataset of 960 successfully parsed runs. Each run has four associated dates:
    (a) The test date, (b) the submission date, as well as (c) hardware and (d) software
    availability dates. As we discuss trends in hardware, we use the hardware availability
    date, which indicates the month at which the system became "generally available"
    [\[11\]](#page-4-10). Hence, even though the earliest results were published in
    2007, some runs are associated with earlier dates.


    Figure [1](#page-0-1) shows general trends over time. For the whole duration from
    2005 to 2023, an average of 44.2 runs were submitted per year. Between 2013 and
    2017, this drops to 15.2 runs per year. The increased number of submissions from
    2018 onward coincides with an increase in submissions using Linux (from 2.2 %
    before 2018 to 36.3 % after 2018), and an increase in submissions using AMD processors
    (from 13.0 % to 31.3 %). The latter observation can be explained by AMD''s introduction
    of its EPYC server CPUs in 2017.


    To keep the systems within the dataset comparable, we exclude uncommon configurations:
    Runs with CPUs made by neither Intel nor AMD (9), and all runs not on server or
    workstation CPUs[5](#page-1-1) (6) are filtered. Finally, we remove runs with
    more than one node or more than two sockets (269). After all filtering, 676 runs
    remain as the base for all further analysis.


    <span id="page-1-0"></span>![](_page_1_Figure_6.jpeg)


    <span id="page-1-1"></span><sup>5</sup> I.e. CPUs marketed neither as *Xeon*,
    *Opteron*, nor *EPYC*.


    <span id="page-1-3"></span>![](_page_1_Figure_8.jpeg)


    Figure 3: Overall efficiency trend


    ## III. PERFORMANCE AND EFFICIENCY TRENDS


    Across all runs, the maximum power consumed rapidly increases over the years,
    as Figure [2](#page-1-2) depicts. This trend is consistent across both AMD and
    Intel, although the spread increased in recent years. The dataset shows a broad
    insight into both low and high Thermal Design Power (TDP) processors. Overall,
    this upward trend of TDP cannot continue indefinitely, as cooling infrastructure
    has to be scaled with the power consumed, with air cooling becoming unfeasible
    at around 400W TDP [\[12\]](#page-4-11).


    The increase in power consumption per socket is most pronounced at full load (100
    %), where the mean since 2022 increased ∼2.5x compared to runs up to 2010 (119.0W
    to 303.3W). However, power consumption at all other load levels increases as well,
    e.g., by ∼1.8x at 20 % or by ∼2.2x at 70 % load using means across the same time
    spans.


    Dividing the achieved performance (ssj\_ops rate) by the mean power consumption
    results in the energy efficiency in ssj\_ops/W. Across all runs and load levels,
    this energy efficiency improved over the years, as Figure [3](#page-1-3) shows.[6](#page-1-4)
    Here, AMD emerges as the driver of the upward energy efficiency trend, in particular
    from ∼2018 onward. Even though Intel''s efficiency is also growing, out of the
    100 most efficient runs 98 use AMD processors. Similarly to the overall score,
    the runs record the achieved ssj\_ops and average power at every load level, which
    enables us to compute the efficiency per load level. We then scale this to the
    efficiency at full load, yielding the *relative efficiency* per load level. A
    relative efficiency <1 is less, >1 more efficient than full load; a relative efficiency
    of 1 at all load levels essentially corresponds to *energy proportionality*. We
    summarize these relative efficiencies from 60 % to 90 % load, binned by CPU vendor
    and year in Figure [4.](#page-2-0)


    In the early years, lower load is consistently less efficient compared to full
    load. Over time, the relative efficiency approaches 1 also for lower load levels.
    Since 2012 Intel systems have a mean relative efficiency >1 with all load levels
    ≥70 %, but from 2017 on, we observe a regression


    <span id="page-1-4"></span><sup>6</sup> P The overall efficiency (overall ssj\_ops/W)
    for SPEC Power is defined as ssj\_ops/ PP across all load levels including active
    idle [\[11\]](#page-4-10).


    <span id="page-2-0"></span>![](_page_2_Figure_0.jpeg)


    Figure 4: Distribution of relative efficiency at 60 % to 90 %, binned by year
    and CPU vendor.


    back to ∼1. This likely stems from overlapping related effects of increased power
    reduction at low lower load, but also the use of inefficient turbo states at full
    load, which were particularly pronounced around 2017. For AMD, the relative efficiency
    approaches 1 around 2021. Even though there are still visible differences between
    AMD and Intel results from 2021 onward, both have a large spread: No CPU vendor
    has a universally better relative efficiency at any given load point – their energy
    proportionality is diverse. When comparing absolute instead of relative efficiency
    however, AMD systems still clearly outperforms Intel systems (cf. Figure [3\)](#page-1-3).


    #### IV. IDLE POWER TREND ANALYSIS


    In addition to the full and partial load, the power is measured for an active
    idle interval. During active idle, the SUT is ready to perform work, but no transactions
    are being processed. Partial load configurations already allow for significant
    power reduction by leveraging techniques such as Dynamic Voltage and Frequency
    Scaling (DVFS) and core C-states [\[13\]](#page-4-12). In active idle, the power
    can be reduced further by powering down shared components, e.g., implemented by
    package C-states (see [\[3,](#page-4-2) Chapter 2]).


    The active idle power consumption is particularly relevant for High-Performance
    Computing (HPC) systems. HPC systems strive to maximize utilization and may, on
    average, have a higher utilization than other data center applications. However,
    if no batch job is executed on an HPC node, its load level is truly 0 %. In contrast,
    e.g., a web service during off-hours has a low load level that is typically still
    >0 %.


    The early runs of SPEC Power in Figure [5](#page-2-1) show the widespread introduction
    of power-saving mechanisms targeting active idle: From the earliest runs in 2006,
    where idle consumes a mean 70.1 % power compared to full load (the *idle fraction*),
    the yearly mean drops to its minimum of 15.7 % in 2017. Since then, the yearly
    mean idle fraction has increased again to 25.7 % in 2024, marking a regression
    in idle-specific power optimizations. Intel seems more affected: In Figure [5](#page-2-1)
    Intel''s runs follow an upward trend, whereas AMD has a slightly falling trend
    – although there are both low and high idle fraction systems from both CPU vendors.


    In an attempt to explain some of this recent development, we explored possible
    correlations between various run features, including the idle fraction. This exploration
    of runs since 2021 showed that the CPU vendor lineups, as well as submitted runs
    affect many features, confounding possible correlations. Most prominently, the
    core count of AMD (mean 85.8) is greater than that of Intel (mean 39.5). A more
    subtle example is the nominal frequency, where AMD and Intel share the same mean
    (∼2.3 GHz) but differ by spread (standard deviation 0.3 GHz vs 0.5 GHz). Our correlation
    analysis to explain the recent development of the idle fraction remains inconclusive.


    To better understand idle power optimizations, we introduce the *extrapolated*
    active idle power consumption: We extrapolate the power consumed at active idle
    through linear regression from the power consumed at 20 % and 10 % load. The result
    represents the power consumption during active idle if there would be no specific
    optimizations for full idle (rather than just individual idle cores). We then
    divide this extrapolated by the measured active idle power consumption and plot
    it over time in Figure [6.](#page-2-2) We refer to this quotient as *extrapolated
    idle quotient*. Higher values correspond to more effective idle-specific power
    optimization, 1 corresponds to none at all. However, higher values might also
    indicate a worse energy proportionality at


    <span id="page-2-1"></span>![](_page_2_Figure_9.jpeg)


    <span id="page-2-2"></span>![](_page_2_Figure_10.jpeg)


    Figure 6: Trend of extrapolated vs measured active idle power


    <span id="page-3-0"></span>Table I: Comparison of two dual processor Lenovo systems,
    for the benchmarks SPECpower\_ssj 2008, SPEC CPU Floating Point Rate Base, and
    SPEC CPU Integer Rate Base; *Factor* refers to the relative AMD/Intel performance
    difference


    | Benchmark      | Result | Factor | System   | CPU                       | TDP  |
    Date   | OS                                          | RAM  |

    |----------------|--------|--------|----------|---------------------------|------|--------|---------------------------------------------|------|

    | power_ssj 2008 | 15112  | 1      | SR650 V3 | Intel Xeon                | 350W
    | Feb 23 | Windows Server 2019 Datacenter              | 256  |

    | CPU 2017 FP    | 926    | 1      |          | Platinum 8490H            |      |        |
    SUSE Linux Enterprise Server 15 SP4         | 512  |

    | CPU 2017 Int   | 902    | 1      |          | 1.90 GHz                  |      |        |
    Red Hat Enterprise Linux release 9.0 (Plow) | 512  |

    | power_ssj 2008 | 31634  | 2.09   | SR645 V3 | AMD EPYC 9754<br>2.25 GHz | 360W
    | Aug 23 | Windows Server 2022 Datacenter              | 384  |

    | CPU 2017 FP    | 1420   | 1.53   |          |                           |      |        |
    SUSE Linux Enterprise Server 15 SP4         | 1536 |

    | CPU 2017 Int   | 1830   | 2.03   |          |                           |      |        |
    SUSE Linux Enterprise Server 15 SP4         | 1536 |


    low loads. Although Figure [6](#page-2-2) has an upward trend overall, there is
    a large spread, in particular in newer runs. Idle-specific optimizations are not
    universally effective in recent runs.


    The reasons behind this trend are obscured by two indistinguishable mechanisms.
    On the one hand, we suspect that processor architectures have an increasingly
    large share of power being used by shared resources, such as caches and on-chip
    communication, but also corresponding idle powersaving implementations for these
    resources. This effect could increase the extrapolated idle quotient if the latter
    effectively benefits from the energy-saving techniques. On the other hand, we
    speculate that it is becoming more difficult to effectively leverage idle power-saving
    mechanisms.


    Consider, for instance, background tasks that are replicated for each logical
    CPU. Their activity prevents the system from fully utilizing idle states for short
    times each. With increasing core counts in recent processor generations, more
    of those tasks are running, reducing the relative time spent in the most efficient
    idle states. The first effect – architecturally low measured active idle – can
    explain an increased ceiling of the extrapolated idle quotient. The second effect
    – more difficult effective idle – can explain the large variation. From the quantitative
    data alone, we cannot fully distinguish the compound effects.


    #### V. LIMITATIONS AND GENERALIZATION


    Since we only evaluate data from one benchmark, there is an argument to be made
    regarding the possibility of generalizing our observed energy efficiency trends.
    We evaluated other benchmarks that include power/efficiency: The TOP500/Green500
    [\[6\]](#page-4-5) are of limited value with respect to single-node performance
    due to their additional complexity (e.g., scalability challenges, inter-node networks).
    SPEC OMP 2012 [\[7\]](#page-4-6) and SPEC ACCEL [\[14\]](#page-4-13) perform floating-point
    heavy parallel workloads, representing typical parallel scientific workloads.
    However, their power measurement support never gained track with submitters; only
    8 and 27 submissions include them, respectively. Other benchmarks, such as SPEC
    CPU [\[15\]](#page-4-14), include much more general workloads than SPEC Power,
    with equally rich and well-reviewed publicly available datasets, but lack the
    power measurements required for efficiency analyses.


    To assess the similarity to floating-point workloads, we resorted to screening
    the SPEC CPU results for recent runs of similar class CPUs with similar TDP from
    the same vendor that are also available in our SPEC Power dataset. Table [I](#page-3-0)
    shows one example of two Lenovo nodes with Intel/AMD CPUs, both powered by 1100W
    power supply units, and compares the results of SPEC Power and SPEC CPU Rate Base
    (throughput). In accordance with our expectations, the relative performance difference
    between the two systems is similar for SPEC Power and SPEC CPU integer, while
    AMD''s performance advantage in the SPEC CPU floating-point benchmark suite is
    less pronounced. The integer-heavy SPEC Power workload favors AMD CPUs, while
    Intel''s 2x advantage in AVX register width reduces the performance gap for floating-point
    calculations on wide vectors. Therefore, the observed energy efficiency trends
    can not be generalized to floating-point workloads.


    Our results can also not be generalized to accelerator-based systems. While alternative
    benchmarks can make use of such devices (Green500, SPEC ACCEL), we did not consider
    them for the reasons mentioned previously.


    #### VI. CONCLUSION AND FUTURE WORK


    Our analysis of 16 years of SPEC Power benchmark results shows continuous increases
    in *power consumption* of x86 processors. While there are certainly physical limitations
    to this growth, they are not yet visible in the data.


    *Energy efficiency* also increases continuously and substantially, with AMD clearly
    providing superior efficiency in recent years. Due to the integer-heavy properties
    of the SPEC Power benchmark, this observation may not be generalized to more floating-point-intensive
    workloads. We also observe a positive trend towards better *energy proportionality*
    for both CPU vendors; although this trend is not universal.


    Our analysis of *active idle power* data shows a conclusive trend towards lower
    consumption between 2006 and 2017, driven by the introduction of successively
    more effective sleep state mechanisms. Since then, a substantial share of runs
    show a regression in idle-specific power optimizations. We believe that the high
    variation in the results serves as an indication that particular attention should
    be paid to practical active idle power in the hardware selection and system operation.
    Especially for systems that may spend substantial time in active idle, such as
    HPC systems, idle power optimizations can improve economical and ecological performance.


    The major *limitations* of this analysis are a lack of data for floating-point
    workloads, for other processor architectures such as ARM, and for accelerators
    such as AMD or NVIDIA graphics processing units (GPUs). The latter will hopefully
    be addressed by the SPECpowerNext benchmark [\[9\]](#page-4-8). This would be
    the industry-standard, vendor-driven benchmark, filling an important gap and enabling
    *future work* on GPU energy efficiency trend analysis.


    ### ACKNOWLEDGMENTS


    This work is supported in part by the German National High Performance Computing
    (NHR@TUD), funded in equal parts by the state of Saxony and the Federal Ministry
    of Education and Research. Additionally, this work is supported by the Federal
    Ministry of Education and Research via the EECliPs research project (16ME0602).
    The authors want to express their gratitude to everyone involved in creating the
    used SPEC Power dataset, from the contributors at SPEC creating the benchmark
    to the countless submitters. Further, we thank Florian Mros for his work expanding
    the parsing scripts.


    #### AVAILABILITY


    We provide all scripts to reproduce this paper (parsing, analysis, and plotting),
    together with all data (raw and processed) online [\[16\]](#page-4-15).


    #### REFERENCES


    - <span id="page-4-0"></span>[1] SPEC, "Design document ssj workload specpower\_ssj2008,"
    2012, [https:](https://www.spec.org/power/docs/SPECpower_ssj2008-Design_ssj.pdf)
    [//www.spec.org/power/docs/SPECpower\\_ssj2008-Design\\_ssj.pdf.](https://www.spec.org/power/docs/SPECpower_ssj2008-Design_ssj.pdf)

    - <span id="page-4-1"></span>[2] J. von Kistowski, K.-D. Lange, J. A. Arnold,
    S. Sharma, J. Pais, and H. Block, "Measuring and benchmarking power consumption
    and energy efficiency," in *Companion of the 2018 ACM/SPEC International Conference
    on Performance Engineering*, ser. ICPE ''18. New York, NY, USA: Association for
    Computing Machinery, 2018, p. 57–65, [DOI:](https://doi.org/10.1145/3185768.3185775)
    [10.1145/3185768.3185775.](https://doi.org/10.1145/3185768.3185775)

    - <span id="page-4-2"></span>[3] C. Gough, I. Steiner, and W. Saunders, *Energy
    Efficient Servers: Blueprints for Data Center Optimization*, 2015, [DOI: 10.1007/978-1-](https://doi.org/10.1007/978-1-4302-6638-9)
    [4302-6638-9.](https://doi.org/10.1007/978-1-4302-6638-9)

    - <span id="page-4-3"></span>[4] C.-H. Hsu and S. W. Poole, "Power signature analysis
    of the specpower\_ssj2008 benchmark," in *(IEEE ISPASS) IEEE International Symposium
    on Performance Analysis of Systems and Software*, 2011, pp. 227–236, [DOI: 10.1109/ISPASS.2011.5762739.](https://doi.org/10.1109/ISPASS.2011.5762739)

    - <span id="page-4-4"></span>[5] ——, "Revisiting server energy proportionality,"
    in *2013 42nd International Conference on Parallel Processing*, 2013, pp. 834–840,
    [DOI:](https://doi.org/10.1109/ICPP.2013.99) [10.1109/ICPP.2013.99.](https://doi.org/10.1109/ICPP.2013.99)

    - <span id="page-4-5"></span>[6] W.-c. Feng and K. Cameron, "The green500 list:
    Encouraging sustainable supercomputing," *Computer*, vol. 40, no. 12, pp. 50–55,
    2007, [DOI:](https://doi.org/10.1109/MC.2007.445) [10.1109/MC.2007.445.](https://doi.org/10.1109/MC.2007.445)

    - <span id="page-4-6"></span>[7] M. S. Müller, J. Baron, W. C. Brantley, H. Feng,
    D. Hackenberg, R. Henschel, G. Jost, D. Molka, C. Parrott, J. Robichaux, P. Shelepugin,
    M. van Waveren, B. Whitney, and K. Kumaran, "Spec omp2012 an application benchmark
    suite for parallel systems using openmp," in *OpenMP in a Heterogeneous World*,
    B. M. Chapman, F. Massaioli, M. S. Müller, and M. Rorro, Eds. Berlin, Heidelberg:
    Springer Berlin Heidelberg, 2012, pp. 223–236, [DOI: 10.1007/978-3-642-30961-8\\_17.](https://doi.org/10.1007/978-3-642-30961-8_17)

    - <span id="page-4-7"></span>[8] U.S. Environmental Protection Agency, "Energy
    star® program requirements product specification for computer servers final draft
    test method," 2023, [https://www.energystar.gov/sites/default/files/](https://www.energystar.gov/sites/default/files/asset/document/Computer%20Servers%20Version%204.0%20Final%20Draft%20Test%20Method.pdf)
    [asset/document/Computer%20Servers%20Version%204.0%20Final%](https://www.energystar.gov/sites/default/files/asset/document/Computer%20Servers%20Version%204.0%20Final%20Draft%20Test%20Method.pdf)
    [20Draft%20Test%20Method.pdf.](https://www.energystar.gov/sites/default/files/asset/document/Computer%20Servers%20Version%204.0%20Final%20Draft%20Test%20Method.pdf)

    - <span id="page-4-8"></span>[9] N. Schmitt, K.-D. Lange, S. Sharma, N. Rawtani,
    C. Ponder, and S. Kounev, "The specpowernext benchmark suite, its implementation
    and new workloads from a developer''s perspective," in *Proceedings of the ACM/SPEC
    International Conference on Performance Engineering*, ser. ICPE ''21. New York,
    NY, USA: Association for Computing Machinery, 2021, p. 225–232, [DOI: 10.1145/3427921.3450239.](https://doi.org/10.1145/3427921.3450239)

    - <span id="page-4-9"></span>[10] M. Meissner, K.-D. Lange, S. Sharma, J. Arnold,
    A. Cragin, P. Galizia, M. Petrich, B. Zhang, and S. Kounev, "Challenges and future
    directions in efficiency benchmarking (vision paper)," in *Companion of the 2023
    ACM/SPEC International Conference on Performance Engineering*, ser. ICPE ''23
    Companion. New York, NY, USA: Association for Computing Machinery, 2023, p. 51–55,
    [DOI: 10.1145/3578245.3585034.](https://doi.org/10.1145/3578245.3585034)

    - <span id="page-4-10"></span>[11] SPEC, "SPECpower\_ssj2008 Result File Fields,"
    2018, [https://www.spec.](https://www.spec.org/power/docs/SPECpower_ssj2008-Result_File_Fields.html)
    [org/power/docs/SPECpower\\_ssj2008-Result\\_File\\_Fields.html.](https://www.spec.org/power/docs/SPECpower_ssj2008-Result_File_Fields.html)

    - <span id="page-4-11"></span>[12] American Society of Heating Refrigerating and
    Air-Conditioning Engineers, "Emergence and Expansion of Liquid Cooling in Mainstream
    Data Centers," Tech. Rep., May 2021. [Online]. Available: [https:](https://www.ashrae.org/file%20library/technical%20resources/bookstore/emergence-and-expansion-of-liquid-cooling-in-mainstream-data-centers_wp.pdf)
    [//www.ashrae.org/file%20library/technical%20resources/bookstore/](https://www.ashrae.org/file%20library/technical%20resources/bookstore/emergence-and-expansion-of-liquid-cooling-in-mainstream-data-centers_wp.pdf)
    [emergence-and-expansion-of-liquid-cooling-in-mainstream-data-centers\\_](https://www.ashrae.org/file%20library/technical%20resources/bookstore/emergence-and-expansion-of-liquid-cooling-in-mainstream-data-centers_wp.pdf)
    [wp.pdf](https://www.ashrae.org/file%20library/technical%20resources/bookstore/emergence-and-expansion-of-liquid-cooling-in-mainstream-data-centers_wp.pdf)

    - <span id="page-4-12"></span>[13] D. Hackenberg, R. Schöne, T. Ilsche, D. Molka,
    J. Schuchart, and R. Geyer, "An energy efficiency feature survey of the intel
    haswell processor," in *2015 IEEE International Parallel and Distributed Processing
    Symposium Workshop*, 2015, pp. 896–904, [DOI: 10.1109/IPDPSW.2015.70.](https://doi.org/10.1109/IPDPSW.2015.70)

    - <span id="page-4-13"></span>[14] G. Juckeland, W. Brantley, S. Chandrasekaran,
    B. Chapman, S. Che, M. Colgrove, H. Feng, A. Grund, R. Henschel, W.-M. W. Hwu,
    H. Li, M. S. Müller, W. E. Nagel, M. Perminov, P. Shelepugin, K. Skadron, J. Stratton,
    A. Titov, K. Wang, M. van Waveren, B. Whitney, S. Wienke, R. Xu, and K. Kumaran,
    "Spec accel: A standard application suite for measuring hardware accelerator performance,"
    in *High Performance Computing Systems. Performance Modeling, Benchmarking, and
    Simulation*, S. A. Jarvis, S. A. Wright, and S. D. Hammond, Eds. Cham: Springer
    International Publishing, 2015, pp. 46–67, [DOI: 10.1007/978-3-](https://doi.org/10.1007/978-3-319-17248-4_3)
    [319-17248-4\\_3.](https://doi.org/10.1007/978-3-319-17248-4_3)

    - <span id="page-4-14"></span>[15] J. Bucek, K.-D. Lange, and J. v. Kistowski,
    "Spec cpu2017: Nextgeneration compute benchmark," in *Companion of the 2018 ACM/SPEC
    International Conference on Performance Engineering*, ser. ICPE ''18. New York,
    NY, USA: Association for Computing Machinery, 2018, p. 41–42, [DOI: 10.1145/3185768.3185771.](https://doi.org/10.1145/3185768.3185771)

    - <span id="page-4-15"></span>[16] H. Tröpgen, R. Schöne, T. Ilsche, and D. Hackenberg,
    "Artifacts to Reproduce "16 Years of SPEC Power: An Analysis of x86 Energy Efficiency
    Trends"," Jul. 2024. [Online]. Available: <https://doi.org/10.5281/zenodo.12656360>'
- title: "RPCAcc: A High-Performance and Reconfigurable PCIe-attached RPC\n  Accelerator"
  abstract: "The emerging microservice/serverless-based cloud programming paradigm\
    \ and the\nrising networking speeds leave the RPC stack as the predominant data\
    \ center\ntax. Domain-specific hardware acceleration holds the potential to disentangle\n\
    the overhead and save host CPU cycles. However, state-of-the-art RPC\naccelerators\
    \ integrate RPC logic into the CPU or use specialized low-latency\ninterconnects,\
    \ hardly adopted in commodity servers.\n  To this end, we design and implement\
    \ RPCAcc, a software-hardware co-designed\nRPC on-NIC accelerator that enables\
    \ reconfigurable RPC kernel offloading.\nRPCAcc connects to the server through\
    \ the most widely used PCIe interconnect.\n  To grapple with the ramifications\
    \ of PCIe-induced challenges, RPCAcc\nintroduces three techniques:(a) a target-aware\
    \ deserializer that effectively\nbatches cross-PCIe writes on the accelerator's\
    \ on-chip memory using compacted\nhardware data structures; (b) a memory-affinity\
    \ CPU-accelerator collaborative\nserializer, which trades additional host memory\
    \ copies for slow cross-PCIe\ntransfers; (c) an automatic field update technique\
    \ that transparently codifies\nthe schema based on dynamic reconfigure RPC kernels\
    \ to minimize superfluous\nPCIe traversals. We prototype RPCAcc using the Xilinx\
    \ U280 FPGA card. On\nHyperProtoBench, RPCAcc achieves 3.2X lower serialization\
    \ time than a\ncomparable RPC accelerator baseline and demonstrates up to 2.6X\
    \ throughput\nimprovement in the end-to-end cloud workload."
  url: http://arxiv.org/abs/2411.07632v2
  keywords: ''
  document: '# RPCAcc: A High-Performance and Reconfigurable PCIe-attached RPC Accelerator


    Jie Zhang†,<sup>1</sup> , Hongjing Huang†,<sup>1</sup> , Xuzheng Xu<sup>1</sup>
    , Xiang Li<sup>1</sup> , Jieru Zhao<sup>2</sup> , Ming Liu<sup>3</sup> , Zeke
    Wang<sup>1</sup>


    <sup>1</sup> Zhejiang University


    2 Shanghai Jiao Tong University


    <sup>3</sup> University of Wisconsin-Madison


    *Abstract*—The emerging microservice/serverless-based cloud programming paradigm
    and the rising networking speeds leave the RPC stack as the predominant data center
    tax. Domainspecific hardware acceleration holds the potential to disentangle the
    overhead and save host CPU cycles. However, state-ofthe-art RPC accelerators integrate
    RPC logic into the CPU or use specialized low-latency interconnects, hardly adopted
    in commodity servers.


    To this end, we design and implement RPCAcc, a softwarehardware co-designed RPC
    on-NIC accelerator that enables reconfigurable RPC kernel offloading. RPCAcc connects
    to the server through the most widely used PCIe interconnect. To grapple with
    the ramifications of PCIe-induced challenges, RPCAcc introduces three techniques:
    (a) a target-aware deserializer that effectively batches cross-PCIe writes on
    the accelerator''s SRAM using compacted hardware data structures; (b) a memory-affinity
    CPU-accelerator collaborative serializer, which trades additional host memory
    copies for slow cross PCIe-transfers; (c) an automatic field update technique
    that transparently codifies the schema based on dynamic reconfigure RPC kernels
    to minimize superfluous PCIe traversals. We prototype RPCAcc using the Xilinx
    U280 FPGA card. On HyperProtoBench, RPCAcc achieves an average of 2.3× lower RPC
    layer processing time than a comparable RPC accelerator baseline and demonstrates
    2.6× achievable throughput improvement in the end-to-end cloud workload.


    #### I. INTRODUCTION


    Remote Procedure Call (RPC) is a paramount service block of today''s cloud system
    stacks [\[1\]](#page-12-0), [\[33\]](#page-12-1), [\[42\]](#page-12-2), [\[65\]](#page-13-0).
    It abstracts remote computing resources and provides a simple and familiar programming
    model. Developers only prescribe type information for each remote procedure, and
    a compiler generates a stub code linked to an application to pass arguments via
    message. The RPC model has been widely adopted in many distributed applications,
    such as cloud storage [\[17\]](#page-12-3), [\[79\]](#page-13-1), file systems
    [\[39\]](#page-12-4), [\[82\]](#page-13-2), data analytics [\[72\]](#page-13-3),
    consensus protocols [\[80\]](#page-13-4), [\[81\]](#page-13-5), and machine learning
    systems [\[45\]](#page-13-6), [\[53\]](#page-13-7).


    The RPC stack comprises two key components: (a) RPC protocol handling that parses
    the RPC headers, identifies the triggered message and the carried payload, and
    determines the target function; (b) serialization and deserialization, transforming
    between in-memory data fields and architecture/languageagnostic formats. A recent
    study from Google Cloud [\[65\]](#page-13-0) reports that the RPC processing occupies
    ∼7.1% of CPU cycles across the entire fleet. Thus, it is important to accelerate


    †Contributes equally


    the RPC execution, reduce this data center tax, and release more CPU cycles for
    revenue-generated applications.


    Domain-specific hardware acceleration is a promising solution to build performant
    computing systems in the post-Moore''s Law era. However, designing an RPC hardware
    accelerator is very challenging because the RPC stack is tightly coupled with
    the networking stack and application layer, whose processing should be efficiently
    streamlined into the data plane. As a result, researchers propose to use specialized
    on-chip interconnects and closely integrate the RPC acceleration module in the
    host CPU chips [\[32\]](#page-12-5), [\[34\]](#page-12-6), [\[42\]](#page-12-2),
    [\[59\]](#page-13-8), [\[60\]](#page-13-9). For example, Cereal [\[32\]](#page-12-5)
    introduces a special memory access interface to allow low-latency host memory
    accesses from the RPC accelerator. Dagger [\[42\]](#page-12-2) leverages Intel
    UPI [\[31\]](#page-12-7) interconnect to facilitate RPC stack processing.


    Unfortunately, none of these proposals can be easily adopted on commodity servers
    due to the lack of interconnect support. RPC stack is continuously and rapidly
    evolving. For example, widely used gRPC [\[1\]](#page-12-0) has 9 major releases
    over the last twelve months. As such, integrating RPC logic into the real host
    CPU lacks enough flexibility. Besides, developing a function- and performance-capable
    interconnect that can be integrated into a server system takes many years of engineering
    efforts, such as the ECI bus from the pioneering Enzian platform [\[10\]](#page-12-8).
    The emerging Compute Express Link [\[11\]](#page-12-9) looks promising, but its
    physical layer runs atop PCIe, yielding sub-microsecond access latency [\[44\]](#page-13-10),
    [\[68\]](#page-13-11), which cannot satisfy the latency requirement of the above
    accelerators. This leads to an interesting question: How to accelerate RPC on
    top of de facto and predominant server interconnect, i.e., PCIe?


    In this paper, we design and implement RPCAcc, a software-hardware co-designed
    PCIe-attached accelerator for reconfigurable RPC offloading. RPCAcc colocates
    with the PCIe-attached NIC. RPCAcc'' hardware part comprises three building blocks:
    (1) a target-aware deserializer that takes RPC requests, deserializes the messages,
    and forwards the results to the host or accelerator memory; (2) a memory-affinity
    serializer, which fetches computed data from both the host and accelerator memory,
    performs serialization, and fabricates the response; (3) programable computing
    units, dynamically offloading RPC computing kernels. In sum, RPCAcc is a lowprofile
    immediately deployable PCIe-attached on-NIC accelerator with a software abstraction
    to load the RPC stack and related computing kernels on demand.


    Building RPCAcc is non-trivial because of the high cross-PCIe overheads, jeopardizing
    the interaction performance between the RPC stack and other system layers. First,
    the RPC deserialization process needs to write deserialized results in a field-by-field
    scheme, whose throughput is bounded by the number of PCIe transactions. For example,
    our empirical evaluation using HyperProtoBench [\[34\]](#page-12-6) shows that
    this limitation can degrade the attainable deserialization throughput by 2.8×
    in geometric mean. RPCAcc proposes a target-aware deserializer that temporarily
    batches the deserialized fields within one RPC message in the accelerator''s SRAM
    and performs cross-PCIe writes only when necessary. We realize this by designing
    two compacted hardware data structures (schema table and temp buffer) and revamping
    the deserialization process.


    Second, the RPC serialization process is hindered by the high PCIe latency. A
    nested RPC message or dereference field (strings/bytes/repeated/sub-messages)
    would require multiple memory accesses in a pointer-chasing manner since the memory
    location of the sub-fields can only be known after the parent''s content is fetched.
    The sub-microsecond latency of PCIe would significantly increase the overall serialization
    time (∼4.6× compared with an on-chip accelerator). RPCAcc designs a memory-affinity
    CPU-Accelerator collaborative serializer that trades additional host memory copies
    for slow cross-PCIe transfer. We introduce a lightweight pre-serialization phase
    to materialize the data layout on the host memory and facilitate the accelerator-side
    serialization execution. Besides, we leverage the memcpy (memory copy) engines
    [\[30\]](#page-12-10), [\[41\]](#page-12-11) residing in modern CPUs [\[29\]](#page-12-12)
    to alleviate host CPU usage for large fields'' copy.


    Third, computation partition between host and RPC kernels within the RPC handler
    would cause suboptimal data placement and incur superfluous PCIe traversals. People
    eagerly co-locate domain-specific logic along with the RPC stack to maximize the
    hardware specialization benefits [\[18\]](#page-12-13), [\[35\]](#page-12-14),
    [\[50\]](#page-13-12), [\[62\]](#page-13-13), [\[79\]](#page-13-1). However, unlike
    on-chip cache-coherent interconnects, dynamic splitting computation logic across
    the host and accelerator over PCIe is inflexible and cause inferior data placement.
    Therefore, RPCAcc develops an automatic field update technique that transparently
    codifies the schema based on host/RPC kernel layout. As such, the accelerator
    deserializer can place the fields in suitable locations to avoid PCIe traversals.


    We built RPCAcc over an Xilinx Alevo U280 FPGA and evaluated it in several real-world
    scenarios. In a cloud image compression application, RPCAcc increases the achievable
    throughput by 2.6× and reduces the average (99th percentile) latency by 2.6× (1.9×)
    compared with an RPC accelerator baseline. Using Google''s HyperProtoBench [\[34\]](#page-12-6),
    RPCAcc reduces the data serialization time by 4.3× in geometric mean. RPCAcc achieves
    similar performance as prior specialized onchip accelerators from the literature.
    The source code will be open-sourced.


    ### II. BACKGROUND AND MOTIVATION


    #### <span id="page-1-0"></span>*A. Remote Procedure Calls*


    A typical RPC layer consists of two execution logics: RPC protocol handling and
    de/serialization.


    - *RPC protocol handling.* In the transmitting path (TX), the protocol handling
    mainly involves creating an RPC header. In the receiving path (RX), the protocol
    handling involves parsing the RPC header and dispatching the deserialized message
    to an idle CPU core to execute the target caller function. This process is usually
    lightweight compared with RPC payload processing;

    - *Serialization and deserialization.* Object serialization and deserialization
    are heavyweight operations and exist in RPC TX/RX, respectively. RPC serialization
    transforms the in-memory fields into architecture and language-agnostic formats
    that can traverse through the network [\[71\]](#page-13-14). Deserialization operates
    conversely.


    Protobuf. We focus on the Protocol Buffer serialization library [\[19\]](#page-12-15),
    widely used by many cloud applications.


    - *Protobuf message definition.* It defines the logical transformation between
    the in-memory format and the wire format. A protobuf message is a collection of
    fields, usually called "schema". Each message field has a type, name, field number,
    and labels (e.g., "repeated"). Field types can be (a) basic scalar types such
    as integers and strings; or (b) a nested user-defined message, also called a sub-message.
    Based on the memory layout, these fields can be further classified into two classes.
    One uses direct addressing, meaning that the value is within the memory location
    of its parent message, such as doubles and integers. The other uses indirect addressing
    (dereferences), indicating that their actual value is in a pointer-referenced
    memory location, such as strings, bytes, or sub-messages. In real-world applications
    the RPC message''s depth can reach up to a dozen levels or more [\[34\]](#page-12-6);

    - *Varint encoding of protobuf.* Data decoding/encoding is one of the most time-consuming
    operations in the de/serialization process [\[34\]](#page-12-6), especially for
    small data fields. Data encoding widely exists in many popular serialization frameworks
    [\[5\]](#page-12-16), [\[19\]](#page-12-15) for message reduction. Protobuf uses
    variable-length integer encoding (known as varint). The encoding uses the most
    significant bit in each byte to indicate if the next byte is part of the same
    integer and the remaining 7 bits are used to store the actual value. Protobuf
    uses the tag-length-value format (TLV) [\[20\]](#page-12-17) for lengthdelimited
    fields (such as string and sub-message) and tagvalue format (TV) for varints or
    fixed-length fields (such as double). Handling these byte-wise and bit-wise operations
    on general-purpose modern CPUs is costly [\[34\]](#page-12-6), [\[60\]](#page-13-9),
    [\[71\]](#page-13-14), but can be easily accelerated via hardware specialization.


    ### *B. Prior Hardware RPC Acceleration*


    Researchers have developed several hardware-accelerated RPC solutions [\[32\]](#page-12-5),
    [\[34\]](#page-12-6), [\[42\]](#page-12-2), [\[59\]](#page-13-8), [\[60\]](#page-13-9)
    to reduce the RPC


    <span id="page-2-0"></span>![](_page_2_Figure_0.jpeg)


    Fig. 1: High-level system architecture and request workflow of a PCIe-attached
    RPC accelerator.


    stack processing overheads and save host CPU cycles. For example, Cereal [\[32\]](#page-12-5)
    introduces a special memory access interface, enabling low-latency host memory
    access from the RPC accelerator. Dagger [\[42\]](#page-12-2) leverages a cache-coherent
    onchip interconnect (UPI) to facilitate collaborative RPC stack processing between
    an FPGA-based SmartNIC and the host CPU. However, Dagger does not support nested
    structures or pointers, which are heavily used in today''s applications. Optimus
    Prime [\[59\]](#page-13-8) and Cerebros [\[60\]](#page-13-9) place an on-chip
    accelerator for handling the de/serialization phase of the RPC stack. ProtoACC
    [\[34\]](#page-12-6) then develops a near-core de/serialization accelerator for
    Protobuf using a RISC-V SoC.


    Limitation. As shown in Table [I,](#page-3-0) all existing solutions require on-chip
    design or specialized interconnects for low latency. The main challenge of on-chip
    accelerator solutions is that integrating a specialized but not generalized function
    (e.g., RPC) into the commercial CPUs is very intrusive to the server CPU design.
    Considering that RPCs are evolving rapidly while the CPU design cycle takes years,
    integrating RPCs into the CPU chip is very costly and impractical. As a result,
    none can be easily employed on commodity servers and immediately deployed at scale.
    Therefore, we aim to build an immediately deployable PCIe-attached RPC accelerator
    that offers the same programmability and comparable performance as prior on-chip
    designs.


    #### <span id="page-2-2"></span>*C. Challenges*


    Integrating a specialized function into the PCIe-attached NIC is much easier,
    takes much less engineering effort, and has been proven to be practical in many
    production systems. For example, Google integrates their transport protocol "Falcon"
    into the Intel IPU [\[21\]](#page-12-18), and AWS integrates storage function
    into their Nitro SmartNIC [\[4\]](#page-12-19).


    Anathor option is to implement a standalone PCIe-based RPC accelerator. However,
    both the transmitting and receiving paths incur multiple redundant cross-PCIe
    RPC message movements between the RPC accelerator and the NIC. Therefore, we prefer
    integrating the RPC accelerator in the NIC.


    Figure [1](#page-2-0) sketches such a high-level design that offloads the RPC
    stack to the PCIe-attached NIC. When an RPC request arrives at the accelerator
    ( 1 ), the message is first deserialized via the hardware engine, where the deserialized
    fields are written into the host memory ( 2 ). Next, the host ( 3 ) and RPC kernel
    ( 4 and 5 ) are triggered collaboratively to process the RPC message. Then, the
    serialization engine retrieves the processing results, performs the serialization
    task ( 6 ), and sends back data through the network ( 7 ). The overall design
    seems straightforward but imposes three unique challenges.


    <span id="page-2-1"></span>![](_page_2_Figure_8.jpeg)


    Fig. 2: Normalized serialization time when increasing the simulated PCIe latency
    for different messages.


    C1: The limited number of concurrent PCIe transactions hinders the deserialization
    throughput. The deserialization engine writes back deserialized results into the
    host memory in a field-by-field manner [\[34\]](#page-12-6), [\[60\]](#page-13-9).
    However, these writing objects are small, incurring numerous DMA writes and smallsized
    PCIe transactions, quickly saturating the PCIe transaction rate. We built a deserialization
    accelerator (on the Xilinx U280 FPGA) based on ProtoACC [\[34\]](#page-12-6),
    and enforced it to put deserialized results into either the host memory (crossing
    PCIe) or the FPGA off-chip memory. Our evaluations on HyperProtoBench [\[34\]](#page-12-6)
    show that cross-PCIe deserialization can only achieve 5.6× lower throughput compared
    with writing results to the FPGA''s off-chip memory.


    C2: The high PCIe interconnect latency drastically decelerates the serialization
    performance. As described in §[II-A,](#page-1-0) an RPC message can be a deeply
    nested memory object, causing multiple pointer-chasing memory accesses under retrieval
    during the serialization phase ( 6 ), which is extremely inefficient when crossing
    PCIe (taking sub-microseconds).


    To illustrate this, we implement a protobuf serialization accelerator based on
    ProtoACC [\[34\]](#page-12-6). We measure the serialization time when varying
    interconnect latency through the Xilinx Vivado simulator [\[74\]](#page-13-15).
    Figure [2](#page-2-1) illustrates the normalized serialization time for all messages
    of Bench2 in HyperProtoBench [\[34\]](#page-12-6). As expected, when the interconnect
    latency increases from DDR5''s 70ns latency to commercial PCIe''s 1250ns, the
    end-to-end serialization''s time increases by 3.4× in geometric mean, due to the
    complex nested message structure. The exception is that the two messages (M4 and
    M10) only present a marginal increase. This is because when the RPC message becomes
    large and flat (1.6MB and 0.6MB), the serialization performance is dominated by
    the data transfer time and is not sensitive to the interconnect latency.


    C3: Suboptimal data placement causes superfluous PCIe accesses from host/RPC kernels.
    People eagerly offload domain-specific logic along with the RPC stack for core
    savings and performance maximization [\[18\]](#page-12-13), [\[35\]](#page-12-14),
    [\[42\]](#page-12-2), [\[50\]](#page-13-12), [\[62\]](#page-13-13), [\[79\]](#page-13-1).
    These offloadable kernels are generally parallelfriendly with less data dependency.
    For example, researchers place a data compression engine for the cloud block storage
    application [\[79\]](#page-13-1). Ideally, one should divide incoming data and
    place them accordingly, such that the host and the offloaded RPC kernels only
    access their data locally. However, in reality, since the offloaded kernels (
    5 ) are only part of the RPC handler and the offloaded kernels may dynamically
    change in a multi-tenant environment, it becomes extremely challenging


    <span id="page-3-0"></span>


    | System             | Interconnect        | Latency | Throughput | Accelerated
    RPC Stack                         | Accelerated RPC Kernels |

    |--------------------|---------------------|---------|------------|-----------------------------------------------|-------------------------|

    | Cereal [32]        | MAI                 | 40 ns   | 76.8 GB/s  | Customized
    De/Serialization                   | N/A                     |

    | Optimus Prime [59] | 2D mesh NoC         | 45 ns   | 64 GB/s    | Protobuf/Thrift-based
    De/Serialization        | N/A                     |

    | Cerebros [60]      | 2D mesh NoC         | 45 ns   | 64 GB/s    | Thrift-based
    De/Serialization, RPC protocol   | N/A                     |

    | ProtoACC [34]      | TileLink System Bus | 30 ns   | N/A        | Protobuf-based
    De/Serialization               | N/A                     |

    | Dagger [42]        | Intel UPI           | 125 ns  | 19.2 GB/s  | Customized
    De/Serialization, RPC protocol,    | Yes                     |

    | RPCAcc             | PCIe                | 1250 ns | 12.8 GB/s  | Protobuf-based
    De/Serialization, RPC protocol | Yes                     |


    TABLE I. Hardware specification comparison.


    to design a clean and optimal partition. A suboptimal data placement is very costly
    for a PCIe accelerator considering its high latency for cross-PCIe traversals.


    To illustrate this, we develop an RPC-based network function accelerator that
    co-locates with a PCIe-attached NIC. It serves as the cloud gateway [\[55\]](#page-13-16).
    It performs L2/L3 protocol processing, network address translation (NAT), and
    packet de/encryption. We explore different computing-driven data placement strategies
    and find out that the worst-case placement can decrease the achievable throughput
    by 2.2× than the bestcase placement.


    III. RPCACC: DESIGN AND IMPLEMENTATION


    ## *A. Overview*


    RPCAcc is a software-hardware co-designed PCIe-attached accelerator that allows
    offloading user-defined RPC kernels. Figure [3](#page-4-0) provides the system
    overview. The hardware part consists of 1) a target-aware deserializer (§[III-B\)](#page-3-1)
    that takes RPC requests, deserializes the messages, and forwards the results to
    the host or accelerator; 2) a memory-affinity serializer (§[III-C\)](#page-4-1),
    which fetches computed data, performs serialization, and fabricates the response;
    3) programable computing units (§[III-D\)](#page-5-0), dynamically offloading
    RPC computing kernels; and 4) a transport layer. RPCAcc adopts a RoCE-based transport
    layer [\[67\]](#page-13-17), which is entirely offloaded to the NIC just like
    an RDMA NIC. The RPC acceleration logic is in the NIC and sits between the transport
    layer and the PCIe controller. When the RPC message is fabricated in the NIC RPC
    layer, the hardware will send the message using an "RDMA Send" verb and the remote
    side uses an "RDMA Recv" verb to receive incoming RPC requests. The benefit of
    putting the RPC acceleration and transport layer together in NIC hardware is that
    it avoids redundant data movement or PCIe traversals between the transport processing
    and RPC processing.


    We place (1), (2), and (4) in the static region of the board, while (3) in the
    partial reconfiguration region. Our software stack (§[III-E](#page-5-1) and §[III-F\)](#page-6-0)
    of RPCAcc consists of (a) a compiler that takes the user-defined RPC message specification
    and outputs both the message structure and accelerator-friendly configurations;
    and (b) a rich set of APIs to describe the RPC kernel task.


    # <span id="page-3-1"></span>*B. Target-aware Deserializer*


    To address challenge #1 (§[II-C\)](#page-2-2), we develop a target-aware deserialization
    engine that forwards deserialized fields to the host or accelerator memory accordingly.
    Our deserialization logic has 4 independent computing lanes (i.e., deserializers).
    Each deserializer processes RPC requests one by one. Each deserializer executes
    the deserialization logic and converts the results into in-memory C++ objects.


    We introduce two hardware data structures (described below) and revamp the deserialization
    process based on them.


    - Schema Table. It is an SRAM region that stores the message structure of incoming
    RPC messages. For each field of an RPC class, we use one bit to indicate its target
    location type for the deserialized results. Fields used by the offloaded RPC kernel
    (host kernel) are forwarded to the accelerator off-chip memory (host CPU memory).
    The "Schema Table" is shared by all deserializers and serializers. Section [III-E](#page-5-1)
    describes how this bit is set and how the "Schema Table" is constructed;

    - Temp Buffer. Deserialized results used by RPC kernels are directly written to
    the accelerator off-chip memory. For others, we use a per-deserializer SRAM buffer
    ("Temp Buffer") to store the deserialized fields temporarily. The buffer is 4KB
    and operates in an append-only mode, simplifying the buffer management. The buffer
    size is configurable. When the buffer is full or the current RPC request''s deserialization
    is finished, the deserializer triggers a DMA write and copies data to the intended
    host CPU memory. We call this batching mechanism "One-shot DMA write". Note that
    this batching mechanism would barely increase the deserialization latency, since
    it only batches the fields within an RPC request instead of batching fields from
    different requests.


    Deserialization Procedure. An incoming RPC message is assigned to an idle deserializer
    or buffered when there are no idle deserializers. To avoid software allocation
    overhead, we reserve a host CPU memory region and an accelerator offchip memory
    region for the deserializer. These two memory regions are divided into 4KB [1](#page-3-2)
    chunks. There is a 16K-entry TLB[2](#page-3-3) to perform host CPU memory address
    translation on the accelerator. We use two SRAM-based FIFOs (called freelist FIFOs)
    to store the free chunks of the host/accelerator memory region. Allocating/freeing
    memory is translated to poping/pushing a 4KB chunk from/into the FIFO, simplifying
    hardware complexity.


    The deserializer first pre-allocates a host CPU memory chunk and an accelerator
    off-chip memory chunk. It then parses the RPC header to obtain the RPC message
    class ID and message length and queries the "Schema Table" based on the class
    ID, which returns the schema of this message


    <span id="page-3-2"></span><sup>1</sup>4KB chunks lead to a small allocation time
    (0.68% of the total deserialization time) and a small fragmentation overhead (3.6%
    in HyperProtoBench). The size is configurable at system initialization and the
    users can choose a suitable value that balances both allocation time and memory
    fragmentation.


    <span id="page-3-3"></span><sup>2</sup> We adopt a simple TLB implementation,
    which can only store pages with contiguous virtual addresses. 16K entries only
    occupy 0.29% of the total SRAM resources in our FPGA prototype.


    <span id="page-4-0"></span>![](_page_4_Figure_0.jpeg)


    Fig. 3: Software stack and hardware architecture of RPCAcc.


    class. The accelerator then deserializes the message data in a field-by-field
    manner accordingly. When encountering a dereference sub-message, the deserializer
    pushes the current message schema into an SRAM-based stack and deserializes the
    sub-message recursively.


    During the deserialization, each deserialized field has one of the two target
    locations:


    - Host CPU memory: The deserialized result is assigned a CPU memory location from
    the pre-allocated CPU memory chunk. As described above, we would temporarily save
    it in the "Temp Buffer".

    - Accelerator off-chip memory: The deserialized data is assigned an accelerator
    memory location from the preallocated off-chip memory chunk. Then the result would
    be directly written to this location. The corresponding field pointer in the parent
    message would be updated to point to this off-chip memory location.


    When the deserializer exhausts the pre-allocated chunks, it allocates a new chunk
    from one of the two free-list FIFOs. When exhausting pre-allocated host CPU memory
    chunks, the deserializer additionally uses a DMA write to flash the "Temp Buffer"
    into the corresponding host CPU memory. Upon deserialization completion, the accelerator
    then notifies the host CPU of an incoming RPC message.


    Summary. Compared with the traditional field-by-field deserialization scheme [\[34\]](#page-12-6),
    [\[59\]](#page-13-8), [\[60\]](#page-13-9), our target-aware deserializer uses
    effective batching and reduces unnecessary PCIe traffic by storing certain fields
    in the accelerator local offchip memory. Besides, we directly store the fields
    that are not needed by the host CPU in the NIC accelerator''s off-chip memory,
    greatly reducing unnecessary PCIe transactions.


    #### <span id="page-4-1"></span>*C. Memory-affinity Serializer*


    We next discuss how to address challenge #2 (§[II-C\)](#page-2-2) in RPCAcc. There
    are two general serialization design choices:


    • Option#1: CPU-only Serialization. Figure [4-](#page-4-2)a depicts the process
    of CPU-only serialization. Upon the compute unit (CU) finishing computation, it
    writes the result back to the host CPU memory ( 1 ). The CPU then retrieves all
    the fields and serializes them ( 2 ), writing them into a DMA-safe memory region.
    At last, the accelerator reads data from the DMA-safe region, fabricates the RPC
    response, and sends


    <span id="page-4-2"></span>![](_page_4_Figure_11.jpeg)


    Fig. 4: Compaison of three serialization strategies.


    it to the network ( 3 ). As CPU memory access latency is very low (∼70ns), this
    approach can tolerate nested RPC messages well. However, it wastes host CPU cycles
    on CPU-inefficient encoding, while wasting PCIe bandwidth (GB/s, not transaction
    rate) drastically in stage 1 ;


    • Option #2: Accelerator-only Serialization. Figure [4-](#page-4-2)b shows how
    an on-NIC accelerator performs serialization independently. Compared with CPU-only
    serialization, the difference is that the serialization is fully offloaded to
    the NIC hardware. Accelerator-only serialization consumes minimal host CPU cycles.
    However, as discussed in §[II-C,](#page-2-2) the high cross-PCIe latency would
    jeopardize the serialization time, especially for deeply nested RPC messages.


    Our approach: Memory-affinity CPU-Accelerator Collaborative Serialization (Option
    #3). To address the limitations of the above two, we distribute the serialization
    logic across the host CPU and accelerator, aiming to achieve the best of two worlds:
    minimizing PCIe transfers while consuming the fewest host CPU cycles. Our key
    idea is to add a lightweight CPU pre-serialization phase to the host to materialize
    the data layout for fields residing in the host memory, which trades additional
    fast host memory copies for slow PCIe accesses. Figure [4-](#page-4-2)c highlights
    the process. The compute unit writes results into the accelerator memory instead
    of the host memory ( 1 ). Then the host CPU retrieves all local fields and preserializes
    them ( 2 ) without CPU-inefficient encoding. Next, the CPU sends the pre-serialized
    data to the serializer ( 3 ), which encodes these data and further serializes
    fields that reside in the accelerator memory ( 4 ). At last, the serialization
    module merges the CPU and accelerator memory fields and sends the merged result
    out to the network as an RPC response ( 5 ). Modern server CPUs [\[29\]](#page-12-12)
    are integrated with on-chip memcpy engines (Data Stream Accelerator [\[30\]](#page-12-10),
    [\[41\]](#page-12-11)) and the pre-serialization process offload the copies of
    large fields to the memcpy engines to save host CPU cycles during the preserialization..


    Next, we discuss the detailed serialization procedure:


    Stage 1: CPU Pre-serialization. We maintain a small DMAsafe buffer to store the
    CPU pre-serialization output. The process iterates the to-be-serialized object
    in a field-by-field manner. The process scans each encountered field and writes
    the non-contiguous results into the contiguous DMA-safe buffer. Upon finishing,
    the software uses an MMIO write to notify the accelerator of the address and length
    of the preserialized data, and other required information to construct an RPC
    header. The pre-serialization has three unique properties that can help reduce
    CPU cycles:


    - Memcpy Offload. The copy of large CPU memory fields in the host CPU memory can
    be offloaded to the memcpy engines. The CPU asynchronously invokes the memcpy
    engines to reduce the required CPU cycles at most.

    - Encoding Offload. The pre-serialization process would not perform CPU-inefficient
    encoding, deferred to the hardware accelerator.

    - Skipping Accelerator Fields. The pre-serialization process only pre-serializes
    fields residing in the host CPU memory. If encountering a field residing in accelerator
    memory, it only writes the pointer value and data length into the DMAsafe buffer.


    Stage 2: Accelerator Serialization. When the accelerator is notified after the
    completion of CPU serialization, the accelerator constructs an RPC header in an
    SRAM region, which is called "TX Arena" and is used to store the final RPC message
    that is to be sent to the network. The hardware serializer then uses a DMA read
    to fetch the pre-serialized data from the host CPU memory, iterates pre-serialized
    data, and performs varint encoding. The serializer encodes the preserialized data
    in a per-512-bit manner. For each 512-bit, the encoding can be done within one
    cycle. If a pointer referring to the accelerator off-chip memory is found, the
    serializer reads the referred data from the accelerator off-chip memory, serializes
    it, and writes the result into the "TX Arena". After fetching the data from the
    accelerator off-chip memory, the engine serializes it, writes the result to the
    corresponding address in the "TX Arena", and continues the iteration. When the
    iteration finishes, the RPC header, and the serialization results now lie contiguously
    in the "TX Arena". The transport layer can transmit these data into the network.


    Summary. Our memory-affinity CPU-accelerator collaborative serializer trades fast
    host memory copies for slow cross-PCIe transfer. It introduces a lightweight pre-serialization
    phase to materialize the data layout and facilitate the accelerator-side serialization
    execution. To further reduce CPU overhead during pre-serialization, we leverage
    memcpy engines to perform data copies of large fields.


    # <span id="page-5-0"></span>*D. Compute Unit*


    In addition to accelerating the RPC stack itself, RPCAcc allows user to program
    the compute units (CUs) with their hardware logic to further offload compute-intensive
    computations in the RPC requests. We call these offloaded computations as RPC
    kernels. A compute unit in RPCAcc is a partially reconfigurable FPGA block. Each
    CU has a memory interface connected to the accelerator off-chip memory.


    CUs interact with the host software uniformly and provide a set of APIs (Table
    [II\)](#page-6-1). Each CU has a descriptor ring in the accelerator SRAM and a
    notification ring in the host CPU memory. To activate a CU, the host software
    has to submit a descriptor to the descriptor ring using an MMIO write ("submitTask()").
    After submission, the address of an available entry in the notification ring would
    be returned to the software. Submitting computation tasks is an asynchronous process,
    and the software can poll the returned address to be aware of this task''s completion
    ("poll()"), akin to the BlueFlame [\[76\]](#page-13-18) mechanism in the Mellanox
    NICs.


    - Descriptor ring. Entries in the descriptor ring are submitted by the host CPU
    software, where each entry consists of the input address, input length, output
    address, and output buffer size. When a CU becomes idle, it fetches the next ready
    descriptor from the ring, and reads data from the off-chip memory using the input
    address and input length;

    - Notification ring. When computation finishes, the CU first writes the results
    into the output address. Then the result length and the completion signal would
    be written into the corresponding notification entry in the host CPU memory using
    one DMA write.


    In addition to submitting tasks to the CU and polling the completion, the user
    can use ".program()" to reprogram the CU with a given FPGA bit file and use "getType()"
    to check the currently supported computation of the compute unit. In the current
    implementation, we create four PR blocks of the same size. Equal-size PR blocks
    expose limitations on flexibility. It''s also possible to leverage the techniques
    proposed in prior works [\[38\]](#page-12-20), [\[77\]](#page-13-19), [\[78\]](#page-13-20)
    to dynamically manage the PR region and this could be our future work.


    # <span id="page-5-1"></span>*E. RPCAcc Software Stack*


    RPCAcc provides a compiler toolchain and programming APIs that enable dynamic
    and reconfigurable RPC stack and computing kernel offloading.


    *1) RPCAcc Compiler:* Our compiler takes a user-provided .proto file that contains
    the RPC message structure, and generates (1) a header file for applications running
    on the host CPU and (2) a schema definition stored in the "Schema Table" of the
    RPCAcc accelerator for orchestrating RPC request flow.


    RPCAcc fully supports Protobuf3 [\[19\]](#page-12-15) format. Programmers first
    define the RPC message format in the .proto file and specify a field with labels
    such as "optional" and "repeated". In RPCAcc, the user can additionally specify
    a dereference field (string/bytes/repeated/sub-message) with a custom label


    <span id="page-6-1"></span>


    | Member Function | Parameter                                                                                                                                                     |
    Description                                                                                                                                                                                                                                                                     |  |

    |-----------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--|

    | .program()      | The<br>file<br>location<br>of<br>the<br>bitstream<br>(bitFilePath),
    the programmed RPC<br>kernel (kernelType)                                                 |
    Program the compute unit with the provided bit file; the compute unit is labeled
    as "kernelType".                                                                                                                                                                               |  |

    | .getType()      | N/A                                                                                                                                                           |
    Return the string of the labeled "kernelType".                                                                                                                                                                                                                                  |  |

    | .submitTask()   | Address of input to the CU (inputAddr),<br>input size in bytes
    (inputSize), address of<br>output result (outputAddr), output size in<br>bytes
    (outputBufSize) | Submit a new task to the compute unit, and an asynchronous task
    event will be returned. The<br>CU would fetch inputSize bytes from the accelerator
    memory address inputAddr. When the engine<br>completes, it writes the result into
    the accelerator memory address outputAddr. |  |

    | .poll()         | The completion signal of the compute unit<br>(taskEvent)                                                                                                      |
    Busy polling the taskEvent until it completes.                                                                                                                                                                                                                                  |  |


    TABLE II. Member functions of compute units in RPCAcc.


    <span id="page-6-3"></span>


    | Member Function | Description                                                                                           |  |  |

    |-----------------|-------------------------------------------------------------------------------------------------------|--|--|

    | .isInAcc()      | Check whether the data is in the accelerator memory.                                                  |  |  |

    | .moveToAcc()    | Move data to the accelerator memory, and the field''s<br>pointer
    will be updated to this new location. |  |  |

    | .moveToCPU()    | Move data to the host CPU memory, and the field''s<br>pointer
    will be updated to this new location.    |  |  |


    TABLE III. Dereference fields functions in RPCAcc.


    "Acc"[3](#page-6-2) . The compiler first scans the .proto file, recording the
    structures of each message and attribute of each field. Then the compiler generates
    a header file and a schema definition based on the scanned results.


    The header file mainly consists of generated RPC message classes including de/serialization
    functions and three unique member functions (i.e., "moveToAcc", "moveToCPU", and
    "isInAcc") for each dereference field (Table [III\)](#page-6-3). "isInAcc" checks
    whether the pointer refers to the accelerator off-chip memory. "moveToAcc" moves
    data from the pointed CPU memory to the accelerator off-chip memory while "move-ToCPU"
    operates reversely.


    The schema definition stores the RPC message structure and the attributes of each
    field. The schema definition is stored in the accelerator "Schema Table". During
    the deserialization, the deserializer selectively puts the deserialized field
    in the host/accelerator memory based on whether the field has an "Acc" label.


    *2) RPCAcc Programming Interface:* We take an RPC-based compression accelerator
    as an example. Listing [1](#page-6-4) shows the pseudo implementation. The RPC
    request message (User) and response message (Photo) are defined in Figure [3.](#page-4-0)
    This application is representative as it involves a lightweight host kernel (authorization)
    and a compute-intensive RPC kernel (compression). The authorization only involves
    lightweight processing and usually has many data dependencies. The compression
    is compute-intensive, easy to parallelize, and does not have data dependencies.


    Next, we show how to realize the application using the provided programming interfaces
    assuming that the compression bit file is ready and has been programmed in the
    CU. When an RPC request arrives, the software first checks whether the user is
    authorized (L1). After authorization, the software checks whether the CU can perform
    compression (L4).


    • If so (L5-10), the application first ensures that the raw avatar data of the
    request is put in the accelerator memory (L5). Otherwise, it moves the data to
    accelerator memory (L6). It


    <span id="page-6-2"></span>3 "Acc" represents the accelerator''s off-chip memory.
    then invokes the compute unit to execute the compression RPC kernel (L8), poll
    the result (L9), and set the size in the RPC response (L10);


    • Otherwise (L13-16), we first ensure that the raw avatar data is in the CPU memory
    (L13). If not, we move the data to the host memory (L14). We then perform CPU
    compression and set the compressed image size (L16).


    Finally, the RPC response is fabricated (L18). RPCAcc allows developers to focus
    on high-level application logic instead of dealing with the RPC layer and host-accelerator
    interactions.


    <span id="page-6-4"></span>


    | Algorithm 1: RPCACC PROGRAMMING EXAMPLE   |  |  |  |  |

    |-------------------------------------------|--|--|--|--|

    | Define : req: RPC request,                |  |  |  |  |

    | res: RPC response,                        |  |  |  |  |

    | cu: compute unit                          |  |  |  |  |

    | 1 if isIdAuthorized(req.id) == false then |  |  |  |  |

    | /* Host kernel, omitted<br>*/             |  |  |  |  |

    | 2 end                                     |  |  |  |  |

    | 3 reqData = req.avatar.image;             |  |  |  |  |

    | 4 if cu.getType() == "compress" then      |  |  |  |  |

    | if reqData.isInAcc() == false then<br>5   |  |  |  |  |

    | reqData.moveToAcc();<br>6                 |  |  |  |  |

    | end<br>7                                  |  |  |  |  |

    | e = cu.submitTask();<br>8                 |  |  |  |  |

    | poll(e);<br>9                             |  |  |  |  |

    | res.size = e.size;<br>10                  |  |  |  |  |

    | 11 end                                    |  |  |  |  |

    | 12 else                                   |  |  |  |  |

    | if reqData.isInAcc() == true then<br>13   |  |  |  |  |

    | reqData.moveToCPU();<br>14                |  |  |  |  |

    | end<br>15                                 |  |  |  |  |

    | res.size = compressOnCPU();<br>16         |  |  |  |  |

    | 17 end                                    |  |  |  |  |

    | 18 return res;                            |  |  |  |  |


    # <span id="page-6-0"></span>*F. Automatic Field Updating*


    At default, developers manually assign "Acc" labels for data fields, indicating
    that they are likely to be used by a compute unit in the accelerator. This approach
    works well for pre-known request traffic because one can profile the data access
    pattern and devise an optional data placement scheme. However, such an assumption
    is no longer held in the cloud setting [\[51\]](#page-13-21), [\[79\]](#page-13-1),
    where computing demand varies continuously. Thus, developers would reprogram the
    computing unit and determine which RPC kernels would benefit the most from hardware
    acceleration, completely breaking the established data partition layout and causing
    unnecessary PCIe traversals across two computing domains.


    We instead propose an automatic field updating mechanism that allows modifying
    message schema at runtime. Specifically, it automatically adds/removes the "Acc"
    label for all dereference fields at runtime when calling "moveToAcc" and "moveToCPU"
    member functions.


    When "moveToAcc" or "moveToCPU" is invoked, not only an MMIO would be issued to
    notify the accelerator to move the field''s content across PCIe, but also the
    corresponding entry in the "Schema Table" would be updated. If "moveToNAcc" is
    called, it indicates that the field should be added with a "Acc" label, while
    "moveToCPU" would remove the "Acc" label of this field. In this way, when the
    next same RPC arrives, the deserializer can use the updated "Schema Table" to
    deserialize RPC messages, thus avoiding redundant data movement within subsequent
    RPC requests.


    Limitation. If a field is needed by both the CPU and the RPCAcc compute unit,
    the field must be fetched over the PCIe bus. This is unavoidable in the current
    design as the CPU and PCIe device are not in a coherent domain. However, we believe
    this is relatively rare as users should try to ensure that the offloaded function
    is a standalone logic and does not reuse the CPU-side data.


    Summary. Automatic field updating poses three benefits. First, it frees users
    from the tedious task of manual and explicit assignment of "Acc" labels in .proto
    file. Second, our .proto file can be completely the same as Protobuf3. As such,
    existing applications can easily adopt RPCAcc without modification of .proto files.
    Third, it can accommodate partial reconfiguration''s dynamic offloading. When
    the RPC kernels change from CPU execution to compute unit execution or reversely,
    the deserializers can place the corresponding fields correctly after one incorrect
    placement.


    #### IV. EVALUATION


    Our evaluations aim to answer the following questions:


    - How effective is the target-aware deserializer (§[IV-B\)](#page-7-0)?

    - How effective is the memory-affinity serializer (§[IV-C\)](#page-8-0)?

    - How does the performance of RPCAcc compare to that of an SoC SmartNIC (§[IV-D\)](#page-8-1),
    Dagger (§[IV-E\)](#page-8-2), and an on-chip accelerator (§[IV-F\)](#page-9-0)?

    - How can automatic field updating address the data layout problem under compute
    reconfiguration (§[IV-G\)](#page-9-1)?

    - How much performance acceleration can RPCAcc achieve for RPC-based workloads
    (§[IV-H\)](#page-10-0)?

    - How many resources do the accelerator use (§[IV-I\)](#page-10-1)?


    # *A. Experimental Setup*


    Hardware Testbed. Our hardware testbed consists of two servers, each having two
    16-core Xeon Silver 4514Y CPUs running at 2.0GHz, 512 GiB (8x64 GiB) 4800 MHz
    DDR5 memory, and a 60 MiB LLC. Each core has a memcpy engine (Intel DSA [\[30\]](#page-12-10),
    [\[41\]](#page-12-11)). Each server is equipped with a Mellanox Dual-port ConnectX-5
    100 Gb NIC (×16) and a Xilinx U280 (×16) [\[73\]](#page-13-22) FPGA which features
    an 8 GiB off-chip high bandwidth memory (HBM).


    CPU Baseline. We implement the baseline "CPU-only" (2K LoCs of C++) that runs
    the entire RPC stack and all computa-


    <span id="page-7-1"></span>![](_page_7_Figure_15.jpeg)


    tions on the CPU. It uses the DPDK-based eRPC [\[33\]](#page-12-1) library and
    adopts Protobuf3 [\[19\]](#page-12-15) as the de/serialization format. It is well-optimized
    under optimization methods such as zero-copy, huge page, kernel-bypass, and polling
    mode driver.


    ProtoACC-OnChip Baseline. As no on-chip RPC accelerator hardware exists, we implement
    the "ProtoACC-OnChip" simulation baseline based on ProtoACC [\[34\]](#page-12-6).
    We mainly modify the frequency to 250 MHz /2 GHz and set the memory access latency
    to 70 nanoseconds (same level as our host CPU memory latency).


    ProtocACC-PCIe Baseline. We implement baseline "ProtocACC-PCIe" (3K LoCs of C++
    and 3.7K LoCs of Chisel3) and prototype it on U280 FPGA hardware. It offloads
    the entire RPC stack and computation kernels to the hardware. The FPGA performs
    the de/serialization logic following ProtoAcc [\[34\]](#page-12-6), the difference
    is that our implementation uses PCIe interconnect. The serialization presedure
    is similar to the "Accelerator-only" approach as introduced in § [III-C.](#page-4-1)
    The RPC protocol is close to eRPC [\[33\]](#page-12-1) and the transport layer
    adopts a modified version of Strom [\[67\]](#page-13-17). The RPC/transport logic
    runs at 250 MHz.


    BF3 Baseline. We implement baseline "BF3" that offloads the RPC layer to SoC SmartNIC
    Bluefield-3 [\[54\]](#page-13-23). The software stack is the same as the CPU baseline.


    RPCAcc Implementation. We prototype RPCAcc in the U280 FPGA (3K LoCs of C++ and
    3.5K LoCs of Chisel3). The RPC protocol is similar to eRPC [\[33\]](#page-12-1).
    The transport layer adopts a modified version of StRoM [\[67\]](#page-13-17).
    It features targetaware deserialization and memory-affinity CPU-Accelerator co-serialization.
    The RPC/transport logic runs at 250 MHz.


    ## <span id="page-7-0"></span>*B. Target-aware Deserializer*


    We first examine the performance of our target-aware deserializer. We set all
    RPC data fields'' destinations to the host memory and compare RPCAcc with the
    conventional field-by-field one. Figure [5](#page-7-1) reports the deserialization
    throughput improvement of "one-shot DMA write" over "field-by-field" when running
    HyperProtoBench [\[34\]](#page-12-6) (including six workloads, each containing
    10 messages) for messages with different average field sizes. On average, RPCAcc
    outperforms the field-by-field solution by 2.2×. For messages with small-sized
    fields (less than 1KB), RPCAcc achieves a higher speedup (3.1×) because the field-by-field
    solution suffers from inefficient DMA that transfers numerous small objects. Instead,
    our one-shot DMA write scheme can combine small DMA writes into one large contiguous
    DMA write, yielding higher PCIe link utilization.


    <span id="page-8-3"></span>![](_page_8_Figure_0.jpeg)


    Fig. 6: Normalized CPU cycles of memory-affinity serializer with/without memcpy
    and encoding offload.


    <span id="page-8-4"></span>![](_page_8_Figure_2.jpeg)


    # <span id="page-8-0"></span>*C. Memory-affinity Serializer*


    We validate the effectiveness of the memory-affinity CPU-Accelerator collaborative
    serialization scheme by measuring the CPU cycle savings and serialization time.
    We use HyperProtoBench [\[34\]](#page-12-6) and five representative microservices
    in DeathStarBench [\[16\]](#page-12-21) as workloads.


    Effect of Encoding/memcpy Offload. Figure [6](#page-8-3) demonstrates the normalized
    host CPU cycles with/without encoding/memcpy offloading for "memory-affinity"
    CPU-Accelerator coserialization. The memcpy offload can reduce the host CPU cycles
    by an average of 55% (23%) in HyperProtoBench (DeathStarBench). Memcpy offload
    together with encoding offload can save the host CPU cycles by an average of 74%
    (74%) in HyperProtoBench (DeathStarBench). RPCAcc greatly reduces the host CPU
    cycles, indicating the effectiveness of "memory-affinity" CPU-Accelerator co-serialization.
    RPCAcc trades a few CPU cycles for the large decrease in the overall serialization
    time. The pre-serialization uses a geometric mean of 22% CPU cycles compared with
    performing serialization in CPUs, while the geometric mean of the overall serialization
    time is decreased by 57%.


    Serialization Performance. We then use HyperProtoBench to measure the end-to-end
    serialization time. We measure the serialization time of three design choices
    in Figure [4.](#page-4-2) As shown in Figure [7,](#page-8-4) "Memory-affinity"
    serialization spends the least time while "CPU-only" spends the most time. "Memoryaffinity"
    outperforms "ProtoACC-PCIe" by 2.3× in geometric mean, because "memory-affinity"
    leverages the CPU or memcpy engine to perform pre-serialization for fields residing
    in the host CPU memory, instead through high-latency PCIe interconnect. "Memory-affinity"
    outperforms "CPU-only" by 4.3× in geometric mean, because pure software implementation
    of serialization incurs significant CPU overhead.


    # <span id="page-8-1"></span>*D. Comparison to SoC SmartNIC/DPU*


    In this section, we evaluate how RPCAcc optimizations can improve the performance
    when offloading RPC to a SoC-based SmartNIC, i.e., Nvidia Bluefield-3. "BF3" naively
    offloads the


    <span id="page-8-6"></span><span id="page-8-5"></span>![](_page_8_Figure_9.jpeg)


    Fig. 8: Serialization time comparison and deserialization speedup of "BF3-Oneshot"
    over "BF3".


    <span id="page-8-7"></span>![](_page_8_Figure_11.jpeg)


    Fig. 9: Serialization time of applying ProtoACC/RPCAcc serialization approach
    to Dagger.


    entire RPC to BF3, "BF3-MemoryAffinity" uses the host CPU to perform a pre-serialization
    process and uses BF3 cores to perform encoding/decoding. "BF3-DSA" is similar
    to "BF3- MemoryAffinity" and leverages the DSA memcpy engines during the host
    pre-serialization. "BF3-Oneshot" offloads the entire RPC to BF3 and coalesces
    small DMA requests into a large request during the deserialization.


    Figure [8a](#page-8-5) shows the normalized serialization time on six benches
    of HyperProtoBench. We have three observations. First, applying pre-serialization
    optimization to the BF3 can reduce the serialization time by 1.58× on average.
    Second, applying memcpy offload optimization in the pre-serialization phase can
    additionally reduce the serialization time by 1.18× on average. The main reason
    for the speedup is that the pre-serialization can greatly reduce the high-latency
    cross-PCIe travesals and the memcpy offload can free host CPUs from copying large
    data fields. These results indicate applying RPCAcc optimizations to SoC-based
    SmartNIC/DPU can effectively reduce the overall serialization time. Third, RPCAcc
    still outperforms well-optimized BF3 implementation, this is mainly because RPCAcc
    uses hardware to perform CPUinefficient encoding, while BF3 does this in the Arm
    cores.


    Figure [8b](#page-8-6) shows the deserialization throughput improvement of "BF3-Oneshot"
    over "BF3". Averagely applying oneshot DMA write optimization to BF3 can improve
    the deserialization throughput by 1.78×, because one-shot DMA write can coalesce
    small DMA writes into a single large DMA write, improving PCIe transaction rate
    utilization. RPCAcc averagely achieves 5.9× higher deserialization throughput
    than "BF3-Oneshot". This is mainly because RPCAcc additionally offloads memory
    management and decoding to hardware.


    The above experiments indicate that RPCAcc optimizations can also works well on
    a SoC-based SmartNIC.


    # <span id="page-8-2"></span>*E. Comparison to Dagger*


    Dagger does not support (de)serialization of structured and nested formats and
    naively adopting a hardware (de)serializer to Dagger would also suffer from long
    latency issue (FPGA''s


    <span id="page-9-2"></span>![](_page_9_Figure_0.jpeg)


    Fig. 10: Time spent on the RPC layer.


    access over UPI incurs 400ns one-way latency, still much higher than CPU''s memory
    access time). However, we can apply the optimizations proposed by RPCAcc to Dagger
    to improve RPC offloading performance for structured and nested RPC messages.
    We perform a cycle-accurate experiment that simulates integrating ProtoACC/RPCAcc
    (de)serialization methods into Dagger, called Dagger-ProtoACC and Dagger-RPCAcc,
    respectively. Both are clocked at 2 GHz.


    Figure [9](#page-8-7) shows the serialization time for two implementations. Dagger-RPCAcc
    averagely reduces the serialization time by 2.9×, because applying RPCAcc serialization
    methods to Dagger can eliminate many cross-UPI traversals, where UPI has slightly
    lower latency than PCIe but still much higher than the normal memory access.


    For deserialization, RPCAcc''s one-shot DMA write mechanism can be easily adopted
    by Dagger to batch data writes within one RPC request, thus improving deserialization
    throughput at the cost of slightly increased deserialization latency. We do not
    present a throughput simulation, because how the PCIe/UPI transaction rate varies
    according to the data size is not known and we do not have real UPI-based hardware.
    However, Dagger paper has an inter-RPC batching mechanism and the authors claim
    that it can help improve data transfer efficiency. As such, we believe the intra-RPC
    batching (one-shot DMA write) can increase Dagger deserialization efficiency.
    Unlike throughput, the deserialization latency can be accurately simulated with
    one-way UPI latency (400ns) provided in the Dagger paper. Our evaluation on HyperProtoBench
    shows that adopting the one-shot DMA will only slightly increase the deserialization
    latency (geometric mean latency increases by 1.048x), which is acceptable considering
    the potential throughput benefits.


    #### <span id="page-9-0"></span>*F. Comparison to On-chip Accelerator*


    In this section, we compare the de/serialization time for RP-CAcc and "ProtoACC-OnChip",
    using the HyperProtoBench. RPCAcc runs on our hardware platform while ''ProtoACC-OnChip"
    reports Xilinx Vivado simulation results as there is no real hardware. For RPCAcc
    at 250 Mhz, we report the measured RPC TX/RX time measured in the real hardware.
    The TX time is measured from when the CPU issues send RPC command to when the
    serialized data enters the NIC transport layer. The RX time is measured from when
    the data leaves the NIC transport layer to when the deserialized data arrives
    at the host CPU memory. For RPCAcc at 2 GHz, we first simulate the time spent
    on the accelerator. Then we manually add a


    <span id="page-9-4"></span><span id="page-9-3"></span>Fig. 11: Per-RPC execution
    time under kernel reconfiguration for the image compression example.


    PCIe transfer time and the time spent on the host CPU, both of which are measured
    when running 250 MHz real RPCAcc hardware. For the on-chip accelerator simulation
    result, we first measure the simulated time spent on the RPC layer. Since the
    on-chip accelerator does not sit in the NIC, we then add an extra traversal time
    between the NIC and CPU memory.


    Figure [10](#page-9-2) shows the time consumed in the RPC layer, in receiving
    path (RX) and transmitting path (TX), respectively. We have two observations.


    First, when clocked at the same frequency, RPCAcc barely increases RX time compared
    to "ProtoACC-OnChip". This is because deserialization does not require pointer-chasing
    memory access, as such our PCIe-based deserializer would not incur performance
    degradation compared with a low-latency on-chip deserializer accelerator.


    Second, given that PCIe''s latency (1250ns) is 17.9× higher than the memory latency
    (70ns) setting of "ProtoACC-OnChip", "ProtoACC-OnChip" only achieves 1.4×/1.24×
    lower TX time on average over RPCAcc when clocked at 250 MHz/2 GHz, respectively.
    This is mainly because RPCAcc enables the CPU pre-serialization to trade fast
    CPU memory copies for slow PCIe access. What''s more, when offloading an RPC kernel
    in RPCAcc, skipping accelerator fields can effectively reduce the TX time, further
    narrowing the serialization time gap.


    In sum, RPCAcc enables a PCIe-attached accelerator to achieve nearly the same
    deserialization performance and close serialization performance, compared to an
    on-chip de/serialization accelerator.


    # <span id="page-9-1"></span>*G. Automatic Field Updating*


    In this section, we evaluate automatic field updating using the image compression
    example in Listing [1.](#page-6-4) The accelerator is configured with one compression
    compute unit, which is reconfigured to an unavailable state at runtime. It simulates
    the scenario when other applications have preempted the compute unit. When the
    compute unit is unavailable, the compression would switch to host CPU execution.


    At the beginning of Figure [11a,](#page-9-3) the large data field is put at accelerator
    memory and the compression is originally performed at the accelerator. After the
    3rd request finishes, we set the compute unit as unavailable for the image compression
    service. The 4th request would suffer high execution latency since the large "image"
    field is put at the accelerator memory


    <span id="page-10-3"></span><span id="page-10-2"></span>![](_page_10_Figure_0.jpeg)


    Fig. 12: Performanc comparison of three approaches when running an RPC-based image
    compression service.


    after serialization, CPU software has to manually move this field to CPU memory
    before performing compression on the CPU. Without automatic field updating, the
    execution time would remain high. With automatic field updating, all following
    requests'' execution time would drop by several microseconds, because the explicit
    movement of the "image" field would update the schema table in the accelerator.
    This lets the deserializer put this field into the CPU memory next time, avoiding
    the CPU''s explicit memory movement.


    Similarly, Figure [11b](#page-9-4) shows the situation that the CU is unavailable
    at the beginning and is available until the 4th request. With automatic field
    updating, the deserialization module can adapt to the dynamic change of compute
    units and put the field in the correct memory (CPU or accelerator memory). Besides,
    it eliminates the need for users to manually specify where fields should be placed
    after deserialization, saving a lot of hassle. As such, the automatic field updating
    mechanism yields high programmability.


    #### <span id="page-10-0"></span>*H. End-to-end Application Performance*


    We evaluate RPCAcc using a practical cloud workload, which provides a high-performance
    and secure compression service. Our workload mainly comprises three tasks for
    each RPC request: request authorization, compute-intensive compression, and encryption/decryption.
    For the "CPU-only" baseline, all three tasks run on the host CPU. For "ProtoACC-PCIe"
    and RPCAcc, compression and encryption/decryption run on the accelerator hardware,
    while the request authorization task is still conducted on the host CPU. The request
    authorization task is not offloaded because it usually only involves lightweight
    computation and changes frequently Both "ProtoACC-PCIe" and RPCAcc are able to
    process the offloaded tasks at line-rate (100 Gbps).


    Figure [12a](#page-10-2) shows the achieved throughput using the different numbers
    of host CPU cores. We observe that RPCAcc''s achievable throughput is 2.6× higher
    than the "ProtoACC-PCIe" baseline and 31.8 × higher than the "CPU-only" baseline.
    "CPU-only" performs worst because running computeintensive compression, encryption/decryption,
    and RPC stack in the software is very inefficient compared with offloading them
    to hardware. RPCAcc outperforms "ProtoACC-PCIe" mainly because RPCAcc effectively
    offloads the RPC stack to a PCIe-attached accelerator with the proposed schemes.
    We also observe that skipping accelerator fields in the host CPU pre-serialization
    can save 65% CPU cycles. This is because RPCAcc allows the KB-level large field
    to always reside in


    <span id="page-10-5"></span><span id="page-10-4"></span>![](_page_10_Figure_7.jpeg)


    the accelerator memory and the hardware is responsible for its serialization.


    Figures [12b](#page-10-3) shows the average latency of three implementations.
    We observe that RPCAcc can achieve 2.6 × (9.6 ×) lower average latency than the
    "ProtoACC-PCIe" ("CPU-only") solution under the same throughput. RPCAcc outperforms
    the "ProtoACC-PCIe" baseline mainly because target-aware deserialization avoids
    much redundant data movement and memory-affinity serialization can greatly reduce
    the serialization time over the high-latency PCIe interconnect.


    To prove that RPCAcc can fit small-size RPCs, we perform an end-to-end comparison
    of five representative services (UniqueId, User, UrlShorten, SocialGraph, and
    ComposePost) in the widely-used DeathStarBench microservice suit. Figure [13](#page-10-4)
    shows the end-to-end execution time. We observe that the geometric mean execution
    time of RPCAcc is 1.57×/1.34× lower than the software baseline "CPU-only"/"ProtoACC-PCIe".
    This indicates that RPCAcc can also accelerate RPCs with a small message size.


    #### <span id="page-10-1"></span>*I. Hardware Resource Usage*


    Table [IV](#page-10-5) shows the FPGA resource consumption of "ProtoACC-PCIe"
    and RPCAcc. The resources of the offloaded RPC kernel are not reported as it is
    applicationspecific. RPCAcc is resource-frugal thanks to the compacted hardware
    data structures (schema table and temp buffer) and the streamlined serialization
    and serialization process.


    # V. DISCUSSION


    How to accomodate differnt/evloving formats. Currently, RPCAcc focuses on the
    widely used Protobuf format. But it is relatively easy to extend RPCAcc to support
    other formats such as Thrift [\[5\]](#page-12-16). In the following, we present
    two modifications. From the hardware perspective, we mainly need to modify the
    deserializer/serializer module and add transformation logic for Thrift fields.
    From the software perspective, we need to modify the compiler, adding parsing
    logic for ".thrift" files (which is similar to ".proto" files in Protobuf). (De)serialization-free
    formats. Formats [\[22\]](#page-12-22), [\[36\]](#page-12-23) such as Cap''n Proto
    [\[36\]](#page-12-23) are proposed to avoid the (de)serialization overheads during
    runtime. These formats sacrifice object mutability. However, the RPC message size
    is usually not known when it is created, so they need to allocate a large fixed
    buffer for each message which wastes memory space. Besides, the zero elements
    or unset fields would still occupy the space in the wire, incurring a larger transfer
    size than traditional formats like protobuf. To avoid these limitations, these
    formats also need additional designs to balance the (de)serialization overhead
    and waste of memory space/transfer size. Take Cap''n Proto as an example, to avoid
    memory waste, it allows a message to be split across multiple non-contiguous memory
    segments. Users can dynamically allocate more segments and use inter-segment pointers
    to link these segments together. To reduce the transfer size, Cap''n Proto adopts
    an operation called packing to compress these zero bytes in the wire format in
    serialization and unpack these zeros in deserialization. The packing/unpacking
    operations involve many bit-wise/bytewise operations, and these CPU-inefficient
    operations incur similar overheads to that of encoding/decoding in traditional
    formats. We believe RPCAcc''s optimizations can mitigate the inefficiency introduced
    by the multiple segments and CPUinefficient packing/unpacking. During serialization,
    we could add a CPU pre-process that copies incontiguous segments into a contiguous
    buffer, and the copy of large segments can be offloaded to CPU''s on-chip memcpy
    engines, while the packing can be later executed in the NIC hardware. During deserialization,
    we could refer to the key idea of RPCAcc''s one-shot DMA write and decoding offloading.
    The unpacking operations can be offloaded to the NIC hardware and the DMA writes
    of different segments of one RPC message can be batched together to improve DMA
    transfer efficiency. In summary, it is easy to generalize RPCAcc to other formats.


    CXL. RPCAcc''s idea still works well on top of a coherent fabric like CXL. Although
    the CXL coherence allows the host to access the accelerator memory using load/store
    (or reversely), the latency is still several times that of local memory access.
    We believe the key ideas of RPCAcc still hold: 1) putting the deserialized fields
    accordingly and letting CPU/Accelerator access their local memory as much as possible
    during the RPC process; 2) doing intra-RPC batching during the deserialization
    to improve transfer efficiency; 3) letting CPU/Accelerator serialize the fields
    in their local memory; 4) offloading bitwise/byte-wise decoding/encoding and memory
    management to hardware. A coherent fabric like CXL will enhance RPCAcc in two
    main aspects. First, we can replace the costly MMIObased mechanism with coherent
    memory access to implement the CPU-NIC interface. As such, the transaction rate
    for small RPC requests would not be bottlenecked by the low MMIO throughput. Second,
    it can avoid explicit cross-PCIe data movement at runtime. Our current implementation
    has to move the field explicitly, when the deserialized field is not in proper
    memory (CPU memory or accelerator memory). With CXL, both the CPU and accelerator
    can access the memory of each other using load/store instructions. CXL will enhance
    RPCAcc in two main aspects. First, we can replace the costly MMIO-based mechanism
    with coherent memory access to implement the CPU-NIC interface. As such, the transaction
    rate for small RPC requests would not be bottlenecked by low MMIO throughput.
    Second, it can avoid explicit cross-PCIe data movement at runtime. In the current
    implementation, when the deserialized field is not in proper memory (CPU memory
    or accelerator memory), the users have to move the field explicitly. With CXL,
    both the CPU and accelerator can access the memory of each other using load/store
    instructions.


    # VI. RELATED WORKS


    Software-based RPC Acceleration. Cornflakes [\[61\]](#page-13-24) leverages the
    scatter-gather capability to let NIC directly read the noncontiguous data from
    the host memory during the serialization. However, it requires that the data resides
    in the pinned DMAsafe region, which greatly harms memory utilization, especially
    in the cloud scenario. Besides, its serialization format does not contain encoding
    and is not compatible with existing applications.


    Hardware-based RPC Acceleration. Prior works [\[32\]](#page-12-5), [\[34\]](#page-12-6),
    [\[42\]](#page-12-2), [\[59\]](#page-13-8), [\[60\]](#page-13-9) offload the RPC
    stack or de/serialization to hardware to alleviate the CPU pressure. Cereal [\[32\]](#page-12-5)
    adopts a special memory access interface to provide low-latency host memory access
    for the accelerator. Optimus Prime [\[59\]](#page-13-8) and Cerebros [\[60\]](#page-13-9)
    place an on-chip accelerator for de/serialization (entire RPC stack). ProtoACC
    [\[34\]](#page-12-6) proposes a novel nearcore hardware accelerator for Protobuf.
    In contrast, RPCAcc focuses on optimizing RPC in a PCIe-attached accelerator,
    which is much more widely used in the modern cloud.


    New Network Architecture. The nanoPU [\[28\]](#page-12-24) is a new NIC-CPU co-design
    for RPC acceleration. It adds a fast path from the NIC directly to the CPU register
    file to achieve ultra-low latency packet access (∼70ns). RAMBDA [\[75\]](#page-13-25)
    uses RDMA NICs and a standalone cache-coherent accelerator to accelerate data
    center applications. RPCAcc focus on the accelerations of RPC and de/serialization.
    NetDIMM [\[3\]](#page-12-25) integrates a fullblown NIC into the buffer device
    of a DIMM for fast data remote data access. FlexDriver [\[12\]](#page-12-26) allows
    the accelerator to control the NIC execution directly for high scalability.


    Offloading to SmartNIC. Many prior works [\[6\]](#page-12-27)–[\[9\]](#page-12-28),
    [\[13\]](#page-12-29), [\[15\]](#page-12-30), [\[24\]](#page-12-31), [\[24\]](#page-12-31)–[\[27\]](#page-12-32),
    [\[37\]](#page-12-33), [\[43\]](#page-12-34), [\[46\]](#page-13-26)–[\[49\]](#page-13-27),
    [\[52\]](#page-13-28), [\[56\]](#page-13-29), [\[58\]](#page-13-30), [\[64\]](#page-13-31),
    [\[66\]](#page-13-32), [\[69\]](#page-13-33), [\[70\]](#page-13-34), [\[79\]](#page-13-1)
    offload host tasks to FPGA or SoC SmartNICs to alleviate the host CPU pressure.
    None of these works tackle the problem of RPC tax. In contrast, RPCAcc offloads
    the RPC stack and computing kernels.


    Header-payload Data Split. Researchers have studied the header-payload split extensively
    [\[2\]](#page-12-35), [\[14\]](#page-12-36), [\[23\]](#page-12-37), [\[40\]](#page-12-38),
    [\[57\]](#page-13-35), [\[63\]](#page-13-36). IDIO [\[2\]](#page-12-35) selectively
    disables Direct Cache Access (DCA) for the payload of received packets while always
    keeping DCA enabled for packet headers. SplitRPC [\[40\]](#page-12-38) splits
    data using a fixed offset without de/serialization. In contrast, RPCAcc splits
    deserialized RPC messages according to RPC''s fields and forwards them to either
    host CPU or Accelerator memory.


    # VII. CONCLUSION


    This paper presents RPCAcc, a hardware-software codesigned accelerator for reconfigurable
    RPC offloading. To tackle the ramifications of introducing PCIe, RPCAcc introduces
    three techniques: a target-aware deserializer, a memoryaffinity CPU-accelerator
    collaborative serializer, and a runtime automatic field updating scheme. RPCAcc
    is an immediately deployed solution and provides the software abstraction to load
    the RPC stack and compute kernels on demand.


    Acknowledgement. The work is supported by the following grants: the National Key
    R&D Program of China (Grant No. 2022ZD0119301), the National Natural Science Foundation
    of China under the grant number (62472384), the Fundamental Research Funds for
    the Central Universities.


    #### REFERENCES


    - <span id="page-12-0"></span>[1] grpc. [https://grpc.io/,](https://grpc.io/)
    2022.

    - <span id="page-12-35"></span>[2] Mohammad Alian, Siddharth Agarwal, Jongmin
    Shin, Neel Patel, Yifan Yuan, Daehoon Kim, Ren Wang, and Nam Sung Kim. Idio: Networkdriven,
    inbound network data orchestration on server processors. In *MICRO*, 2022.

    - <span id="page-12-25"></span>[3] Mohammad Alian and Nam Sung Kim. Netdimm: Low-latency
    nearmemory network interface architecture. In *MICRO*, 2019.

    - <span id="page-12-19"></span>[4] Amazon. AWS Nitro System. [https://aws.amazon.com/cn/ec2/nitro/,](https://aws.amazon.com/cn/ec2/nitro/)
    2023.

    - <span id="page-12-16"></span>[5] Apache. Apache Thrift. [https://thrift.apache.org/,](https://thrift.apache.org/)
    2021.

    - <span id="page-12-27"></span>[6] Marco Bonola, Giacomo Belocchi, Angelo Tulumello,
    Marco Spaziani Brunella, Giuseppe Siracusano, Giuseppe Bianchi, and Roberto Bifulco.
    Faster software packet processing on fpga nics with ebpf program warping. In *ATC*,
    2022.

    - [7] Marco Spaziani Brunella, Giacomo Belocchi, Marco Bonola, Salvatore Pontarelli,
    Giuseppe Siracusano, Giuseppe Bianchi, Aniello Cammarano, Alessandro Palumbo,
    Luca Petrucci, and Roberto Bifulco. hxdp: Efficient software packet processing
    on fpga nics. *Communications of the ACM*, 2022.

    - [8] Xinyi Chen, Liangcheng Yu, Vincent Liu, and Qizhen Zhang. Cowbird: Freeing
    cpus to compute by offloading the disaggregation of memory. In *SIGCOMM*, 2023.

    - <span id="page-12-28"></span>[9] Sean Choi, Muhammad Shahbaz, Balaji Prabhakar,
    and Mendel Rosenblum. λ-nic: Interactive serverless compute on programmable smartnics.
    In *ICDCS*, 2020.

    - <span id="page-12-8"></span>[10] David Cock, Abishek Ramdas, Daniel Schwyn,
    Michael Giardino, Adam Turowski, Zhenhao He, Nora Hossle, Dario Korolija, Melissa
    Licciardello, Kristina Martsenko, et al. Enzian: an open, general, cpu/fpga platform
    for systems software research. In *ASPLOS*, 2022.

    - <span id="page-12-9"></span>[11] CXL Consortium. CXL Specification. [https://computeexpresslink.org/](https://computeexpresslink.org/cxl-specification/)
    [cxl-specification/,](https://computeexpresslink.org/cxl-specification/) 2024.

    - <span id="page-12-26"></span>[12] Haggai Eran, Maxim Fudim, Gabi Malka, Gal
    Shalom, Noam Cohen, Amit Hermony, Dotan Levi, Liran Liss, and Mark Silberstein.
    Flexdriver: A network driver for your accelerator. In *ASPLOS*, 2022.

    - <span id="page-12-29"></span>[13] Haggai Eran, Lior Zeno, Maroun Tork, Gabi
    Malka, and Mark Silberstein. Nica: An infrastructure for inline acceleration of
    network applications. In *ATC*, 2019.

    - <span id="page-12-36"></span>[14] Alireza Farshin, Amir Roozbeh, Gerald Q Maguire
    Jr, and Dejan Kostic.´ Make the most out of last level cache in intel processors.
    In *EuroSys*, 2019.

    - <span id="page-12-30"></span>[15] Daniel Firestone, Andrew Putnam, Sambhrama
    Mundkur, Derek Chiou, Alireza Dabagh, Mike Andrewartha, Hari Angepat, Vivek Bhanu,
    Adrian Caulfield, Eric Chung, Harish Kumar Chandrappa, Somesh Chaturmohta, Matt
    Humphrey, Jack Lavier, Norman Lam, Fengfen Liu, Kalin Ovtcharov, Jitu Padhye,
    Gautham Popuri, Shachar Raindel, Tejas Sapre, Mark Shaw, Gabriel Silva, Madhan
    Sivakumar, Nisheeth Srivastava, Anshuman Verma, Qasim Zuhair, Deepak Bansal, Doug
    Burger, Kushagra Vaid, David A. Maltz, and Albert Greenberg. Azure accelerated
    networking:smartnics in the public cloud. In *NSDI*, 2018.

    - <span id="page-12-21"></span>[16] Yu Gan, Yanqi Zhang, Dailun Cheng, Ankitha
    Shetty, Priyal Rathi, Nayan Katarki, Ariana Bruno, Justin Hu, Brian Ritchken,
    Brendon Jackson, Kelvin Hu, Meghna Pancholi, Yuan He, Brett Clancy, Chris Colen,
    Fukang Wen, Catherine Leung, Siyuan Wang, Leon Zaruvinsky, Mateo Espinosa, Rick
    Lin, Zhongling Liu, Jake Padilla, and Christina Delimitrou. An open-source benchmark
    suite for microservices and their hardware-software implications for cloud & edge
    systems. In *ASPLOS*, 2019.

    - <span id="page-12-3"></span>[17] Yixiao Gao, Qiang Li, Lingbo Tang, Yongqing
    Xi, Pengcheng Zhang, Wenwen Peng, Bo Li, Yaohui Wu, Shaozong Liu, Lei Yan, Fei
    Feng, Yan Zhuang, Fan Liu, Pan Liu, Xingkui Liu, Zhongjie Wu, Junping Wu, Zheng
    Cao, Chen Tian, Jinbo Wu, Jiaji Zhu, Haiyong Wang, Dennis Cai, and Jiesheng Wu.
    When cloud storage meets rdma. In *NSDI*, 2021.

    - <span id="page-12-13"></span>[18] Abraham Gonzalez, Aasheesh Kolli, Samira Khan,
    Sihang Liu, Vidushi Dadu, Sagar Karandikar, Jichuan Chang, Krste Asanovic, and
    Parthasarathy Ranganathan. Profiling hyperscale big data processing. In *ISCA*,
    2023.

    - <span id="page-12-15"></span>[19] Google. Protocol Buffers Documentation. [https://protobuf.dev/,](https://protobuf.dev/)
    2023.

    - <span id="page-12-17"></span>[20] Google. Protocol Buffers Encoding. [https://protobuf.dev/programming](https://protobuf.dev/programming-guides/encoding/)[guides/encoding/,](https://protobuf.dev/programming-guides/encoding/)
    2023.

    - <span id="page-12-18"></span>[21] Google. Falcon: A Reliable and Low Latency
    Hardware Transport. [https://netdevconf.info/0x18/docs/netdev-0x18-paper43-talk](https://netdevconf.info/0x18/docs/netdev-0x18-paper43-talk-slides/Introduction%20to%20Falcon%20Reliable%20Transport.pdf)[slides/Introduction%20to%20Falcon%20Reliable%20Transport.pdf,](https://netdevconf.info/0x18/docs/netdev-0x18-paper43-talk-slides/Introduction%20to%20Falcon%20Reliable%20Transport.pdf)
    2024.

    - <span id="page-12-22"></span>[22] Google. FlatBuffers. [https://github.com/google/flatbuffers,](https://github.com/google/flatbuffers)
    2024.

    - <span id="page-12-37"></span>[23] Swati Goswami, Nodir Kodirov, Craig Mustard,
    Ivan Beschastnikh, and Margo Seltzer. Parking packet payload with p4. In *CoNEXT*,
    2020.

    - <span id="page-12-31"></span>[24] Stewart Grant, Anil Yelam, Maxwell Bland,
    and Alex C Snoeren. Smartnic performance isolation with fairnic: Programmable
    networking for the cloud. In *SIGCOMM*, 2020.

    - [25] Zerui Guo, Jiaxin Lin, Yuebin Bai, Daehyeok Kim, Michael Swift, Aditya
    Akella, and Ming Liu. Lognic: A high-level performance model for smartnics. 2023.

    - [26] Zerui Guo, Hua Zhang, Chenxingyu Zhao, Yuebin Bai, Michael Swift, and Ming
    Liu. Leed: A low-power, fast persistent key-value store on smartnic jbofs. In
    *SIGCOMM*, 2023.

    - <span id="page-12-32"></span>[27] Zhiyuan Guo, Yizhou Shan, Xuhao Luo, Yutong
    Huang, and Yiying Zhang. Clio: A hardware-software co-designed disaggregated memory
    system. In *ASPLOS*, 2022.

    - <span id="page-12-24"></span>[28] Stephen Ibanez, Alex Mallery, Serhat Arslan,
    Theo Jepsen, Muhammad Shahbaz, Changhoon Kim, and Nick McKeown. The nanopu: A
    nanosecond network stack for datacenters. In *OSDI*, 2021.

    - <span id="page-12-12"></span>[29] Intel. Intel Products formerly Sapphire Rapids.
    [https://ark.intel.com/](https://ark.intel.com/content/www/us/en/ark/products/codename/126212/products-formerly-sapphire-rapids.html
    ) [content/www/us/en/ark/products/codename/126212/products-formerly](https://ark.intel.com/content/www/us/en/ark/products/codename/126212/products-formerly-sapphire-rapids.html
    )[sapphire-rapids.html,](https://ark.intel.com/content/www/us/en/ark/products/codename/126212/products-formerly-sapphire-rapids.html
    ) 2024.

    - <span id="page-12-10"></span>[30] Intel. Intel® Data Streaming Accelerator.
    [https://www.intel.com/](https://www.intel.com/content/www/us/en/products/docs/accelerator-engines/data-streaming-accelerator.html
    ) [content/www/us/en/products/docs/accelerator-engines/data-streaming](https://www.intel.com/content/www/us/en/products/docs/accelerator-engines/data-streaming-accelerator.html
    )[accelerator.html,](https://www.intel.com/content/www/us/en/products/docs/accelerator-engines/data-streaming-accelerator.html
    ) 2024.

    - <span id="page-12-7"></span>[31] Intel. Intel® Ultra Path Interconnect. [https://www.intel.com/content/](https://www.intel.com/content/www/us/en/silicon-innovations/6-pillars/interconnect.html)
    [www/us/en/silicon-innovations/6-pillars/interconnect.html,](https://www.intel.com/content/www/us/en/silicon-innovations/6-pillars/interconnect.html)
    2024.

    - <span id="page-12-5"></span>[32] Jaeyoung Jang, Sung Jun Jung, Sunmin Jeong,
    Jun Heo, Hoon Shin, Tae Jun Ham, and Jae W Lee. A specialized architecture for
    object serialization with applications to big data analytics. In *ISCA*, 2020.

    - <span id="page-12-1"></span>[33] Anuj Kalia, Michael Kaminsky, and David G Andersen.
    Datacenter rpcs can be general and fast. *NSDI*, 2019.

    - <span id="page-12-6"></span>[34] Sagar Karandikar, Chris Leary, Chris Kennelly,
    Jerry Zhao, Dinesh Parimi, Borivoje Nikolic, Krste Asanovic, and Parthasarathy
    Ranganathan. A hardware accelerator for protocol buffers. In *MICRO*, 2021.

    - <span id="page-12-14"></span>[35] Sagar Karandikar, Aniruddha N Udipi, Junsun
    Choi, Joonho Whangbo, Jerry Zhao, Svilen Kanev, Edwin Lim, Jyrki Alakuijala, Vrishab
    Madduri, Yakun Sophia Shao, Borivoje Nikolic, Krste Asanovic, and Parthasarathy
    Ranganathan. Cdpu: Co-designing compression and decompression processing units
    for hyperscale systems. In *ISCA*, 2023.

    - <span id="page-12-23"></span>[36] Kenton Varda. Cap''n Proto. [https://capnproto.org/,](https://capnproto.org/)
    2024.

    - <span id="page-12-33"></span>[37] Mikhail Khalilov, Salvatore Di Girolamo, Marcin
    Chrapek, Rami Nudelman, Gil Bloch, and Torsten Hoefler. Network-offloaded bandwidthoptimal
    broadcast and allgather for distributed ai. In *SC*, 2024.

    - <span id="page-12-20"></span>[38] Ahmed Khawaja, Joshua Landgraf, Rohith Prakash,
    Michael Wei, Eric Schkufza, and Christopher J Rossbach. Sharing, protection, and
    compatibility for reconfigurable fabric with amorphos. In *OSDI*, 2018.

    - <span id="page-12-4"></span>[39] Jongyul Kim, Insu Jang, Waleed Reda, Jaeseong
    Im, Marco Canini, Dejan Kostic, Youngjin Kwon, Simon Peter, and Emmett Witchel.
    ´ Linefs: Efficient smartnic offload of a distributed file system with pipeline
    parallelism. In *SOSP*, 2021.

    - <span id="page-12-38"></span>[40] Adithya Kumar, Anand Sivasubramaniam, and
    Timothy Zhu. Splitrpc: A control+ data path splitting rpc stack for ml inference
    serving. *POMACS*, 2023.

    - <span id="page-12-11"></span>[41] Reese Kuper, Ipoom Jeong, Yifan Yuan, Ren
    Wang, Narayan Ranganathan, Nikhil Rao, Jiayu Hu, Sanjay Kumar, Philip Lantz, and
    Nam Sung Kim. A quantitative analysis and guidelines of data streaming accelerator
    in modern intel xeon scalable processors. In *ASPLOS*, 2024.

    - <span id="page-12-2"></span>[42] Nikita Lazarev, Shaojie Xiang, Neil Adit, Zhiru
    Zhang, and Christina Delimitrou. Dagger: efficient and fast rpcs in cloud microservices
    with near-memory reconfigurable nics. In *ASPLOS*, 2021.

    - <span id="page-12-34"></span>[43] Bojie Li, Kun Tan, Layong Luo, Yanqing Peng,
    Renqian Luo, Ningyi Xu, Yongqiang Xiong, Peng Cheng, and Enhong Chen. Clicknp:
    Highly


    flexible and high performance network processing with reconfigurable hardware.
    In *SIGCOMM*, 2016.


    - <span id="page-13-10"></span>[44] Huaicheng Li, Daniel S Berger, Lisa Hsu, Daniel
    Ernst, Pantea Zardoshti, Stanko Novakovic, Monish Shah, Samir Rajadnya, Scott
    Lee, Ishwar Agarwal, et al. Pond: Cxl-based memory pooling systems for cloud platforms.
    In *ASPLOS*, 2023.

    - <span id="page-13-6"></span>[45] Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar,
    Pieter Noordhuis, Teng Li, Adam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania,
    et al. Pytorch distributed: Experiences on accelerating data parallel training.
    *arXiv preprint arXiv:2006.15704*, 2020.

    - <span id="page-13-26"></span>[46] Yunkun Liao, Jingya Wu, Wenyan Lu, Xiaowei
    Li, and Guihai Yan. Dpu-direct: Unleashing remote accelerators via enhanced rdma
    for disaggregated datacenters. *TC*, 2024.

    - [47] Jiaxin Lin, Kiran Patel, Brent E Stephens, Anirudh Sivaraman, and Aditya
    Akella. Panic: A high-performance programmable nic for multitenant networks. In
    *OSDI*, 2020.

    - [48] Ming Liu, Tianyi Cui, Henry Schuh, Arvind Krishnamurthy, Simon Peter, and
    Karan Gupta. Offloading distributed applications onto smartnics using ipipe. In
    *SIGCOMM*. 2019.

    - <span id="page-13-27"></span>[49] Ming Liu, Simon Peter, Arvind Krishnamurthy,
    and Phitchaya Mangpo Phothilimthana. E3:energy-efficient microservices on smartnicaccelerated
    servers. In *ATC*, 2019.

    - <span id="page-13-12"></span>[50] Andrea Lottarini, Alex Ramirez, Joel Coburn,
    Martha A Kim, Parthasarathy Ranganathan, Daniel Stodolsky, and Mark Wachsler.
    vbench: Benchmarking video transcoding in the cloud. In *ASPLOS*, 2018.

    - <span id="page-13-21"></span>[51] Rui Miao, Lingjun Zhu, Shu Ma, Kun Qian, Shujun
    Zhuang, Bo Li, Shuguang Cheng, Jiaqi Gao, Yan Zhuang, Pengcheng Zhang, Rong Liu,
    Chao Shi, Binzhang Fu, Jiaji Zhu, Jiesheng Wu, Dennis Cai, and Hongqiang Harry
    Liu. From luna to solar: the evolutions of the computeto-storage networks in alibaba
    cloud. In *SIGCOMM*, 2022.

    - <span id="page-13-28"></span>[52] Jaehong Min, Ming Liu, Tapan Chugh, Chenxingyu
    Zhao, Andrew Wei, In Hwan Doh, and Arvind Krishnamurthy. Gimbal: enabling multi-tenant
    storage disaggregation on smartnic jbofs. In *SIGCOMM*, 2021.

    - <span id="page-13-7"></span>[53] Philipp Moritz, Robert Nishihara, Stephanie
    Wang, Alexey Tumanov, Richard Liaw, Eric Liang, Melih Elibol, Zongheng Yang, William
    Paul, Michael I Jordan, et al. Ray: A distributed framework for emerging {AI}
    applications. In *OSDI*, 2018.

    - <span id="page-13-23"></span>[54] Nvidia. NVIDIA BLUEFIELD-3 DPU. [https://www.nvidia.com/](https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/documents/datasheet-nvidia-bluefield-3-dpu.pdf)
    [content/dam/en-zz/Solutions/Data-Center/documents/datasheet-nvidia](https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/documents/datasheet-nvidia-bluefield-3-dpu.pdf)[bluefield-3-dpu.pdf,](https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/documents/datasheet-nvidia-bluefield-3-dpu.pdf)
    2022.

    - <span id="page-13-16"></span>[55] Tian Pan, Kun Liu, Xionglie Wei, Yisong Qiao,
    Jun Hu, Zhiguo Li, Jun Liang, Tiesheng Cheng, Wenqiang Su, Jie Lu, et al. Luoshen:
    A hyper-converged programmable gateway for multi-tenant multi-service edge clouds.
    In *NSDI*, 2024.

    - <span id="page-13-29"></span>[56] Phitchaya Mangpo Phothilimthana, Ming Liu,
    Antoine Kaufmann, Simon Peter, Rastislav Bodik, and Thomas Anderson. Floem: A
    programming system for nic-accelerated network applications. In *OSDI*, 2018.

    - <span id="page-13-35"></span>[57] Boris Pismenny, Liran Liss, Adam Morrison,
    and Dan Tsafrir. The benefits of general-purpose on-nic memory. In *ASPLOS*, 2022.

    - <span id="page-13-30"></span>[58] Salvatore Pontarelli, Roberto Bifulco, Marco
    Bonola, Carmelo Cascone, Marco Spaziani, Valerio Bruschi, Davide Sanvito, Giuseppe
    Siracusano, Antonio Capone, Michio Honda, Felipe Huici, and Giuseppe Bianchi.
    Flowblaze: Stateful packet processing in hardware. In *NSDI*, 2019.

    - <span id="page-13-8"></span>[59] Arash Pourhabibi, Siddharth Gupta, Hussein
    Kassir, Mark Sutherland, Zilu Tian, Mario Paulo Drumond, Babak Falsafi, and Christoph
    Koch. Optimus prime: Accelerating data transformation in servers. In *ASPLOS*,
    2020.

    - <span id="page-13-9"></span>[60] Arash Pourhabibi, Mark Sutherland, Alexandros
    Daglis, and Babak Falsafi. Cerebros: Evading the rpc tax in datacenters. In *MICRO*,
    2021.

    - <span id="page-13-24"></span>[61] Deepti Raghavan, Shreya Ravi, Gina Yuan, Pratiksha
    Thaker, Sanjari Srivastava, Micah Murray, Pedro Henrique Penna, Amy Ousterhout,
    Philip Levis, Matei Zaharia, and Irene Zhang. Cornflakes: Zero-copy serialization
    for microsecond-scale networking. 2023.

    - <span id="page-13-13"></span>[62] Parthasarathy Ranganathan, Daniel Stodolsky,
    Jeff Calow, Jeremy Dorfman, Marisabel Guevara, Clinton Wills Smullen IV, Aki Kuusela,
    Raghu Balasubramanian, Sandeep Bhatia, Prakash Chauhan, Anna Cheung, In Suk Chong,
    Niranjani Dasharathi, Jia Feng, Brian Fosco, Samuel Foss, Ben Gelb, Sara J. Gwin,
    Yoshiaki Hase, Da-ke He, C. Richard Ho, Roy W. Huffman Jr., Elisha Indupalli,
    Indira Jayaram, Poonacha Kongetira, Cho Mon Kyaw, Aaron Laursen, Yuan Li, Fong
    Lou, Kyle A. Lucke, JP Maaninen, Ramon Macias, Maire Mahony, David Alexander Munday,
    Srikanth Muroor, Narayana Penukonda, Eric Perkins-Argueta, Devin Persaud, Alex
    Ramirez, Ville-Mikko Rautio, Yolanda Ripley,


    Amir Salek, Sathish Sekar, Sergey N. Sokolov, Rob Springer, Don Stark, Mercedes
    Tan, Mark S. Wachsler, Andrew C. Walton, David A. Wickeraad, Alvin Wijaya, and
    Hon Kwan Wu. Warehouse-scale video acceleration: co-design and deployment in the
    wild. In *ASPLOS*, 2021.


    - <span id="page-13-36"></span>[63] Anirudh Sarma, Hamed Seyedroudbari, Harshit
    Gupta, Umakishore Ramachandran, and Alexandros Daglis. Nfslicer: Data movement
    optimization for shallow network functions. *arXiv preprint arXiv:2203.02585*,
    2022.

    - <span id="page-13-31"></span>[64] Henry N Schuh, Weihao Liang, Ming Liu, Jacob
    Nelson, and Arvind Krishnamurthy. Xenic: Smartnic-accelerated distributed transactions.
    In *ASPLOS*, 2021.

    - <span id="page-13-0"></span>[65] Korakit Seemakhupt, Brent E Stephens, Samira
    Khan, Sihang Liu, Hassan Wassel, Soheil Hassas Yeganeh, Alex C Snoeren, Arvind
    Krishnamurthy, David E Culler, and Henry M Levy. A cloud-scale characterization
    of remote procedure calls. In *SOSP*, 2023.

    - <span id="page-13-32"></span>[66] Hamed Seyedroudbari, Srikar Vanavasam, and
    Alexandros Daglis. Turbo: Smartnic-enabled dynamic load balancing of µs-scale
    rpcs. In *HPCA*, 2023.

    - <span id="page-13-17"></span>[67] David Sidler, Zeke Wang, Monica Chiosa, Amit
    Kulkarni, and Gustavo Alonso. Strom: smart remote memory. In *EuroSys*, 2020.

    - <span id="page-13-11"></span>[68] Yan Sun, Yifan Yuan, Zeduo Yu, Reese Kuper,
    Chihun Song, Jinghan Huang, Houxiang Ji, Siddharth Agarwal, Jiaqi Lou, Ipoom Jeong,
    et al. Demystifying cxl memory with genuine cxl-ready systems and devices. In
    *Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture*,
    pages 105–121, 2023.

    - <span id="page-13-33"></span>[69] Zeke Wang, Hongjing Huang, Jie Zhang, Fei
    Wu, and Gustavo Alonso. Fpganic: An fpga-based versatile 100gb smartnic for gpus.
    In *ATC*, 2022.

    - <span id="page-13-34"></span>[70] Xingda Wei, Rongxin Cheng, Yuhan Yang, Rong
    Chen, and Haibo Chen. Characterizing off-path SmartNIC for accelerating distributed
    systems. In *OSDI*, 2023.

    - <span id="page-13-14"></span>[71] Adam Wolnikowski, Stephen Ibanez, Jonathan
    Stone, Changhoon Kim, Rajit Manohar, and Robert Soule. Zerializer: Towards zero-copy
    ´ serialization. In *HosOS*, 2021.

    - <span id="page-13-3"></span>[72] Mingyu Wu, Shuaiwei Wang, Haibo Chen, and Binyu
    Zang. Zerochange object transmission for distributed big data analytics. In *ATC*,
    2022.

    - <span id="page-13-22"></span>[73] Xilinx. Xilinx ALVEO™ U280. [https://www.xilinx.com/publications/](https://www.xilinx.com/publications/product-briefs/alveo-u280-product-brief.pdf)
    [product-briefs/alveo-u280-product-brief.pdf,](https://www.xilinx.com/publications/product-briefs/alveo-u280-product-brief.pdf)
    2021.

    - <span id="page-13-15"></span>[74] Xilinx. Xilinx Vivado Design Suite. [https://www.xilinx.com/products/](https://www.xilinx.com/products/design-tools/vivado.html)
    [design-tools/vivado.html,](https://www.xilinx.com/products/design-tools/vivado.html)
    2023.

    - <span id="page-13-25"></span>[75] Yifan Yuan, Jinghan Huang, Yan Sun, Tianchen
    Wang, Jacob Nelson, Dan RK Ports, Yipeng Wang, Ren Wang, Charlie Tai, and Nam
    Sung Kim. Rambda: Rdma-driven acceleration framework for memoryintensive µs-scale
    datacenter applications. In *HPCA*, 2023.

    - <span id="page-13-18"></span>[76] Rohit Zambre, Aparna Chandramowlishwaran,
    and Pavan Balaji. Scalable communication endpoints for mpi+ threads applications.
    In *IC-PADS*, 2018.

    - <span id="page-13-19"></span>[77] Yue Zha and Jing Li. Virtualizing fpgas in
    the cloud. In *ASPLOS*, 2020.

    - <span id="page-13-20"></span>[78] Yue Zha and Jing Li. Hetero-vital: A virtualization
    stack for heterogeneous fpga clusters. In *ISCA*, 2021.

    - <span id="page-13-1"></span>[79] Jie Zhang, Hongjing Huang, Lingjun Zhu, Shu
    Ma, Dazhong Rong, Yijun Hou, Mo Sun, Chaojie Gu, Peng Cheng, Chao Shi, and Zeke
    Wang. Smartds: Middle-tier-centric smartnic enabling application-aware message
    split for disaggregated block storage. In *ISCA*, 2023.

    - <span id="page-13-4"></span>[80] Siyuan Zhou and Shuai Mu. {Fault-Tolerant}
    replication with {Pull-Based} consensus in {MongoDB}. In *NSDI*, 2021.

    - <span id="page-13-5"></span>[81] Yang Zhou, Zezhou Wang, Sowmya Dharanipragada,
    and Minlan Yu. Electrode: Accelerating distributed protocols with {eBPF}. In *NSDI*,
    2023.

    - <span id="page-13-2"></span>[82] Bohong Zhu, Youmin Chen, Qing Wang, Youyou
    Lu, and Jiwu Shu. Octopus+: An rdma-enabled distributed persistent memory file
    system. *TOS*, 2021.'
- title: Web-Based Simulator of Superscalar RISC-V Processors
  abstract: 'Mastering computational architectures is essential for developing fast
    and

    power-efficient programs. Our advanced simulator empowers both IT students and

    professionals to grasp the fundamentals of superscalar RISC-V processors, HW/SW

    co-design and HPC optimization techniques. With customizable processor and

    memory architecture, full C compiler support, and detailed runtime statistics,

    this tool offers a comprehensive learning experience. Enjoy the convenience of

    a modern, web-based GUI to enhance your understanding and skills.'
  url: http://arxiv.org/abs/2411.07721v1
  keywords: Processor Simulator, Superscalar Processor, RISC-V, HW-SW Co-design, Web
    Application.
  document: '# Web-Based Simulator of Superscalar RISC-V Processors


    Jiri Jaros


    *Faculty of Information Technology Brno University of Technology* Brno, Czech
    Republic 0000-0002-0087-8804


    Michal Majer *Faculty of Information Technology Brno University of Technology*
    Brno, Czech Republic misa@majer.cz


    Jan Vavra *Faculty of Information Technology Brno University of Technology* Brno,
    Czech Republic jv369@seznam.cz


    Jakub Horky *Faculty of Information Technology Brno University of Technology*
    Brno, Czech Republic horkykuba@email.cz


    *Abstract*—Mastering computational architectures is essential for developing fast
    and power-efficient programs. Our advanced simulator empowers both IT students
    and professionals to grasp the fundamentals of superscalar RISC-V processors,
    HW/SW co-design and HPC optimization techniques. With customizable processor and
    memory architecture, full C compiler support, and detailed runtime statistics,
    this tool offers a comprehensive learning experience. Enjoy the convenience of
    a modern, webbased GUI to enhance your understanding and skills.


    *Index Terms*—Processor Simulator, Superscalar Processor, RISC-V, HW-SW Co-design,
    Web Application.


    ### I. INTRODUCTION


    In the rapidly evolving field of computer architecture, a deep understanding of
    superscalar processors is crucial for both IT students and professionals, particularly
    those focusing on writing high-performance and power-efficient code. However,
    mastering the intricacies of these architectures is challenging, especially when
    existing educational tools fall short.


    Current processor simulators are often either too complex and low level aiming
    at cycle accurate simulation of complex codes of yet non-existent processors,
    such as Intel Simcs Simulator [1], or lacking intuitive graphical interface, features
    such as supercalar out-of-order execution, processor customization, memory and
    cache hierarchy, or detailed runtime statistic.


    ### *A. State of the Art*


    A comprehensive list of RISC-V simulators can be found on the RISC FIVE website
    [2]. The Creator RISC-V RV32IMFD Online Assembly Simulator [3] is a powerful web-based
    tool that allows users to write, compile, and step through RISC-V RV32IMFD assembly
    code to observe program behavior. Its key features include processor and memory
    layout customization, runtime statistics collection, and online debugging. However,
    it only supports scalar processors and lacks a commandline interface (CLI) for
    benchmarking large program segments.


    This work was supported by Brno University of Technology under project FIT-S-23-8141.


    The Venus RISC-V Simulator [4] is a RISC-V instruction set simulator designed
    for educational purposes. It allows the simulation of more complex codes, but
    only on a scalar RISC-V processor, without the capability to inspect pipeline
    stages, hazards, or other detailed processor behaviors.


    The Vulcan RISC-V Simulator for Education [5] offers several RISC-V instruction
    set extensions, along with sideby-side visualization of the program counter (PC),
    machine code, and original instructions, as well as register and memory visualization.
    However, it only supports a scalar core, and the web interface is still in the
    alpha stage.


    Other notable simulators for RISC-V processors include Ripes [6] and Jupiter [7].
    However, neither supports a superscalar pipeline or a web-based interface.


    In the search for inspiration in superscalar processor simulators, we must mention
    the excellent VSIM simulator [8], which our group has used for years in the Computer
    Architecture course. Developed in 2001, VSIM offers five architectures of superscalar
    processors from that era: Compaq Alpha 21264, Hewlett-Packard PA-8500, IBM Power3,
    Intel Pentium Pro/I-I/III, and MIPS R10000. VSIM allows partial customization
    of the processor architecture, the ability to load user-defined or random programs,
    and step-by-step simulation of program execution, including visualization of instruction
    and data flows between processor components. Unfortunately, this simulator is
    quite outdated and only runs on 32-bit Windows.


    ### *B. Objectives*


    The primary objective of the proposed web-based simulator is to bridge this educational
    gap by providing HPC developers with an accessible and illustrative tool to explore
    and understand the architecture of superscalar RISC-V processors. The simulator
    is designed to visually demonstrate each phase an instruction undergoes within
    the processor pipeline, allowing developers to identify potential bottlenecks
    and understand how different implementations of the same algorithm can impact
    runtime metrics such as execution time, cost or power consumption. By interacting
    with the simulator, developers can experiment with different processor configurations
    and observe their impact on runtime metrics.


    Since the primary purpose of the simulator is educational, the initial version
    currently supports only the RV32IMFD instruction set. Future versions will add
    support for the 64 bit instruction set as well as vector extensions.


    This hands-on approach aims to equip developers with the knowledge and skills
    needed to answer critical questions: Given an algorithm, how should one design
    a processor and optimize the code for the best performance, reasonable manufacturing
    cost and power consumption? By offering a user-friendly interface and comprehensive
    support for customization and performance analysis, our simulator seeks to enhance
    the learning experience and prepare developers for the challenges of modern computing.


    ### II. KEY FEATURES OF THE SIMULATOR


    To address the challenges associated with understanding and teaching superscalar
    RISC-V processors, we have developed a comprehensive web-based platform-independent
    simulator. Recognizing the need for both interactive and automated analysis, the
    simulator also includes a command-line interface (CLI) that allows for the analyzing
    of large programs in a batch processing manner, catering to advanced users who
    require more extensive testing capabilities. The proposed simulator offers following
    key features:


    - User-Friendly Interface: The simulator features an intuitive web interface that
    visually presents each block and instruction in the processor pipeline. It includes
    comprehensive documentation and tutorials, making it accessible for students and
    educators alike.

    - Fully Configurable Processors: Users can customize various processor parameters,
    including fetch and issue width, size of register fields, reorder, load and store
    buffers, branch predictors implementations, number of functional units, supported
    operations and corresponding delays. The simulator also allows for detailed configuration
    of cache memory settings such as capacity, associativity, cache line size, and
    replacement strategy. This flexibility enables users to explore different processor
    designs and understand their impact on performance.

    - Forward and Backward Simulation: The simulator supports both forward and backward
    instruction simulation, allowing users to step through the execution process in
    either direction. This feature aids in understanding the flow of instructions
    and the effects of architectural decisions on execution.

    - GCC Compiler Interface: Integrated with the GCC compiler, the simulator enables
    users to compile C code into assembly, offering various optimization levels. The
    interface includes syntax highlighting and links between C and assembly code,
    helping users understand how different coding strategies impact low-level operations.

    - Comprehensive Runtime Statistics: The simulator provides detailed performance
    metrics such as FLOPs, IPC,


    branch prediction accuracy, functional unit utilization, and cache hit rates.
    These metrics help users identify bottlenecks and optimize their code for better
    performance and efficiency.


    - Benchmark CLI: For more advanced users, the simulator includes a command-line
    interface that allows for the benchmarking of complex programs in an automated,
    batch-processing manner.

    - Open Source: The simulator''s source code is available on GitHub, encouraging
    collaboration and allowing users to modify and extend the tool according to their
    needs.


    ### *A. Main Window with the Processor View*


    The simulator''s web interface is a multi-window application. Specific windows,
    including the main simulator window, code editor, memory editor, architecture
    settings, and runtime statistics, can be accessed from the left toolbar.


    The main simulator window, as shown in Fig. 12, serves as the core interface of
    the simulator. It features processor schematics, a top simulation control bar,
    and a right-hand status bar. The processor schematics display essential components
    such as fetch and decode blocks, reorder (retire) buffer, and issue windows for
    the FX and FP ALUs, branch unit, and load/store (LS) components. Additionally,
    it includes a variable number of FX, FP, LS units, load/store buffers, and a memory
    unit connected to the cache. FX and FP registers are shown with their renamed
    tags and values, alongside the cache memory organized into lines. The simulation
    is fully controllable via mouse, keyboard, or on smartphones, with a slightly
    adjusted layout for mobile devices.


    All blocks share the same control elements, as shown in Fig. 1. The top left corner
    displays the name of the block (1), while the top right corner features a button
    (4) that opens a pop-up window with detailed information about the current status
    of the block. The second line (2) provides the most crucial realtime information
    about the block. The bottom right corner (5)


    ![](_page_1_Figure_17.jpeg)


    Fig. 1. Graphical representation of the fetch block with (1) block name, (2) simulation
    information, (3) active instructions, (4) pop-up details, and (5) resize bar."


    |       | Label                                                                                                                                                                          |      |
    Address |

    |-------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------|---------|

    | Array |                                                                                                                                                                                |
    1440 |         |

    | ರಿ    |                                                                                                                                                                                |
    640  |         |

    | b     |                                                                                                                                                                                |
    1040 |         |

    | · L2  |                                                                                                                                                                                |
    16   |         |

    | main  |                                                                                                                                                                                |
    0    |         |

    |       |                                                                                                                                                                                |      |         |

    | LC0   | Memory Inspector shows the memory up to the highest touched address                                                                                                            |
    1456 |         |

    |       | שיר השיר היישוב היישוב של המשטע של שני שני שני שני שני שני שני ישר ישי
    יוש יוש<br>0x03f0 0000e0 400000e0 4000000 e0 40 00000 e0 40 @ @ @ @ @ @ @ @ @
    @ @ @ @ @ @               |      |         |

    |       | 0x0400 0000e0 4000000e0 4000000 e0 40 00000 e0 40 @ @ @ @ @ @ @ @ @
    @ @ @ @ @ @<br>0x0410 0000803f00000803f0000803f00000803f ? ? ? ?                                           |      |         |

    |       | 0x0420 0000803f00000803f0000803f0000803f 0000803f 0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000
    |      |         |

    |       | 0x0430 0000803f00000803f0000803f0000803f ? ? ? ?                                                                                                                               |      |         |

    |       | 0x0440 0000803f00000803f00000803f00000803f ? ? ? ?                                                                                                                             |      |         |

    |       | 0x0450 0000803f00000803f00000803f00000803f ? ? ? ?                                                                                                                             |      |         |

    |       | 0x0460 0000803f0000803f0000803f0000803f ? ? ? ? ?                                                                                                                              |      |         |


    Fig. 2. A pop-up window displaying the current state of the memory, including
    allocated arrays, their starting addresses, and a memory dump."


    allows for resizing the block. The remaining area of the block (3) is specific
    to its function, typically containing a list of active instructions and their
    details, such as the state of the branch predictor, actual names of the registers,
    valid bits, etc.


    The schematic view of the simulation state offers a general overview but lacks
    detailed information. For additional details, pop-up previews are available. Clicking
    on a block or instruction opens a window displaying relevant data in a tabular
    format. For instructions, this includes timestamps of key phases (fetch, decode,
    ...), parameter values, and flags (e.g., validity flag). The details are block
    specific; for instance, the main memory block (as shown in Fig. 2) reveals all
    program pointers, their addresses, and an expanded view of the entire memory.


    Hovering over an instruction or register highlights all its occurrences across
    other blocks, making it easier to comprehend the simulation state. Additionally,
    hovering over an instruction parameter reveals a tooltip with its value, and for
    registers, information about their renaming is also displayed. Finally, clicking
    on an instruction opens a pop-up window with detailed information about its state,
    as shown in Fig. 3.


    The right-hand panel displays selected statistics and the debug log. It has two
    states; default and expanded. In the default view, it shows the number of cycles,
    committed instructions, IPC, and branch prediction accuracy, while the expanded
    view includes additional metrics such as FLOPs and cache hit rate. Complete statistics
    are available on a separate page (see Sec. II-D). Each log message is timestamped
    with the cycle in


    | addi tg0,sp,-32                                                                                                                                                                                             |                                                                                                    |
    ×         |

    |-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------|-----------|

    | Detailed view of instruction #0                                                                                                                                                                             |                                                                                                    |           |

    | Operands<br>tg0: renamed from x2<br>Value 0                                                                                                                                                                 |
    ID: 0<br>Type: Arithmetic (int)<br>Address: 0x00 (0)<br>Committed: 0 times<br>Exception
    Raised: No |           |

    | Type<br>kint<br>No<br>Valid                                                                                                                                                                                 |
    Timestamps                                                                                         |           |

    | Bin<br>0b0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000<br>0x000000000<br>Hex
    | Stage                                                                                              |
    Timestamp |

    |                                                                                                                                                                                                             |
    Fetch                                                                                              |
    0         |

    |                                                                                                                                                                                                             |
    Issue                                                                                              |
    2         |

    | 640<br>Value 640                                                                                                                                                                                            |
    Result Ready                                                                                       |
    N/A       |

    | Type<br>kLong<br>Valid<br>Yes                                                                                                                                                                               |
    Commit                                                                                             |
    N/A       |

    | 0b000000000000000000000001010000000<br>Bin<br>Hex<br>0x00000280                                                                                                                                             |
    Flags                                                                                              |           |

    |                                                                                                                                                                                                             |
    Flag                                                                                               |
    Value     |

    | -32                                                                                                                                                                                                         |
    Speculative                                                                                        |
    No        |

    | Value -32<br>Type<br>kint                                                                                                                                                                                   |
    Busy                                                                                               |
    Yes       |

    | Valid<br>Yes<br>0b11?111111111111111111111111100000<br>Bin                                                                                                                                                  |
    Ready To Execute                                                                                   |
    Yes       |

    | 0xfffffffe0<br>Hex                                                                                                                                                                                          |
    Ready To Commit                                                                                    |
    No        |

    |                                                                                                                                                                                                             |
    Finished                                                                                           |
    No        |

    |                                                                                                                                                                                                             |
    Failled                                                                                            |
    No        |

    |                                                                                                                                                                                                             |                                                                                                    |           |


    Fig. 3. A pop-up window that displays instruction current state, parameters, renaming
    details, values and validity, along with instruction flags and the timestamps
    of phase completions.


    which it was generated, and clicking on the message number navigates the simulation
    to that specific cycle.


    ### *B. Code Editor*


    The code editor allows users to input programs in both C and RISC-V assembly languages,
    see Fig. 4. The entry point can be set to the first instruction or any specified
    label. When the code is entered in C, it can be compiled into assembly using four
    optimization levels. In this case, the C and assembly codes are linked through
    highlighting, enabling visualization of how C statements are translated into assembly
    instructions, see Fig. 5. If the code requires global arrays, the C language keyword
    extern can be used, and the array contents can be filled in the Memory Settings
    window (see Sec. II-C). In assembly code, users can use labels such as .word to
    define memory arrays. Users can also load basic assembly and C examples or load
    and save complex code from and to files.


    The editor is implemented using the CodeMirror library<sup>1</sup> , which provides
    a robust user interface with features such as syntax highlighting, keyboard shortcuts,
    line numbering, error highlighting (Fig. 6 and 7), and more.


    <sup>1</sup>https://codemirror.net/5/


    | Code Editor |

    |-------------|

    |             |


    | UUUS LUILUI                                                                                                                                  |                                                                                                                                  |                                                                                                                                                                                                                                                                          |  |  |

    |----------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--|--|

    | Optimization                                                                                                                                 |
    C Code I Load & Save C                                                                                                           |
    ASM Code   2 Load   & Save                                                                                                                                                                                                                                               |  |  |

    | O Do not optimize<br>O<br>Optimize (02)<br>O<br>Optimize (03)<br>O<br>Optimize
    for size<br>Compile<br>ಕಾ<br>Load Example<br>Show Memory<br>= | int ptr (32) ;<br>1<br>2<br>3<br>int
    writeMem() {<br>4<br>for(int i = 0: i < 32: i++) {<br>5<br>ptr (1) = 1;<br>6<br>7<br>7<br>8
    | Check<br>D Entry Point: writeMem<br>1<br>writeMem.<br>2<br>lla a4,ptr<br>3<br>li
    a5.0<br>li a3,32<br>4<br>5<br>.L2:<br>6<br>sw a5.0(34)<br>7<br>addi a5.a5.1<br>8<br>addi
    =4,24,4<br>9<br>bne a5,a3 . L2<br>10<br>ret<br>11<br>.align 2<br>12<br>ptr:<br>13<br>.zero
    128 |  |  |

    | Tips for writing code                                                                                                                        |                                                                                                                                  |                                                                                                                                                                                                                                                                          |  |  |


    Fig. 4. Code editor displaying C and Assembly codes, with compiler parameters
    and control buttons.


    ![](_page_3_Picture_0.jpeg)


    Fig. 5. Code editor showing the link between C and Assembly codes, with instruction
    details displayed in a bubble window.


    |   | C Code                                               |

    |---|------------------------------------------------------|

    | 1 | int ptr[32];                                         |

    | 2 |                                                      |

    | 3 | int writeMem() {                                     |

    | 4 | for(int i = 0; i < 32; i++) {                        |

    | 5 | ptr[i] = riscy;                                      |

    | 6 |                                                      |

    | 7 | ''riscv'' undeclared (first use in this function)<br>} |

    | 8 |                                                      |


    Fig. 6. Syntax error visualization in the C code.


    |   | Check   ▷ Entry Point: 0         |

    |---|----------------------------------|

    | 1 | addi x1,x0,10                    |

    | 2 | addi x2,x0,10                    |

    | 3 |                                  |

    | 4 | addi x3,x0,20                    |

    | 5 | sw x1,0(80)                      |

    | 6 | sw x2,0(                         |

    | 7 | Argument "80" is not a register. |

    | 8 | addi x4,x0,20                    |

    | 9 | lw x5,0(x4)                      |


    Fig. 7. Syntax error visualization in the Assembly code.


    # *C. Memory and Processor Architecture Settings*


    The Memory Settings window allows users to populate memory with custom data, see
    Fig. 8. Users can define static global arrays of various basic data types and
    specify their alignment. Arrays can be populated with user-specified values separated
    by commas, repeated constants (e.g., zeros), or random values. Additionally, memory
    dumps can be imported and exported in binary or CSV format.


    The Architecture Settings window enables users to customize the processor architecture
    and cache in detail, see Fig. 9. The window is organized into several tabs, each
    grouping related settings. At the top, users can switch between different architectures
    and import or export configurations using JSON files.


    The first tab allows users to name the architecture and set the core and memory
    clock speeds in Hz. The second tab, titled Buffers, controls the superscalar processor''s
    width by adjusting the reorder buffer size, the number of instructions fetched
    and committed per cycle, flush penalty, and the number


    | New Object              | Pointer Name (1)<br>Array                                                                                             |  |  |  |  |  |

    |-------------------------|-----------------------------------------------------------------------------------------------------------------------|--|--|--|--|--|

    | Import Export           | Data Type (1)<br>Alignment (log Bytes) (1)<br>4<br>Integer<br>V                                                       |  |  |  |  |  |

    | Memory Objects<br>Array | Values                                                                                                                |  |  |  |  |  |

    |                         | Random Values<br>List<br>Repeated Constant                                                                            |  |  |  |  |  |

    |                         | Random values will be generated in an inclusive range.<br>Number
    of Elements<br>Inclusive Range<br>4<br>0<br>100<br>- |  |  |  |  |  |

    |                         |                                                                                                                       |  |  |  |  |  |


    Fig. 8. Memory editor allowing to define static global arrays and fill them with
    user data.


    |                              | Active configuration |                        |
    Realistic  |                  |                                              |
    C                                                     | Export |

    |------------------------------|----------------------|------------------------|------------|------------------|----------------------------------------------|-------------------------------------------------------|--------|

    | No memory locations defined. |                      |                        |            |                  |                                              |                                                       |        |

    |                              | Name                 | Buffers                |            |
    Functional Units | Cache                                        | Memory                                                |
    Branch |

    |                              |                      | Functional Units       |            |                  |                                              |                                                       |        |

    |                              | Unit                 | Latency                |
    Operations |                  |                                              |                                                       |        |

    |                              | Adder                | 2                      |            |                  |
    Bitwise   1   Addition   1   Special   2     |                                                       |
    ×      |

    |                              | Mult                 | 2                      |            |                  |
    Multiplication   2 Division   10 Special   2 |                                                       |
    ×      |

    |                              | Eb                   | 2                      |            |                  |                                              |
    Bitwise  1 Addition  1   Multiplication  2   Division | ×      |

    |                              | LS                   | 1                      |
    N/A        |                  |                                              |                                                       |
    ×      |

    |                              | Branch               | 2                      |
    N/A        |                  |                                              |                                                       |
    ×      |

    |                              | Memory               | 2                      |
    N/A        |                  |                                              |                                                       |
    ×      |

    |                              |                      |                        |            |                  |                                              |                                                       |        |

    |                              |                      | Add a Unit             |            |                  |                                              |                                                       |        |

    |                              |                      | FX                     |
    FP         | L_S              | Branch                                       |
    Memory                                                |        |

    |                              | Name                 |                        |            |                  |
    Base latency                                 |                                                       |        |

    |                              |                      |                        |            |                  |
    1                                            |                                                       |        |

    |                              |                      |                        |            |                  |                                              |                                                       |        |

    |                              |                      | This unit can perform:
    |            |                  |                                              |                                                       |        |

    |                              |                      | Addition (+)           |            |                  |
    1                                            |                                                       |        |

    |                              |                      | Bitwise (and, or, xor)
    |            |                  | 1                                            |                                                       |        |

    |                              |                      | Division (/)           |            |                  |
    1                                            |                                                       |        |

    |                              |                      | Multiplication (*)     |            |                  |
    1                                            |                                                       |        |

    |                              |                      | Special (sqrt,  )      |            |                  |
    1                                            |                                                       |        |


    Fig. 9. Architecture settings for custom processor and memory configuration, including
    processor width, functional units, cache organization, memory subsystem, and branch
    predictor.


    of jumps the fetch unit can handle within a single cycle. The third tab addresses
    the functional units, categorized into FX, FP, LS, branch, and memory. FX and
    FP units can vary in supported instructions and associated latencies, while LS,
    memory and branch units allow for latency specification only.


    The Cache tab provides options to enable or disable the L1 cache, define the number
    of cache lines, their size, and associativity. Users can also choose a replacement
    policy from LRU, FIFO, or Random, and determine the store behavior, either write-back
    or write-through. Additionally, users can control the cache line replacement delay
    and cache access delay. The Memory tab allows for the configuration of the load
    and store buffer size, load and store latency, call stack size, and the register
    rename file size. Finally, the Branch prediction tab lets users set the branch
    target buffer size, pattern history table size, predictor type (zero, one, or
    two-bit), predictor default state, and choose between local or global history
    shift registers.


    ### *D. Runtime Statistics*


    The Runtime Statistics window provides detailed useful insight into the code execution,
    including both static and dynamic instruction mix in tabular and graphical formats.
    It also summarizes the number and percentage of busy cycles for each unit, as
    well as cache statistics, including the number of accesses, hit and miss ratios,
    and bytes written. Additionally, the window displays various detailed metrics
    such as predictor accuracy, total executed cycles, total number of committed instructions,
    number of reorder buffer flushes, FLOPS, IPC, wall time, and many more, see Fig.
    10.


    ### *E. Command-Line Interface*


    The Command-Line Interface (CLI) allows users to execute large programs written
    in C or assembly language and collect runtime statistics. The CLI requires two
    mandatory arguments: the assembly language source code in a text file and the
    architecture description in JSON format. Additional parameters allow to specify
    the program''s entry point, memory configuration, data dump, and various levels
    of output verbosity and format (either text or JSON). The CLI must be connected
    to the server using host and port parameters, with an optional connection to the
    GCC compiler.


    ### III. IMPLEMENTATION AND DEPLOYMENT


    The proposed web-based simulator is a client-server application with two client
    interfaces: a command-line interface (CLI) and a web-based interface. Both clients
    interact with the server API to present simulation results, while all simulation
    logic is handled server-side. The web client is developed in JavaScript, using
    the React library<sup>2</sup> and the Next.js framework<sup>3</sup> . It communicates
    with the simulation server via HTTP, using a JSONbased API. The HTTP server is
    implemented with the Undertow library<sup>4</sup> . HTTPS support is provided
    through an NGINX proxy server<sup>5</sup> . The global state, facilitating communication
    between modules, is managed using sessions wrapped in React context and maintained
    by the Redux library<sup>6</sup> .


    https://react.dev/ https://nextjs.org/ https://undertow.io/ https://nginx.org
    https://redux.js.org/


    ![](_page_4_Figure_10.jpeg)


    Fig. 10. Collected runtime statistics cover a wide range of performance statistics.


    ### *A. Simulator Architecture and Simulation Step Manager*


    The server-side, which encompasses the entire simulator logic, is written in Java
    and utilizes the JavaFX library. The simulator is organized into modules, taking
    full advantage of object-oriented design principles. The central class BlockScheduleTask
    maintains the list of references on all simulated blocks in the processor, using
    the observer design pattern to broadcast the changes in the processor state.


    The simulator''s memory is represented as a 1D byte array with a predefined capacity.
    Memory modules operate in a transactional mode. Functional blocks that request
    data from memory generate an object representing a transaction. Upon registration,
    memory management populates this object with information about the transaction''s
    completion time. Transactions enable easy configuration of memory access times,
    support cache line flushing, and include metadata useful for interactive simulation.


    The initialization of the simulation involves several steps, including configuration
    validation, loading of register and instruction definitions, initialization of
    statistics and the simulation step manager, parsing of the assembly program, memory
    setup, construction of all processor components, initialization of the register
    file with specific values, and setting the PC register to the program''s entry
    point.


    The simulation can proceed either clock cycle by clock cycle (step by step) or
    run continuously to completion. Each simulation clock cycle (a step) is executed
    by sequentially calling all blocks, which are arranged in a queue based on their
    position in the pipeline. The simulation of functional units is divided into two
    sub-steps to allow the completion of the current instruction and the loading of
    the next one within a single clock cycle. It is important to note that the functional
    blocks currently do not support internal pipelining, which is commonly used in
    components such as the floating-point ALU. The simulation ends when the pipeline
    is empty or when the stack pointer reaches the bottom of the call stack, indicating
    process completion as the main routine is exited.


    The simulation step manager also collects runtime statistics.


    ### *B. Register Representation and Instruction Interpretation*


    Registers are represented as 64-bit arrays, even though the simulator currently
    supports only 32-bit instructions. The interpretation of their values depends
    on the type of instruction being executed. Each register also contains metadata
    defining the data type in use, making code debugging in the GUI more intuitive
    by displaying the intended value (e.g., a char) instead of a raw bit array. Additionally,
    registers maintain all necessary information for renaming. Each register tracks
    the number of references; architectural registers use a list of all renamed copies,
    while renamed (speculative) registers hold a pointer to the corresponding architectural
    register. This setup allows for detailed tracking of register renaming.


    The simulator fully supports the RV32I instruction set with the M and F extensions,
    including pseudo-instructions and directives (e.g., .word). However, privileged
    instructions and instructions for context switching are not supported, as the
    simulator does not run an operating system. Branching and memory instructions
    have been modified to work with indices into arrays representing code and data
    memory segments, rather than with memory addresses.


    The instruction set is defined in a configuration JSON file and can be easily
    extended, see Listings 1.


    ```

    1 {

    2 "name": "add",

    3 "instructionType": "kArithmetic",

    4 "arguments": [

    5 {

    6 "name": "rd",

    7 "type": "kInt",

    8 "writeBack": true

    9 },

    10 {

    11 "name": "rs1",

    12 "type": "kInt"

    13 },

    14 {

    15 "name": "rs2",

    16 "type": "kInt"

    17 }

    18 ],

    19 "interpretableAs": "\rs1 \rs2 + \rd ="

    20 },

    ```

    Listing 1. Definition of the add instruction, its parameters and interpretation.


    The execution of an instruction is managed by the Expression class, which implements
    a simple stack-based interpreter using postfix notation, as shown in Listing 1
    under interpretableAs. The interpreter also handles operands directly encoded
    in the instruction opcode, such as the PC register in jump and branch instructions.
    The output of an expression may be twofold: the first possible output is the value
    that remains on the stack after the interpretation is executed, a mechanism used
    by expressions to calculate jump addresses or conditions. The second possible
    output is the assignment to a variable within the expression. The binary operator
    = in the expression has a side effect, writing the value into the register. Exceptions
    are generated during code execution (e.g., when accessing an unauthorized address,
    division by zero). The existence of an exception is checked when the instruction
    is committed.


    The simulator also supports backward simulation, enabling users to inspect changes
    in the processor state in detail. This is implemented as a forward simulation
    with t − 1 clock cycles. While this approach significantly simplifies the implementation,
    it requires the simulation to be deterministic and imposes higher computational
    demands on the server. Backward simulation is only available in the web application,
    where it is intended for use with small programs running over a few thousand clock
    cycles.


    # *C. Compiler Integration*


    3


    7


    The simulator utilizes the GCC compiler''s crosscompilation to translate C programs
    into RISC-V assembly. When the code is ready for compilation, the web client packages
    the source code and sends it to the server via a POST request. The server then
    generates a shell script to execute the compiler, collecting the compiled assembly
    program along with a log of any potential compiler errors. The result is sent
    back to the web client.


    For processing the compiled or user defined assembly code, a two-pass approach
    was chosen. In the first pass, instructions and memory definitions (directives)
    are processed. The program text is divided into language units (tokens such as
    symbols, comments, or new lines). The tokens are then processed sequentially in
    a loop according to the grammar, and the individual instructions are stored. Instruction
    objects are linked by references to objects that describe their behavior and to
    register objects.


    Listing 2 shows examples of memory definitions in the assembly program. The compiler
    supports the following directives: .byte, .hword, .word, .align, .ascii, .asciiz,
    .string, .skip a .zero.


    ```

    1 x:

    2 .word 5 # integer variable x

    4 .align 4

    5 arr:

    6 .zero 64 # 64 bytes with 16B alignment

    8 hello:

    9 .asciiz "Hello World" # null-terminated

    10 # string

    ```

    Listing 2. Example of memory definitions in the assembly language. Memory defined
    in this way is referenced in the program using labels, e.g. arr.


    After the first pass, not all operand values are defined, because an operand may
    refer to a label that has not yet been processed. The second pass fills in the
    missing values, completing the program processing. A complication, when filling
    in the values, is the support for arithmetic expressions in instruction arguments
    (e.g., lla x4, arr+64). This feature is implemented because the compiler often
    generates such expressions. Therefore, memory allocation takes place between the
    first and second pass. After allocation, all label values are known, and the final
    values of instruction arguments can be calculated. Jump instructions use relative
    values for jumps, so it is sometimes necessary to subtract the instruction''s
    position from the absolute value of the label. Expressions are evaluated by a
    simple evaluation program, which must have access to the label values.


    Finally, the assembler output may contain a large amount of information that is
    redundant for the simulator and also reduces the readability of the code. Therefore,
    the compiler output is passed through a filter that removes unnecessary directives,
    labels, and data.


    Once the program is compiled, it is loaded into memory. User data defined on the
    memory settings page (and with the extern keyword in the code) must be statically
    allocated in memory. The allocation is performed with respect to the data type
    and alignment requirements. The compiled code then operates with labels set to
    the starting addresses of specific memory arrays. Since the compiler''s application
    binary interface (ABI) requires a call stack, the memory initialization process
    allocates space for the stack at the beginning of the memory and stores the corresponding
    pointer in register x2 (the stack pointer). User data is then stored after the
    stack.


    ### *D. Deployment*


    Continuous integration and development is managed under the GitHub version control
    system. Deployment is carried out using three Docker containers. The first container,
    called simserver, is responsible for all the simulation logic, while the second
    container, called web, mediates requests between the client application and the
    simulation server using web technologies. Both containers open a single external
    communication port. The third container, nginx\_proxy, provides HTTPS encapsulation
    for internal HTTP communication using the supplied server SSL certificate. The
    containers can be compiled and deployed automatically using the docker-compose
    command.


    ### IV. TESTING AND EVALUATION


    The simulator is quite extensive piece of software with almost 33 thousand lines
    of code. During the implementation of the simulator, unit testing was intensively
    used and gradually expanded. The project in its current state contains more than
    400 unit tests. The code coverage in the simulator is 83%, and the coverage in
    the simulator''s blocks is 94%.


    The system was tested as a whole from many aspects. Each instruction has its own
    test to verify its correct behavior. This type of test typically checks the state
    at the end of the simulation. Additionally, the test script ensures that all provided
    code samples run on the simulator without errors. The functionality of several
    more complex programs is also tested, such as array sorting using the quicksort
    algorithm, working with a linked list, and polymorphism (dynamic dispatch).


    The web application was manually tested using Google Chrome and Mozilla Firefox.
    The web interface was also evaluated with Google Lighthouse<sup>7</sup> . Performance
    tests showed that rendering typically takes around 80 ms.


    The simulator also underwent user testing. Twenty participants, consisting of
    IT students and faculty members, were asked to complete several tasks and evaluate
    the user experience. The simulator achieved an average score of 8.4/10. The testing
    also revealed several minor bugs, which were subsequently fixed.


    # *A. Performance Evaluation*


    For performance evaluation, the simulator was profiled in server mode. Additionally,
    a simple benchmark was developed using the Java Microbenchmark Harness (JMH)<sup>8</sup>
    .


    The most important conclusion from the performance testing is the following: in
    server mode, about 60% of the request handling time is consumed by working with
    the JSON format. This format is inherently unfavorable for performance. As a result,
    the dominance of the communication overhead means that further performance gains
    from optimizing the simulation itself are diminishing. Exploring a change in the
    communication protocol is a direction worth investigating in future work.


    Load testing was also conducted using Apache JMeter<sup>9</sup> . The characteristics
    of the test are as follows:


    - two test sizes: 30 and 100 users,

    - each user interactively simulates 40 steps of the simulation for one of two
    programs,

    - ramp-up time of 4 seconds, with a 1-second pause between each user''s request
    (think time),

    - use of gzip.


    Using gzip compression increased throughput on the local server by 40%. Table
    I presents the measured data. All measurements were conducted locally on a laptop
    with an Intel i5 8300H processor and 16 GB of DDR4 RAM. The conclusion is that
    the server handles a smaller number of concurrent users well, regardless of the
    runtime mode, although Docker has a noticeable impact on application performance.
    A larger number of users significantly affects latency, degrading the user experience.
    During the test, there were no application crashes or query failures. In real-world
    operation, latency is likely to be higher due to the longer travel distance of
    packets over the internet. A larger number of users can be managed by running
    the application on more powerful hardware or by distributing the load across multiple
    servers.


    <sup>7</sup>https://developer.chrome.com/docs/lighthouse/overview


    <sup>8</sup>https://www.baeldung.com/java-microbenchmark-harness


    <sup>9</sup>https://jmeter.apache.org/


    TABLE I THE MEASURED LATENCY VALUES FOR THE FOUR SPECIFIED SCENARIOS.


    | Mode   | #users | Latency [ms] |                 |                      |

    |--------|--------|--------------|-----------------|----------------------|

    |        |        | Median       | 90th percentile | Throughput [trans/s] |

    | Direct | 30     | 70,66        | 118             | 25.96                |

    |        | 100    | 680          | 1248.9          | 53.61                |

    | Docker | 30     | 77           | 283             | 24.49                |

    |        | 100    | 1135         | 2031.9          | 42.07                |


    ## V. CONCLUSIONS


    The proposed web-based simulator for superscalar RISC-V processors is a substantial
    contribution to computer architecture education and research. By offering an accessible
    and interactive platform, it facilitates a deeper understanding of complex processor
    architectures and encourages experimentation and innovation.


    To the best of our knowledge, this is the most advanced web-based simulator for
    a superscalar RISC-V processor with L1 cache support, designed for educational
    use, benchmarking code snippets, and architectural evaluation. The simulator boasts
    several significant features. It offers a user-friendly graphical interface, with
    fully configurable processor components, allowing users to export and share configurations.
    The simulator supports C and assembly language programs, various levels of code
    optimization through the integrated GCC compiler, and an intuitive code editor.
    Performance analysis is enabled by a range of built-in metrics, and for more complex
    programs, the command-line interface allows batch evaluation of different algorithm
    versions, facilitating direct comparisons across different architectures.


    The intended users of this simulator are primarily IT students specializing in
    processor design and HPC computing. We believe that a deep understanding of processor
    architecture will contribute to the development of highly optimized RISC-V processors
    for custom applications. Additionally, understanding how processors handle specific
    code patterns and snippets will lead to better optimization of HPC codes across
    various architectures. We hope that even advanced HPC developers will benefit
    from this simulator by evaluating different implementations and processor configurations.


    The simulator will be used in the upcoming academic year during the Computation
    Systems Architectures course at the Faculty of Information Technology, Brno University
    of Technology. Nearly 250 students will leverage its features to solve assignments
    focused on optimizing specific code patterns concerning the provided architecture.


    Future work will focus on expanding the simulator''s capabilities. Several directions
    are under consideration. One direction is to enhance the processor architecture
    with additional features, such as vector units, advanced branch predictors, pipelined
    functional units, or deeper cache hierarchies. Another potential area of development
    is improving the code development and simulation environment by adding breakpoints,
    watches, dynamic memory allocation, atomic operations, and more. Additionally,
    runtime statistics could be expanded to measure the chip area consumed by specific
    blocks based on their complexity or estimate the processor''s power consumption
    using realistic manufacturing technology.


    Finally, we provide links to the source code on GitHub and a live instance of
    the simulator, as shown in Fig. 11.


    ![](_page_7_Picture_10.jpeg)


    Fig. 11. QR codes with source codes on GitHub and live demo.


    ### REFERENCES


    - [1] Intel Corporation, "Intel® simics® simulator." [Online]. Available: \https://www.intel.com/content/www/us/en/developer/articles/tool/
    simics-simulator.html

    - [2] RISC-V International, "RISC-V Simulators," 2024. [Online]. Available: https://www.riscfive.com/risc-v-simulators/

    - [3] D. C. Alonso, F. G. Carballeira, A. C. Mateos, and E. del Pozo Punal, "CreatorSim:
    A RISC-V Simulator," 2024. [Online]. ˜ Available: https://creatorsim.github.io/creator/

    - [4] S. Kaminsky and K. Vakil, "Venus RISC-V Simulator," 2024. [Online]. Available:
    https://github.com/ThaumicMekanism/venus?tab= readme-ov-file

    - [5] V. M. de Morais Costa, "Vulcan RISC-V Instruction Set Simulator Built For
    Education," 2024. [Online]. Available: https://github.com/ vmmc2/Vulcan

    - [6] M. B. Petersen, "Ripes Introduction," online, 2022. [Online]. Available:
    https://github.com/mortbopet/Ripes/blob/master/docs/introduction.md

    - [7] A. Castellanos, "Jupiter, RISC-V Assembler and Runtime Simulator," online,
    2022. [Online]. Available: https://github.com/andrescv/Jupiter

    - [8] Arpad Mikl ´ os and D. Sima, "VSIM A Superscalar CPU Simulator," ´ John
    von Neumann Faculty of Informatics, Budapest Polytechnic, PH-1034 Budapest, Nagyszombat
    u. 19., Tech. Rep., 2001.

    - [9] K. Kocˇ´ı, "Graphicall CPU Simulator with Cache Visualization," diploma
    thesis, Faculty of Electrical Engineering CVUT in Prague, May 2018. ˇ

    - [10] H. Rajaei, "An empirical study for multi-level cache associativity," *2020
    ASEE Virtual Annual Conference Content Access*, no. 31008, 2020. [Online]. Available:
    https://peer.asee.org/ an-empirical-study-for-multilevel-cache-associativity.pdf

    - [11] S. Metzlaff, A. Vogelgsang, and N. Krezic-Luger, "DLX/MIPS processor simulator,"
    online, 2013. [Online]. Available: https://github. com/smetzlaff/openDLX


    ![](_page_8_Figure_0.jpeg)


    Fig. 12. The main simulator window, displaying all processor components, including
    registers, cache, and main memory, with control buttons positioned at the top
    and basic statistics on the right.'
- title: "MANTIS: A Mixed-Signal Near-Sensor Convolutional Imager SoC Using\n  Charge-Domain\
    \ 4b-Weighted 5-to-84-TOPS/W MAC Operations for Feature\n  Extraction and Region-of-Interest\
    \ Detection"
  abstract: 'Recent advances in artificial intelligence have prompted the search for

    enhanced algorithms and hardware to support the deployment of machine learning

    at the edge. More specifically, in the context of the Internet of Things (IoT),

    vision chips must be able to fulfill tasks of low to medium complexity, such as

    feature extraction or region-of-interest (RoI) detection, with a sub-mW power

    budget imposed by the use of small batteries or energy harvesting. Mixed-signal

    vision chips relying on in- or near-sensor processing have emerged as an

    interesting candidate, thanks to their favorable tradeoff between energy

    efficiency (EE) and computational accuracy compared to digital systems for

    these specific tasks. In this paper, we introduce a mixed-signal convolutional

    imager system-on-chip (SoC) codenamed MANTIS, featuring a unique combination of

    large 16$\times$16 4b-weighted filters, operation at multiple scales, and

    double sampling, well suited to the requirements of medium-complexity tasks.

    The main contributions are (i) circuits called DS3 units combining delta-reset

    sampling, image downsampling, and voltage downshifting, and (ii) charge-domain

    multiply-and-accumulate (MAC) operations based on switched-capacitor amplifiers

    and charge sharing in the capacitive DAC of the successive-approximation ADCs.

    MANTIS achieves peak EEs normalized to 1b operations of 4.6 and 84.1 TOPS/W at

    the accelerator and SoC levels, while computing feature maps with a root mean

    square error ranging from 3 to 11.3$\%$. It also demonstrates a face RoI

    detection with a false negative rate of 11.5$\%$, while discarding 81.3$\%$ of

    image patches and reducing the data transmitted off chip by 13$\times$ compared

    to the raw image.'
  url: http://arxiv.org/abs/2411.07946v1
  keywords: Charge-domain, CMOS image sensor (CIS), convolutional neural network (CNN),
    feature extraction, mixedsignal, multiply-and-accumulate (MAC) operations, near-sensor,
    region-of-interest (RoI) detection, system-on-chip (SoC).
  document: '# MANTIS: A Mixed-Signal Near-Sensor Convolutional Imager SoC Using Charge-Domain
    4b-Weighted 5-to-84-TOPS/W MAC Operations for Feature Extraction and Region-of-Interest
    Detection


    Martin Lefebvre, *Graduate Student Member, IEEE*, and David Bol, *Senior Member,
    IEEE*


    *Abstract*—Recent advances in artificial intelligence have prompted the search
    for enhanced algorithms and hardware to support the deployment of machine learning
    at the edge. More specifically, in the context of the Internet of Things (IoT),
    vision chips must be able to fulfill tasks of low to medium complexity, such as
    feature extraction or region-of-interest (RoI) detection, with a sub-mW power
    budget imposed by the use of small batteries or energy harvesting. Mixed-signal
    vision chips relying on in- or near-sensor processing have emerged as an interesting
    candidate, thanks to their favorable tradeoff between energy efficiency (EE) and
    computational accuracy compared to digital systems for these specific tasks. In
    this paper, we introduce a mixed-signal convolutional imager system-on-chip (SoC)
    codenamed MANTIS, featuring a unique combination of large 16×16 4b-weighted filters,
    operation at multiple scales, and double sampling, well suited to the requirements
    of mediumcomplexity tasks. The main contributions are (i) circuits called DS3
    units combining delta-reset sampling, image downsampling, and voltage downshifting,
    and (ii) charge-domain multiplyand-accumulate (MAC) operations based on switched-capacitor
    amplifiers and charge sharing in the capacitive DAC of the successive-approximation
    ADCs. MANTIS achieves peak EEs normalized to 1b operations of 4.6 and 84.1 TOPS/W
    at the accelerator and SoC levels, while computing feature maps with a root mean
    square error ranging from 3 to 11.3%. It also demonstrates a face RoI detection
    with a false negative rate of 11.5%, while discarding 81.3% of image patches and
    reducing the data transmitted off chip by 13× compared to the raw image.


    *Index Terms*—Charge-domain, CMOS image sensor (CIS), convolutional neural network
    (CNN), feature extraction, mixedsignal, multiply-and-accumulate (MAC) operations,
    near-sensor, region-of-interest (RoI) detection, system-on-chip (SoC).


    ### I. INTRODUCTION


    R ECENT years have seen artificial intelligence (AI) rise as a key component of
    numerous engineered systems, reaching an unprecedented level of pervasiveness
    at the applications level. Among them, the Internet of Things (IoT) has elicited
    a particular interest as the large amount of data generated by sensor nodes calls
    for the development of specialized machine learning (ML) algorithms and hardware
    to efficiently


    Received 26 June 2024; revised 5 September 2024 and 14 October 2024; accepted
    18 October 2024. This article was approved by Associate Editor Yoonmyung Lee.
    *(Corresponding author: Martin Lefebvre.)*


    The authors are with the Institute of Information and Communication Technologies,
    Electronics and Applied Mathematics, Universite catholique ´ de Louvain, 1348
    Louvain-la-Neuve, Belgium. M. Lefebvre is also with the Department of Microelectronics,
    Delft University of Technology, 2628 CD Delft, The Netherlands (e-mail: m.lefebvre@tudelft.nl).


    Color versions of one or more figures in this article are available at https://doi.org/10.1109/JSSC.2024.3484766.


    Digital Object Identifier 10.1109/JSSC.2024.3484766


    ![](_page_0_Figure_12.jpeg)


    <span id="page-0-0"></span>Sub-40-nm CMOS process 0.18-µm to 90-nm (CIS) process


    Fig. 1. (a) Vision chip architectures ranging from mixed-signal processing in
    or near the pixel array to conventional digital processing outside of it, and
    (b) strengths and limitations of these architectures. (c) Envisioned system based
    on a cascaded processing scheme similar to [\[4\]](#page-13-0), in which only
    relevant image patches are transmitted from the image sensor to the digital processor.


    process data at the edge, a concept coined as edge AI or tiny ML. More specifically,
    in the context of vision sensors, edge devices must be able to solve vision tasks
    of low to medium complexity, e.g., feature extraction (FE) and region-of-interest
    (RoI) detection, within a sub-mW power budget, as IoT nodes are often supplied
    by limited-capacity batteries. Mixed-signal vision chips have thus emerged as
    a suitable candidate, since they outperform digital chips in terms of energy efficiency
    (EE) while maintaining a sufficient computational accuracy. This improved EE also
    stems from a reduced number of ADC conversions compared to digital implementations,
    leading to significant energy and area savings.


    Mixed-signal vision chip architectures can be divided into two main categories,
    namely in- [\[1\]](#page-13-1), [\[2\]](#page-13-2), [\[3\]](#page-13-3) and near-sensor
    [\[4\]](#page-13-0), [\[5\]](#page-13-4), [\[6\]](#page-13-5), [\[7\]](#page-13-6),
    [\[8\]](#page-13-7) vision chips, respectively implemented


    This document is the paper as accepted for publication in JSSC, the fully edited
    paper is available at https://ieeexplore.ieee.org/document/10750406. ©2024 IEEE.
    Personal use of this material is permitted. Permission from IEEE must be obtained
    for all other uses, in any current or future media, including reprinting/republishing
    this material for advertising or promotional purposes, creating new collective
    works, for resale or redistribution to servers or lists, or reuse of any copyrighted
    component of this work in other works.


    with analog processing elements (PEs) inside or in the periphery of the pixel
    array. A third category, referred to as hybrid vision chips [\[9\]](#page-13-8),
    [\[10\]](#page-13-9), is not represented in Fig. [1\(](#page-0-0)a) but simply
    combines elements from both categories. On the one hand, in-sensor vision chips
    are massively parallel and do not require any memory, be it analog or digital.
    However, connections between pixel-level PEs are usually local and limited to
    neighboring pixels, hampering the calculation of image-level features and thereby,
    the operation at multiple spatial scales. In addition, pixel-level PEs also lead
    to a relatively large pixel pitch above 10 µm. At last, in-sensor processing is
    often limited to low-complexity tasks as it relies on binary (1b) or ternary (1.5b)
    weights, and on raw [\[1\]](#page-13-1), [\[2\]](#page-13-2), [\[3\]](#page-13-3)
    or amplified [\[9\]](#page-13-8) photocurrents subject to significant fixed-pattern
    noise (FPN), i.e., local mismatch between pixel responses. On the other hand,
    near-sensor and hybrid vision chips usually present a decreased throughput compared
    to insensor ones, but are better suited to the execution of mediumcomplexity tasks
    thanks to the use of large-size 1.5b Haarlike filters [\[4\]](#page-13-0), [\[5\]](#page-13-4),
    [\[9\]](#page-13-8), [\[10\]](#page-13-9) or to an increased 4b filter weight
    resolution [\[7\]](#page-13-6), [\[8\]](#page-13-7). They make use of conventional
    pixel structures such a three- or four-transistor (3T or 4T) active pixel sensor
    (APS) or pulse-width-modulated (PWM) digital pixel sensor, which are compatible
    with double sampling techniques to compensate FPN. 3T/4T pixels respectively use
    rolling and global shutters, and can either rely on a voltagebased readout with
    a source follower (SF), or on a time-based one with a PWM structure, allowing
    to reduce the supply voltage without degrading the output dynamic range. Besides,
    near-sensor and hybrid vision chips can operate at multiple spatial scales thanks
    to image downsampling (DS) [\[4\]](#page-13-0), [\[5\]](#page-13-4) or filter
    dilation [\[9\]](#page-13-8). Finally, an analog memory generally based on capacitors
    (caps) is required to store a few rows of the image, ultimately leading to power
    and/or area overheads. Nevertheless, existing works fall short of preserving EE
    while simultaneously supporting medium-complexity tasks, which require sufficient
    computational accuracy brought by FPNcompensated inputs and increased weight resolution,
    as well as multiscale operation and large filters for tasks as RoI detection.


    In this work, we present a mixed-signal near-sensor convolutional imager system-on-chip
    (SoC) codenamed MANTIS, fabricated in United Microelectronics Corporation (UMC)
    0.11-µm CMOS technology and supporting both FE and RoI detection. It includes
    two main contributions providing an effective answer to the aforementioned limitations
    of existing vision chips. First, circuits called DS3 units combining three operations
    which can be abbreviated as DS, namely double sampling, to mitigate the impact
    of FPN, voltage downshifting, to reduce the voltage level from the pixel array
    to the convolution processor, and image downsampling, to allow for multiscale
    operation. Second, a mixed-signal convolution processor implementing 4b-weighted
    multiply-and-accumulate (MAC) operations in the charge domain, based on a modified
    switched-capacitor (SC) amplifier structure to compute the partial sum (psum)
    of a row of an image patch, and on a charge sharing operation in the capacitive
    digital-to-analog converter (DAC) of the following successive-approximation (SAR)
    analog-to-digital converter (ADC) to aggregate psums


    ![](_page_1_Figure_4.jpeg)


    <span id="page-1-0"></span>Fig. 2. MANTIS CMOS imager SoC (a) modes of operation
    and (b) architecture, detailing the different blocks in the digital core and image
    sensor analog core with their respective power domains.


    ![](_page_1_Figure_6.jpeg)


    <span id="page-1-2"></span>Fig. 3. Block diagram of (a) the convolution and (b)
    the imaging pipelines.


    of different rows. In our vision, MANTIS would be used as the first stage of a
    cascaded processing system [Fig. [1\(](#page-0-0)c)] supporting low- to medium-complexity
    processing tasks, while a highcomplexity processing based on convolutional or
    deep neural networks (CNNs or DNNs) would be executed by a digital processor.
    The major benefits of such a system are to limit the amount of I/O data transfers
    from the image sensor to the digital processor, and to only dedicate energy to
    the processing of relevant data. This paper extends our conference paper [\[11\]](#page-13-10)
    by providing a more in-depth description of the circuits constituting the convolution
    pipeline, highlighted in Fig. [2,](#page-1-0) as well as additional experimental
    results. The remainder of this paper is organized as follows. First, Section [II](#page-1-1)
    describes the architecture of the SoC. Then, Section [III](#page-2-0) discusses
    the design and implementation of the proposed mixed-signal convolution pipeline,
    while Section [IV](#page-8-0) presents measurement results of the SoC. Finally,
    Section [V](#page-11-0) compares this work to the state of the art, and Section
    [VI](#page-12-0) offers some concluding remarks.


    ### II. SYSTEM-ON-CHIP DESCRIPTION


    ### <span id="page-1-1"></span>*A. Architecture*


    This section describes the modes of operation and architecture of MANTIS CMOS
    imager SoC, respectively depicted in Figs. [2\(](#page-1-0)a) and (b). Three modes
    of operations are supported. First, the imaging mode produces 8b 128×128 images,
    which


    ![](_page_2_Figure_1.jpeg)


    <span id="page-2-1"></span>Fig. 4. (a) Schematic, (b) timing diagram, and (c)
    90◦-rotated layout of a single column-parallel DS3 unit. VCM = 1.2 V and VREF
    = 0.6 V in (a).


    are necessary to thoroughly compare the mixed-signal onchip execution of the convolution
    operations, subject to analog nonidealities, to an ideal software baseline. Second,
    FE can be performed using 2D convolution operations between the image and 4b-weighted
    filters of fixed size F = 16. All parameters are programmable, with the filter
    stride S and the downsampling factor DS respectively taking any power-of-two value
    between 2 and 16, and between 1 to 4, and the number of filters ranging from 1
    to 32. This mode generates feature maps (fmaps) with a programmable power-of-two
    resolution between 1 and 8 bits. Finally, an RoI detection mode supports the comparison
    of fmap values with a different threshold for each filter directly in the SAR
    ADCs. These thresholds are implemented as offsets modifying the fmap values. In
    this last mode of operation, 1b fmaps are created by the imager, which are subsequently
    combined to yield an RoI heatmap and 1b detection map, as illustrated in Fig.
    [2\(](#page-1-0)a) for the detection of faces.


    Furthermore, the SoC architecture revolves around a Cortex-M4 central processing
    unit (CPU) from ARM, embedding a mixed-signal image sensor macro. First, regarding
    the *digital part* of the SoC, efficient data transfers are supported by a direct
    memory access (DMA) peripheral allowing to move data through the advanced high-performance
    (AHB) bus from the imager output registers to a master digital camera interface
    (DCMI), which then transmits this data off chip in an 8b-parallel fashion. Moving
    on to the image sensor macro [Fig. [2\(](#page-1-0)b) right], it includes several
    configuration registers for the parameters of the convolution operations discussed
    here-


    ![](_page_2_Figure_5.jpeg)


    <span id="page-2-2"></span>Fig. 5. Schematics of (a) the inverter-based OTA proposed
    in [\[12\]](#page-13-11), and of (b) the enable circuit shared by all 128 column-parallel
    DS3 units.


    above, among others, a 4-kB local SRAM memory, denoted as LMEM, which can store
    up to 32 4b 16×16 filters, and 32 8b registers for the corresponding thresholds
    or offsets in RoI mode. These registers impact the behavior of the digital controller
    piloting the analog core of the imager. Next, the *analog part* of the SoC relies
    on a 3T 128×128 pixel array with two readout pipelines: (i) a convolution pipeline
    supporting both FE and RoI detection modes, and (ii) an imaging pipeline used
    in imaging mode. It also includes a bias generation circuit used by both pipelines.
    The digital core is supplied at 1.2 V while the analog circuitry relies on two
    supplies at 1.2 and 2.5 V, respectively.


    ## *B. Convolution Pipeline*


    In the convolution pipeline [Fig. [3\(](#page-1-2)a)], raw pixel voltages go through
    128 column-parallel DS3 units, which zero out the FPN by a double sampling technique
    known as delta-reset sampling (DRS). It consists in subtracting the signal voltage
    of a pixel from its reset voltage, thereby suppressing the impact of local mismatch
    on its output. In addition, DS3 units also perform image DS to support multiscale
    operation. The output voltage of DS3 units are then stored in an analog memory
    with a capacity of 16 rows. Next, the stored pixel values are employed as inputs
    to 128 MAC units, connected to eight SC amplifiers computing partial convolution
    results or psums in the analog domain, under the form of voltages. These psums
    are stored in the capacitive DAC of the SAR ADCs following the SC amplifiers,
    before being aggregated by a charge sharing operation in the capacitive DAC and
    digitized to produce convolution results, with the resolution of the produced
    fmaps being a power of two between 1 and 8 bits.


    ## *C. Imaging Pipeline*


    In the imaging pipeline [Fig. [3\(](#page-1-2)b)], DRS is used to mitigate FPN,
    as is done in DS3 units in the convolution pipeline. DRS units also implement
    voltage downshifting to adapt the 2.5-V signals from the pixel array to the 1.2-V
    input of the 8b SAR ADCs. Note that the outputs of column-parallel DRS units in
    16 adjacent columns are multiplexed to a single ADC, leading to a total of eight
    ADCs to digitize a complete row.


    ## <span id="page-2-0"></span>III. CIRCUIT IMPLEMENTATION AND DESIGN OF THE MIXED-SIGNAL
    CONVOLUTION PIPELINE


    The objective of this section is to present the circuit implementation of the
    mixed-signal convolution pipeline, and to provide insights regarding the design
    of the circuits it contains.


    ![](_page_3_Figure_1.jpeg)


    <span id="page-3-1"></span>Fig. 6. Illustration of image DS by 4× with (a) the
    timing diagram, (b) the operation principle, and (c) the schematic of four DS3
    units in four neighboring columns. (d) Connection of the switches shorting the
    inputs and outputs of inverter-based OTAs for different DS factors.


    ![](_page_3_Figure_3.jpeg)


    <span id="page-3-2"></span>Fig. 7. (a) VPIX in process corners, and (b) variability
    of VPIX for 10<sup>3</sup> MC simulations with local mismatch. Image DS by 2×
    for 2×10<sup>3</sup> random combinations of inputs drawn from a uniform distribution,
    with (c) a comparison between ideal and simulated results, and distributions of
    the error ∆VPIX = (VPIX − VPIX, ideal) in (d) pre- and (e) post-layout simulations.
    All figures correspond to the TT 25 ◦C corner, except for (a) which covers all
    five process corners.


    To do so, this section is divided in two parts. Section [III-A](#page-3-0) deals
    with the calculation of FPN-compensated downsampled pixel voltages and their storage
    in the analog memory, and consequently covers the DS3 units and the analog memory.
    Section [III-B](#page-5-0) discusses the charge-domain MAC operations and the
    digitization of the convolution results, and examines the MAC units, the SC amplifiers,
    and the SAR ADCs.


    ## <span id="page-3-0"></span>*A. Image Readout, Downsampling, and Storage*


    *1) DS3 Units for Image Readout and Downsampling:* Fig. [4](#page-2-1) illustrates
    the operation of a column-parallel DS3 unit to read a single pixel value while
    performing DRS and voltage downshifting. This operation consists of three steps
    [Fig. [4\(](#page-2-1)b)]. In step ⃝1 , the signal coming from the 3T APS, resulting
    from the discharge of the internal pixel node VPD by the photocurrent during the
    exposure time, is read on the column voltage VCOL. To do so, we rely on the partial
    settling or dynamic SF readout from [\[13\]](#page-13-12), which consists in resetting
    VCOL to ground using the COL\_RST switch, before enabling the SF during a finite
    amount of time (ROW\_SEL[i] = 1). This readout is more energy-efficient than the
    conventional one using a current source at the bottom of the column, as it eliminates
    any static current consumption. It also presents an optimal settling time [\[13\]](#page-13-12)
    which minimizes the variability of (VRST − VSIG), that we find to be 0.5 µs in
    our design. At the end of step ⃝1 , the signal value has been sampled on the 26-fF
    MOM cap CSIG. Then, in step ⃝2 , the pixel is reset and the resulting value is
    sampled on CRST, whose capacitance is the same as CSIG. Finally, during step ⃝3
    , these two caps are connected with opposite polarities, and their charges are
    dumped on a 58-fF MOM feedback cap CFB, resulting in a voltage VPIX in which (VRST
    − VSIG) is multiplied by the capacitance ratio CS/CFB = 0.45. Thus, the operations
    of (i) DRS and (ii) voltage downshifting are realized. Moreover, in the schematic
    depicted in Fig. [4\(](#page-2-1)a), switches connected to ground or VDD are respectively
    implemented with single nMOS or pMOS, while other switches are transmission gates
    (TGs). They rely on 3.3-V I/O transistors to withstand the 2.5-V supply, with
    W = 0.25 µm and L = Lmin = 0.34 µm, except for the TGs connected to VREF and VCM
    for which L = 0.68 µm to reduce leakage. Regarding the capacitors, they are chosen
    to ensure that local mismatch, noise, and layout parasitics have a minimal impact
    on the circuit behavior, but they could be downsized as long as the uncertainty
    and voltage attenuation remain within the specifications of the target algorithm.
    Further reduction could be achieved by accounting for these nonidealities in the
    training algorithm, as is done in [\[14\]](#page-13-13) for in-memory computing
    (IMC). A similar design choice is made for the other mixed-signal circuits in
    this work.


    Besides, to compensate for the offset of the inverter-based operational transconductance
    amplifier (OTA) [\[12\]](#page-13-11), it is put in autozero (AZ) during steps
    ⃝1 and ⃝2 . This corresponds to sampling on the two 50-fF MIM caps CAZ the difference
    between the common-mode voltage VCM and the VGS of M1−<sup>2</sup> with a fixed
    1-µA bias current, imposed by the floating current source formed by M5−<sup>6</sup>
    [Fig. [5\(](#page-2-2)a)]. Moreover, a key feature to reach a high EE is the enable
    circuit shared by all amplifiers [Fig. [5\(](#page-2-2)b)], implemented by respectively
    clamping bias voltages VBN1,INT and VBP1,INT to ground and VDDAH, and thus allowing
    to duty cycle DS3 units to save power.


    Next, the image DS relies on a principle proposed in [\[6\]](#page-13-5) and represented
    for a DS by 4× in Fig. [6.](#page-3-1) As illustrated in Fig. [6\(](#page-3-1)c),
    the inputs and outputs of the OTAs in four adjacent columns are shorted together
    by switches whose configuration depends on the DS factor [Fig. [6\(](#page-3-1)d)].
    The average of each row of a 4×4 image patch is computed and stored in the hold
    cap C<sup>H</sup> of one of the four columns, as shown in steps ⃝1 to ⃝4 [Figs.
    [4\(](#page-2-1)a) to (c)]. Once all row averages have been computed, all C<sup>H</sup>
    caps are simultaneously connected during step ⃝5 , and the resulting voltage is
    the average of row averages, or in other words, the average of the image patch.
    The proposed DS3 unit fits into the 6.03-µm pixel pitch and occupies 74.73 µm
    in height [Fig. [4\(](#page-2-1)c)], a dimension which could be further reduced
    if transistors could be placed below MIM caps CAZ.


    ![](_page_4_Figure_1.jpeg)


    <span id="page-4-0"></span>Fig. 8. (a) Schematic and (b) layout of an analog memory
    cell. (c) Connections at the input and output of a column of 16 row of memory
    cells and (d) timing diagram for write and read operations.


    ![](_page_4_Figure_3.jpeg)


    <span id="page-4-1"></span>Fig. 9. (a) Voltage change of VMEM after 100 ms in
    TT and FF, and at 25 ◦C and 85 ◦C. (b) Retention time tret in TT 85 ◦C and FF
    85 ◦C (worst case). tret is defined as the time at which the initial voltage has
    changed by more than 2.35 mV, corresponding to half an LSB for a 1.2-V 8b ADC.
    At 25 ◦C, (c) transfer function from VPIX to VBUF in process corners and (d) variability
    of VBUF for 10<sup>3</sup> MC simulations with local mismatch.


    This circuit is robust to process, voltage, and temperature (PVT) variations due
    to its ratiometric nature, as long as the OTA is designed to operate in the relevant
    corners. Thus, this article does not aim at providing an exhaustive characterization
    of the proposed circuits in PVT corners, but focuses on their main performance
    and limitations. Post-layout simulations in Fig. [7](#page-3-2) confirm the independence
    with respect to process [Fig. [7\(](#page-3-2)a)] and show that the σ and σ/µ
    of VPIX due to local mismatch for a single DS3 unit are respectively below 2.2
    mV and 0.4% across the input range. Regarding the output voltage noise, a theoretical
    expression is given by v<sup>n</sup> = C<sup>S</sup> <sup>C</sup>FB <sup>q</sup>2kT
    C<sup>S</sup> , where k is Boltzmann''s constant and T the absolute temperature.
    At 25 ◦C, v<sup>n</sup> = 0.25 mV and its impact is significantly lower than that
    of mismatch. In addition, the performance of DS is evaluated for DS by 2× in Figs.
    [7\(](#page-3-2)c) to (e), for 2×10<sup>3</sup> combinations of input voltages
    drawn from


    ![](_page_4_Figure_6.jpeg)


    <span id="page-4-2"></span>Fig. 10. (a) Storage pattern of the image into the
    analog memory for different DS factors. Filter striding for a convolution operation
    with a stride S = 4 (b) without image DS (DS = 1) and (c) with image DS by 2×
    (DS = 2).


    a uniform distribution. The σ of the error ∆VPIX increases from less than 1 mV
    pre-layout to approximately 10 mV postlayout, as highlighted in Figs. [7\(](#page-3-2)d)
    and (e), due to capacitive coupling between nodes VIN, VPIX and V<sup>H</sup>
    [Fig. [4\(](#page-2-1)a)] which could be reduced by investing more effort into
    the layout.


    *2) Analog Memory for Image Storage:* The schematic and operation of the analog
    memory are described in Fig. [8.](#page-4-0) A memory cell with a structure close
    to [\[4\]](#page-13-0), [\[5\]](#page-13-4) [Fig. [8\(](#page-4-0)a)] consists
    of a 32-fF MOS cap MCAP, an access transistor M<sup>W</sup> with a dummy transistor
    MW,DUM with half the length, to compensate for the charge injection of MW, and
    an SF MSF employed in a dynamic fashion for its reduced mismatch of VBUF and decreased
    static power consumption, similar to the pixel readout in Section [III-A.](#page-3-0)
    This memory cell occupies a silicon area of 6.03 µm × 6.075 µm [Fig. [8\(](#page-4-0)b)],
    close to that of a pixel. To read or write a cell located within a column of the
    analog memory [Figs. [8\(](#page-4-0)c) and (d)], several switches are used to
    connect the column internal voltage VPIX,INT to the output of the DS3 units, or
    to ground/VDDAL. During a write operation, in step ⃝1 , VPIX,INT, VMEM and VSF
    are grounded to overwrite the memory cell content without any impact from previously
    stored values. Then, in step ⃝2 , VMEM is driven to VPIX by the DS3 unit connected
    to the column, before disconnecting VMEM from VPIX,INT. During a read operation,
    VBUF is first reset to ground in step ⃝3 , before reading the memory cell by partial
    settling in step ⃝4 .


    When the memory is not written or read, the retention of the memory cells needs
    to be maximized in the worst-case corner, here FF 85 ◦C. To do so, we minimize
    the leakage of the access transistor M<sup>W</sup> by implementing it with a 3.3-V
    I/O nMOS with W = 0.18 µm and L = 1 µm, and additionally, by driving VPIX,INT
    to VDDAL = 1.2 V in retention to limit the VDS of M<sup>W</sup> and further reduce
    the leakage, given that VPIX approximately ranges from 0.6 to 1.5 V [Fig. [7\(](#page-3-2)a)].
    Continuing with the post-layout characterization of the memory, Fig. [9\(](#page-4-1)a)
    highlights that the typical voltage change of the stored voltage after 100 ms
    is respectively 2.61 and 2.18 mV in the TT and FF process corners at 85 ◦C, while
    Fig. [9\(](#page-4-1)b) indicates retention times of 90.3 and 106.9 ms in the
    same conditions, the retention time being defined as a change of ±LSB/2 with respect
    to the initially stored voltage, i.e., ±2.35 mV for a


    ![](_page_5_Figure_1.jpeg)


    <span id="page-5-1"></span>Fig. 11. (a) Principle of the convolution operation
    between a 16×16 image patch and a 4b 16×16 filter. (b) Schematic of the SC amplifier
    realizing the MAC operations between a single row of the image patch and filter,
    with (c) the corresponding timing diagram. The common-mode voltage VCM is equal
    to VDDAL/2 = 0.6 V. (d) Detailed schematic and (e) 90◦-rotated layout of one of
    the 16 MAC units connected to the SC amplifier.


    1.2-V supply and an 8b resolution. In addition, in Fig. [9\(](#page-4-1)a), the
    linear increase of ∆VMEM in TT 25 ◦C for VPIX < 1 V can be explained by a slow
    transient of VSF lightly affecting VMEM through capacitive coupling. In Fig. [9\(](#page-4-1)c),
    we observe that the transfer function of the SF has a slope ASF below 1 V/V due
    to the body effect resulting from MSF''s body being grounded, and that it is impacted
    by variations of MSF''s threshold voltage in process corners even though the slope
    remains around 0.83 V/V. Finally, VBUF has a σ around 3.5 mV in the usable part
    of the input range, corresponding to a maximum σ/µ of 2.3% for VPIX = 0.6 V, while
    in comparison, the output noise <sup>v</sup><sup>n</sup> <sup>=</sup> <sup>A</sup>SF<sup>q</sup>
    kT CMEM equal to 0.3 mV at 25 ◦C is negligible. Future designs could compensate
    the SF mismatch by making use of an OTA-based feedback loop to write the analog
    memory, as proposed by Seo et al. in [\[15\]](#page-13-14).


    ### <span id="page-5-0"></span>*B. Charge-Domain Multiply-and-Accumulate Operations*


    *1) Operation Principle:* When no DS is applied to the image, the columns of the
    pixel array match that of the analog memory in a one-to-one fashion, and the whole
    width of the analog memory is used to store the image [Fig. [10\(](#page-4-2)a)].
    The convolution operation is thus performed between several replicas of the 4b
    16×16 filter and different image patches without overlap [Fig. [10\(](#page-4-2)b)].
    As the filter is shifted to the right, the connections between the analog memory
    and the eight SC amplifiers are modified over time to follow the movement of the
    filter. However, when a DS by 2× is applied, the image is only 64-columns wide,
    so the first half of the analog memory stores the downsampled image, while the
    second half stores a version of the image shifted by eight columns to the left
    [Fig. [10\(](#page-4-2)a)]. This routing from the outputs of the DS3 units to
    the analog memory is ensured by switches changing the connections depending on
    the DS factor. When computing the convolution operation, this storage pattern
    of the image into the analog memory allows to improve throughput, by executing
    in parallel operations corresponding to two different shifts of the image in the
    execution without DS [Fig. [10\(](#page-4-2)c)]. The throughput is thereby increased
    by the DS factor, here 2×. The same reasoning holds for a DS by 4× for which three
    shifted versions of the image are used, as shown in Fig. [10\(](#page-4-2)a).


    *2) Switched-Cap Amplifiers for Multiplication:* We now zoom in on the convolution
    operation between a 16×16 image patch and a 4b 16×16 filter, computed by the process
    depicted in Fig. [11](#page-5-1) using an SC amplifier. Phase ⃝1 of this process,
    presented in Fig. [11\(](#page-5-1)a), consists in successively computing the
    psums resulting from the convolution of a row of pixels stored in the analog memory
    with the corresponding row of 4b filter weights stored in the LMEM. Each psum
    is stored in a 16th of the SAR ADC capacitive DAC (CDAC), until all psums have
    been computed. In phase ⃝2 , the CDAC is disconnected from the SC amplifier (VIN\_CONNECT
    = 0) and all capacitors storing psums are shorted together to compute the final
    convolution result by charge sharing on node VSH.


    Going one step further, the psum of a row is computed by the SC amplifier circuit
    drawn in Fig. [11\(](#page-5-1)b), whose timing diagram is detailed in Fig. [11\(](#page-5-1)c).
    In step ⃝1 , the OTA, based on a two-stage Miller architecture, is enabled and
    its feedback is activated. As for the DS3 units, power gating the OTA is a key
    feature to save energy and improve the EE of the accelerator. Then, steps ⃝2 and
    ⃝3 respectively consist in resetting the columns of the analog memory corresponding
    to positiveweighted inputs and in reading these inputs from the analog memory.
    Next, in step ⃝4 , the columns of the analog memory corresponding to a negative
    weight are reset, while connecting them to the input of the corresponding MAC
    units. Lastly, during step ⃝5 , the negative-weighted inputs are read, and charges
    are dumped on node VAMP,IN<sup>−</sup> by caps C+,<sup>i</sup> and C<sup>−</sup>,<sup>j</sup>
    , yielding an output voltage VMAC containing the psum


    ![](_page_6_Figure_1.jpeg)


    <span id="page-6-0"></span>Fig. 12. (a) Standard deviation and (b) distribution
    of VAMP,IN<sup>−</sup> and VOUT, for 10<sup>3</sup> MC simulations with local
    mismatch. (b) depicts the histograms for ∆VBUF = 0.3 V. (c) Mean and (d) standard
    deviation of the error ∆VMAC = (VMAC − VMAC, ideal) for 10<sup>4</sup> random
    combinations of inputs and weights without mismatch and noise, with only local
    mismatch, and with only intrinsic noise. All figures correspond to the TT 25 ◦C
    corner.


    ∆VCONV referred to VCM. The formula given in Fig. [11\(](#page-5-1)b) can be intuitively
    understood by noticing that the inputs of caps C+,i are applied when ϕ1,SC = 1,
    while the inputs of caps C<sup>−</sup>,<sup>j</sup> are applied when ϕ2,SC = 1.
    The inputs associated with C+,<sup>i</sup> thus follow the behavior of a non-inverting
    SC amplifier, while those associated with C<sup>−</sup>,<sup>j</sup> follow that
    of an inverting one, thus explaining the formula for VMAC. Despite the fact that
    the computation is performed in the mixed-signal domain and suffers from analog
    nonidealities, the proposed structure features several properties ensuring the
    robustness of the computation. (i) It has a single-ended output which does not
    rely on intermediate differential voltages, avoiding an incorrect result when
    the differential voltage is small but the common mode is large and potentially
    subject to saturation. This is an issue encountered in previous charge-domain
    near-sensor architectures [\[5\]](#page-13-4). (ii) The proposed structure is
    ratiometric and robust to PVT variations. (iii) It is not impacted by the statistical
    offset of the OTA thanks to the offset-insensitive switching scheme. Indeed, the
    charges at node VAMP,IN<sup>−</sup> are


    $$\begin{aligned} Q\_1 &= -\sum\_{i=1}^{N\_+} C\_{+,i} (V\_{\text{BUF}+,i} - V\_{\text{AMP,IN}-})
    \\ &- \sum\_{j=1}^{N\_-} C\_{-,j} (-V\_{\text{AMP,IN}-}) + C\_{\text{FB}} (V\_{\text{AMP,IN}-}
    - V\_{\text{CM}}) \tag{1} \end{aligned}$$


    for ϕ1,SC = 1, where N<sup>+</sup> and N<sup>−</sup> respectively stand for the
    number of positive- and negative-weighted inputs, and


    $$\begin{aligned} Q\_2 &= -\sum\_{j=1}^{N\_-} C\_{-,j} (V\_{\text{BUF}-,j} - V\_{\text{AMP,IN}-})
    \\ &- \sum\_{i=1}^{N\_+} C\_{+,i} (-V\_{\text{AMP,IN}-}) + C\_{\text{FB}} (V\_{\text{AMP,IN}-}
    - V\_{\text{MAC}}) \tag{2} \end{aligned}$$


    for ϕ2,SC = 1. Interestingly, the resulting expression for VMAC based on the conservation
    of charge at node VAMP,IN<sup>−</sup>, i.e., Q<sup>1</sup> = Q2, does not depend
    on the value of VAMP,IN<sup>−</sup> when ϕ1,SC = 1 and therefore, is independent
    of the OTA''s offset.


    ![](_page_6_Figure_9.jpeg)


    <span id="page-6-1"></span>Fig. 13. Leakage current through TGs in the MAC unit
    can lead to variability of VMAC due to global process variations. (a) Simplified
    schematic of the MAC unit with transistor-level switch implementation, illustrating
    the origin of this leakage, and (b) standard deviation of the error ∆VMAC for
    10<sup>4</sup> random combinations of inputs and weights with local mismatch and
    global process variations, for TGs realized with LVT core devices with L = 120
    and 240 nm, or HVT core ones with L = 120 nm.


    Furthermore, the implementation of the 4b weights is given in Fig. [11\(](#page-5-1)d),
    with the most-significant bit (MSB) W[3] corresponding to the sign bit and least-significant
    bits (LSBs) W[2:0] to the magnitude bits. The sign bit determines which signals
    control the connections at the input of the MAC unit, while the magnitude bits
    determine the number of 7-fF unitary MOM caps C<sup>U</sup> connected in parallel
    in each MAC unit. This circuit thus implements integer weights ranging from -7
    to 7, multiplied by a factor 0.25×, originating from the fact that each column
    includes a part of the feedback cap CFB equal to 4CU. The MAC unit fits inside
    the pixel pitch and occupies a height of 28.85 µm [Fig. [11\(](#page-5-1)e)].


    The performance of these MAC units is characterized with post-layout simulations
    in Fig. [12.](#page-6-0) First, Figs. [12\(](#page-6-0)a) and (b) correspond to
    a setup in which eight MAC units have their weight set to +7 with a shared input
    voltage VBUF,+, while the other eight units have their weight set to -7 with input
    voltage VBUF,<sup>−</sup>. The impact of local mismatch is studied in this context
    with 10<sup>3</sup> Monte Carlo (MC) simulations. Across the input range ∆VBUF
    = (VBUF,<sup>+</sup> − VBUF,<sup>−</sup>), the σ of VMAC remains below 1 mV while
    that of VAMP,IN<sup>−</sup> is affected by the statistical offset of the OTA,
    leading to a 2.5-mV σ. More specifically, for ∆VBUF = 0.3 V, the proposed offsetinsensitive
    structure reduces the σ by 14.2× from 2.55 to 0.18 mV. However, this first setup
    only accurately describes a specific realization of input voltages and weights.
    Therefore, Figs. [12\(](#page-6-0)c) and (d) extend this analysis to a baseline
    of 10<sup>4</sup> random combinations of inputs and weights drawn from uniform
    distributions, with local mismatch and intrinsic noise subsequently applied on
    top of it. In the VMAC output range between 0.15 and 1.05 V, avoiding transistors
    to be biased outside of saturation, the average error in Fig. [12\(](#page-6-0)c)
    shows a deterministic behavior hinting at a slope error which can be related to
    parasitic capacitances or charge injection, while the average standard deviation
    of the error over the considered output range increases from 0.42 mV to respectively
    0.80 and 0.74 mV with local mismatch and intrinsic noise. These variabilities
    are predominantly due to the mismatch of MOM caps and the thermal kT /C noise
    due to the sampling of input voltages on the capacitors in the MAC units. These
    results demonstrate the robustness of the proposed architecture to these analog
    nonidealities. However, Fig. [13](#page-6-1) highlights one of the architectural
    details which could be further improved. As emphasized in Fig. [13\(](#page-6-1)a),
    when one of the magnitude


    ![](_page_7_Figure_1.jpeg)


    <span id="page-7-0"></span>Fig. 14. (a) Schematic, (b) timing diagram, and (c)
    layout of the 8b SAR ADC spanning over 16 columns of the analog memory. Detailed
    schematic of (d) the dynamic comparator, consisting of two preamplification stages
    and a StrongARM latch, and (e) the capacitive DAC, employing the split-MSB and
    split-array techniques.


    bits is equal to zero, a leakage current flows through the TG, implemented with
    high-speed low-Vth (LVT) core devices with a minimum length of 120 nm. Hence,
    this introduces a stochastic error on VMAC when both local mismatch and global
    process variations are considered, with an average σ around 7.46 mV [Fig. [13\(](#page-6-1)b)]
    compared to 0.80 mV with mismatch only. To mitigate this problem, the transistors
    constituting the TG can either be made longer or rely on low-leakage high-Vth
    (HVT) core devices, respectively reducing the average σ to 2.12 and 0.40 mV. Interestingly,
    the 0.40-mV σ obtained with an HVT TG is even lower than the one obtained with
    local mismatch only with the LVT TG, suggesting that the sensitivity to local
    mismatch not only stems from MOM caps, but also from the TG leakage.


    *3) SAR ADCs for Aggregation and Digitization:* The SAR ADCs employed in this
    work follow the topology presented in Fig. [14\(](#page-7-0)a) with the associated
    timing diagram shown in Fig. [14\(](#page-7-0)b). Similarly to the SC amplifiers,
    each SAR ADC spans 16 pixel columns [Fig. [14\(](#page-7-0)c)]. The two main functions
    of the SAR ADC employed in this work are ⃝1 the aggregation of psums by charge
    sharing, following the calculation of the psums of rows by the SC amplifiers,
    and ⃝2 the analog-todigital (A2D) conversion of the convolution result, following
    the SAR principle. In terms of circuit implementation, the detailed comparator
    architecture in Fig. [14\(](#page-7-0)d) features two differential preamplification
    stages based on a differential pair driving a load of diode-connected transistors,
    and providing a total gain of approximately 10 V/V. These stages both embed AZ
    capabilities based on caps at their output, and are followed by a dynamic StrongARM
    latch. In addition, the CDAC combines two existing techniques to reduce its power
    and area overheads. First, it relies on a split-MSB array [\[16\]](#page-13-15),
    based on two identical DACs for the MSB and all the LSBs, respectively called
    MSB DAC and main DAC in Figs. [14\(](#page-7-0)c) and (e). More


    ![](_page_7_Figure_5.jpeg)


    <span id="page-7-1"></span>Fig. 15. (a) Transfer function of the SAR ADC with
    the corresponding (b) DNL and (c) INL. (d) Power consumption on VDDAL = 1.2 V
    (CDAC, comparator, and drivers) as a function of the input voltage. (e) Statistical
    offset of the StrongARM latch referred to the input of the first preamplifier.
    All figures correspond to the TT 25 ◦C corner.


    specifically, as illustrated in Fig. [14\(](#page-7-0)b), the MSB DAC is initially
    switched to VDD. Then, when the output data DOUT of the comparator is equal to
    zero, the current bit of the MSB DAC is switched to ground, whereas when DOUT
    is equal to one, the current bit of the main DAC is switched to VDD. This allows
    to harmonize power consumption across the input voltage range by optimizing the
    switching of the DAC. Then, it also makes use of a split-capacitive array [\[17\]](#page-13-16),
    employing an attenuation cap C<sup>A</sup> in Fig. [14\(](#page-7-0)e) to reduce
    the impact of the LSBs, ultimately allowing to diminish the capacitance of the
    MSBs. Besides, possible further improvements in terms of EE and silicon area can
    be obtained by employing parasitic caps instead of explicit MOM ones, as outlined
    by Harpe in [\[18\]](#page-13-17). Interestingly, in the RoI detection mode generating
    1b fmaps, the offset associated with each filter is also implemented with the
    CDAC, by switching up (resp. down) bits of the main (resp.


    ![](_page_8_Figure_1.jpeg)


    <span id="page-8-1"></span>Fig. 16. (a) Chip microphotograph with overlaid layout.
    (b) Example of captured images with an exposure time of 20 ms and (c) imaging
    characteristics of the proposed imager.


    ![](_page_8_Figure_3.jpeg)


    <span id="page-8-3"></span>Fig. 17. (a) Output code, (b) PRNU and TN, and (c)
    SNR as a function of illuminance, computed for 10 images for each illuminance.
    (d) Power breakdown in imaging mode. All figures correspond to an exposure of
    20 ms.


    MSB) DAC to implement a positive (resp. negative) offset.


    Regarding the post-layout simulation results in typical conditions (TT 25 ◦C)
    in Fig. [15,](#page-7-1) a relatively good linearity is achieved in Fig. [15\(](#page-7-1)a),
    with the differential nonlinearity (DNL) comprised between -0.07 and 0.55 LSB,
    and the integral one (INL) between -1.17 and 0.62 LSB [Figs. [15\(](#page-7-1)b)
    and (c)]. Regarding power consumption, it is relatively constant across the input
    voltage range thanks to the split-MSB CDAC, with a mean value of 3.59 and 3.78
    µW in pre- and postlayout simulations [Fig. [15\(](#page-7-1)d)]. At last, the
    input-referred statistical offset of the comparator features a ±3σ value of 1.62
    mV corresponding to 0.35 LSB, hence ensuring that the comparator operation is
    robust to local mismatch [Fig. [15\(](#page-7-1)e)].


    ## IV. EXPERIMENTAL RESULTS


    <span id="page-8-0"></span>MANTIS imager SoC has been fabricated in UMC 0.11-µm
    bulk CMOS technology. The chip microphotograph is shown in Fig. [16\(](#page-8-1)a),
    together with examples of captured images in Fig. [16\(](#page-8-1)b). This section
    presents the experimental results obtained with the fabricated chip, respectively
    focusing on the characterization of the image sensor in Section [IV-A](#page-8-2)
    and of the mixed-signal near-sensor convolution processor in Section [IV-B.](#page-9-0)
    Finally, the applicative performance is evaluated on a face RoI detection task
    in Section [IV-C.](#page-11-1)


    ## <span id="page-8-2"></span>*A. Imaging Performance*


    We first characterize the performance of MANTIS in imaging mode with the results
    summarized in Figs. [16\(](#page-8-1)c) and [17.](#page-8-3) This characterization
    is performed by exposing the imager to an uniform light flux, generated by an
    Olympus KL 1500 halogen light source going through an integrating sphere, and
    by capturing 10 images for each light flux level. The transfer function between
    the imager 8b output code and the light flux per unit area, i.e., the illuminance
    expressed in lm/m<sup>2</sup> or lx, is depicted in Fig. [17\(](#page-8-3)a).
    It highlights that, for a 20-ms exposure time, the usable illuminance ranges from
    120 to 1500 lx. In addition, the transfer function levels off at low illuminance
    due to the relatively low photocurrent values compared to leakages inside the
    pixel, the photoresponse of the n+/psub diodes available in this CMOS logic process
    being far from optimal compared to a CMOS image sensor (CIS) process. Next, Fig.
    [17\(](#page-8-3)b) evaluates two types of noise affecting the image quality,
    namely photoresponse non-uniformity (PRNU), capturing the variability of pixel
    responses to light due to mismatch, and temporal noise (TN). PRNU and TN are worth
    2.44 and 0.75% of the full scale (FS) at 50% of the FS, and correspond to a signal-to-noise
    ratio (SNR) slightly above 20 dB in the usable illuminance range, dominated by
    the PRNU [Fig. [17\(](#page-8-3)c)]. Taking a more theoretical perspective, the
    voltage noise due to thermal noise at the output of the DRS units can be expressed
    as


    $$

    \overline{v\_{\rm n}} = \sqrt{2kT} \sqrt{\frac{A\_{\rm SF}^2}{C\_{\rm PD}} + \frac{1}{C\_{\rm
    S}}} \tag{3}

    $$


    with CPD = 12.2 fF the pixel capacitance, C<sup>S</sup> = 29 fF the sampling capacitance,
    and ASF = 0.69 V/V the gain of the pixel''s SF. This yields v<sup>n</sup> = 0.78
    mV at 25 ◦C corresponding to a 0.65% error with a 1.2-V dynamic range of VPIX,
    which is in the same order of magnitude as the TN in Fig. [17\(](#page-8-3)b).
    A 4T pixel array could easily be integrated to the proposed design by switching
    to pinned photodiodes and modifying the digital controller, and would reduce the
    contribution of TN thanks to correlated double sampling (CDS). The imaging characteristics
    are summarized in Fig. [16\(](#page-8-1)c). Lastly, Fig. [17\(](#page-8-3)d) describes
    the power consumption of the imager for a frame rate of 29 fps. Power is dominated
    by the digital part, which represents 78% of the 335.6-µW SoC power, with the
    following split: 38% for the imager controller, 25% for the CPU, and 13% for data
    transfers by the DMA. It could easily be reduced by moderately scaling VDDD to
    1 V or by making use of power-gating techniques. The power of the analog circuitry
    only amounts to 22% of the SoC power, with most of it (17%) being consumed by
    the pixel array and DRS units supplied at 2.5 V, while the remaining 5% correspond
    to SAR ADCs supplied at 1.2 V. The SoC power corresponds to an energy per pixel
    of 706.3 pJ/(pix·frame), which is larger than state-of-the-art values for low-power
    imagers, typically ranging from 100 to 300 pJ/(pix·frame) [\[13\]](#page-13-12).
    Nonetheless, this is perfectly normal as our SoC is not optimized for imaging,
    and as imagers usually do not include a CPU and a DMA.


    ![](_page_9_Figure_1.jpeg)


    <span id="page-9-1"></span>Fig. 18. Comparison between ideal and measured fmaps,
    respectively computed in software (Matlab) and on chip, obtained by a convolution
    operation between the image and two random 4b 16×16 filters. The parameters used
    for this operation are S = 2, DS = 1, 2 or 4, and an exposure time of 12.5 ms.
    The RMSE for each fmap is written below it, and the error map corresponds to the
    fmap with the worst RMSE among the displayed ones.


    <span id="page-9-3"></span>TABLE I SUMMARY OF THE MEASURED PERFORMANCE OF MANTIS
    IMAGER SOC IN CONVOLUTION MODE FOR FOUR FILTERS (12.5-MS EXPOSURE TIME).


    | Image downsampling (DS)                                  | 1    |                              |      |      |
    2                                                                       |      |      |      |
    4               |                 |      |      |

    |----------------------------------------------------------|------|------------------------------|------|------|-------------------------------------------------------------------------|------|------|------|-----------------|-----------------|------|------|

    | Filter stride (S)                                        | 2    | 4                            |
    8    | 16   | 2                                                                       |
    4    | 8    | 16   | 2               | 4               | 8    | 16   |

    | Frame rate⋆<br>[fps]                                     | 18.2 | 79.7                         |
    79.7 | 79.7 | 79.7                                                                    |
    79.7 | 79.7 | 79.7 | 79.7            | 79.7            | 79.7 | 79.7 |

    | Throughput†<br>[MOPS]                                    | 121  | 137.3 36.7                   |      |      |
    10.5 408.3 110.4 32.0                                                   |      |      |      |
    10.5 211.7 65.3 |                 | 23.5 | 10.5 |

    | Feature map RMSE⋄<br>[%]                                 | 3.01 | 3.25                         |
    4.00 | 4.69 | 3.40                                                                    |
    3.98 | 6.30 | 8.68 |                 | 4.88 11.34 9.19 |      | 8.45 |

    | Power▷<br>(accelerator) [µW]                             |      |                              |      |      |
    66.84 76.20 22.36 8.40 58.74 17.40 6.60                                 |      |      |      |
    4.03 10.07 4.42 |                 | 3.29 | 2.70 |

    | EE◁<br>(accelerator) [TOPS/W]                            | 7.24 | 7.31                         |
    6.57 |      | 4.98 27.80 25.38 19.40 10.37 84.09 59.17 28.61 15.48                    |      |      |      |                 |                 |      |      |

    | Energy/OP◁<br>(accelerator) [fJ/op]                      |      | 138.1 138.7
    152.1 200.9 36.0 |      |      |                                                                         |
    39.4 | 51.6 | 96.4 | 11.9            | 16.9            | 35.0 | 64.6 |

    | Power‡<br>(SoC) [µW]                                     |      |                              |      |      |
    338.5 384.7 297.4 268.9 357.0 288.0 264.7 256.3 271.9 258.3 253.3 250.9 |      |      |      |                 |                 |      |      |

    | EE◁<br>(SoC) [TOPS/W]                                    | 1.43 | 1.43                         |
    0.49 | 0.16 | 4.57                                                                    |
    1.53 | 0.48 | 0.16 | 3.11            | 1.01            | 0.37 | 0.17 |

    | Energy/OP◁<br>(SoC) [pJ/op]                              | 0.70 | 0.70                         |
    2.02 | 6.43 | 0.22                                                                    |
    0.65 | 2.07 | 6.13 | 0.32            | 0.99            | 2.69 | 6.00 |

    | Processing energy (SoC) [pJ/(pix·frame·filt)] 284.1 73.6 |      |                              |
    56.9 | 51.5 | 68.3                                                                    |
    55.1 | 50.7 | 49.0 | 52.0            | 49.4            | 48.5 | 48.0 |


    <sup>⋆</sup> Frame rate is limited by the 12.5-ms exposure time. † Expressed in
    operations with analog inputs and 4b weights. <sup>⋄</sup> Computed over 10 images
    with 10 random filters. <sup>▷</sup> Includes the analog memory, SC amplifiers,
    SAR ADCs and drivers on VDDAL. <sup>◁</sup> Normalized to 1b operations. ‡ Includes
    the imager analog macro (VDDAL and VDDAH), and the digital core, i.e., the Cortex-M4
    CPU, the imager controller, and the SRAM macros (VDDD).


    ![](_page_9_Figure_6.jpeg)


    <span id="page-9-2"></span>Fig. 19. (a) Illustration of sequential and parallel
    exposure and convolution. Measured (b) frame rate and (c) energy per 1b operation
    (SoC) for the sequential and parallel executions and for different DS and S configurations,
    four filters, and a 12.5-ms exposure time.


    ## <span id="page-9-0"></span>*B. Electrical Characterization of Near-Sensor Convolutions*


    To validate the proper operation of the mixed-signal convolution processor, two
    aspects need to be thoroughly quantified: (i) the quality of the fmaps computed
    by the chip with respect to an ideal execution in software [Fig. [18\]](#page-9-1),
    and (ii) the throughput and EE of the MAC operations, at the accelerator and SoC
    levels [Figs. [19](#page-9-2) to [21\]](#page-10-0). This analysis must cover
    the different configurations of the proposed convolution processor in terms of
    image DS factor and filter stride S. For the characterization of throughput and
    EE, we rely on the benchmarking outlined by Shanbhag et al. [\[19\]](#page-13-18)
    in the context of IMC, which shares striking similarities with convolutional imagers,
    except that IMC is weight-stationary while convolutional imagers are input-stationary.
    The concept of *ADC column* originates from this paper, and corresponds in this
    work to a group of 16 columns of the analog memory feeding 16 MAC units, connected
    to an SC amplifier and an 8b SAR ADC.


    We start by comparing ideal fmaps to measured ones based on two important steps.
    First, fmaps need to be normalized, as the pixel values of the 8b 128×128 image
    used in the ideal software execution in Matlab do not reflect the analog values
    employed on chip by the convolution processor. This difference stems from the
    various multiplicative factors applied to raw pixel voltages in the blocks constituting
    the convolution pipeline. For a given fmap denoted as f, the normalized fmap denoted
    as ˆf is computed as


    $$

    \hat{f} = \frac{[f - \mu(f)]}{\sigma(f)},\tag{4}

    $$


    thereby ensuring that the mean µ and the standard deviation σ of the resulting
    fmap are respectively equal to zero and one. The second important step is the
    metric used to assess the quality of the computed fmap. Here, we rely on the root
    mean square error (RMSE) calculated as


    <span id="page-9-4"></span>

    $$\text{RMSE} = \frac{100\%}{2\text{max}(|\hat{f}\_{\text{meas}}|)} \sqrt{\frac{1}{N\_{\text{f}}^{2}}
    \sum\_{i=1}^{N\_{\text{f}}} \sum\_{j=1}^{N\_{\text{f}}} \left(\hat{f}\_{\text{ideal,ij}}
    - \hat{f}\_{\text{meas},ij}\right)^{2}} \tag{5}$$


    where ˆfideal and ˆfmeas are respectively the normalized ideal and measured fmaps,
    and N<sup>f</sup> is the fmap size, obtained from


    $$N\_{\rm f} = \left(\frac{128}{\rm DS} - \rm F\right) \frac{1}{\rm S} + 1\tag{6}$$


    with F = 16 the filter size. The characterization is performed over 10 images,
    among which nine are part of the KODAK dataset of natural images, and with 10
    4b-weighted filters drawn from a uniform distribution. Table [I](#page-9-3) details
    the RMSE results. It indicates that the RMSE is comprised between 3.01 and 11.34%,
    and that it tends to degrade for smaller fmaps with a larger DS factor and/or
    a larger filter stride. This is quite intuitive to understand, given that [\(5\)](#page-9-4)
    relies on max(| ˆfmeas|) to approximate the range of values contained in an fmap,
    and is hence sensitive to errors in large values of ˆfmeas. However, we believe
    that the proposed metric provides both an intuition and a quantification of the
    magnitude of the error, despite these inaccuracies. A few fmaps are displayed
    in Fig. [18](#page-9-1) with the corresponding RMSE, as well as an error map for
    the fmap with the worst RMSE among the displayed ones. It reveals that the measured
    fmaps strongly resemble the ideal ones and properly capture the image features.
    Errors are barely noticeable with the naked eye and consist of slightly different
    values between fmaps.


    Then, we turn to the assessment of the throughput and EE of the MAC operations.
    We first introduce the throughput as


    <span id="page-10-1"></span>

    $$\text{Throughput} = \text{fps} \cdot N\_{\text{filt}} \cdot N\_{\text{f}}^2
    \cdot (2 \cdot \text{F}^2 \cdot \text{DS}^2) \tag{7}$$


    where Nfilt corresponds to the number of filters. This definition of the throughput
    does not account for the resolution of the inputs and weights involved in the
    MAC operations. Next, we can define the energy per 1b operation as


    $$\text{Energy/OP} = \frac{\text{Power}}{\text{fps} \cdot N\_{\text{filt}} \cdot
    N\_{\text{f}}^2 \cdot (2 \cdot \text{F}^2 \cdot \text{DS}^2) \cdot (B\_{\text{X}}
    \cdot B\_{\text{W}})} \tag{8}$$


    where B<sup>X</sup> and B<sup>W</sup> respectively stand for the resolution of
    the inputs and weights. In the proposed SoC, MAC operations are based on analog
    inputs and 4b weights. Hence, we use B<sup>X</sup> = 1 and B<sup>W</sup> = 4,
    even though using B<sup>X</sup> equal to the effective number of bits (ENOB) at
    the input of the MAC units could be possible to compare the results with accelerators
    such as IMC ones, for which the resolution of inputs is clearly defined. Throughput
    can also be normalized to 1b operations by multiplying its expression in [\(7\)](#page-10-1)
    by B<sup>X</sup> · BW.


    Fig. [19\(](#page-9-2)a) illustrates different cases regarding how the exposure
    and convolution operations intertwine. A sequential execution is inefficient as
    pixels can start being exposed as soon as they have been stored in the analog
    memory. Therefore, a parallel execution is preferable. The current version of
    the imager controller only supports the case in which the exposure time Texp is
    longer than the duration of the convolution operation Tconv (case 2), but could
    easily be modified to support a parallel execution for Texp < Tconv (case 3).
    This modification would be beneficial from an applicative standpoint, as it would
    allow to maximize the frame rate in all the configurations of the accelerator.
    Figs. [19\(](#page-9-2)b) and (c) correspond to an execution with four filters
    and a 12.5-ms exposure time for all possible configurations of DS and S. Fig.
    [19\(](#page-9-2)b) reveals that a higher frame rate can be achieved


    ![](_page_10_Figure_10.jpeg)


    <span id="page-10-2"></span>Fig. 20. Breakdown of (a) the total SoC power and
    (b) the power of the analog macro and digital data transfers. Energy per 1b operation
    for (c) the SoC and (d) the accelerator. All figures correspond to a parallel
    exposure and convolution, different DS and S configurations, four filters, and
    a 12.5 ms exposure time, and present measurement results. The fraction of power
    due to the imager controller in (a)(c) is estimated based on physical simulations.


    ![](_page_10_Figure_12.jpeg)


    <span id="page-10-0"></span>Fig. 21. For sequential exposure and convolution,
    DS = 1, S = 2, and a 12.5 ms exposure time, measured (a) frame rate, EE at the
    SoC and accelerator levels, and (b) energy per 1b operation with the number of
    filters.


    ![](_page_10_Figure_14.jpeg)


    <span id="page-10-3"></span>Fig. 22. Training pipeline of the face RoI detector,
    based on a quantized CNN trained with QKeras and a TensorFlow backend (ideal software
    execution). The frame rate is 27 fps and is dominated by the duration of the convolution
    operation rather than by the exposure time.


    with parallel execution, the limit being the exposure time. In terms of energy/OP
    at the SoC level, the parallel execution yields a reduction of 12 to 44% with
    the strongest reductions attained for small values of DS and S.


    Besides, Fig. [20](#page-10-2) presents breakdowns of the power consumption and
    energy/OP at the levels of the accelerator and of the SoC. Regarding the SoC power
    [Fig. [20\(](#page-10-2)a)], it ranges from 245 to 379 µW. The CPU and imager
    controller have a relatively constant consumption around 0.2 mW across configurations,
    while the consumption related to the analog circuitry and data transfers is highly
    dependent on the configuration [Fig. [20\(](#page-10-2)b)]. The power on VDDAL
    and of the DMA declines for a larger DS and/or S, as N<sup>f</sup> becomes smaller,
    while the power on VDDAH and of the DCMI remains fairly constant. The former is
    indeed related to the pixel readout and DS3 units, and does not change as the
    frame rate is the same for all configurations except for DS = 1 and S = 2. This
    is the case because a parallel execution is employed and the frame rate is limited
    by the exposure time as in Fig. [19\(](#page-9-2)a). The latter is related to
    internal switching of the DCMI and does not account for the I/O power which would
    otherwise scale with the amount of data, similarly to the DMA. Therefore, the
    energy/OP at the SoC level [Fig. [20\(](#page-10-2)c)] goes from 0.22 to 6.43
    pJ and degrades for large strides as the power is amortized over a smaller number
    of operations. Finally, the energy/OP at the accelerator level [Fig. [20\(](#page-10-2)d)]
    is comprised between 12 and 201 fJ, corresponding to an EE of 84.09 and 4.98 TOPS/W.
    Fig. [20\(](#page-10-2)d) and Table [I](#page-9-3) further reveal that the energy/OP
    at the accelerator level improves with a larger DS, as the filter is applied to
    a larger number of pixels in the original image thanks to the DS operation. Interestingly,
    the two key features to achieve a high EE at the accelerator level are the power
    gating of OTAs and the amortization of the MAC operation energy over a large number
    of pixels thanks to image DS.


    Finally, Fig. [21](#page-10-0) studies the impact of the number of filters Nfilt
    on the frame rate, EE and energy/OP, for sequential exposure and convolution.
    Fig. [21\(](#page-10-0)a) highlights that increasing Nfilt causes the frame rate
    to drop as Tconv becomes longer while Texp remains constant, and that the accelerator
    EE remains relatively constant while the SoC one slightly improves, as the fraction
    of time without convolution operations decreases and the digital power is amortized
    over a larger number of operations. The same trend is reflected by the energy/OP
    at the SoC level in Fig. [21\(](#page-10-0)b).


    ### <span id="page-11-1"></span>*C. Face Region-of-Interest Detection*


    This last experiment consists in demonstrating the operation of MANTIS in a face
    RoI detection use case. The structure and training pipeline of the RoI detector,
    implemented as a quantized CNN, are illustrated in Fig. [22.](#page-10-3) The
    first part of the RoI detector is a convolution layer executed on chip, using
    16 4b 16×16 filters and 8b offsets, and operating over the image downsampled by
    2×. It is followed by an off-chip fullyconnected (FC) layer with 8b weights, combining
    1b fmaps to generate a 1b RoI detection map. Most of the workload is executed
    on chip, with 20.48 million operations in the convolution layer against 21.25
    thousands in the FC one. Note that an ad-hoc digital accelerator could realize
    the FC layer on chip. An interesting feature of this detector is that it reduces
    the data that needs to be transmitted off chip to 7.63% of the raw 8b image, thus
    cutting down the I/O bandwidth by 13.1×. As the EE at the SoC level for a sequential
    execution is 4.57 TOPS/W [Table [I\]](#page-9-3) and the difference between the
    two execution types does not exceed 44% [Fig. [19\(](#page-9-2)c)], we expect
    the EE for a parallel execution to be above 2.56 TOPS/W. The network is trained
    with QKeras on a dataset consisting of background and face images used by Moons
    et al. in [\[20\]](#page-13-19), and achieves false and true negative rates (FNR
    and TNR) of 8.5 and 96.9% on the test set, respectively, for an ideal software
    execution. At last, Fig. [23\(](#page-11-2)a) shows detailed results of one of
    the test images, and provides the overall performance over the 10 test images.
    MANTIS achieves an 11.5% FNR while


    ![](_page_11_Figure_6.jpeg)


    <span id="page-11-2"></span>Fig. 23. (a) Measured face RoI detection results,
    with details of a single test image and overall results for the 10 test images.
    (b) Face RoI results over four additional test images.


    respectively discarding 81.9% and 81.3% of image patches for the ideal and measured
    executions. These results are in line with the software execution but with a slight
    degradation coming from the fact that images generated by the imager are different
    from the ones in the dataset. Fig. [23\(](#page-11-2)b) displays face RoI results
    over four additional images, with a measured percentage of discarded image patches
    between 76.5 and 84.3%, and a single discarded face. Interestingly, the overall
    performance remains largely similar whereas RoI detection maps are different for
    the ideal and measured executions, due to the adaptation of the biases of the
    convolution layer in measurement and an approximate modelling of raw pixel voltages''
    transformations inside the convolution pipeline.


    ## V. COMPARISON TO THE STATE OF THE ART


    <span id="page-11-0"></span>In this section, we compare our work to the state
    of the art of mixed-signal vision chips in Table [II,](#page-12-1) and to other
    relevant accelerators. The proposed SoC relies on a 3T APS with DRS to compensate
    FPN. This a key enabler to achieve a 6.03-µm pixel pitch and a 54% fill factor,
    which are superior to existing vision chips, especially in-sensor ones for which
    the pixel pitch usually exceeds 30 µm. In terms of functionality, MANTIS is the
    first work to combine large 16×16 filters, a 4b weight resolution, and operation
    at multiple scales, making it suitable for medium-complexity vision tasks such
    as RoI detection. The charge-domain MAC operations computed by the accelerator
    have an EE normalized to 1b operations (1b EE) between 4.98 and 84.09 TOPS/W,
    so 2.4× better than [\[3\]](#page-13-3). This information is however missing from
    other works which only report EE at the SoC level. The 1b EE at the SoC level
    ranges from 0.16 to 4.57 TOPS/W and is on par or better than existing


    <span id="page-12-1"></span>


    |                                                                                                                                                                            |
    In-sensor                                         |                                                                   |                                                               |
    Hybrid                                                                                                                                                                                  |                                                        |
    Near-sensor                                               |                                                          |                                                  |                                                      |                                                                                              |  |

    |----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------|-------------------------------------------------------------------|---------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------|-----------------------------------------------------------|----------------------------------------------------------|--------------------------------------------------|------------------------------------------------------|----------------------------------------------------------------------------------------------|--|

    |                                                                                                                                                                            |
    Jendernalik<br>[1]                                | Carey<br>[2]                                                      |
    Xu<br>[3]                                                     | Lefebvre<br>[9]                                                                                                                                                                         |
    Song<br>[10]                                           | Kim / Bong<br>[4] / [5]                                   |
    Young<br>[6]                                             | Hsu<br>[7]                                       |
    Hsu<br>[8]                                           | Lefebvre<br>This work                                                                        |  |

    | Publication<br>Year                                                                                                                                                        |
    TCAS-I<br>2013                                    | VLSI<br>2013                                                      |
    TCAS-I<br>2022                                                | ISSCC<br>2021                                                                                                                                                                           |
    VLSI<br>2021                                           | ESSCIRC / JSSC<br>2017
    / 2018                             | JSSC<br>2019                                             |
    JSSC<br>2021                                     | JSSC<br>2023                                         |
    JSSC<br>2024                                                                                 |  |

    | Technology<br>Area [mm2<br>]<br>Supply voltage [V]                                                                                                                         |
    9.8<br>3.3                                        | 0.35µm CMOS 0.18µm CMOS<br>10×10<br>1.8
    (Digital)<br>1.5 (Analog) | 0.18µm CMOS<br>N/A<br>0.8–1.8                                 |
    65nm CMOS<br>2×2<br>0.8/1 (Digital)<br>0.95/1.05 (Analog)                                                                                                                               |
    0.18µm CIS<br>4.1×5.2<br>1.8 (Digital)<br>2.5 (Analog) | 65nm CMOS<br>3.3×3.6<br>0.5–0.8
    (Digital)<br>2.5 (Analog) | 0.13µm CIS<br>4×4<br>0.9 (Digital)<br>1.5/2.5 (Analog)   |
    2.46×2<br>0.5                                    | 0.18µm CMOS 0.18µm CMOS<br>2.46×2.18<br>0.8          |
    0.11µm CMOS<br>1.37×2.18<br>1.2 (Digital)<br>1.2/2.5/3.3 (Analog)                            |  |

    | Resolution<br>Shutter<br>Double sampling<br>Frame rate [fps]<br>Pixel pitch
    [µm]<br>Pixel complexity                                                                       |
    64×64<br>Global<br>No<br>100<br>35<br>18T APS +   | 256×256<br>Global<br>No<br>100,000<br>32.3<br>176T
    APS            | 32×32<br>Global<br>DRS<br>156<br>35<br>61T APS                |
    160×128<br>Rolling<br>No<br>24–268<br>9<br>40T log(I) +                                                                                                                                 |
    240×240<br>Global<br>CDS<br>120<br>9.8<br>14T APS +    | 320×240<br>Rolling<br>No<br>1<br>7<br>3T
    APS              | 320×240<br>Global<br>CDS<br>30<br>4<br>4T APS            |
    128×128<br>Rolling<br>No<br>480<br>7.6<br>4T PWM | 126×126<br>Rolling<br>No<br>50–250<br>7.6<br>4T
    PWM  | 128×128<br>Rolling<br>DRS<br>18.2–79.7<br>6.03<br>3T APS                                     |  |

    | Fill factor [%]<br>DR [dB]                                                                                                                                                 |
    2 MOS caps.<br>23<br>58                           | 6.2<br>N/A                                                        |
    9.1<br>N/A                                                    | 1 MIM cap.<br>12.9<br>47.1                                                                                                                                                              |
    4 caps.<br>20.1<br>N/A                                 | N/A<br>N/A                                                |
    60.4<br>59.3                                             | 36<br>52.3                                       |
    36<br>47.8                                           | 54<br>57.7                                                                                   |  |

    | Feature type<br>Multiscale                                                                                                                                                 |
    - 3×3 kernels<br>No                               | - Median filtering<br>Arbitrary                                   |
    No                                                            | - Edge detection
    - 32×32 kernels - 2×2 to 64×64 kernels - Log. Haar filters - 20×20 lin. Haar filters
    - Log. gradients - 3×3 kernels<br>- 16×16 lin. Haar filters<br>- 6 scales (conv.)
    | Arbitrary                                              | 3 scales                                                  |
    Arbitrary                                                | No                                               |
    - 3×3 kernels<br>No                                  | - 16×16 kernels<br>3 scales                                                                  |  |

    | Computation type<br>Weight resolution<br>Feature resolution                                                                                                                |
    Current<br>Analog<br>Analog                       | Current<br>N/A<br>1b or 8b                                        |
    Current<br>1b<br>1b                                           | - 3 scales (Haar)<br>Current<br>1.5b<br>1b
    or 8b                                                                                                                                        |
    Charge<br>1.5b<br>1b                                   | Charge<br>1.5b<br>1b                                      |
    Charge<br>N/A<br>1.5b or 2.75b                           | Current<br>4b<br>1b
    to 8b                        | Current<br>4b<br>1b to 8b                            |
    Charge<br>4b<br>1b, 2b, 4b, or 8b                                                            |  |

    | Throughput◁<br>[MOPS]<br>Throughput◁‡<br>[MOPS]<br>Power (accel.) [µW]<br>EE‡<br>(accel.)
    [TOPS/W]<br>Power (SoC) [µW]<br>EE‡<br>(SoC) [TOPS/W]<br>Processing energy (SoC)
    | 7.4<br>N/A<br>N/A<br>N/A<br>280<br>0.026<br>683.6 | 655,000<br>N/A<br>N/A<br>N/A<br>1,230,000<br>0.53<br>187.7        |
    5.1<br>5.1<br>0.147–0.537<br>9.52–34.77<br>8.5<br>0.60<br>3.3 | 15.1–252.1<br>22.7–378.2<br>N/A<br>N/A<br>42–206<br>0.23–5.46<br>2.5–103.9⋆                                                                                                             |
    N/A<br>N/A<br>N/A<br>N/A<br>2,900<br>N/A<br>16.8†      | N/A<br>N/A<br>N/A<br>N/A<br>24–96<br>N/A<br>6.0–24.0⋄     |
    N/A<br>N/A<br>N/A<br>N/A<br>229–262<br>N/A<br>49.7–56.9▷ | 137.2<br>548.7<br>N/A<br>N/A<br>117<br>4.67      |
    63.5<br>254<br>N/A<br>N/A<br>80.4–134.5<br>0.63–1.89 | 10.5–408.3<br>42–1633.2<br>2.7–76.2<br>4.98–84.09<br>250.9–384.7<br>0.16–4.57<br>48.0–284.1⋆
    |  |


    TABLE II COMPARISON TABLE OF STATE-OF-THE-ART MIXED-SIGNAL VISION CHIPS.


    <sup>⋆</sup> For 4 filters. † For 25 filters. <sup>⋄</sup> For 52 filters. <sup>▷</sup>
    Horizontal and vertical gradients are considered as two filters. <sup>◁</sup>
    Not normalized to the resolution of inputs and weights. ‡ Normalized to 1b operations.


    works [\[7\]](#page-13-6), [\[9\]](#page-13-8), while respectively supporting
    a larger filter size and an increased weight resolution. Regarding the processing
    energy, it is larger than for other works as this metric does not account for
    the filter characteristics. Next, let us compare near-sensor and hybrid vision
    chips. The latter usually present larger pixels which contain registers to store
    weights [\[9\]](#page-13-8), or large capacitors to store and read pixel values
    [\[10\]](#page-13-9). Regarding double sampling, it is possible when the readout
    is voltagebased [\[10\]](#page-13-9) but not when photocurrents are used [\[9\]](#page-13-8).
    Lastly, near-sensor vision chips are generally more flexible as different configurations
    of the mixed-signal processor can easily be implemented. In contrast, hybrid architectures
    often involve hardwiring of connections between pixels, and either require to
    move weights in the pixel array [\[9\]](#page-13-8) or to combine row and column
    signals [\[10\]](#page-13-9). These architectural features thus limit the use
    of hybrid vision chips to specific applications.


    Let us now consider other types of convolution accelerators, starting with those
    for which the digital control is implemented with an FPGA (not included in the
    power consumption). Two relevant works employ charge-domain computations, either
    using IMC with analog inputs and 1b weights [\[21\]](#page-13-20), or incolumn
    SC amplifiers with 3b weights, followed by ADCs performing a nonlinear quantization
    [\[22\]](#page-13-21). Their 1b EE at the accelerator level is respectively 1.25
    and 0.017 TOPS/W, which is approximately 4× lower than the worst-case EE obtained
    with MANTIS. Other works have their digital control embarked on chip. First, [\[23\]](#page-13-22)
    proposes a current-domain insensor processor with 2b weights based on resistive
    RAM, and attaining a 1b EE of 2.98 TOPS/W. Then, [\[24\]](#page-13-23) features
    a hybrid optical-electronic CNN processor with 1b weights, realizing the first
    convolution layer with a mask in the optical domain, and reaching a 1b EE of 0.37
    TOPS/W. Finally, [\[25\]](#page-13-24) introduces a digital CNN processor stacked
    on an image sensor exhibiting a peak EE of 4.97 TOPS/W for 8b and 32b integer
    operations. MANTIS is on par with [\[23\]](#page-13-22), [\[24\]](#page-13-23)
    in terms of EE, but offers more flexibility with its programmable convolution
    parameters. However, it is not as efficient as [\[25\]](#page-13-24), leveraging
    the performance improvements due to technology scaling.


    ## VI. CONCLUSION


    <span id="page-12-0"></span>In this work, we presented MANTIS, a mixed-signal
    nearsensor convolutional imager SoC intended for FE and RoI detection. It is the
    first mixed-signal vision chip to combine large 16×16 filters with 4b weight resolution,
    operation at three different scales, and DRS to remove FPN and improve computational
    accuracy. MANTIS is enabled by two main circuit innovations. First, DS3 units
    combining DRS, voltage downshifting, and image DS. Next, near-sensor MAC operations
    computed in the charge domain, using SC amplifiers to compute psums and charge
    sharing in the capacitive DAC of the SAR ADCs to aggregate psums and compute the
    convolution result. MANTIS respectively reaches peak EEs normalized to 1b operations
    of 4.57 and 84.09 TOPS/W at the accelerator and SoC levels, while producing fmaps
    with an RMSE between 3.01 and 11.34%. Lastly, face RoI detection was demonstrated
    with an FNR of 11.5%, while discarding 81.3% of image patches and reducing the
    imager output data to 7.63% of the raw image. This work demonstrates that nearsensor
    vision chips can successfully tackle tasks requiring a higher resolution of inputs
    and weights, as opposed to insensor vision chips which are currently limited to
    noisy inputs and low-resolution weights. Further works should focus on the digital
    part of the SoC, albeit some analog blocks could also benefit from the utilization
    of more advanced techniques. New opportunities for the implementation of mixed-signal
    vision chips also arise from the advent of 2.5D/3D packaging.


    ### ACKNOWLEDGMENTS


    The authors would like to thank Prof. Marian Verhelst and Dr. Bert Moons for granting
    us access to their face detection dataset, Dr. Remi Dekimpe for his help with
    the DMA, Prof. ´ Charlotte Frenkel and Dr. Adrian Kneip for fruitful discussions,
    and Eleonore Masarweh for the chip microphotograph. ´


    ### REFERENCES


    - <span id="page-13-1"></span>[1] W. Jendernalik, G. Blakiewicz, J. Jakusz, S.
    Szczepanski, and R. Piotrowski, "An Analog Sub-Milliwatt CMOS Image Sensor with
    Pixel-Level Convolution Processing," *IEEE Trans. Circuits Syst. I, Reg. Papers*,
    vol. 60, no. 2, pp. 279–289, Feb. 2013.

    - <span id="page-13-2"></span>[2] S. J. Carey, A. Lopich, D. R. Barr, B. Wang,
    and P. Dudek, "A 100,000 fps Vision Sensor with Embedded 535GOPS/W 256×256 SIMD
    Processor Array," in *2013 Symp. VLSI Circuits*. IEEE, 2013, pp. C182–C183.

    - <span id="page-13-3"></span>[3] H. Xu, N. Lin, L. Luo, Q. Wei, R. Wang, C. Zhuo,
    X. Yin, F. Qiao, and H. Yang, "Senputing: An Ultra-Low-Power Always-On Vision
    Perception Chip Featuring the Deep Fusion of Sensing and Computing," *IEEE Trans.
    Circuits Syst. I, Reg. Papers*, vol. 69, no. 1, pp. 232–243, Jan. 2022.

    - <span id="page-13-0"></span>[4] C. Kim, K. Bong, I. Hong, K. Lee, S. Choi, and
    H.-J. Yoo, "An Ultra-Low-Power and Mixed-Mode Event-Driven Face Detection SoC
    for Always-On Mobile Applications," in *IEEE 43rd Eur. Solid-State Circuits Conf.*
    IEEE, 2017, pp. 255–258.

    - <span id="page-13-4"></span>[5] K. Bong, S. Choi, C. Kim, D. Han, and H.-J.
    Yoo, "A Low-Power Convolutional Neural Network Face Recognition Processor and
    a CIS Integrated with Always-On Face Detector," *IEEE J. Solid-State Circuits*,
    vol. 53, no. 1, pp. 115–123, Jan. 2018.

    - <span id="page-13-5"></span>[6] C. Young, A. Omid-Zohoor, P. Lajevardi, and
    B. Murmann, "A Data-Compressive 1.5/2.75-bit Log-Gradient QVGA Image Sensor with
    Multi-Scale Readout for Always-On Object Detection," *IEEE J. Solid-State Circuits*,
    vol. 54, no. 11, pp. 2932–2946, Nov. 2019.

    - <span id="page-13-6"></span>[7] T.-H. Hsu, Y.-R. Chen, R.-S. Liu, C.-C. Lo,
    K.-T. Tang, M.-F. Chang, and C.-C. Hsieh, "A 0.5-V Real-Time Computational CMOS
    Image Sensor with Programmable Kernel for Feature Extraction," *IEEE J. Solid-State
    Circuits*, vol. 56, no. 5, pp. 1588–1596, May 2021.

    - <span id="page-13-7"></span>[8] T.-H. Hsu, G.-C. Chen, Y.-R. Chen, R.-S. Liu,
    C.-C. Lo, K.-T. Tang, M.-F. Chang, and C.-C. Hsieh, "A 0.8 V Intelligent Vision
    Sensor with Tiny Convolutional Neural Network and Programmable Weights using Mixed-Mode
    Processing-in-Sensor Technique for Image Classification," *IEEE J. Solid-State
    Circuits*, vol. 58, no. 11, pp. 3266–3274, Nov. 2023.

    - <span id="page-13-8"></span>[9] M. Lefebvre, L. Moreau, R. Dekimpe, and D. Bol,
    "7.7 A 0.2-to-3.6 TOPS/W Programmable Convolutional Imager SoC with In-Sensor
    Current-Domain Ternary-Weighted MAC Operations for Feature Extraction and Region-of-Interest
    Detection," in *2021 IEEE Int. Solid-State Circuits Conf.*, vol. 64. IEEE, 2021,
    pp. 118–120.

    - <span id="page-13-9"></span>[10] H. Song, S. Oh, J. Salinas, S.-Y. Park, and
    E. Yoon, "A 5.1 ms Low-Latency Face Detection Imager with In-Memory Charge-Domain
    Computing of Machine-Learning Classifiers," in *2021 Symp. VLSI Circuits*. IEEE,
    2021, pp. 1–2.

    - <span id="page-13-10"></span>[11] M. Lefebvre and D. Bol, "A Mixed-Signal Near-Sensor
    Convolutional Imager SoC with Charge-Based 4b-Weighted 5-to-84-TOPS/W MAC Operations
    for Feature Extraction and Region-of-Interest Detection," in *2024 IEEE Custom
    Integr. Circuits Conf.* IEEE, 2024, pp. 1–2.

    - <span id="page-13-11"></span>[12] B. Gonen, F. Sebastiano, R. Quan, R. van Veldhoven,
    and K. A. ¨ Makinwa, "A Dynamic Zoom ADC with 109-dB DR for Audio Applications,"
    *IEEE J. Solid-State Circuits*, vol. 52, no. 6, pp. 1542–1550, June 2017.

    - <span id="page-13-12"></span>[13] I. Park, W. Jo, C. Park, B. Park, J. Cheon,
    and Y. Chae, "A 640 × 640 Fully Dynamic CMOS Image Sensor for Always-On Operation,"
    *IEEE J. Solid-State Circuits*, vol. 55, no. 4, pp. 898–907, Apr. 2020.

    - <span id="page-13-13"></span>[14] A. Kneip, M. Lefebvre, J. Verecken, and D.
    Bol, "IMPACT: A 1-to-4b 813-TOPS/W 22-nm FD-SOI Compute-in-Memory CNN Accelerator
    Featuring a 4.2-POPS/W 146-TOPS/mm 2 CIM-SRAM With Multi-Bit Analog Batch-Normalization,"
    *IEEE J. Solid-State Circuits*, vol. 58, no. 7, pp. 1871–1884, May 2023.

    - <span id="page-13-14"></span>[15] J.-O. Seo, M. Seok, and S. Cho, "A 44.2-TOPS/W
    CNN Processor With Variation-Tolerant Analog Datapath and Variation Compensating
    Circuit," *IEEE J. Solid-State Circuits*, vol. 59, no. 5, May 2023.

    - <span id="page-13-15"></span>[16] B. P. Ginsburg and A. P. Chandrakasan, "An
    Energy-Efficient Charge Recycling Approach for a SAR Converter with Capacitive
    DAC," in *2005 IEEE Int. Symp. Circuits Syst.* IEEE, 2005, pp. 184–187.

    - <span id="page-13-16"></span>[17] Y. Li and Y. Lian, "Improved Binary-Weighted
    Split-Capacitive-Array DAC for High-Resolution SAR ADCs," *Electron. Lett.*, vol.
    50, no. 17, pp. 1194–1195, Aug. 2014.

    - <span id="page-13-17"></span>[18] P. Harpe, "A Compact 10-b SAR ADC with Unit-Length
    Capacitors and a Passive FIR Filter," *IEEE J. Solid-State Circuits*, vol. 54,
    no. 3, pp. 636–645, Mar. 2019.

    - <span id="page-13-18"></span>[19] N. R. Shanbhag and S. K. Roy, "Benchmarking
    In-Memory Computing Architectures," *IEEE Open J. Solid-State Circuits Soc.*,
    vol. 2, pp. 288– 300, Dec. 2022.

    - <span id="page-13-19"></span>[20] B. Moons, D. Bankman, L. Yang, B. Murmann,
    and M. Verhelst, "BinarEye: An Always-On Energy-Accuracy-Scalable Binary CNN Processor
    with All Memory on Chip in 28nm CMOS," in *2018 IEEE Custom Integr. Circuits Conf.*
    IEEE, 2018, pp. 1–4.

    - <span id="page-13-20"></span>[21] H. Valavi, P. J. Ramadge, E. Nestler, and
    N. Verma, "A 64-Tile 2.4-Mb In-Memory-Computing CNN Accelerator Employing Charge-Domain
    Compute," *IEEE J. Solid-State Circuits*, vol. 54, no. 6, pp. 1789–1799, June
    2019.

    - <span id="page-13-21"></span>[22] B. Jeong, J. Lee, J. Choi, M. Song, Y. Son,
    and S. Y. Kim, "A 0.57 mW@1 FPS In-Column Analog CNN Processor Integrated Into
    CMOS Image Sensor," *IEEE Access*, vol. 11, June 2023.

    - <span id="page-13-22"></span>[23] M. Abedin, A. Roohi, M. Liehr, N. Cady, and
    S. Angizi, "MR-PIPA: An Integrated Multilevel RRAM (HfOx)-Based Processing-in-Pixel
    Accelerator," *IEEE J. Explor. Solid-State Comput. Devices Circuits*, vol. 8,
    no. 2, pp. 59–67, Sept. 2022.

    - <span id="page-13-23"></span>[24] X. Wang, Z. Huang, T. Liu, W. Shi, H. Chen,
    and M. Zhang, "6.9 A 0.35 V 0.367 TOPS/W Image Sensor with 3-Layer Optical-Electronic
    Hybrid Convolutional Neural Network," in *2024 IEEE Int. Solid-State Circuits
    Conf.*, vol. 67. IEEE, 2024, pp. 116–118.

    - <span id="page-13-24"></span>[25] R. Eki, S. Yamada, H. Ozawa, H. Kai, K. Okuike,
    H. Gowtham, H. Nakanishi, E. Almog, Y. Livne, G. Yuval *et al.*, "9.6 A 1/2.3inch
    12.3Mpixel with On-Chip 4.97 TOPS/W CNN Processor Back-Illuminated Stacked CMOS
    Image Sensor," in *2021 IEEE Int. Solid-State Circuits Conf.*, vol. 64. IEEE,
    2021, pp. 154–156.


    ![](_page_13_Picture_29.jpeg)


    Martin Lefebvre (Graduate Student Member, IEEE) received the M.Sc. and Ph.D. degrees
    in engineering sciences from the Universite catholique de Louvain ´ (UCLouvain),
    Belgium, in 2017 and 2024.


    He is currently a post-doctoral researcher with the cognitive sensor nodes and
    systems (CogSys) laboratory led by Prof. C. Frenkel at TU Delft, The Netherlands,
    working on neuromorphic hardware/software co-design for efficient on-chip learning.
    His research interests include hardware-aware machine learning algorithms, mixed-signal
    vision chips for embedded image processing, and low-power current reference architectures.


    Dr. Lefebvre serves as a reviewer for various IEEE journals and conferences including
    IEEE JOURNAL OF SOLID-STATE CIRCUITS AND IEEE TRANS-ACTIONS ON CIRCUITS AND SYSTEMS
    I AND II.


    ![](_page_13_Picture_33.jpeg)


    David Bol (Senior Member, IEEE) received the Ph.D. degree in engineering science
    from the Universite catholique de Louvain (UCLouvain), in 2008, in ´ the field
    of ultra-low-power digital nanoelectronics.


    In 2005, he was a visiting Ph.D. student at the CNM, Sevilla, and in 2009, a post-doctoral
    researcher at intoPIX, Louvain-la-Neuve. In 2010, he was a visiting post-doctoral
    researcher at the UC Berkeley Lab for Manufacturing and Sustainability, Berkeley.
    In 2015, he participated to the creation of e-peas semiconductors spin-off company.
    Prof.


    Bol leads the Electronic Circuits and Systems (ECS) group, UCLouvain, focused
    on ultra-low-power design of integrated circuits for environmental and biomedical
    IoT applications including computing, power management, sensing and wireless communications.
    He is currently an Associate Professor with UCLouvain. He has authored more than
    150 papers and conference contributions and holds three delivered patents. He
    is actively engaged in a social-ecological transition in the field of information
    and communication technologies (ICT) research with a post-growth approach.


    Prof. Bol (co-)received five Best Paper/Poster/Design Awards in IEEE conferences
    (ICCD 2008, SOI Conf. 2008, FTFC 2014, ISCAS 2020, ESSCIRC 2022) and supervised
    the Ph.D. thesis of Charlotte Frenkel who received the 2021 Nokia Bell Scientific
    Award and the 2021 IBM Innovation Award for her Ph.D. He serves as a reviewer
    for various IEEE journals and conferences and presented several keynotes in international
    conferences.'
- title: A Novel Extensible Simulation Framework for CXL-Enabled Systems
  abstract: 'Compute Express Link (CXL) serves as a rising industry standard, delivering

    high-speed cache-coherent links to a variety of devices, including host CPUs,

    computational accelerators, and memory devices. It is designed to promote

    system scalability, enable peer-to-peer exchanges, and accelerate data

    transmissions. To achieve these objectives, the most recent CXL protocol has

    brought forth several innovative features, such as port-focused routing,

    device-handled coherence, and PCIe 6.0 compatibility. However, due to the

    limited availability of hardware prototypes and simulators compatible with CXL,

    earlier CXL research has largely depended on emulating CXL devices using remote

    NUMA nodes. Unfortunately, these NUMA-based emulators have difficulties in

    accurately representing the new features due to fundamental differences in

    hardware and protocols. Moreover, the absence of support for non-tree topology

    and PCIe links makes it complex to merely adapt existing simulators for CXL

    simulation. To overcome these problems, we introduce ESF, a simulation

    framework specifically designed for CXL systems. ESF has been developed to

    accurately reflect the unique features of the latest CXL protocol from the

    ground up. It uses a specialized interconnect layer to facilitate connections

    within a wide range of system topologies and also includes key components to

    carry out specific functions required by these features. By utilizing ESF, we

    thoroughly investigate various aspects of CXL systems, including system

    topology, device-handled coherence, and the effects of PCIe characteristics,

    leading to important findings that can guide the creation of high-performance

    CXL systems. The ESF source codes are fully open-source and can be accessed at

    https://anonymous.4open.science/r/ESF-1CE3.'
  url: http://arxiv.org/abs/2411.08312v1
  keywords: ''
  document: "# A Novel Extensible Simulation Framework for CXL-Enabled Systems\n\n\
    Yuda An<sup>1</sup> , Shushu Yi<sup>1</sup> , Bo Mao<sup>2</sup> , Qiao Li<sup>2</sup>\
    \ , Mingzhe Zhang<sup>3</sup>\n\nKe Zhou<sup>4</sup> , Nong Xiao<sup>5</sup> ,\
    \ Guangyu Sun<sup>1</sup>,<sup>6</sup> , Xiaolin Wang<sup>1</sup> , Yingwei Luo<sup>1</sup>\
    \ , Jie Zhang<sup>1</sup>\n\n*Computer Hardware and System Evolution Laboratory,*\n\
    \nPeking University<sup>1</sup> , Xiamen University<sup>2</sup> ,\n\nInstitute\
    \ of Information Engineering, Chinese Academy of Sciences<sup>3</sup> ,\n\nWuhan\
    \ National Laboratory for Optoelectronics of Huazhong University of Science and\
    \ Technology<sup>4</sup>\n\nSun Yat-sen University<sup>5</sup> ,\n\nBeijing Advanced\
    \ Innovation Center for Integrated Circuits<sup>6</sup>\n\nhttps://www.chaselab.wiki\n\
    \n*Abstract*—Compute Express Link (CXL) serves as a rising industry standard,\
    \ delivering high-speed cache-coherent links to a variety of devices, including\
    \ host CPUs, computational accelerators, and memory devices. It is designed to\
    \ promote system scalability, enable peer-to-peer exchanges, and accelerate data\
    \ transmissions. To achieve these objectives, the most recent CXL protocol has\
    \ brought forth several innovative features, such as port-focused routing, device-handled\
    \ coherence, and PCIe 6.0 compatibility. However, due to the limited availability\
    \ of hardware prototypes and simulators compatible with CXL, earlier CXL research\
    \ has largely depended on emulating CXL devices using remote NUMA nodes. Unfortunately,\
    \ these NUMA-based emulators have difficulties in accurately representing the\
    \ new features due to fundamental differences in hardware and protocols. Moreover,\
    \ the absence of support for non-tree topology and PCIe links makes it complex\
    \ to merely adapt existing simulators for CXL simulation. To overcome these problems,\
    \ we introduce ESF, a simulation framework specifically designed for CXL systems.\
    \ ESF has been developed to accurately reflect the unique features of the latest\
    \ CXL protocol from the ground up. It uses a specialized interconnect layer to\
    \ facilitate connections within a wide range of system topologies and also includes\
    \ key components to carry out specific functions required by these features. By\
    \ utilizing ESF, we thoroughly investigate various aspects of CXL systems, including\
    \ system topology, device-handled coherence, and the effects of PCIe characteristics,\
    \ leading to important findings that can guide the creation of high-performance\
    \ CXL systems. The ESF source codes are fully open-source and can be accessed\
    \ at https://anonymous.4open.[science/r/ESF-1CE3.](https://anonymous.4open.science/r/ESF-1CE3)\n\
    \n#### I. INTRODUCTION\n\nWith the prevalence of large-scale data-intensive applications\
    \ such as artificial intelligence, life science, and climate modelling [\\[17\\\
    ]](#page-12-0), [\\[22\\]](#page-12-1), [\\[24\\]](#page-12-2), [\\[30\\]](#page-12-3),\
    \ [\\[38\\]](#page-13-0), [\\[41\\]](#page-13-1), [\\[45\\]](#page-13-2), [\\\
    [46\\]](#page-13-3), [\\[63\\]](#page-13-4), there are increasing demands to aggregate\
    \ tons of computation and memory resources into a uniform system. Peripheral component\
    \ interconnect express (PCIe) [\\[4\\]](#page-12-4), [\\[8\\]](#page-12-5), as\
    \ one of the most popular interconnect standards, has been widely adopted in the\
    \ computing system to connect between the host CPU and diverse peripheral devices\
    \ including graph processing units (GPUs) and solid-state drives (SSDs) [\\[34\\\
    ]](#page-12-6), [\\[36\\]](#page-13-5), [\\[56\\]](#page-13-6), [\\[65\\]](#page-13-7).\n\
    \nCompared to other types of interconnects (e.g., Ethernet [\\[1\\]](#page-12-7),\
    \ SATA [\\[3\\]](#page-12-8), and DDR [\\[13\\]](#page-12-9)), PCIe can deliver\
    \ much higher aggregated throughput (*e.g.*, 256 GB/s in 16 PCIe 6.0 lanes [\\\
    [8\\]](#page-12-5)). In addition, PCIe supports various communication protocols\
    \ (*e.g.*, NVMe [\\[2\\]](#page-12-10)), exhibiting high compatibility. However,\
    \ PCIe fails to extend the host local memory with external PCIe memory devices\
    \ due to the lack of coherence mechanisms [\\[36\\]](#page-13-5). Specifically,\
    \ the memory accesses that target PCIe device memory address space are required\
    \ to be *noncachable*. CPU cores must directly access the PCIe device memory and\
    \ are not allowed to store copies of data from the device memory within their\
    \ internal caches. Software involvement is necessary to maintain data coherence.\
    \ This limitation significantly worsens the memory access performance. Thus, building\
    \ computation and memory pools atop PCIe cannot satisfy the demands of large-scale\
    \ data-intensive applications.\n\n,\n\nCompute Express Link (CXL) is an emerging\
    \ industry standard that offers high-performance cache-coherent interconnect capability\
    \ to heterogeneous devices, including host CPUs, computational accelerators, and\
    \ memory devices [\\[9\\]](#page-12-11), [\\[50\\]](#page-13-8). CXL is designed\
    \ to operate over the existing PCIe infrastructure, which utilizes the same physical\
    \ and electrical interfaces. This design philosophy aids CXL with the high performance\
    \ and backward compatibility of PCIe technology. CXL also provides the features\
    \ of cache coherency and memory semantic support, which can seamlessly extend\
    \ the hostside processor and memory with the external CXL accelerators and memory\
    \ devices. Thus, CXL enables efficient data sharing and communication within computation\
    \ and memory pools.\n\nWhile CXL has great potential to change the existing computer\
    \ architecture, most of the prior studies on CXL [\\[18\\]](#page-12-12), [\\\
    [40\\]](#page-13-9), [\\[44\\]](#page-13-10) leverage remote NUMA nodes to emulate\
    \ CXL devices due to the lack of hardware prototypes. As highversion CXL is still\
    \ at the proof of concept (PoC) stage, we believe constructing a CXL simulator\
    \ would be the wheel to drive the high-performance interconnect research forward.\
    \ Nevertheless, it is challenging to simply extend the existing simulators and\
    \ emulators to support the CXL simulation. Specifically, the CXL standard aims\
    \ to achieve ultra-high scalability by providing complicated non-tree system topology\
    \ and coherent peer-to-peer communication. However, NUMAbased emulators, which\
    \ have been adopted in previous work, face strict physical constraints (*e.g.*,\
    \ socket number) and fail to extend system scalability. Prior computation-centric\
    \ simulators, such as gem5 [\\[21\\]](#page-12-13), [\\[33\\]](#page-12-14) and\
    \ GPGPUsim [\\[19\\]](#page-12-15), focus on accurate processor unit modeling,\
    \ however, support only legacy interconnects (*e.g.*, gem5 only supports legacy\
    \ PCI links), which are unable to operate in non-tree topologies. On the other\
    \ hand, network-centric simulators, such as BookSim [\\[35\\]](#page-13-11) and\
    \ Garnet [\\[16\\]](#page-12-16), pay attention to diverse network topologies\
    \ and flow control mechanisms. These simulators lack the support of coherency\
    \ management, which is considered a key promise of the CXL standard. In summary,\
    \ existing tools struggle to reflect critical features of the CXL standard.\n\n\
    Tackling the aforementioned challenges, we propose our novel extensible simulation\
    \ framework, *ESF*, that is built atop the CXL backbone. This framework introduces\
    \ two function layers for an accurate simulation of highly scalable CXL systems,\
    \ namely *interconnect layer* and *device layer*. The interconnect layer is dedicated\
    \ to supporting complicated system topologies. Upon system initialization, this\
    \ layer constructs a topology graph of the system and provides detailed routing\
    \ information to the devices for intercommunication uses. On the other hand, the\
    \ device layer models several types of fundamental CXL devices, including CXL\
    \ accelerators, memory devices, and CXL switches. During simulation, these devices\
    \ conduct CXL protocol functions and communicate with each other by leveraging\
    \ the communication function of the interconnect layer. For example, CXL switches\
    \ build internal routing tables based on the topology information provided by\
    \ the interconnect layer and route different requests to the correct destinations.\
    \ The tight collaboration of these two layers ensures ESF to accurately simulate\
    \ a highly scalable system defined by the CXL standard. The validation experiment\
    \ proves the accuracy of ESF with errors ranging from 0.1% to 10%. With accurate\
    \ simulation, ESF can uncover several issues that the existing simulators are\
    \ unable to figure out, including the performance impacts of diverse system topologies\
    \ and the design choices for device-managed coherence.\n\nOur contributions are\
    \ summarized as follows:\n\n• *CXL simulation challenge analysis of existing research\
    \ tools:* The CXL standard is aimed at supporting rack-level systems with scalable\
    \ performance, which requires complicated nontree system topology and coherent\
    \ peer-to-peer communication. To meet these requirements, the CXL protocol introduces\
    \ several novel features, including port-based routing, devicemanaged coherence\
    \ and the adoption of high-version PCIe physical links. Unfortunately, existing\
    \ simulation and emulation tools face challenges in accurately reflecting these\
    \ critical features. Most of the prior works adopt remote NUMA nodes as CXL hardware\
    \ emulators. However, the physical limitation of NUMA platforms prevents them\
    \ from emulating port-based routing. Meanwhile, existing computation-centric simulators\
    \ lack the support of PCIe simulation, while network-centric simulators fail to\
    \ provide coherence management functionality. • *Novel simulation framework customized\
    \ for CXL systems:* To address the challenges in existing tools comprehensively,\
    \ we propose a customized simulation framework, ESF, which consists of two fundamental\
    \ layers, namely the interconnect layer and the device layer. While the interconnect\
    \ layer is dedicated to providing interconnection and scalability of the simulated\
    \ system, the device layer performs device-specific functions, such as coherence\
    \ management. The novel framework carefully implements a set of components to\
    \ model the essential features of CXL. Firstly, it provides a switch component\
    \ that supports PBR. Secondly, it implements a device-side inclusive snoop filter\
    \ as an example of device coherency agent (DCOH). Lastly, it implements the bus\
    \ components while considering unique characteristics of PCIe buses to accurately\
    \ reflect the behaviors of real CXL platforms.\n\n• *Exploration on the performance\
    \ impacts of multiple new CXL features:* We perform a set of experiments to explore\
    \ the performance impacts of emerging CXL features in multiple representative\
    \ systems implemented with our novel simulation framework. Our investigation focuses\
    \ on three main aspects: (1) the impacts of different system topologies, (2) the\
    \ impacts of device-managed coherence, and (3) the unique full-duplex feature\
    \ of PCIe transmission. From the experimental results, we derive three key observations.\
    \ First, the traditional treelike system topology experiences severe bandwidth\
    \ and latency bottlenecks at the root, leading to potential performance degradation\
    \ similar to systems with a chain-like topology. Second, the device-side inclusive\
    \ snoop filter receives unique request patterns because most of the requests that\
    \ reach the snoop filter are cache misses. Therefore, a customized structure is\
    \ essential for the snoop filter to achieve optimal performance. Third, we observe\
    \ from read-write mixed workloads that fullduplex transmission of PCIe buses results\
    \ in a bandwidth improvement compared to those with a single type of access pattern.\
    \ These observations pave the road to future CXL system designs.\n\n# <span id=\"\
    page-1-0\"></span>II. BACKGROUND, MOTIVATION AND CHALLENGES\n\n# *A. Basic Features\
    \ in CXL Protocol*\n\nCompute Express Link (CXL) is an emerging standard that\
    \ provides high-performance and cache-coherent interconnects for heterogeneous\
    \ devices ranging from host CPUs to memory devices [\\[9\\]](#page-12-11), [\\\
    [50\\]](#page-13-8). To this end, the CXL standard introduces various new features.\
    \ In the following, we will elaborate on these features in detail.\n\nCXL sub-protocols\
    \ and Flex Bus layers. CXL provides both backward-compatible and incremental functions\
    \ over PCIe physical and electrical interfaces via three sub-protocols: *CXL.io*,\
    \ *CXL.cache* and *CXL.mem*, as depicted in Figure [1.](#page-2-0)\n\nThe CXL.io\
    \ protocol, highlighted in dashed yellow lines in the figure, is the fundamental\
    \ protocol that all CXL devices and host CPUs need to support. It is responsible\
    \ for all basic I/O operations, including PCIe backward-compatible operations,\
    \ device enumeration, and device configuration. CXL.io mainly adopts the command\
    \ set of traditional PCIe links with slight enhancements. In contrast, the other\
    \ two protocols, CXL.cache and CXL.mem, employ customized command sets.\n\n<span\
    \ id=\"page-2-0\"></span>![](_page_2_Figure_0.jpeg)\n\nFig. 1: CXL sub-protocols,\
    \ endpoint types, and root complex.\n\nA traditional PCIe device counts on DMA\
    \ mechanism [\\[48\\]](#page-13-12), [\\[54\\]](#page-13-13) to access data that\
    \ reside in host memory. This mechanism brings two drawbacks. First, DMA is optimized\
    \ for massive contiguous accesses and shows suboptimal performance when servicing\
    \ small accesses. Second, DMA does not provide data coherence guarantee, which\
    \ necessitates software assistance, limiting the access speed. To address this\
    \ challenge, CXL proposes CXL.cache protocol, as highlighted in solid green lines\
    \ in the figure, which enables cacheline-grained coherent access from devices\
    \ to the host memory via hardware assistance, eliminating software involvement.\n\
    \nThe CXL.mem protocol exposes the CXL device internal memory to the host as coherent\
    \ memory space (cf. dotted blue lines in Figure [1\\)](#page-2-0). It provides\
    \ byte-addressable memory semantics, allowing the host to access the device local\
    \ memory via load/store instructions. The CXL.mem enables the expansion of coherent\
    \ physical memory through PCIe ports, constructing a highly scalable memory system.\n\
    \nTo achieve fast and reliable transmission, PCIe protocol consist of three layers,\
    \ namely *transaction* layer, *link* layer, and *physical* layer [\\[4\\]](#page-12-4),\
    \ [\\[8\\]](#page-12-5). CXL, built on top of PCIe, adopts these layers. Moreover,\
    \ as only the CXL.io protocol, rather than CXL.cache and CXL.mem, operates on\
    \ a PCIe backwardcompatible command set, CXL extends the transaction and link\
    \ layers for CXL.cache and CXL.mem. The entire hierarchy is called *Flex Bus*\
    \ [\\[9\\]](#page-12-11), depicted in Figure [2.](#page-2-1) The transaction layer\
    \ is where requests are handled by different CXL protocols. The link layer transforms\
    \ one or more requests into packets and is responsible for transmission reliability.\
    \ After the packets are ready, the physical layer performs the actual transmission.\
    \ Note that the physical layer is shared by all protocols. Therefore, an arbitrator/multiplexer\
    \ (CXL ARB/MUX) between the physical layer and the link layer is required to prepare\
    \ the physical bus for a specific protocol and distributes a physical packet to\
    \ the correct link-layer handler. CXL endpoint types and root complex. CXL enables\
    \ the integration of various types of peripheral devices in the computing system.\
    \ These devices are distinct from the hosts and named as *Endpoints* (EP). On\
    \ the contrary, the various components on the host side that conduct CXL functions\
    \ are named *Root Complex* (RC). As shown in Figure [1,](#page-2-0) the endpoints\
    \ are categorized into three types. As a backward compatibility support, all the\
    \ types of devices support CXL.io protocol for I/O operations. In the root complex,\
    \ the CXL.io protocol is managed by the I/O bridge. In addition to I/Os, different\
    \ types of CXL devices target different functions.\n\n<span id=\"page-2-1\"></span>![](_page_2_Figure_5.jpeg)\n\
    \nFig. 2: Hierarchy of CXL Flex Bus layers.\n\nType-1 devices are endpoints with\
    \ a fully-coherent cache but without a global-visible device local memory. The\
    \ cache buffers data using the CXL.cache protocol from the host-side memory to\
    \ utilize the potential data locality. Devices that do not expose their local\
    \ memory, such as SmartNIC [\\[29\\]](#page-12-17), match with type-1. In the\
    \ root complex, the CXL.cache protocol is mainly served by the coherency bridge.\
    \ It responds to CXL.cache requests from devices and records coherence metadata\
    \ of cached data. It may also actively send requests to devices when the host\
    \ asks for ownership or copies of the cached data.\n\nA type-2 device contains\
    \ local memory components (*e.g.*, DDR DRAM or HBM modules) in addition to a coherent\
    \ local cache. Traditional computer architecture cannot coherently access the\
    \ memory of peripheral devices. As a result, it relies on explicit data migration\
    \ between host and device to feed the computational cores in PCIe-attached type-2\
    \ devices, causing underutilized performance [\\[47\\]](#page-13-14), [\\[64\\\
    ]](#page-13-15), [\\[66\\]](#page-13-16). To address this issue, the CXL protocol\
    \ provides methods for the host and devices to efficiently communicate with each\
    \ other. Specifically, the host can push operands to and pull results from the\
    \ device memory via CXL.mem protocol with hardware assistance, and the devices\
    \ can directly access data in host memory through CXL.cache protocol without waiting\
    \ on explicit data copy.\n\nTo support this intercommunication, a CXL type-2 device\
    \ needs to expose its local memory address space to the entire system. This address\
    \ space is integrated into the host coherent physical address space and is named\
    \ *Host-managed Device Memory* (HDM). Figure [1](#page-2-0) shows the integration\
    \ of HDM, with the host local memory and HDM regions organized in the same physical\
    \ address space. In the root complex, in addition to the I/O bridge, two components\
    \ are involved in communication with type-2 devices. The coherency bridge serves\
    \ the CXL.cache requests, similar to the case of type-1 devices. The home agent\
    \ handles CXL.mem transactions, such as issuing load/store instructions to type-2\
    \ devices.\n\nSimilar to type-2 devices, the CXL type-3 devices also include local\
    \ memory components that are exposed as fullycoherent HDMs. However, typical type-3\
    \ devices do not contain computational cores or coherent local caches. These devices\
    \ are categorized as memory expanders [\\[37\\]](#page-13-17), which extend the\
    \ total memory capacity of the entire system. In the root complex, the I/O bridge\
    \ and the home agent manage CXL.io and CXL.mem transactions to type-3 devices,\
    \ respectively.\n\nHDM coherence management modes. Type-2 and type-3 devices expose\
    \ their local memory as HDMs, which are required to be fully coherent. CXL proposes\
    \ three modes to manage the coherence of HDMs. The first mode is hostmanaged coherence,\
    \ denoted as HDM-H. When an HDM is in HDM-H mode, the host is fully responsible\
    \ for managing its coherence (based on either software or hardware), and the device\
    \ is not required to act for coherency. The second mode is called device-managed\
    \ coherence using *Back-Invalidate Snoop* (BISnp), denoted as HDM-DB. In HDM-DB\
    \ mode, the device is responsible for managing the coherence of its local memory\
    \ and can actively send BISnp requests to other devices, including the host. A\
    \ BISnp request may ask the device for a cacheline that has been cached previously.\
    \ Upon receiving a BISnp request, the device needs to check whether it has cached\
    \ the corresponding cacheline, and needs to flush it back through *Back-Invalidate\
    \ Response* (BIRsp). There are two critical facts in the latest CXL 3.1 version\
    \ about HDM-DB mode. First, the BISnp/Rsp transactions are sent through CXL.mem\
    \ protocol, but not through CXL.cache. The CXL.mem provides two dedicated channels\
    \ used only for BISnp and BIRsp. Second, to utilize the 64GT/s PCIe 6.0 transmission\
    \ speed, type-2/3 devices that support device-managed coherence must operate in\
    \ HDM-DB mode. This means that the HDM-DB will be the main mode used to achieve\
    \ optimal performance with device-managed coherence. The third mode is called\
    \ \"device coherent\" and is denoted as HDM-D. In the latest version of CXL, this\
    \ mode is left for backward compatibility for devices that manage HDM coherence\
    \ through the CXL.cache protocol. In the rest of this paper, we mainly consider\
    \ HDM-DB as the typical mode of *Device-Managed Coherence* (DMC).\n\n#### <span\
    \ id=\"page-3-1\"></span>*B. The Scale-up of CXL-Enabled Systems*\n\nTo fulfill\
    \ the growing computational needs of emerging large-scale applications (*e.g.*,\
    \ machine learning [\\[22\\]](#page-12-1), [\\[41\\]](#page-13-1), [\\[46\\]](#page-13-3),\
    \ life science [\\[17\\]](#page-12-0), [\\[24\\]](#page-12-2), [\\[38\\]](#page-13-0)\
    \ and climate modelling [\\[30\\]](#page-12-3), [\\[45\\]](#page-13-2), [\\[63\\\
    ]](#page-13-4)), CXL aims to support the scale-up of a CXL-enabled system from\
    \ a single node to rack-level and even further to extend the memory capacity and\
    \ computation capability. To this end, the CXL protocol projects the design of\
    \ CXL switch as the key component, which is responsible for large-scale interconnection.\
    \ It introduces port-based routing (PBR) and multi-level switching as the key\
    \ features, which distinguish CXL switches from traditional PCIe switches [\\\
    [4\\]](#page-12-4), [\\[8\\]](#page-12-5). In this part, we briefly introduce\
    \ the novel CXL switches.\n\nPCIe-compatible CXL switch. CXL switches can operate\
    \ in a PCIe-compatible configuration. The basic PCIe-compatible configuration\
    \ of a CXL switch is known as a *Single Virtual CXL Switch* (Single VCS), which\
    \ is depicted in Figure [3a](#page-3-0). A Single VCS consists of a single *Upstream\
    \ Port* (USP) and one or more *Downstream Ports* (DSP). Similar to traditional\
    \ PCIe switches, the ports are connected via *virtual PCI-to-PCI Bridges* (vPPB).\
    \ The USP of a Single VCS links to a root port, leading to a host or another switch.\
    \ Each DSP of a Single VCS links to a CXL or legacy PCIe device including another\
    \ switch. A Single VCS behaves identically to a PCIe switch except that it supports\
    \ CXL protocols at the link and transaction layers.\n\nFigure [3b](#page-3-0)\
    \ shows another PCIe-compatible configuration, namely the *Multiple VCS*. This\
    \ configuration supports multiple USPs, each linked to a root port, allowing multiple\
    \ *requesters* (hosts and accelerators) to issue downstream requests. The multiple\
    \ VCS offers additional features compared to the Single VCS. During initialization,\
    \ a Multiple VCS presents itself as\n\n<span id=\"page-3-0\"></span>![](_page_3_Figure_5.jpeg)\n\
    \nFig. 3: CXL system switching configuration examples.\n\nmultiple Single VCS\
    \ instances, corresponding to the number of USPs. The association between DSPs\
    \ and USPs can be dynamically configured or even software-composed during execution.\
    \ Additionally, multiple vPPBs can be combined into a single physical port, thereby\
    \ exposing the physical device under this physical port as multiple logical devices,\
    \ providing resource isolation and pooling of a single CXL device.\n\nMulti-level\
    \ switching. Although multiple VCSs offer some innovative features, fundamentally,\
    \ they still operate in a PCIecompatible manner. A key limitation is that each\
    \ logical CXL device under a DSP is linked to a single USP. Peer-to-peer communication\
    \ without host assistance is not supported. Thus, the scalability of the entire\
    \ system is limited. To address this challenge, the CXL protocol proposes *multi-level\
    \ switching* on top of *Port-Based Routing* (PBR). A system that supports multi-level\
    \ switching mainly consists of two parts: the *fabric network* and the peripheral\
    \ components, depicted in Figure [3c](#page-3-0). The fabric network is constructed\
    \ from multiple PBR switches. Ports connecting to devices/hosts that are not PBR\
    \ switches (*i.e.*, on the edge of the fabric network) are called *edge ports*.\
    \ PBR is used to route requests between different edge ports. In a multi-level\
    \ switching system, each edge port is assigned a 12-bit port ID, supporting up\
    \ to 4096 edge ports. When a CXL.mem request arrives at the edge port, the PBR\
    \ switches route it to the correct edge port based on its internal routing table.\
    \ This mechanism enables peer-to-peer communication among CXL devices and allows\
    \ diverse non-tree system topologies in the fabric network. The design philosophy\
    \ of such topology flexibility is aimed to enhance overall system performance.\n\
    \n#### *C. The Necessity of CXL Simulator*\n\nThere exist multiple challenges\
    \ that necessitate a CXL simulator to conduct relevant computer architecture research.\
    \ The lack of hardware prototypes supporting the latest CXL specification. Many\
    \ of the aforementioned features of CXL are proposed in the latest CXL 3.1 specification,\
    \ including PCIe 6.0 64GT/s transmission speed, multi-level switching, and coherent\
    \ peer-to-peer communication support. These features are critical to establishing\
    \ a CXL system with high scalability and enhanced performance. However, none of\
    \ the current CXL hardware prototypes [\\[7\\]](#page-12-18), [\\[10\\]](#page-12-19)–[\\\
    [12\\]](#page-12-20) are compatible with CXL 3.1. This lack of hardware has greatly\
    \ hindered research progress in the field of CXL. A software simulator can help\
    \ researchers study the newest features of CXL standards, boosting the development\
    \ of high-performance design of CXL systems.\n\nCXL researches demand a highly\
    \ configurable multi-level system. As an interconnect standard, different parts\
    \ in a CXL system tightly work together with each other. Thus, CXL research involves\
    \ multiple aspects including the specification, the devices, and the whole system.\
    \ Simply evaluating an individual aspect is inefficient for studying the overall\
    \ system behaviors. Moreover, building a high-performance CXL system requires\
    \ tuning the system configuration by taking all these aspects into consideration.\
    \ However, hardware implementation costs tremendous labor and time efforts. Leveraging\
    \ a CXL simulator, researchers can easily test a wide range of system setups without\
    \ suffering from implementing specific hardware. Existing tools struggle to simulate\
    \ CXL systems. Due to the lack of hardware, most of the prior works leverage an\
    \ emulation methodology to study CXL systems [\\[18\\]](#page-12-12), [\\[40\\\
    ]](#page-13-9), [\\[44\\]](#page-13-10). This methodology treats remote NUMA nodes\
    \ as an emulator of CXL devices. However, as reported in [\\[55\\]](#page-13-18),\
    \ there are critical differences between remote NUMA nodes and real CXL devices.\
    \ With the new features proposed in the CXL 3.1 specification, this gap of behavior\
    \ and performance between NUMA-based emulators and CXL devices will become more\
    \ severe, making NUMA-based emulators an inaccurate choice for extensive studies.\
    \ In addition to NUMA-based emulators, there exist plenty of software simulators\
    \ for computer architecture. These simulators can be categorized in two types:\
    \ (1) computation-centric simulators, including gem5 [\\[21\\]](#page-12-13),\
    \ [\\[33\\]](#page-12-14) and GPGPUsim [\\[19\\]](#page-12-15), and (2) network-centric\
    \ simulators, including BookSim [\\[35\\]](#page-13-11) and Garnet [\\[16\\]](#page-12-16).\
    \ Different types of simulators focus on different aspects of the system. The\
    \ computationcentric simulators focus on accurate processor unit modeling, while\
    \ the network-centric simulators pay attention to diverse network topologies and\
    \ flow control. However, since CXL systems are highly collaborative systems, in\
    \ which all the aspects play an important role in system performance, these simulators\
    \ struggle to provide comprehensive simulation results.\n\nSpecifically, there\
    \ are three major characteristics of CXLenabled systems that existing tools are\
    \ unable to accurately simulate. First, CXL 3.1 specification introduces PBR and\
    \ multi-level switching to support high system scalability and non-tree topologies.\
    \ Second, the specification defines DMC as a key feature to support direct peer-to-peer\
    \ communication between devices without the assistance of the host. Devices featuring\
    \ DMC functionality are required to have a *Device COHerency agent* (DCOH) to\
    \ manage coherence for the cachelines from their local memory. With the help of\
    \ DMC devices, the system can offload expensive coherence management to multiple\
    \ individual devices, eliminating the need for a central coherence engine and\
    \ thereby enhancing performance scalability. Third, the adopted PCIe bus and its\
    \ full-duplex nature notably influence system performance. None of the existing\
    \ tools can accurately simulate all these features. NUMA-based emulators, being\
    \ limited by physical constraints such as the number of sockets and slots, can\
    \ hardly provide the required scalability. They are unable to simulate either\
    \ DMC or PCIe bus, because they can only adopt a traditional memory system with\
    \ DDR DRAM that doesn't\n\n<span id=\"page-4-0\"></span>![](_page_4_Figure_2.jpeg)\n\
    \nFig. 4: Overview of the ESF architecture and components.\n\nsupport DMC, and\
    \ the data paths of NUMA interfaces are quite different from CXL-enabled PCIe\
    \ paths. Existing computational simulators can only be configured to basic switch\
    \ setups and often leverage a central engine (e.g., host-side CPU) for coherence\
    \ management. Thus, they struggle to simulate PBR and DMC. On the other hand,\
    \ existing network-centric simulators lack the basic support of coherence management.\
    \ In addition, existing simulators provide poor support for highversion PCIe simulation\
    \ (*e.g.*, gem5 only supports legacy PCI simulation). To sum up, the existing\
    \ simulation tools face intractable difficulties in accurately simulating CXL\
    \ systems. This lack of tools urges the development of a CXL simulator.\n\n####\
    \ III. MODELLING DETAILS\n\nIn this work, we present, *ESF*, our novel simulation\
    \ framework developed from scratch to support accurate simulation of the aforementioned\
    \ critical features. Figure [4](#page-4-0) depicts the architectural overview\
    \ of ESF and its major components. In the rest of this section, we introduce the\
    \ design principles of ESF and describe these major components.\n\n#### *A. Architectural\
    \ Overview*\n\nTo support the simulation of highly scalable CXL-based systems,\
    \ our novel simulation framework utilizes a dedicated architecture consisting\
    \ of two major layers, which tightly work together to provide accurate and detailed\
    \ results during the simulation.\n\nThe first layer is the interconnect layer.\
    \ As mentioned, to provide high scalability, the CXL protocol proposes multi-level\
    \ switching and port-based routing. These features allow CXL systems to adopt\
    \ non-tree interconnect topologies that most of the existing tools cannot support.\
    \ The interconnect layer in ESF is dedicated to the simulation of diverse system\
    \ topologies. Upon initialization, the interconnect layer receives a set of device\
    \ pairs, which are configured as directly connected through physical links. Then,\
    \ the interconnect layer constructs an internal topology graph and builds a default\
    \ routing strategy based on the shortest-path algorithm [\\[27\\]](#page-12-21),\
    \ [\\[52\\]](#page-13-19). During the simulation, the interconnect layer provides\
    \ routing information to all devices. While most of the devices can directly employ\
    \ the default routing, devices like switches can access detailed graph information\
    \ to create dedicated routing for their special functionalities. Therefore, the\
    \ interconnect layer enables peerto-peer communication between different devices\
    \ in a system with an arbitrary topology.\n\nThe next layer on top of the interconnect\
    \ layer is the device layer. This layer models several kinds of fundamental devices\
    \ in a CXL system, such as memory devices and physical buses. To fully support\
    \ peer-to-peer communication as required by the CXL standard, all the devices\
    \ are treated equally. They can actively operate without involving any central\
    \ device, such as a host CPU. This enables ESF to simulate key features proposed\
    \ by CXL protocol, including device-managed coherence (DMC). In addition, the\
    \ device layer adopts a decoupling design to model device functions. For example,\
    \ the device coherency agent (DCOH), which mainly conducts DMC function, is decoupled\
    \ from the memory device. Therefore, ESF can easily configure a wide range of\
    \ hardware setups, adjusting parameters such as coherence management policies\
    \ for each DCOH without bothering the memory devices. This function decoupling\
    \ enhances the flexibility of the simulation configuration, expanding the exploration\
    \ spaces. Based on the fundamental architecture, ESF implements several primary\
    \ components to support the simulation of multiple classical scenarios in CXL-based\
    \ systems.\n\nImplementation and usage details. ESF is primarily written in C++.\
    \ By default, users can simply prepare configuration files and pass them to the\
    \ simulator to setup and simulate a proposed system. Furthermore, ESF provides\
    \ essential abstraction and interfaces in both the interconnect and device layers,\
    \ which allow users to easily hack it and implement components for their own purposes.\
    \ The detailed usage instructions are included in the open-sourced code repository.\n\
    \n### *B. Computational Components*\n\nESF provides a straightforward abstraction\
    \ of computational components (*i.e.*, requesters) in CXL systems, namely *hosts*\
    \ and *accelerators*. Each computational component consists of three primary units:\
    \ *request queue*, *address translation unit*, and *cache coherence management\
    \ unit*. A request queue is defined by the queue capacity and the time interval\
    \ between issued requests. It models the capability of the computational component\
    \ to issue requests to other devices. An address translation unit simulates various\
    \ interleaving policies and can be used to investigate the impacts of different\
    \ policies on system performance. It can adjust the strategy of interleaving requests\
    \ among multiple memory endpoints to improve system bandwidth [\\[9\\]](#page-12-11),\
    \ [\\[39\\]](#page-13-20), [\\[60\\]](#page-13-21). The cache coherence management\
    \ unit simulates an internal cache, which records the metadata (*e.g.*, source\
    \ endpoints) of fetched cachelines during simulation. Upon receiving a coherent\
    \ request (*e.g.*, BISnp) from any endpoint, the unit searches the cache for the\
    \ corresponding cacheline and flushes it back if necessary. This unit collaborates\
    \ with DCOH to execute device-managed coherence functions. The computational component\
    \ supports the simulation of various access patterns. It can be configured with\
    \ a stream pattern or random pattern and will automatically generate requests\
    \ during the simulation. It can also be set in trace-based mode, which receives\
    \ external trace files and replays the recorded requests. This mode helps conduct\
    \ simulations of real-world workloads. It is important to note that the implementation\
    \ of computational components is offered as an easy-to-use default configuration.\n\
    \n## *C. Interconnective components*\n\nESF implements two main components that\
    \ are responsible for the simulation of interconnection: *bus* and *switch*. They\
    \ model interconnect features in the CXL specification (*i.e.*, the full-duplex\
    \ PCIe bus transfer and the port-based routing of CXL switch) that allow non-tree\
    \ system topologies. To accurately reflect the full-duplex feature of PCIe buses,\
    \ ESF implements a bandwidth allocation module for the bus component. First, with\
    \ the help of the interconnect layer, the bus detects all the data transfer directions\
    \ that pass through it. Then, to simulate full-duplex functionality, the bus allocates\
    \ full bandwidth for each direction (cf. the bandwidth allocation unit shown in\
    \ Figure [4\\)](#page-4-0). ESF also ensures the bus component is highly configurable.\
    \ The bandwidth can be configured during initialization. Additionally, the bus\
    \ can be set to halfduplex with configurable turnaround overheads. This flexibility\
    \ enables researchers to explore multiple hardware setups. While the bus component\
    \ mainly operates in two directions (*i.e.*, upstream and downstream), the switch\
    \ component is used for conducting more complicated port-based routing as required\
    \ by the CXL specification. During the initialization, the switch can receive\
    \ multiple connections from different devices up to its number of ports. Then,\
    \ with the help of routing information provided by the interconnect layer, the\
    \ switch constructs an internal routing table for different sources and destinations.\
    \ Upon the arrival of a packet, based on the source, receiving port, and destination,\
    \ the switch forwards it to the corresponding port according to the routing table.\n\
    \n## *D. Device-side snoop filter*\n\nTo support DMC, We implements a device-side\
    \ snoop filter as an example DCOH. As required by the CXL specification, the snoop\
    \ filter operates in inclusive mode. An inclusive snoop filter is a buffer that\
    \ records all the cachelines from its corresponding endpoints that are cached\
    \ by other devices, such as the host and accelerators. Each entry in the buffer\
    \ stores the coherence metadata of a cacheline, including the coherence state\
    \ and the owner list. When a new coherent request is received (*e.g.*, a host\
    \ requires exclusive ownership of a cacheline), the snoop filter allocates a new\
    \ entry to record the updated metadata. In cases of conflicts with other devices\
    \ that have already owned the cacheline, the snoop filter sends BISnp requests\
    \ to the original owners before proceeding with the new request. Also, when the\
    \ buffer is run out of new entry, the snoop filter selects a victim entry and\
    \ sends the corresponding BISnp requests to clear the entry before serving the\
    \ new request. To support the above functions, we implements the snoop filter\
    \ as a fully-associative buffer of its corresponding endpoints. It performs entry\
    \ allocation, metadata update, BISnp requesting, and victim selection for coherent\
    \ requests targeting these endpoints. When it is necessary to clear an entry (*i.e.*,\
    \ conflict or victim eviction), the snoop filter sends BISnp requests to the original\
    \ owners using the default routing strategy provided by the interconnect layer.\
    \ Once all the BIRsps are collected, the snoop filter clears the entry for the\
    \ next request. It may also write back the cacheline\n\n<span id=\"page-6-0\"\
    ></span>![](_page_6_Figure_0.jpeg)\n\n<span id=\"page-6-1\"></span>TABLE II: Comparison\
    \ with other simulation frameworks.\n\nto the corresponding endpoint if the cacheline\
    \ is flushed in a dirty state. We also modularizes the victim selection to allow\
    \ the researchers to evaluate various policies.\n\n#### *E. Integration with existing\
    \ simulators*\n\nIn order to demonstrate the extensibility of our framework, we\
    \ integrate it with several existing simulators. Table [I](#page-6-0) provides\
    \ an overview of these simulators. The first integrated simulator is *gem5* [\\\
    [21\\]](#page-12-13), [\\[33\\]](#page-12-14). As a widely adopted simulator,\
    \ gem5 models processors and memory systems with extensive details. We integrate\
    \ ESF with gem5 to take advantage of its processor simulation and to enable the\
    \ end-to-end evaluation of real-world applications. Specifically, the gem5 memory\
    \ system contains three major layers (*i.e.*, cache, memory controller, and underlying\
    \ memory). Among the three layers, the memory controller performs as an interconnect\
    \ level similar to the level of CXL protocol, which passes the memory accesses\
    \ from caches to the underlying memory. It also manages different types of memories\
    \ and provides a general view of memory to the CPU. To cooperate with gem5's native\
    \ memory system, we extends gem5 MemCtrl with the interfaces to ESF to add CXL\
    \ interconnection into the simulation. Specifically, as shown in Figure [5a](#page-6-0),\
    \ we implemented a Wrapper object, which utilizes the memory management functions\
    \ of the MemCtrl. Each wrapper is integrated with two ESF devices, namely *UpInterface*\
    \ and *DownInterface*. When a memory packet arrives, it is firstly passed to the\
    \ UpInterface. The interface then transforms it into a packet and passes it to\
    \ the DownInterface through a standalone ESF simulation to simulate the additional\
    \ latency of the CXL system. This procedure is performed by reusing the gem5 event\
    \ engine (*i.e.*, registering and simulating a set of gem5-style events). Upon\
    \ the arrival of the packet at DownInterface, it will be transformed back to the\
    \ gem5 memory packet, and the functions of the original gem5 MemCtrl are conducted.\
    \ After the procedure in the underlying memory, the packet is passed back from\
    \ DownInterface to UpInterface to simulate the response procedure. One of the\
    \ advantages of this implementation is that the wrapper can utilize the underlying\
    \ memory objects from the original gem5, which enhances its extensibility. To\
    \ support DMC functionality, we also implement a coherency interface using gem5's\
    \ native tool SLICC. When the DCOH in ESF issues a back-invalidation request,\
    \ it will be forwarded to the CohInterface. The interface will use gem5 native\
    \ events to invalidate corresponding cachelines in the cache hierarchy to simulate\
    \ the DMC function.\n\nWe also integrate ESF with two representative memory and\
    \ storage simulators, namely *DRAMsim3* [\\[42\\]](#page-13-22) and *SimpleSSD*\
    \ [\\[31\\]](#page-12-22). These simulators provide architectural details of various\
    \ types of endpoints, including DDRx/HBM DRAM and SSD, which may exist in future\
    \ CXL systems. In particular, we implemented wrappers for these simulators. For\
    \ cycle-based simulator (*i.e.*, DRAMsim3), the wrapper periodically register\
    \ a clocking event to make progress in DRAMsim3 simulation. For event-driven simulator\
    \ (*i.e.*, SimpleSSD), the wrapper transforms the event format and registers them\
    \ in the ESF event engine. In summary, ESF is capable of evaluating CXLbased processors\
    \ and endpoints by leveraging the existing simulators.\n\nTable [II](#page-6-1)\
    \ summarizes the differences between ESF and several related simulation frameworks\
    \ that simulate computing systems based on either CXL protocol or traditional\
    \ interconnects. Specifically, most of the existing networkextended computation-centric\
    \ simulation frameworks (*e.g.*, gem5-garnet [\\[16\\]](#page-12-16), [\\[20\\\
    ]](#page-12-23) and BZSim [\\[53\\]](#page-13-23)) are mainly designed for on-chip\
    \ interconnection simulation and lack detailed simulation for off-chip interconnection,\
    \ such as the PCIe bus used by CXL. On the other hand, existing CXL-oriented simulators\
    \ (*e.g.*, CXLMemSim [\\[62\\]](#page-13-24) and MQSim CXL [\\[61\\]](#page-13-25))\
    \ are constrained to simulate only one specific type of CXL devices (*i.e.*, memory\
    \ for CXLMemSim and SSD for MQSim CXL) rather than a general computing system\
    \ that have various types of CXL devices attached. In contrast, ESF can simulate\
    \ the off-chip device interconnection, support diverse peripheral devices in CXL\
    \ systems, and integrate with commonly-used simulators (*e.g.*, gem5, DRAMsim3,\
    \ and SimpleSSD) for strong extensibility. ESF is provided as a unified research\
    \ wheel for researchers who are interested in different fields, including architecture,\
    \ distributed systems, and networks.\n\n# IV. VALIDATION\n\n<span id=\"page-6-2\"\
    ></span>Methodology. We validate ESF using a dual-socket platform with commercially\
    \ available CXL hardware. The top view of our platform is shown in Figure [6](#page-7-0)\
    \ (the CXL memory expander is unplugged for a better view). Each socket is equipped\
    \ with an Intel Xeon Gold 6416H CPU [\\[12\\]](#page-12-20) and eight DDR5-4800\
    \ DRAM DIMMs, providing 128GB of main memory. One of the sockets is attached by\
    \ a CXL memory\n\n<span id=\"page-7-0\"></span>![](_page_7_Picture_0.jpeg)\n\n\
    Fig. 6: Top view of the hardware platform.\n\n| Requester process time       \
    \  | 10ns | PCIe port delay | 25ns |\n|--------------------------------|------|-----------------|------|\n\
    | Cache access time              | 12ns | Bus time        | 1ns  |\n| Device controller\
    \ process time | 40ns | Switching time  | 20ns |\n|                          \
    \      |      |                 |      |\n\nTABLE III: Latency configurations\
    \ of critical components in validation.\n\nexpander with a CXL memory expander\
    \ controller (MXC) from Montage Technology [\\[10\\]](#page-12-19), supporting\
    \ up to CXL 2.0 protocol and PCIe 5.0 ×16 standard. The memory expander consists\
    \ of four DDR5-4800 DRAM DIMMs, providing 64GB of CXL HDM-H memory. To simulate\
    \ the CXL system, we construct a sample system in ESF, which includes a requester,\
    \ an interconnect bus, and four memory endpoints. For a fair comparison, we adjust\
    \ the number of DRAM DIMMs in each NUMA node to four, which is equal to the DIMM\
    \ number in the CXL memory expander. In ESF we use the integrated DRAMsim3 [\\\
    [42\\]](#page-13-22) as the default endpoint components for an accurate DRAM simulation.\
    \ For calibration, we follow the statistics measured on real hardware platforms\
    \ and those from multiple prior works [\\[5\\]](#page-12-24), [\\[26\\]](#page-12-25),\
    \ [\\[32\\]](#page-12-26), [\\[40\\]](#page-13-9), [\\[44\\]](#page-13-10), [\\\
    [49\\]](#page-13-26), [\\[55\\]](#page-13-18), and configure the components in\
    \ ESF with these latency statistics. The detailed configurations are listed in\
    \ Table [III.](#page-7-0) For validation, we measure three major metrics: idle\
    \ latency, peak bandwidth under different read-write ratio, and loaded-latency\
    \ under different request intensity. To measure these metrics on hardware platform,\
    \ we adopt Intel Memory Latency Checker (MLC) [\\[6\\]](#page-12-27), a widely\
    \ used tool. The MLC is designed to evaluate a certain memory region for its idle\
    \ latency and peak bandwidth. In addition, the read-write ratio of the requests\
    \ generated by MLC is configurable. It can also run in loadedlatency mode, which\
    \ varies the request intensity to measure the change of latency and bandwidth\
    \ under different system loads. We run the tests of idle latency, peak bandwidth\
    \ and loaded-latency with MLC based on CXL MXC memory and NUMA remote memory respectively\
    \ to extract the metrics of CXL hardware and NUMA-emulator. For simulation platform,\
    \ we directly generate memory accesses from the requester component to measure\
    \ these metrics. By adjusting the queue size and issuing delay between requests,\
    \ we can modify the request\n\n<span id=\"page-7-1\"></span>![](_page_7_Figure_5.jpeg)\n\
    \nFig. 8: Latency-bandwidth curves of different platforms.\n\n| Compared platforms\
    \ | SpecCPU2017 workloads |               |  |\n|--------------------|-----------------------|---------------|--|\n\
    |                    | gcc                   | mcf           |  |\n| CXL Hardware\
    \       | 18.0% (0%)            | 24.2% (0%)    |  |\n| ESF standalone     | 18.7%(+0.7%)\
    \          | 29.8% (+5.6%) |  |\n| gem5-ESF           | 15.6% (-2.4%)        \
    \ | 19.8% (-4.4%) |  |\n| NUMA emulation     | 20.0% (+2.0%)         | 15.0% (-9.2%)\
    \ |  |\n| gem5-garnet        | 12.2% (-5.8%)         | 15.2% (-9.0%) |  |\n\n\
    TABLE IV: Simulated execution time overhead incurred by CXL memory of applications\
    \ on different platforms.\n\n|  | Workloads | Compared platforms |       |  |\n\
    |--|-----------|--------------------|-------|--|\n|  | gem5-ESF  | gem5-garnet\
    \        |       |  |\n|  | gcc       | 1.7%               | 21.5% |  |\n|  |\
    \ mcf       | 2.7%               | 24.5% |  |\n\nTABLE V: Simulation time overhead\
    \ incurred to vanilla gem5.\n\nintensity to evaluate the loaded-latency. The idle\
    \ latency and peak bandwidth are measured under fixed low and high system loads,\
    \ respectively. The total amount of generated requests during each simulation\
    \ test is 16000, and each endpoint receives 4000 requests. To warm up the system\
    \ to the steady-state, 16000 additional requests are performed before collecting\
    \ the final results. We also compare the simulation accuracy and speed of ESF\
    \ (both standalone and gem-integration modes) with other platforms by running\
    \ two example workloads from SPEC CPU2017 [\\[23\\]](#page-12-28). The cache hierarchy\
    \ is configured to match our hardware platform (*i.e.*, 1.7MB L1D cache, 72MB\
    \ L2 cache and 96MB L3 cache). For hardware platforms, the workloads are directly\
    \ run by specifying the used socket and memory with numactl. For standalone mode,\
    \ the memory access traces of the workloads are firstly collected with Intel PIN\
    \ [\\[14\\]](#page-12-29) and filtered with a simulated cache hierarchy, then\
    \ passed to ESF. For gem5-integrated simulators (*i.e.*, gem5-ESF and gem5-garnet),\
    \ the workloads are run in gem5 SE mode.\n\nResults. Figure [7](#page-7-1) presents\
    \ the results of idle latency and bandwidth. In addition to the CXL hardware and\
    \ ESF, we also demonstrate the results of local DRAM and remote DRAM. As can be\
    \ observed, after calibration, ESF exhibits an outstanding latency accuracy compared\
    \ to NUMA-based emulators using remote DRAM. Regarding bandwidth, ESF shows acceptable\
    \ errors ranging from 0.1% to 10% when compared to CXL hardware, while the remote\
    \ DRAM modules do not accurately reflect the absolute value of CXL hardware. We\
    \ also derive an observation that, as the read-write mixed ratio increases, the\
    \ bandwidth of CXL hardware increases synchronously, which is well captured by\
    \ ESF. In contrast, the results of local and remote DRAM show a decreasing trend\
    \ in bandwidth. Further investigation on this characteristic can be found in Section\
    \ [V.](#page-8-0)\n\nWe also conduct loaded-latency tests on different platforms\
    \ by adjusting the requester intensity. The results are shown as latency-bandwidth\
    \ curves in Figure [8.](#page-7-1) The curves of ESF can accurately align with\
    \ those of CXL hardware for both read and write requests, with an error margin\
    \ up to only 12%, and an average error of 4.3%. In both low and high-intensity\
    \ scenarios, ESF closely reflects the average latency observed on the CXL hardware\
    \ platform. In contrast, the NUMA-based emulator presents curves that are completely\
    \ apart from those of CXL hardware.\n\nTable [IV](#page-7-1) demonstrates the\
    \ accuracy of different platforms on SpecCPU2017 workloads. Since the performance\
    \ of realworld applications is highly related to the exact microarchitecture of\
    \ hardware CPUs, which is unknown and cannot be accurately simulated, we, instead,\
    \ use the execution time overheads incurred by CXL memory as the metric. This\
    \ approach excludes the influence of CPU micro-architecture, allowing us to concentrate\
    \ only on the memory systems. As observed, both ESF-standalone and gem5-ESF can\
    \ accurately reflect CXL overhead in real-world workloads, with errors as low\
    \ as 0.7% compared to hardware results. However, NUMAemulation and gem5-garnet\
    \ demonstrate errors up to 9.2% and 9.0%, respectively. In summary, ESF exhibits\
    \ surpassing simulation accuracy compared to the prior approaches.\n\nWe also\
    \ compare the simulation speed of ESF with a representative simulator (*i.e.*,\
    \ gem5-garnet). For a fair comparison, we compare gem5-garnet with the integrated\
    \ mode of ESF (*i.e.*, gem5-ESF) to exclude the influence of gem5. According to\
    \ the results shown in Table [V,](#page-7-1) ESF only increases simulation time\
    \ by 2% compared to vanilla gem5 on average, while garnet incurs 22.5% extra simulation\
    \ time on average. The results indicate that ESF has a faster simulation speed\
    \ than garnet.\n\n#### V. DESIGN SPACE EXPLORATION\n\n<span id=\"page-8-0\"></span>ESF\
    \ is designed to help researchers make deep observations in features of newest\
    \ CXL that traditional tools struggle to support, including PBR, DMC and full-duplex\
    \ transfer. In this section, we explore the performance impacts of these features\
    \ across various system setups. All the following experiments perform warming-up\
    \ requests to prepare the systems to steadystates, and only collect results under\
    \ the steady-states.\n\n#### <span id=\"page-8-3\"></span>*A. Impact of Different\
    \ System Topologies*\n\nAs discussed in Section [II-B,](#page-3-1) the CXL protocol\
    \ introduces PBR to support high scalability, which allows non-tree system topologies.\
    \ To understand the impact on performance of system topologies, we perform a set\
    \ of experiments using systems with N requesters (*i.e.*, hosts and accelerators)\
    \ and N memory devices. The setup of a N-N system is denoted as \"system scale\
    \ = 2N\". In these experiments, the requesters issue random\n\n<span id=\"page-8-2\"\
    ></span><span id=\"page-8-1\"></span>![](_page_8_Figure_8.jpeg)\n\nFig. 10: System\
    \ bandwidth of different system topologies and scales, normalized to the max bandwidth\
    \ of switch port.\n\nmemory requests to all the memory devices. Different requesters\
    \ and memory devices are connected via multiple PBR switches with different topologies.\
    \ The bandwidth of a PBR switch port is constrained to a constant value. We investigated\
    \ five types of topologies: (1) chain, (2) tree, (3) ring, (4) spine-leaf (SL),\
    \ and (5) fully-connected (FC). Figure [9](#page-8-1) shows the example diagrams\
    \ of these topologies. During the simulation, each requester generates 4000 accesses\
    \ to each endpoint, and the total request amount is 4000 ∗ N<sup>2</sup> .\n\n\
    Figure [10](#page-8-2) illustrates the aggregated bandwidth in different systems.\
    \ The bandwidth values are normalized to the maximum port bandwidth. The results\
    \ highlight the bandwidth bottlenecks in different topologies. Both the chain\
    \ and tree include \"bridge\" routes (*i.e.*, all routes between switches in chain\
    \ and routes directly connected to the root switch in tree), which are shared\
    \ by all the requesters, limiting the system bandwidth to the maximum capacity\
    \ of a single switch port. Scaling up the system with these topologies cannot\
    \ improve the performance. The ring can provide an extra route in addition to\
    \ chain and tree. Thus, by scaling up the system, the bandwidth can reach 2× of\
    \ the port capacity. Compared to the former topologies, spine-leaf and fully-connected\
    \ exhibit high scalability. The spine-leaf achieves this by replacing the bottleneck\
    \ routes with a high-performance interconnect network (*i.e.*, the \"spine\").\
    \ However, the competition among requesters on ports in \"leaf\" switches still\
    \ exists. Thus, the spine-leaf can only provide <sup>N</sup> <sup>2</sup> × bandwidth\
    \ of the port capacity. The fully-connected overcomes this limitation with a network\
    \ where each pair of devices can communicate directly. As a result, each requester\
    \ is provided with full port bandwidth, achieving a system bandwidth of N× port\
    \ capacity.\n\nFigure [11](#page-9-0) depicts the average latency of requests\
    \ across various topologies with a system scale of 16. The results are grouped\
    \ by the number of hops the requests experienced. It can be observed that as the\
    \ number of hops increases, the request will experience more switch queuing time,\
    \ harming the performance. The bottleneck of \"bridge\" routes also significantly\
    \ impacts the latency. For example, as shown in Figure [11b](#page-9-0), the queuing\
    \ time of requests with 9 hops is drastically higher than that of other requests.\
    \ This is because these requests are congested on the \"bridge\" routes. In contrast,\
    \ when being\n\n<span id=\"page-9-0\"></span>![](_page_9_Figure_0.jpeg)\n\nFig.\
    \ 11: Latency of different system topologies, grouped by hop counts. The system\
    \ scale is 16.\n\n![](_page_9_Figure_2.jpeg)\n\nFig. 12: Latency of different\
    \ system topologies under isobisection bandwidth condition, grouped by hop counts.\
    \ The system scale is 16.\n\n<span id=\"page-9-1\"></span>![](_page_9_Figure_4.jpeg)\n\
    \nFig. 13: Bandwidth of observed host with different routing strategies, normalized\
    \ to the max bandwidth of switch port.\n\nprovided with an extra route in ring,\
    \ the maximum latency is halved compared to the maximum latency in tree and chain,\
    \ and the variation among latency with different hop counts becomes more balanced.\
    \ Latency degradation due to congestion can also be found in spine-leaf system,\
    \ where requests may congest at leaf switches and experience queuing time increasing.\
    \ We also conduct ISO-bisection bandwidth tests on different system topologies,\
    \ which are configured to deliver the same bisection bandwidth. The average latency\
    \ results are shown in Figure [12.](#page-9-0) Since the maximum port bandwidths\
    \ of different system topologies can vary under ISObisection bandwidth configuration,\
    \ which leads to unmatched switch queuing time, we only show the overall average\
    \ latency values of different hop numbers and omit the breakdown. Although the\
    \ average latency values of chain, tree and ring are decreased, increasing the\
    \ hop number can still cause congestion on critical paths, incur significant latency\
    \ overhead (*i.e.*, about 2× in chain and 1× in tree and ring compared to the\
    \ latency with lowest hop number) and introduce latency unpredictability. In contrast,\
    \ since spine-leaf and fully-connected require fewer hops due to their specific\
    \ network structures, they can provide high stability for latency values, and\
    \ achieve high system scalability. These results indicate that the traditional\
    \ tree-like topology experiences significant system performance bottlenecks.\n\
    \nWe then investigate the impact of routing strategy under a typical high-performance\
    \ and low-cost system topology (*i.e.*, spine-leaf). The routing strategy is categorized\
    \ into two classes, namely Oblivious and Adaptive [\\[51\\]](#page-13-27), [\\\
    [58\\]](#page-13-28). Oblivious routes every packet statically based on the source\
    \ and destination, while Adaptive chooses the next hop dynamically according to\
    \ the congestion condition among the available choices. The system is configured\
    \ to include eight memory endpoints, eight noisy neighbors that intensively access\
    \ the memories, and a host that accesses the memories at a fixed rate. Figure\
    \ [13](#page-9-1) shows the observed bandwidth of the host, normalized to the\
    \ maximum port bandwidth. The results show that Adaptive routing strategies, compared\
    \ to Oblivious, can drastically improve host performance under the pressure of\
    \ noisy neighbors, indicating the importance of the routing strategy.\n\n# <span\
    \ id=\"page-9-2\"></span>*B. Impact of Snoop Filter Victim Selection Policies*\n\
    \nAs mentioned in Section [II,](#page-1-0) the CXL protocol requires devices with\
    \ DMC to implement an inclusive snoop filter (SF). Due to its inclusive nature,\
    \ in cases of insufficient buffer entries, the SF will acquire new clear entries\
    \ by selecting victims and issuing back-invalidate snoop (BISnp) requests to their\
    \ current owners. These BISnp requests will clear the lines in the owners' local\
    \ cache, impacting the system performance. Therefore, a goal of SF victim selection\
    \ policy is to reduce the frequency of BISnp.\n\nTo investigate the impact of\
    \ different victim selection policies in SF, we test five basic policies. The\
    \ tested system includes a requester, which issues coherent requests in a skewed\
    \ pattern with 90% to hot data and 10% to cold data. The amount of hot data takes\
    \ 10% of the total memory footprint. The requester is equipped with a local cache\
    \ that filters the hit events. The size of the cache is configured to 20% of the\
    \ total memory footprint, making sure it can cache all the hot data. In cases\
    \ of a cache miss, the request is routed to the memory device through a bus, which\
    \ is configured with infinite bandwidth to eliminate unexpected performance impact.\
    \ On the device side, an SF filters the requests and issues BISnp whenever necessary.\
    \ A BISnp will be sent to the requester to invalidate the corresponding cacheline.\
    \ The size of the SF is set to be the same with the local cache size in order\
    \ to record states of all cached data. We test the following victim selection\
    \ policies: (1) FIFO (First-In, First-Out), (2) LRU (Least Recently Used), (3)\
    \ LFI (Least Frequently Inserted), (4) LIFO (Last-In, First-Out), and (5) MRU\
    \ (Most Recently Used). The number of endpoints is set to four, and each endpoint\
    \ receives 4000 accesses during the evaluation.\n\nFigure [14](#page-10-0) depicts\
    \ the results of bandwidth, latency, and invalidation count, all normalized to\
    \ FIFO. Since there is little hit event in the SF, FIFO and LRU behave similarly\
    \ to LIFO and MRU, respectively. Compared to FIFO, LIFO improves the bandwidth\
    \ by 5%, while decreasing the average latency and invalidation count by 15% and\
    \ 16%, respectively. The difference between the SF and local cache explains these\
    \ findings. As the system reaches its steady state, most of the hot data reside\
    \ in the local cache, while the SF records the coherence states of these hot data.\
    \ Most of the requests reaching the SF are cache misses, targeting cold data.\
    \ In this scenario, the \"last-in\" or \"most recent\" entries, rather than the\
    \ \"first-in\" or \"least recent\" entries, actually store information for cold\
    \ data and are the suitable victims. In contrast, the FIFO and LRU are more likely\
    \ to invalidate hot data, harming the\n\n<span id=\"page-10-0\"></span>![](_page_10_Figure_0.jpeg)\n\
    \n<span id=\"page-10-1\"></span>![](_page_10_Figure_1.jpeg)\n\nFig. 16: Bandwidth\
    \ under different R:W ratios and header overheads, normalized to read-only scenarios.\n\
    \nsystem performance. To demonstrate the impact of invalidating hot data, we also\
    \ propose and evaluate the LFI policy, which maintains a global counter table\
    \ to record the inserted times of each cacheline. Upon invalidation, LFI selects\
    \ the least frequently inserted address as the victim to avoid invalidating hot\
    \ data. The results show that LFI reduces invalidation count by 15% compared to\
    \ FIFO, proving that FIFO invalidates hot data more frequently. However, since\
    \ the LFI leverages global information, it will periodically invalidate all the\
    \ hot cachelines when their inserted times become equal. This leads to a slightly\
    \ worse performance compared to LIFO and MRU.\n\n## *C. Impact of* InvBlk *Commands*\n\
    \nThe CXL protocol proposes a set of InvBlk commands for the SF. When the SF sends\
    \ a BISnp request, it can additionally send a InvBlk command, which requires the\
    \ owners to invalidate a sequence of cachelines with contiguous addresses. The\
    \ length of these cachelines can range from two to four. This feature is introduced\
    \ to improve the efficiency of BISnp, allowing the SF to clear multiple entries\
    \ with a single request. To understand the impact of InvBlk commands, we perform\
    \ a set of experiments on a system with two requesters issuing sequential requests,\
    \ a local cache in each requester, a bus, and a memory device with an SF. The\
    \ configurations including cache size, SF size and request number are the same\
    \ with those described in Section [V-B.](#page-9-2) To zoom-in the effects of\
    \ InvBlk commands, the SF employs a block-lengthprioritized victim selection policy.\
    \ During victim selection, the SF chooses the longest sequence of entries with\
    \ contiguous addresses. It leverages LIFO policy among multiple possible victims.\
    \ In our experiments, we limit the maximum length of entry sequences to evaluate\
    \ the impact of InvBlk.\n\nFigure [15](#page-10-0) depicts the results of bandwidth,\
    \ average latency, and average waiting time for invalidation. When the InvBlk\
    \ length is larger than one, a single BISnp request can clear more than one entry.\
    \ As a result, subsequent coherent requests no longer need to wait on BISnp, reducing\
    \ the average waiting time. When two lines are cleared in one BISnp, this benefit\
    \ brings the reduction of total average latency and the increase\n\n![](_page_10_Figure_7.jpeg)\n\
    \nFig. 15: Performance of different InvBlk lengths, normalized to length=1.\n\n\
    ![](_page_10_Figure_9.jpeg)\n\nFig. 17: Bus utility and transmission efficiency\
    \ under different R:W ratios and header overheads.\n\nof bandwidth. However, when\
    \ the number of lines in a BISnp exceeds two, the overhead of access requester\
    \ local caches increases, diminishing the benefit of InvBlk. Furthermore, the\
    \ data within the BISnp flows compete for the transmission bandwidth. As a result,\
    \ the performance of larger InvBlk length shows no improvement compared to length=1.\n\
    \n#### <span id=\"page-10-2\"></span>*D. Full Duplex Transmission*\n\nWe then\
    \ present our exploration with ESF on the impact of PCIe bus' full-duplex transfer\
    \ feature. We build a dedicated system to evaluate the effects of this feature,\
    \ which includes a requester issuing random requests based on a read-write ratio\
    \ setup, a bus incurring packet size overheads to the header packets, and four\
    \ memory devices. Besides the bandwidth, we define two other metrics: (1) bus\
    \ utility, indicating the fraction of time when the bus is busy compared to the\
    \ total simulation time (average in all transmission directions), and (2) transmission\
    \ efficiency, denoting the fraction of time the bus spends on payload transmission\
    \ compared to total bus transmission time. Across experiments, we adjust the readwrite\
    \ ratio and the incurred header overheads to understand the impact of full-duplex\
    \ transmission under different scenarios. The system includes four endpoints,\
    \ each of them receives 4000 requests during the simulation.\n\nFigure [16](#page-10-1)\
    \ depicts the bandwidth results. The header overheads are normalized to payload\
    \ length, and the bandwidth values are respectively normalized to read-only scenarios\
    \ for each header overhead setting. The figure shows that, with all other configurations\
    \ unchanged, the bandwidth of a fullduplex bus system is more affected by the\
    \ read-write ratio than that of a half-duplex bus system. Specifically, the system\
    \ bandwidth stays almost constant for a half-duplex bus. On the other hand, mixing\
    \ read and write requests enhances the bandwidth of a full-duplex bus system.\
    \ These findings are consistent with the hardware platform observations discussed\
    \ in Section [IV.](#page-6-2) We also conduct the tests by varying the header\
    \ overhead. As can be observed, with zero header overhead, a 1:1 mix of read and\
    \ write packets can nearly double the system bandwidth. As header overhead increases,\
    \ the improvement\n\n<span id=\"page-11-0\"></span>![](_page_11_Figure_0.jpeg)\n\
    \nFig. 18: Throughput of different real-world traces with different system topologies,\
    \ normalized to Chain.\n\n<span id=\"page-11-1\"></span>![](_page_11_Figure_2.jpeg)\n\
    \nFig. 20: (a) Execution speedup of full-duplex bus against halfduplex bus and\
    \ mix degrees of different real-world traces. (b) Performance of silo with full-duplex\
    \ bus, normalized to the max bandwidth of one bus direction.\n\nof read-write\
    \ mixing decreases. When the headers have the same length of payloads, the improvement\
    \ drops to zero. We explain this phenomenon as follows. In the case of fullduplex\
    \ buses, when memory accesses are only with a single operation type, the packets\
    \ utilize only one direction of the bus, leaving the opposite direction for zero-payload\
    \ headers. Consequently, the opposite direction is not fully utilized. When the\
    \ read and write packets are mixed, both directions of the bus are engaged in\
    \ transmitting payloads, thereby improving the overall bandwidth. On the other\
    \ hand, a half-duplex bus only provides a single direction at one time, thus,\
    \ there are no space left for improving the system bandwidth.\n\nTo further explore\
    \ on this phenomenon, we demonstrate the evaluation results on bus utility and\
    \ transmission efficiency. Figure [17](#page-10-1) shows the results. In the case\
    \ of half-duplex bus, the bus is almost fully utilized, and the utility remain\
    \ unchanged as read-write ratio varies. On the other hand, the bus utility of\
    \ full-duplex bus is heavily affected by read-write ratio. When the header overhead\
    \ is zero, single-type scenario only utilized half of the bus (*i.e.*, one direction\
    \ in a total of two). As the mix ratio increases, the bus utility approaches 1,\
    \ which means both directions of the bus are fully utilized. These results indicate\
    \ that the bandwidth improvement of read-write mix scenarios comes from the improved\
    \ bus utility. By increasing the header overhead, more bus time will be cost on\
    \ nonpayload transmission, decreasing the transmission efficiency. Because more\
    \ time is spent on header transmission, the bus utility of single-type scenarios\
    \ increases, leaving less space for read-write mixing to improve utility. As a\
    \ result, the bandwidth improvement is also weakened, as can be observed in Figure\
    \ [16.](#page-10-1) These results support the explanation that, in scenarios with\
    \ a single type of operation, the headers will waste one of the bus directions,\
    \ and read-write mixed scenarios can optimize bus utility, leading to bandwidth\
    \ improvement.\n\n![](_page_11_Figure_6.jpeg)\n\nFig. 19: Average latency of different\
    \ real-world traces with different system topologies, normalized to Chain.\n\n\
    #### *E. Impact on Real-World Workloads*\n\n We also investigate the impact of\
    \ CXL systems on realworld workloads by replaying the traces of five representative\
    \ workloads (*i.e.*, BTree [\\[15\\]](#page-12-30), liblinear [\\[28\\]](#page-12-31),\
    \ redis [\\[25\\]](#page-12-32), [\\[43\\]](#page-13-29), silo [\\[59\\]](#page-13-30)\
    \ and XSBench [\\[57\\]](#page-13-31)) using ESF. The memory traces, each containing\
    \ one million memory accesses, are collected by using a popular tool proposed\
    \ by prior work [\\[61\\]](#page-13-25).\n\nFigure [18](#page-11-0) and [19](#page-11-0)\
    \ demonstrate the evaluated results of these workloads running on different system\
    \ topologies, which are described in Section [V-A.](#page-8-3) The system topology\
    \ significantly impacts the performance of all real-world workloads. Similar to\
    \ our aforementioned observation, both the chain and the tree exhibit the lowest\
    \ throughput and highest average memory latency compared with other topologies.\
    \ This is because of the bottleneck route in these traditional system topologies.\
    \ By simply widening the system with an extra route, ring can achieve 1.72× throughput\
    \ and 0.57× average latency, compared to chain and tree. The spine-leaf and fully-connected\
    \ topologies achieve higher performance by eliminating the bottleneck routes,\
    \ achieving 2.27×, 3.63× throughput and 0.44×, 0.28× average latency, respectively.\
    \ These results confirm that system topology notably affects the performance of\
    \ CXL memory system in terms of both throughput and latency, and the traditional\
    \ tree-like topologies incur significant overheads.\n\nFigure [20a](#page-11-1)\
    \ and [20b](#page-11-1) depict the impact of PCIe full-duplex transfer on the\
    \ real-world workloads. As discussed in Section [V-D,](#page-10-2) mixing read\
    \ and write requests can improve the system performance with a full-duplex bus\
    \ by utilizing both transfer directions. This impact is also observed in real-world\
    \ workloads. As shown in Figure [20a](#page-11-1), when the mix degree (defined\
    \ as min{read ratio, write ratio}) of a workload increases, its speed-up compared\
    \ to a half-duplex platform also increases. Figure [20b](#page-11-1) further demonstrates\
    \ the relationship between the mix degree and performance. In the figure, each\
    \ point represents the bandwidth of 1000 memory accesses, normalized to the max\
    \ bandwidth of one bus direction. It can be observed that, there is a high-positive\
    \ correlation between mix degree and performance. When the mix degree increases\
    \ by 0.1, the overall bandwidth can be improved by 9%. This observation suggests\
    \ that real-world workloads can mix memory read and write more aggressively when\
    \ running on CXL memory to achieve better performance.\n\n## VI. CONCLUSION\n\n\
    In this work, we introduce ESF, a novel simulation framework that accurately models\
    \ several critical features in CXL 3.1 specification, that existing emulation\
    \ and simulation tools struggle to support. These features help ESF to simulate\
    \ CXL systems with high scalability and coherent peer-to-peer communication. We\
    \ validate ESF on a real CXL-attached hardware platform and demonstrate outstanding\
    \ accuracy compared to emulators adopted by prior works. ESF can uncover important\
    \ issues that existing tools cannot figure out, such as the performance impact\
    \ of device-managed coherence. We hope ESF can assist in the exploration of high-performance\
    \ CXL system design.\n\n#### REFERENCES\n\n- <span id=\"page-12-7\"></span>[1]\
    \ \"Ieee standard for ethernet,\" [https://standards](https://standards.ieee.org/ieee/802.3/7071/).ieee.org/ieee/802.3/7071/.\n\
    - <span id=\"page-12-10\"></span>[2] \"Nvm express link,\" [https://nvmexpress](https://nvmexpress.org/).org/.\n\
    - <span id=\"page-12-8\"></span>[3] \"Serial advanced technology attachment (sata),\"\
    \ [https://sata-io](https://sata-io.org/).org/.\n- <span id=\"page-12-4\"></span>[4]\
    \ \"Pci express® 5.0 specification,\" https://pcisig.[com/specifications/](https://pcisig.com/specifications/pciexpress)\n\
    - <span id=\"page-12-24\"></span>[pciexpress,](https://pcisig.com/specifications/pciexpress)\
    \ 2019. [5] \"Enabling pcie® 5.0 system level testing and low latency mode for\
    \ cxl™,\" https://www.asteralabs.[com/videos/aries-smart-retimer-for-pcie-](https://www.asteralabs.com/videos/aries-smart-retimer-for-pcie-gen-5-and-cxl)\n\
    - <span id=\"page-12-27\"></span>[gen-5-and-cxl,](https://www.asteralabs.com/videos/aries-smart-retimer-for-pcie-gen-5-and-cxl)\
    \ 2021. [6] \"Intel® memory latency checker v3.11,\" https://www.intel.[com/content/](https://www.intel.com/content/www/us/en/developer/articles/tool/intelr-memory-latency-checker.html)\
    \ [www/us/en/developer/articles/tool/intelr-memory-latency-checker](https://www.intel.com/content/www/us/en/developer/articles/tool/intelr-memory-latency-checker.html).html,\
    \ 2021.\n- <span id=\"page-12-18\"></span>[7] \"Micron® smc 2000 smart memory\
    \ controllers,\" [https:](https://www.microchip.com/en-us/products/memory/smart-memory-controllers)\
    \ //www.microchip.[com/en-us/products/memory/smart-memory](https://www.microchip.com/en-us/products/memory/smart-memory-controllers)[controllers,](https://www.microchip.com/en-us/products/memory/smart-memory-controllers)\
    \ 2022.\n- <span id=\"page-12-5\"></span>[8] \"Pcie express® 6.0 specification,\"\
    \ https://pcisig.[com/pci-express-6](https://pcisig.com/pci-express-6.0-specification).0\
    \ [specification,](https://pcisig.com/pci-express-6.0-specification) 2022.\n-\
    \ <span id=\"page-12-11\"></span>[9] \"Cxl® 3.1 specification,\" [https://computeexpresslink](https://computeexpresslink.org/cxl-specification/).org/cxl[specification/,](https://computeexpresslink.org/cxl-specification/)\
    \ 2023.\n- <span id=\"page-12-19\"></span>[10] \"Cxl® memory expander controller\
    \ (mxc) m88mx5891,\" [https://](https://www.montage-tech.com/MXC/M88MX5891) www.montage-tech.[com/MXC/M88MX5891,](https://www.montage-tech.com/MXC/M88MX5891)\
    \ 2023.\n- [11] \"Intel® compute express link® (cxl) fpga ip,\" [https://www](https://www.intel.com/content/www/us/en/products/details/fpga/intellectual-property/interface-protocols/cxl-ip.html).intel.com/\
    \ [content/www/us/en/products/details/fpga/intellectual-property/interface](https://www.intel.com/content/www/us/en/products/details/fpga/intellectual-property/interface-protocols/cxl-ip.html)[protocols/cxl-ip](https://www.intel.com/content/www/us/en/products/details/fpga/intellectual-property/interface-protocols/cxl-ip.html).html,\
    \ 2023.\n- <span id=\"page-12-20\"></span>[12] \"Intel® xeon® gold 6416h processor,\"\
    \ https://www.intel.[com/content/](https://www.intel.com/content/www/us/en/products/sku/232389/intel-xeon-gold-6416h-processor-45m-cache-2-20-ghz/specifications.html)\
    \ [www/us/en/products/sku/232389/intel-xeon-gold-6416h-processor-](https://www.intel.com/content/www/us/en/products/sku/232389/intel-xeon-gold-6416h-processor-45m-cache-2-20-ghz/specifications.html)[45m-cache-2-20-ghz/specifications](https://www.intel.com/content/www/us/en/products/sku/232389/intel-xeon-gold-6416h-processor-45m-cache-2-20-ghz/specifications.html).html,\
    \ 2023.\n- <span id=\"page-12-9\"></span>[13] \"Ddr5 sdram,\" https://www.jedec.[org/standards-documents/docs/jesd79-](https://www.jedec.org/standards-documents/docs/jesd79-5c)\
    \ [5c,](https://www.jedec.org/standards-documents/docs/jesd79-5c) 2024.\n- <span\
    \ id=\"page-12-29\"></span>[14] \"Pin - a dynamic binary instrumentation tool,\"\
    \ [https://www](https://www.intel.com/content/www/us/en/developer/articles/tool/pin-a-dynamic-binary-instrumentation-tool.html).intel.com/\
    \ [content/www/us/en/developer/articles/tool/pin-a-dynamic-binary](https://www.intel.com/content/www/us/en/developer/articles/tool/pin-a-dynamic-binary-instrumentation-tool.html)[instrumentation-tool](https://www.intel.com/content/www/us/en/developer/articles/tool/pin-a-dynamic-binary-instrumentation-tool.html).html,\
    \ 2024.\n- <span id=\"page-12-30\"></span>[15] R. Achermann, A. Panwar, A. Bhattacharjee,\
    \ T. Roscoe, and J. Gandhi, \"Mitosis: Transparently self-replicating page-tables\
    \ for large-memory machines,\" *Proceedings of the Twenty-Fifth International\
    \ Conference on Architectural Support for Programming Languages and Operating\
    \ Systems*, 2019. [Online]. Available: https://api.[semanticscholar](https://api.semanticscholar.org/CorpusID:204509120).org/\
    \ [CorpusID:204509120](https://api.semanticscholar.org/CorpusID:204509120)\n-\
    \ <span id=\"page-12-16\"></span>[16] N. Agarwal, T. Krishna, L.-S. Peh, and N.\
    \ K. Jha, \"Garnet: A detailed on-chip network model inside a full-system simulator,\"\
    \ *2009 IEEE International Symposium on Performance Analysis of Systems and Software*,\
    \ pp. 33–42, 2009. [Online]. Available: https://api.semanticscholar.[org/CorpusID:14839431](https://api.semanticscholar.org/CorpusID:14839431)\n\
    - <span id=\"page-12-0\"></span>[17] S. Angizi, J.-J. Sun, W. Zhang, and D. Fan,\
    \ \"Aligns: A processing-inmemory accelerator for dna short read alignment leveraging\
    \ sot-mram,\" *2019 56th ACM/IEEE Design Automation Conference (DAC)*, pp. 1–\
    \ 6, 2019. [Online]. Available: https://api.[semanticscholar](https://api.semanticscholar.org/CorpusID:163164248).org/CorpusID:\
    \ [163164248](https://api.semanticscholar.org/CorpusID:163164248)\n- <span id=\"\
    page-12-12\"></span>[18] M. Arif, K. Assogba, M. M. Rafique, and S. S. Vazhkudai,\
    \ \"Exploiting cxl-based memory for distributed deep learning,\" *Proceedings\
    \ of the 51st International Conference on Parallel Processing*, 2022. [Online].\
    \ Available: https://api.semanticscholar.[org/CorpusID:255775666](https://api.semanticscholar.org/CorpusID:255775666)\n\
    - <span id=\"page-12-15\"></span>[19] A. Bakhoda, G. L. Yuan, W. W. L. Fung, H.\
    \ Wong, and T. M. Aamodt, \"Analyzing cuda workloads using a detailed gpu simulator,\"\
    \ *2009 IEEE International Symposium on Performance Analysis of Systems and Software*,\
    \ pp. 163–174, 2009. [Online]. Available: https://api.semanticscholar.[org/CorpusID:15461524](https://api.semanticscholar.org/CorpusID:15461524)\n\
    - <span id=\"page-12-23\"></span>[20] S. Bharadwaj, J. Yin, B. M. Beckmann, and\
    \ T. Krishna, \"Kite: A family of heterogeneous interposer topologies enabled\
    \ via accurate interconnect modeling,\" *2020 57th ACM/IEEE Design Automation\
    \ Conference (DAC)*, pp. 1–6, 2020. [Online]. Available: [https:](https://api.semanticscholar.org/CorpusID:222297184)\
    \ //api.semanticscholar.[org/CorpusID:222297184](https://api.semanticscholar.org/CorpusID:222297184)\n\
    - <span id=\"page-12-13\"></span>[21] N. Binkert, B. Beckmann, G. Black, S. K.\
    \ Reinhardt, A. Saidi, A. Basu, J. Hestness, D. R. Hower, T. Krishna, S. Sardashti,\
    \ R. Sen, K. Sewell, M. Shoaib, N. Vaish, M. D. Hill, and D. A. Wood, \"The gem5\
    \ simulator,\" *SIGARCH Comput. Archit. News*, vol. 39, no. 2, p. 1–7, aug 2011.\
    \ [Online]. Available: https://doi.org/10.[1145/2024716](https://doi.org/10.1145/2024716.2024718).2024718\n\
    - <span id=\"page-12-1\"></span>[22] T. B. Brown, B. Mann, N. Ryder, M. Subbiah,\
    \ J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal,\
    \ A. Herbert-Voss, G. Krueger, T. J. Henighan, R. Child, A. Ramesh, D. M. Ziegler,\
    \ J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess,\
    \ J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei,\
    \ \"Language models are few-shot learners,\" *ArXiv*, vol. abs/2005.14165, 2020.\n\
    - <span id=\"page-12-28\"></span>[23] J. Bucek, K.-D. Lange, and J. von Kistowski,\
    \ \"Spec cpu2017: Nextgeneration compute benchmark,\" *Companion of the 2018 ACM/SPEC\
    \ International Conference on Performance Engineering*, 2018. [Online]. Available:\
    \ https://api.semanticscholar.[org/CorpusID:4714535](https://api.semanticscholar.org/CorpusID:4714535)\n\
    - <span id=\"page-12-2\"></span>[24] Z. I. Chowdhury, M. Zabihi, S. K. Khatamifard,\
    \ Z. Zhao, S. Resch, M. Razaviyayn, J. Wang, S. S. Sapatnekar, and U. R. Karpuzcu,\
    \ \"A dna read alignment accelerator based on computational ram,\" *IEEE Journal\
    \ on Exploratory Solid-State Computational Devices and Circuits*, vol. 6, pp.\
    \ 80–88, 2020. [Online]. Available: [https:](https://api.semanticscholar.org/CorpusID:218777033)\
    \ //api.semanticscholar.[org/CorpusID:218777033](https://api.semanticscholar.org/CorpusID:218777033)\n\
    - <span id=\"page-12-32\"></span>[25] B. F. Cooper, A. Silberstein, E. Tam, R.\
    \ Ramakrishnan, and R. Sears, \"Benchmarking cloud serving systems with ycsb,\"\
    \ in *ACM Symposium on Cloud Computing*, 2010. [Online]. Available: https://api.semanticscholar.[org/CorpusID:2589691](https://api.semanticscholar.org/CorpusID:2589691)\n\
    - <span id=\"page-12-25\"></span>[26] T. Do, \"Cxl™ use-cases driving the need\
    \ for low latency performance retimers,\" https://www.microchip.[com/en-us/about/media-center/blog/](https://www.microchip.com/en-us/about/media-center/blog/2020/cxl-use-cases-driving-need-for-low-latency-performance-retimer)\
    \ [2020/cxl-use-cases-driving-need-for-low-latency-performance-retimer,](https://www.microchip.com/en-us/about/media-center/blog/2020/cxl-use-cases-driving-need-for-low-latency-performance-retimer)\
    \ 2020.\n- <span id=\"page-12-21\"></span>[27] W. Fagen-Ulmschneider, \"Shortest\
    \ path,\" in *Encyclopedia of Algorithms*, 2008. [Online]. Available: https://api.[semanticscholar](https://api.semanticscholar.org/CorpusID:14530113).org/CorpusID:\
    \ [14530113](https://api.semanticscholar.org/CorpusID:14530113)\n- <span id=\"\
    page-12-31\"></span>[28] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and\
    \ C.-J. Lin, \"Liblinear: A library for large linear classification,\" *Journal\
    \ of machine learning research*, vol. 9, no. Aug, pp. 1871–1874, 2008.\n- <span\
    \ id=\"page-12-17\"></span>[29] D. Firestone, A. Putnam, S. Mundkur, D. Chiou,\
    \ A. Dabagh, M. Andrewartha, H. Angepat, V. Bhanu, A. M. Caulfield, E. S. Chung,\
    \ H. K. Chandrappa, S. Chaturmohta, M. Humphrey, J. Lavier, N. Lam, F. Liu, K.\
    \ Ovtcharov, J. Padhye, G. Popuri, S. Raindel, T. Sapre, M. Shaw, G. Silva, M.\
    \ Sivakumar, N. Srivastava, A. Verma, Q. Zuhair, D. Bansal, D. Burger, K. Vaid,\
    \ D. A. Maltz, and A. G. Greenberg, \"Azure accelerated networking: Smartnics\
    \ in the public cloud,\" in *Symposium on Networked Systems Design and Implementation*,\
    \ 2018. [Online]. Available: https://api.semanticscholar.[org/CorpusID:4793951](https://api.semanticscholar.org/CorpusID:4793951)\n\
    - <span id=\"page-12-3\"></span>[30] F. Giorgi, \"Thirty years of regional climate\
    \ modeling: Where are we and where are we going next?\" *Journal of Geophysical\
    \ Research: Atmospheres*, vol. 124, pp. 5696 – 5723, 2019. [Online]. Available:\
    \ https://api.semanticscholar.[org/CorpusID:182261868](https://api.semanticscholar.org/CorpusID:182261868)\n\
    - <span id=\"page-12-22\"></span>[31] D. Gouk, M. Kwon, J. Zhang, S. Koh, W. Choi,\
    \ N. S. Kim, M. T. Kandemir, and M. Jung, \"Amber\\*: Enabling precise full-system\
    \ simulation with detailed modeling of all ssd resources,\" *2018 51st Annual\
    \ IEEE/ACM International Symposium on Microarchitecture (MICRO)*, pp. 469–481,\
    \ 2018. [Online]. Available: https://api.semanticscholar.[org/CorpusID:53220814](https://api.semanticscholar.org/CorpusID:53220814)\n\
    - <span id=\"page-12-26\"></span>[32] D. Gouk, S. Lee, M. Kwon, and M. Jung, \"\
    Direct access, highperformance memory disaggregation with directcxl,\" in *USENIX\
    \ Annual Technical Conference*, 2022. [Online]. Available: [https:](https://api.semanticscholar.org/CorpusID:268076421)\
    \ //api.semanticscholar.[org/CorpusID:268076421](https://api.semanticscholar.org/CorpusID:268076421)\n\
    - <span id=\"page-12-14\"></span>[33] A. Hansson, N. Agarwal, A. Kolli, T. Wenisch,\
    \ and A. N. Udipi, \"Simulating dram controllers for future system architecture\
    \ exploration,\" in *2014 IEEE International Symposium on Performance Analysis\
    \ of Systems and Software (ISPASS)*, 2014, pp. 201–210.\n- <span id=\"page-12-6\"\
    ></span>[34] J.-I. Hong, S. Cho, G. Park, W. Yang, Y.-H. Gong, and G. T. Kim,\
    \ \"Bandwidth-effective dram cache for gpu s with storage-class memory,\" *2024\
    \ IEEE International Symposium on High-Performance Computer Architecture (HPCA)*,\
    \ pp. 139–155, 2024. [Online]. Available: https://api.semanticscholar.[org/CorpusID:268385429](https://api.semanticscholar.org/CorpusID:268385429)\n\
    - <span id=\"page-13-11\"></span>[35] N. Jiang, D. U. Becker, G. Michelogiannakis,\
    \ J. D. Balfour, B. Towles, D. E. Shaw, J. Kim, and W. J. Dally, \"A detailed\
    \ and flexible cycle-accurate network-on-chip simulator,\" *2013 IEEE International\
    \ Symposium on Performance Analysis of Systems and Software (ISPASS)*, pp. 86–96,\
    \ 2013. [Online]. Available: [https:](https://api.semanticscholar.org/CorpusID:12013345)\
    \ //api.semanticscholar.[org/CorpusID:12013345](https://api.semanticscholar.org/CorpusID:12013345)\n\
    - <span id=\"page-13-5\"></span>[36] M. Jung, \"Hello bytes, bye blocks: Pcie\
    \ storage meets compute express link for memory expansion (cxl-ssd),\" *Proceedings\
    \ of the 14th ACM Workshop on Hot Topics in Storage and File Systems*, 2022. [Online].\
    \ Available: https://api.semanticscholar.[org/CorpusID:249960254](https://api.semanticscholar.org/CorpusID:249960254)\n\
    - <span id=\"page-13-17\"></span>[37] K. D. Kim, H. Kim, J. So, W. Lee, J.-H.\
    \ Im, S.-R. Y.-C. Park, J. Cho, and H. U. Song, \"Smt: Software-defined memory\
    \ tiering for heterogeneous computing systems with cxl memory expander,\" *IEEE\
    \ Micro*, vol. 43, pp. 20–29, 2023. [Online]. Available: https://api.semanticscholar.[org/CorpusID:256491961](https://api.semanticscholar.org/CorpusID:256491961)\n\
    - <span id=\"page-13-0\"></span>[38] B. Langmead, C. Trapnell, M. Pop, and S.\
    \ L. Salzberg, \"Ultrafast and memory-efficient alignment of short dna sequences\
    \ to the human genome,\" *Genome Biology*, vol. 10, pp. R25 – R25, 2009. [Online].\
    \ Available: https://api.semanticscholar.[org/CorpusID:5057](https://api.semanticscholar.org/CorpusID:5057)\n\
    - <span id=\"page-13-20\"></span>[39] H. Lee, S. Lee, Y. Jung, and D. Kim, \"\
    T-cat: Dynamic cache allocation for tiered memory systems with memory interleaving,\"\
    \ *IEEE Computer Architecture Letters*, vol. 22, pp. 73–76, 2023. [Online]. Available:\
    \ https://api.semanticscholar.[org/CorpusID:259729055](https://api.semanticscholar.org/CorpusID:259729055)\n\
    - <span id=\"page-13-9\"></span>[40] H. Li, D. S. Berger, S. Novakovic, L. R.\
    \ Hsu, D. Ernst, P. Zardoshti, M. Shah, S. Rajadnya, S. Lee, I. Agarwal, M. D.\
    \ Hill, M. Fontoura, and R. Bianchini, \"Pond: Cxl-based memory pooling systems\
    \ for cloud platforms,\" *Proceedings of the 28th ACM International Conference\
    \ on Architectural Support for Programming Languages and Operating Systems, Volume\
    \ 2*, 2022. [Online]. Available: https://api.semanticscholar.[org/CorpusID:252907213](https://api.semanticscholar.org/CorpusID:252907213)\n\
    - <span id=\"page-13-1\"></span>[41] P. Li, Y. Luo, N. Zhang, and Y. Cao, \"Heterospark:\
    \ A heterogeneous cpu/gpu spark platform for machine learning algorithms,\" *2015\
    \ IEEE International Conference on Networking, Architecture and Storage (NAS)*,\
    \ pp. 347–348, 2015.\n- <span id=\"page-13-22\"></span>[42] S.-J. Li, Z. Yang,\
    \ D. Reddy, A. Srivastava, and B. Jacob, \"Dramsim3: A cycle-accurate, thermal-capable\
    \ dram simulator,\" *IEEE Computer Architecture Letters*, vol. 19, pp. 106–109,\
    \ 2020. [Online]. Available: https://api.semanticscholar.[org/CorpusID:214356880](https://api.semanticscholar.org/CorpusID:214356880)\n\
    - <span id=\"page-13-29\"></span>[43] R. Ltd., \"Redis,\" [https://redis](https://redis.io).io,\
    \ 2024.\n- <span id=\"page-13-10\"></span>[44] H. A. Maruf, H. Wang, A. Dhanotia,\
    \ J. Weiner, N. Agarwal, P. Bhattacharya, C. Petersen, M. Chowdhury, S. O. Kanaujia,\
    \ and P. Chauhan, \"Tpp: Transparent page placement for cxlenabled tiered-memory,\"\
    \ *Proceedings of the 28th ACM International Conference on Architectural Support\
    \ for Programming Languages and Operating Systems, Volume 3*, 2022. [Online].\
    \ Available: https://api.semanticscholar.[org/CorpusID:249431676](https://api.semanticscholar.org/CorpusID:249431676)\n\
    - <span id=\"page-13-2\"></span>[45] T. Nguyen, J. Jewik, H. Bansal, P. Sharma,\
    \ and A. Grover, \"Climatelearn: Benchmarking machine learning for weather and\
    \ climate modeling,\" *ArXiv*, vol. abs/2307.01909, 2023. [Online]. Available:\
    \ https://api.semanticscholar.[org/CorpusID:259342316](https://api.semanticscholar.org/CorpusID:259342316)\n\
    - <span id=\"page-13-3\"></span>[46] OpenAI, \"Gpt-4 technical report,\" *ArXiv*,\
    \ vol. abs/2303.08774, 2023.\n- <span id=\"page-13-14\"></span>[47] S. Rajbhandari,\
    \ O. Ruwase, J. Rasley, S. Smith, and Y. He, \"Zero-infinity: Breaking the gpu\
    \ memory wall for extreme scale deep learning,\" *SC21: International Conference\
    \ for High Performance Computing, Networking, Storage and Analysis*, pp. 1–15,\
    \ 2021. [Online]. Available: https://api.semanticscholar.[org/CorpusID:233289729](https://api.semanticscholar.org/CorpusID:233289729)\n\
    - <span id=\"page-13-12\"></span>[48] M. Rhu, M. O'Connor, N. Chatterjee, J. Pool,\
    \ and S. W. Keckler, \"Compressing dma engine: Leveraging activation sparsity\
    \ for training deep neural networks,\" *2018 IEEE International Symposium on High\
    \ Performance Computer Architecture (HPCA)*, pp. 78–91, 2017. [Online]. Available:\
    \ https://api.semanticscholar.[org/CorpusID:3273089](https://api.semanticscholar.org/CorpusID:3273089)\n\
    - <span id=\"page-13-26\"></span>[49] D. D. Sharma, \"Compute express link®: An\
    \ open industry-standard interconnect enabling heterogeneous data-centric computing,\"\
    \ *2022 IEEE Symposium on High-Performance Interconnects (HOTI)*, pp. 5– 12, 2022.\
    \ [Online]. Available: https://api.[semanticscholar](https://api.semanticscholar.org/CorpusID:252997701).org/CorpusID:\
    \ [252997701](https://api.semanticscholar.org/CorpusID:252997701)\n- <span id=\"\
    page-13-8\"></span>[50] D. D. Sharma and M. Wagh, \"Introducing compute express\
    \ link™ (cxl™) 3.1: Significant improvements in fabric connectivity, memory ras,\
    \ security and more!\" [https://computeexpresslink](https://computeexpresslink.org/wp-content/uploads/2023/12/CXL_3.1-White-Paper_FINAL.pdf).org/wp-content/\
    \ [uploads/2023/12/CXL](https://computeexpresslink.org/wp-content/uploads/2023/12/CXL_3.1-White-Paper_FINAL.pdf)\
    \ 3.1-White-Paper FINAL.pdf, 2023.\n- <span id=\"page-13-27\"></span>[51] R. Singh,\
    \ M. K. Bohra, P. Hemrajani, A. Kalla, D. P. Bhatt, N. Purohit, and M. Daneshtalab,\
    \ \"Review, analysis, and implementation of path selection strategies for 2d nocs,\"\
    \ *IEEE Access*, vol. 10, pp. 129 245–\n\n129 268, 2022. [Online]. Available:\
    \ https://api.[semanticscholar](https://api.semanticscholar.org/CorpusID:254450411).org/\
    \ [CorpusID:254450411](https://api.semanticscholar.org/CorpusID:254450411)\n\n\
    - <span id=\"page-13-19\"></span>[52] C. Sommer, \"Shortest-path queries in static\
    \ networks,\" *ACM Computing Surveys (CSUR)*, vol. 46, pp. 1 – 31, 2014. [Online].\
    \ Available: https://api.semanticscholar.[org/CorpusID:1384190](https://api.semanticscholar.org/CorpusID:1384190)\n\
    - <span id=\"page-13-23\"></span>[53] P. Strikos, A. Ejaz, and I. Sourdis, \"\
    Bzsim: Fast, large-scale microarchitectural simulation with detailed interconnect\
    \ modeling,\" *2024 IEEE International Symposium on Performance Analysis of Systems\
    \ and Software (ISPASS)*, pp. 167–178, 2024. [Online]. Available: https://api.semanticscholar.[org/CorpusID:271246055](https://api.semanticscholar.org/CorpusID:271246055)\n\
    - <span id=\"page-13-13\"></span>[54] W. Su, L. Wang, M. Su, and S. Liu, \"A processor-dma-based\
    \ memory copy hardware accelerator,\" *2011 IEEE Sixth International Conference\
    \ on Networking, Architecture, and Storage*, pp. 225–229, 2011. [Online]. Available:\
    \ https://api.semanticscholar.[org/CorpusID:26197371](https://api.semanticscholar.org/CorpusID:26197371)\n\
    - <span id=\"page-13-18\"></span>[55] Y. Sun, Y. Yuan, Z. Yu, R. Kuper, C. Song,\
    \ J. Huang, H. Ji, S. Agarwal, J. Lou, I. Jeong, R. Wang, J. H. Ahn, T. Xu, and\
    \ N. S. Kim, \"Demystifying cxl memory with genuine cxl-ready systems and devices,\"\
    \ *2023 56th IEEE/ACM International Symposium on Microarchitecture (MICRO)*, pp.\
    \ 105–121, 2023. [Online]. Available: https://api.semanticscholar.[org/CorpusID:257766867](https://api.semanticscholar.org/CorpusID:257766867)\n\
    - <span id=\"page-13-6\"></span>[56] A. Tavakkol, J. Gomez-Luna, M. Sadrosadati,\
    \ S. Ghose, and ´ O. Mutlu, \"Mqsim: A framework for enabling realistic studies\
    \ of modern multi-queue ssd devices,\" in *USENIX Conference on File and Storage\
    \ Technologies*, 2018. [Online]. Available: [https:](https://api.semanticscholar.org/CorpusID:3399532)\
    \ //api.semanticscholar.[org/CorpusID:3399532](https://api.semanticscholar.org/CorpusID:3399532)\n\
    - <span id=\"page-13-31\"></span>[57] J. R. Tramm, A. R. Siegel, T. Islam, and\
    \ M. Schulz, \"XSBench - the development and verification of a performance abstraction\
    \ for Monte Carlo reactor analysis,\" in *PHYSOR 2014 - The Role of Reactor Physics\
    \ toward a Sustainable Future*, Kyoto, 2014. [Online]. Available: https://www.mcs.anl.[gov/papers/P5064-0114](https://www.mcs.anl.gov/papers/P5064-0114.pdf).pdf\n\
    - <span id=\"page-13-28\"></span>[58] M. Trik, A. M. N. G. Molk, F. Ghasemi, and\
    \ P. Pouryeganeh, \"A hybrid selection strategy based on traffic analysis for\
    \ improving performance in networks on chip,\" *J. Sensors*, vol. 2022, pp. 1–19,\
    \ 2022. [Online]. Available: https://api.semanticscholar.[org/CorpusID:248448066](https://api.semanticscholar.org/CorpusID:248448066)\n\
    - <span id=\"page-13-30\"></span>[59] S. Tu, W. Zheng, E. Kohler, B. H. Liskov,\
    \ and S. Madden, \"Speedy transactions in multicore in-memory databases,\" *Proceedings\
    \ of the Twenty-Fourth ACM Symposium on Operating Systems Principles*, 2013. [Online].\
    \ Available: https://api.semanticscholar.[org/CorpusID:10076139](https://api.semanticscholar.org/CorpusID:10076139)\n\
    - <span id=\"page-13-21\"></span>[60] J. Yang, J. Kim, M. Hoseinzadeh, J. Izraelevitz,\
    \ and S. Swanson, \"An empirical guide to the behavior and use of scalable persistent\
    \ memory,\" *login Usenix Mag.*, vol. 45, 2019. [Online]. Available: https://api.semanticscholar.[org/CorpusID:199543286](https://api.semanticscholar.org/CorpusID:199543286)\n\
    - <span id=\"page-13-25\"></span>[61] S.-P. Yang, M. Kim, S. Nam, J. Park, J.\
    \ yong Choi, E. H. Nam, E. Lee, S. Lee, and B. S. J. Kim, \"Overcoming the memory\
    \ wall with cxlenabled ssds,\" in *USENIX Annual Technical Conference*, 2023.\
    \ [Online]. Available: https://api.semanticscholar.[org/CorpusID:259859173](https://api.semanticscholar.org/CorpusID:259859173)\n\
    - <span id=\"page-13-24\"></span>[62] Y. Yang, P. Safayenikoo, J. Ma, T. A. Khan,\
    \ and A. Quinn, \"Cxlmemsim: A pure software simulated cxl.mem for performance\
    \ characterization,\" *ArXiv*, vol. abs/2303.06153, 2023. [Online]. Available:\
    \ https://api.semanticscholar.[org/CorpusID:257496563](https://api.semanticscholar.org/CorpusID:257496563)\n\
    - <span id=\"page-13-4\"></span>[63] J. Yuval and P. A. O'Gorman, \"Stable machine-learning\
    \ parameterization of subgrid processes for climate modeling at a range of resolutions,\"\
    \ *Nature Communications*, vol. 11, 2020. [Online]. Available: [https:](https://api.semanticscholar.org/CorpusID:220311961)\
    \ //api.semanticscholar.[org/CorpusID:220311961](https://api.semanticscholar.org/CorpusID:220311961)\n\
    - <span id=\"page-13-15\"></span>[64] H. Zhang, Y. E. Zhou, Y. Xue, Y. Liu, and\
    \ J. Huang, \"G10: Enabling an efficient unified gpu memory and storage architecture\
    \ with smart tensor migrations,\" *2023 56th IEEE/ACM International Symposium\
    \ on Microarchitecture (MICRO)*, pp. 395–410, 2023. [Online]. Available: https://api.semanticscholar.[org/CorpusID:264146434](https://api.semanticscholar.org/CorpusID:264146434)\n\
    - <span id=\"page-13-7\"></span>[65] J. Zhang and M. Jung, \"Zng: Architecting\
    \ gpu multi-processors with new flash for scalable data analysis,\" *2020 ACM/IEEE\
    \ 47th Annual International Symposium on Computer Architecture (ISCA)*, pp. 1064–1075,\
    \ 2020. [Online]. Available: https://api.[semanticscholar](https://api.semanticscholar.org/CorpusID:219103734).org/\
    \ [CorpusID:219103734](https://api.semanticscholar.org/CorpusID:219103734)\n-\
    \ <span id=\"page-13-16\"></span>[66] Q. Zhou, H. Wang, X. Yu, C. Li, Y. Bai,\
    \ F. Yan, and Y. Xu, \"Mpress: Democratizing billion-scale model training on multigpu\
    \ servers via memory-saving inter-operator parallelism,\" *2023 IEEE International\
    \ Symposium on High-Performance Computer Architecture (HPCA)*, pp. 556–569, 2023.\
    \ [Online]. Available: [https:](https://api.semanticscholar.org/CorpusID:257720517)\
    \ //api.semanticscholar.[org/CorpusID:257720517](https://api.semanticscholar.org/CorpusID:257720517)"
